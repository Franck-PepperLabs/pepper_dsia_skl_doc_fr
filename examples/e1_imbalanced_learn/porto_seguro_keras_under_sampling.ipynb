{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Porto Seguro : équilibrer les échantillons en mini-lots avec Keras\n",
        "\n",
        "Cet exemple compare deux stratégies pour entraîner un réseau neuronal sur l'ensemble de données Porto Seguro Kaggle [1]. L'ensemble de données est déséquilibré et nous montrons que l'équilibrage de chaque mini-lot permet d'améliorer les performances et de réduire le temps d'entraînement.\n",
        "\n",
        "## Références\n",
        "\n",
        "[1] https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Automatically created module for IPython interactive environment\n"
          ]
        }
      ],
      "source": [
        "# Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>\n",
        "# License: MIT\n",
        "\n",
        "print(__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chargement des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tout d'abord, vous devriez télécharger l'ensemble de données Porto Seguro depuis Kaggle. Voir le lien dans l'introduction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "training_data = pd.read_csv(\"./input/train.csv\")\n",
        "testing_data = pd.read_csv(\"./input/test.csv\")\n",
        "\n",
        "y_train = training_data[[\"id\", \"target\"]].set_index(\"id\")\n",
        "X_train = training_data.drop([\"target\"], axis=1).set_index(\"id\")\n",
        "X_test = testing_data.set_index(\"id\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "L'ensemble de données est déséquilibré et cela aura un impact sur l'ajustement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The data set is imbalanced: Counter({0: 573518, 1: 21694})\n"
          ]
        }
      ],
      "source": [
        "print(f\"The data set is imbalanced: {Counter(y_train['target'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Définir la chaîne de prétraitement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler\n",
        "\n",
        "\n",
        "def convert_float64(X):\n",
        "    return X.astype(np.float64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nous voulons mettre à l'échelle de manière standard les caractéristiques numériques tout en encodant de manière one-hot les caractéristiques catégorielles. À cet égard, nous utilisons la classe `~sklearn.compose.ColumnTransformer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "numerical_columns = [\n",
        "    name for name in X_train.columns if \"_calc_\" in name and \"_bin\" not in name\n",
        "]\n",
        "numerical_pipeline = make_pipeline(\n",
        "    FunctionTransformer(func=convert_float64, validate=False), StandardScaler()\n",
        ")\n",
        "\n",
        "categorical_columns = [name for name in X_train.columns if \"_cat\" in name]\n",
        "categorical_pipeline = make_pipeline(\n",
        "    SimpleImputer(missing_values=-1, strategy=\"most_frequent\"),\n",
        "    OneHotEncoder(categories=\"auto\"),\n",
        ")\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    [\n",
        "        (\"numerical_preprocessing\", numerical_pipeline, numerical_columns),\n",
        "        (\n",
        "            \"categorical_preprocessing\",\n",
        "            categorical_pipeline,\n",
        "            categorical_columns,\n",
        "        ),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        ")\n",
        "\n",
        "# Create an environment variable to avoid using the GPU. This can be changed.\n",
        "import os\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Créer un réseau neuronal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "\n",
        "def make_model(n_features):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(200, input_shape=(n_features,), kernel_initializer=\"glorot_normal\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(100, kernel_initializer=\"glorot_normal\", use_bias=False))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Dense(50, kernel_initializer=\"glorot_normal\", use_bias=False))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(0.15))\n",
        "    model.add(Dense(25, kernel_initializer=\"glorot_normal\", use_bias=False))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nous créons un décorateur pour reporter le temps de calcul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from functools import wraps\n",
        "\n",
        "\n",
        "def timeit(f):\n",
        "    @wraps(f)\n",
        "    def wrapper(*args, **kwds):\n",
        "        start_time = time.time()\n",
        "        result = f(*args, **kwds)\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f\"Elapsed computation time: {elapsed_time:.3f} secs\")\n",
        "        return (elapsed_time, result)\n",
        "\n",
        "    return wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le premier modèle sera entraîné en utilisant la méthode `fit` et avec des mini-lots déséquilibrés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.utils import parse_version\n",
        "\n",
        "tf_version = parse_version(tensorflow.__version__)\n",
        "\n",
        "\n",
        "@timeit\n",
        "def fit_predict_imbalanced_model(X_train, y_train, X_test, y_test):\n",
        "    model = make_model(X_train.shape[1])\n",
        "    model.fit(X_train, y_train, epochs=2, verbose=1, batch_size=1000)\n",
        "    if tf_version < parse_version(\"2.6\"):\n",
        "        # predict_proba was removed in tensorflow 2.6\n",
        "        predict_method = \"predict_proba\"\n",
        "    else:\n",
        "        predict_method = \"predict\"\n",
        "    y_pred = getattr(model, predict_method)(X_test, batch_size=1000)\n",
        "    return roc_auc_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Au contraire, nous utiliserons imbalanced-learn pour créer un générateur de mini-lots qui générera des mini-lots équilibrés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from imblearn.keras import BalancedBatchGenerator\n",
        "\n",
        "\n",
        "@timeit\n",
        "def fit_predict_balanced_model(X_train, y_train, X_test, y_test):\n",
        "    model = make_model(X_train.shape[1])\n",
        "    training_generator = BalancedBatchGenerator(\n",
        "        X_train, y_train, batch_size=1000, random_state=42\n",
        "    )\n",
        "    model.fit(training_generator, epochs=5, verbose=1)\n",
        "    y_pred = model.predict(X_test, batch_size=1000)\n",
        "    return roc_auc_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Boucle de classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nous effectuons une validation croisée à 10 plis et entraînons le réseau neuronal avec les deux stratégies différentes présentées précédemment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "536/536 [==============================] - 25s 38ms/step - loss: 0.2671 - accuracy: 0.9163\n",
            "Epoch 2/2\n",
            "536/536 [==============================] - 26s 44ms/step - loss: 0.1639 - accuracy: 0.9635\n",
            "60/60 [==============================] - 2s 19ms/step\n",
            "Elapsed computation time: 55.142 secs\n",
            "Epoch 1/5\n",
            "39/39 [==============================] - 4s 17ms/step - loss: 0.8326 - accuracy: 0.4988\n",
            "Epoch 2/5\n",
            "39/39 [==============================] - 1s 20ms/step - loss: 0.7381 - accuracy: 0.5175\n",
            "Epoch 3/5\n",
            "39/39 [==============================] - 1s 17ms/step - loss: 0.7199 - accuracy: 0.5188\n",
            "Epoch 4/5\n",
            "39/39 [==============================] - 1s 19ms/step - loss: 0.7138 - accuracy: 0.5215\n",
            "Epoch 5/5\n",
            "39/39 [==============================] - 1s 18ms/step - loss: 0.7086 - accuracy: 0.5227\n",
            "60/60 [==============================] - 2s 27ms/step\n",
            "Elapsed computation time: 9.535 secs\n",
            "Epoch 1/2\n",
            "536/536 [==============================] - 26s 39ms/step - loss: 0.1890 - accuracy: 0.9607\n",
            "Epoch 2/2\n",
            "536/536 [==============================] - 23s 39ms/step - loss: 0.1618 - accuracy: 0.9636\n",
            "60/60 [==============================] - 1s 15ms/step\n",
            "Elapsed computation time: 51.075 secs\n",
            "Epoch 1/5\n",
            "39/39 [==============================] - 7s 19ms/step - loss: 0.7313 - accuracy: 0.5122\n",
            "Epoch 2/5\n",
            "39/39 [==============================] - 1s 20ms/step - loss: 0.7121 - accuracy: 0.5175\n",
            "Epoch 3/5\n",
            "39/39 [==============================] - 1s 19ms/step - loss: 0.7066 - accuracy: 0.5179\n",
            "Epoch 4/5\n",
            "39/39 [==============================] - 1s 19ms/step - loss: 0.6987 - accuracy: 0.5292\n",
            "Epoch 5/5\n",
            "39/39 [==============================] - 1s 18ms/step - loss: 0.6960 - accuracy: 0.5384\n",
            "60/60 [==============================] - 1s 17ms/step\n",
            "Elapsed computation time: 11.542 secs\n",
            "Epoch 1/2\n",
            "536/536 [==============================] - 35s 57ms/step - loss: 0.1901 - accuracy: 0.9584\n",
            "Epoch 2/2\n",
            "536/536 [==============================] - 36s 62ms/step - loss: 0.1624 - accuracy: 0.9635\n",
            "60/60 [==============================] - 1s 19ms/step\n",
            "Elapsed computation time: 73.459 secs\n",
            "Epoch 1/5\n",
            "39/39 [==============================] - 3s 22ms/step - loss: 0.7366 - accuracy: 0.5067\n",
            "Epoch 2/5\n",
            "39/39 [==============================] - 1s 28ms/step - loss: 0.7133 - accuracy: 0.5168\n",
            "Epoch 3/5\n",
            "39/39 [==============================] - 1s 23ms/step - loss: 0.7055 - accuracy: 0.5223\n",
            "Epoch 4/5\n",
            "39/39 [==============================] - 1s 22ms/step - loss: 0.6999 - accuracy: 0.5306\n",
            "Epoch 5/5\n",
            "39/39 [==============================] - 1s 21ms/step - loss: 0.6960 - accuracy: 0.5354\n",
            "60/60 [==============================] - 1s 19ms/step\n",
            "Elapsed computation time: 8.908 secs\n",
            "Epoch 1/2\n",
            "536/536 [==============================] - 42s 66ms/step - loss: 0.2518 - accuracy: 0.9280\n",
            "Epoch 2/2\n",
            "536/536 [==============================] - 42s 74ms/step - loss: 0.1613 - accuracy: 0.9635\n",
            "60/60 [==============================] - 2s 27ms/step\n",
            "Elapsed computation time: 88.158 secs\n",
            "Epoch 1/5\n",
            "39/39 [==============================] - 4s 24ms/step - loss: 0.7292 - accuracy: 0.5097\n",
            "Epoch 2/5\n",
            "39/39 [==============================] - 1s 22ms/step - loss: 0.7064 - accuracy: 0.5187\n",
            "Epoch 3/5\n",
            "39/39 [==============================] - 1s 22ms/step - loss: 0.6986 - accuracy: 0.5256\n",
            "Epoch 4/5\n",
            "39/39 [==============================] - 1s 22ms/step - loss: 0.6931 - accuracy: 0.5388\n",
            "Epoch 5/5\n",
            "39/39 [==============================] - 1s 22ms/step - loss: 0.6921 - accuracy: 0.5401\n",
            "60/60 [==============================] - 1s 21ms/step\n",
            "Elapsed computation time: 10.044 secs\n",
            "Epoch 1/2\n",
            "536/536 [==============================] - 37s 60ms/step - loss: 0.4087 - accuracy: 0.8247\n",
            "Epoch 2/2\n",
            "536/536 [==============================] - 46s 82ms/step - loss: 0.1638 - accuracy: 0.9635\n",
            "60/60 [==============================] - 2s 27ms/step\n",
            "Elapsed computation time: 87.759 secs\n",
            "Epoch 1/5\n",
            "39/39 [==============================] - 7s 37ms/step - loss: 0.8058 - accuracy: 0.5026\n",
            "Epoch 2/5\n",
            "39/39 [==============================] - 2s 39ms/step - loss: 0.7258 - accuracy: 0.5178\n",
            "Epoch 3/5\n",
            "39/39 [==============================] - 1s 21ms/step - loss: 0.7104 - accuracy: 0.5215\n",
            "Epoch 4/5\n",
            "39/39 [==============================] - 2s 58ms/step - loss: 0.7058 - accuracy: 0.5275\n",
            "Epoch 5/5\n",
            "39/39 [==============================] - 2s 50ms/step - loss: 0.7004 - accuracy: 0.5302\n",
            "60/60 [==============================] - 2s 34ms/step\n",
            "Elapsed computation time: 17.077 secs\n",
            "Epoch 1/2\n",
            "536/536 [==============================] - 60s 93ms/step - loss: 0.2942 - accuracy: 0.8991\n",
            "Epoch 2/2\n",
            "536/536 [==============================] - 57s 101ms/step - loss: 0.1632 - accuracy: 0.9635\n",
            "60/60 [==============================] - 2s 24ms/step\n",
            "Elapsed computation time: 121.454 secs\n",
            "Epoch 1/5\n",
            "39/39 [==============================] - 4s 37ms/step - loss: 0.7463 - accuracy: 0.5075\n",
            "Epoch 2/5\n",
            "39/39 [==============================] - 2s 40ms/step - loss: 0.7147 - accuracy: 0.5171\n",
            "Epoch 3/5\n",
            "39/39 [==============================] - 2s 45ms/step - loss: 0.7079 - accuracy: 0.5217\n",
            "Epoch 4/5\n",
            "39/39 [==============================] - 2s 41ms/step - loss: 0.6998 - accuracy: 0.5319\n",
            "Epoch 5/5\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.6978 - accuracy: 0.5351\n",
            "60/60 [==============================] - 2s 26ms/step\n",
            "Elapsed computation time: 13.812 secs\n",
            "Epoch 1/2\n",
            "536/536 [==============================] - 50s 83ms/step - loss: 0.2228 - accuracy: 0.9484\n",
            "Epoch 2/2\n",
            "536/536 [==============================] - 47s 83ms/step - loss: 0.1627 - accuracy: 0.9635\n",
            "60/60 [==============================] - 2s 25ms/step\n",
            "Elapsed computation time: 100.057 secs\n",
            "Epoch 1/5\n",
            "39/39 [==============================] - 4s 38ms/step - loss: 0.7913 - accuracy: 0.5076\n",
            "Epoch 2/5\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 0.7246 - accuracy: 0.5170\n",
            "Epoch 3/5\n",
            "39/39 [==============================] - 2s 38ms/step - loss: 0.7098 - accuracy: 0.5231\n",
            "Epoch 4/5\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 0.7036 - accuracy: 0.5298\n",
            "Epoch 5/5\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.6991 - accuracy: 0.5360\n",
            "60/60 [==============================] - 2s 26ms/step\n",
            "Elapsed computation time: 12.234 secs\n",
            "Epoch 1/2\n",
            "536/536 [==============================] - 47s 77ms/step - loss: 0.2489 - accuracy: 0.9339\n",
            "Epoch 2/2\n",
            "536/536 [==============================] - 44s 78ms/step - loss: 0.1622 - accuracy: 0.9636\n",
            "60/60 [==============================] - 2s 26ms/step\n",
            "Elapsed computation time: 94.487 secs\n",
            "Epoch 1/5\n",
            "39/39 [==============================] - 4s 41ms/step - loss: 0.7421 - accuracy: 0.5107\n",
            "Epoch 2/5\n",
            "39/39 [==============================] - 2s 38ms/step - loss: 0.7178 - accuracy: 0.5142\n",
            "Epoch 3/5\n",
            "39/39 [==============================] - 2s 39ms/step - loss: 0.7079 - accuracy: 0.5229\n",
            "Epoch 4/5\n",
            "39/39 [==============================] - 1s 37ms/step - loss: 0.7018 - accuracy: 0.5273\n",
            "Epoch 5/5\n",
            "39/39 [==============================] - 2s 40ms/step - loss: 0.6979 - accuracy: 0.5352\n",
            "60/60 [==============================] - 2s 24ms/step\n",
            "Elapsed computation time: 12.636 secs\n",
            "Epoch 1/2\n",
            "536/536 [==============================] - 54s 91ms/step - loss: 0.2541 - accuracy: 0.9271\n",
            "Epoch 2/2\n",
            "536/536 [==============================] - 53s 94ms/step - loss: 0.1614 - accuracy: 0.9635\n",
            "60/60 [==============================] - 2s 25ms/step\n",
            "Elapsed computation time: 109.885 secs\n",
            "Epoch 1/5\n",
            "39/39 [==============================] - 4s 35ms/step - loss: 0.7581 - accuracy: 0.5160\n",
            "Epoch 2/5\n",
            "39/39 [==============================] - 1s 34ms/step - loss: 0.7180 - accuracy: 0.5190\n",
            "Epoch 3/5\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.7044 - accuracy: 0.5297\n",
            "Epoch 4/5\n",
            "39/39 [==============================] - 1s 35ms/step - loss: 0.6990 - accuracy: 0.5343\n",
            "Epoch 5/5\n",
            "39/39 [==============================] - 2s 39ms/step - loss: 0.6949 - accuracy: 0.5393\n",
            "60/60 [==============================] - 2s 23ms/step\n",
            "Elapsed computation time: 11.775 secs\n",
            "Epoch 1/2\n",
            "536/536 [==============================] - 53s 88ms/step - loss: 0.2269 - accuracy: 0.9479\n",
            "Epoch 2/2\n",
            "536/536 [==============================] - 59s 106ms/step - loss: 0.1621 - accuracy: 0.9636\n",
            "60/60 [==============================] - 2s 25ms/step\n",
            "Elapsed computation time: 115.629 secs\n",
            "Epoch 1/5\n",
            "39/39 [==============================] - 4s 39ms/step - loss: 0.7782 - accuracy: 0.5122\n",
            "Epoch 2/5\n",
            "39/39 [==============================] - 2s 39ms/step - loss: 0.7183 - accuracy: 0.5204\n",
            "Epoch 3/5\n",
            "39/39 [==============================] - 2s 38ms/step - loss: 0.7067 - accuracy: 0.5242\n",
            "Epoch 4/5\n",
            "39/39 [==============================] - 1s 36ms/step - loss: 0.7030 - accuracy: 0.5300\n",
            "Epoch 5/5\n",
            "39/39 [==============================] - 2s 38ms/step - loss: 0.6976 - accuracy: 0.5360\n",
            "60/60 [==============================] - 2s 26ms/step\n",
            "Elapsed computation time: 13.019 secs\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "skf = StratifiedKFold(n_splits=10)\n",
        "\n",
        "cv_results_imbalanced = []\n",
        "cv_time_imbalanced = []\n",
        "cv_results_balanced = []\n",
        "cv_time_balanced = []\n",
        "for train_idx, valid_idx in skf.split(X_train, y_train):\n",
        "    X_local_train = preprocessor.fit_transform(X_train.iloc[train_idx])\n",
        "    y_local_train = y_train.iloc[train_idx].values.ravel()\n",
        "    X_local_test = preprocessor.transform(X_train.iloc[valid_idx])\n",
        "    y_local_test = y_train.iloc[valid_idx].values.ravel()\n",
        "\n",
        "    elapsed_time, roc_auc = fit_predict_imbalanced_model(\n",
        "        X_local_train, y_local_train, X_local_test, y_local_test\n",
        "    )\n",
        "    cv_time_imbalanced.append(elapsed_time)\n",
        "    cv_results_imbalanced.append(roc_auc)\n",
        "\n",
        "    elapsed_time, roc_auc = fit_predict_balanced_model(\n",
        "        X_local_train, y_local_train, X_local_test, y_local_test\n",
        "    )\n",
        "    cv_time_balanced.append(elapsed_time)\n",
        "    cv_results_balanced.append(roc_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tracé des résultats et du temps de calcul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_results = pd.DataFrame(\n",
        "    {\n",
        "        \"Balanced model\": cv_results_balanced,\n",
        "        \"Imbalanced model\": cv_results_imbalanced,\n",
        "    }\n",
        ")\n",
        "df_results = df_results.unstack().reset_index()\n",
        "\n",
        "df_time = pd.DataFrame(\n",
        "    {\"Balanced model\": cv_time_balanced, \"Imbalanced model\": cv_time_imbalanced}\n",
        ")\n",
        "df_time = df_time.unstack().reset_index()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure()\n",
        "sns.boxplot(y=\"level_0\", x=0, data=df_time)\n",
        "sns.despine(top=True, right=True, left=True)\n",
        "plt.xlabel(\"time [s]\")\n",
        "plt.ylabel(\"\")\n",
        "plt.title(\"Computation time difference using a random under-sampling\")\n",
        "\n",
        "plt.figure()\n",
        "sns.boxplot(y=\"level_0\", x=0, data=df_results, whis=10.0)\n",
        "sns.despine(top=True, right=True, left=True)\n",
        "ax = plt.gca()\n",
        "ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, pos: \"%i%%\" % (100 * x)))\n",
        "plt.xlabel(\"ROC-AUC\")\n",
        "plt.ylabel(\"\")\n",
        "plt.title(\"Difference in terms of ROC-AUC using a random under-sampling\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
