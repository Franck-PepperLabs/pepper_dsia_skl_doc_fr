{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [**Regroupement double de documents avec l'algorithme de regroupement conjoint spectral**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_4_bicluster/plot_bicluster_newsgroups.ipynb)<br/>([_Biclustering documents with the Spectral Co-clustering algorithm_](https://scikit-learn.org/stable/auto_examples/bicluster/plot_bicluster_newsgroups.html))\n",
    "\n",
    "Cet exemple illustre l'algorithme de Regroupement Conjoint Spectral sur l'ensemble de données des vingt newsgroups. La catégorie « comp.os.ms-windows.misc » est exclue car elle contient de nombreux messages ne contenant que des données.\n",
    "\n",
    "Les messages vectorisés en TF-IDF forment une matrice de fréquence de mots, qui est ensuite regroupée en biclusters à l'aide de l'algorithme de Regroupement Conjoint Spectral de Dhillon. Les biclusters document-mot résultants indiquent les sous-ensembles de mots utilisés plus fréquemment dans ces documents.\n",
    "\n",
    "Pour quelques-uns des meilleurs biclusters, les catégories de documents les plus courantes et leurs dix mots les plus importants sont imprimés. Les meilleurs biclusters sont déterminés par leur coupure normalisée. Les meilleurs mots sont déterminés en comparant leurs sommes à l'intérieur et à l'extérieur du bicluster.\n",
    "\n",
    "Pour comparaison, les documents sont également regroupés à l'aide de [**`MiniBatchKMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html). Les clusters de documents dérivés des biclusters atteignent une meilleure mesure V que les clusters trouvés par `MiniBatchKMeans`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing...\n",
      "Coclustering...\n",
      "Done in 1.48s. V-measure: 0.4415\n",
      "MiniBatchKMeans...\n",
      "Done in 2.93s. V-measure: 0.3015\n",
      "\n",
      "Best biclusters:\n",
      "----------------\n",
      "bicluster 0 : 8 documents, 6 words\n",
      "categories   : 100% talk.politics.mideast\n",
      "words        : cosmo, angmar, alfalfa, alphalpha, proline, benson\n",
      "\n",
      "bicluster 1 : 1948 documents, 4325 words\n",
      "categories   : 23% talk.politics.guns, 18% talk.politics.misc, 17% sci.med\n",
      "words        : gun, guns, geb, banks, gordon, clinton, pitt, cdt, surrender, veal\n",
      "\n",
      "bicluster 2 : 1259 documents, 3534 words\n",
      "categories   : 27% soc.religion.christian, 25% talk.politics.mideast, 25% alt.atheism\n",
      "words        : god, jesus, christians, kent, sin, objective, belief, christ, faith, moral\n",
      "\n",
      "bicluster 3 : 775 documents, 1623 words\n",
      "categories   : 30% comp.windows.x, 25% comp.sys.ibm.pc.hardware, 20% comp.graphics\n",
      "words        : scsi, nada, ide, vga, esdi, isa, kth, s3, vlb, bmug\n",
      "\n",
      "bicluster 4 : 2180 documents, 2802 words\n",
      "categories   : 18% comp.sys.mac.hardware, 16% sci.electronics, 16% comp.sys.ibm.pc.hardware\n",
      "words        : voltage, shipping, circuit, receiver, processing, scope, mpce, analog, kolstad, umass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "from collections import defaultdict\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans, SpectralCoclustering\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.cluster import v_measure_score\n",
    "\n",
    "\n",
    "def number_normalizer(tokens):\n",
    "    \"\"\"Map all numeric tokens to a placeholder.\n",
    "\n",
    "    For many applications, tokens that begin with a number are not directly\n",
    "    useful, but the fact that such a token exists can be relevant.  By applying\n",
    "    this form of dimensionality reduction, some methods may perform better.\n",
    "    \"\"\"\n",
    "    return (\"#NUMBER\" if token[0].isdigit() else token for token in tokens)\n",
    "\n",
    "\n",
    "class NumberNormalizingVectorizer(TfidfVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenize = super().build_tokenizer()\n",
    "        return lambda doc: list(number_normalizer(tokenize(doc)))\n",
    "\n",
    "\n",
    "# exclude 'comp.os.ms-windows.misc'\n",
    "categories = [\n",
    "    \"alt.atheism\",\n",
    "    \"comp.graphics\",\n",
    "    \"comp.sys.ibm.pc.hardware\",\n",
    "    \"comp.sys.mac.hardware\",\n",
    "    \"comp.windows.x\",\n",
    "    \"misc.forsale\",\n",
    "    \"rec.autos\",\n",
    "    \"rec.motorcycles\",\n",
    "    \"rec.sport.baseball\",\n",
    "    \"rec.sport.hockey\",\n",
    "    \"sci.crypt\",\n",
    "    \"sci.electronics\",\n",
    "    \"sci.med\",\n",
    "    \"sci.space\",\n",
    "    \"soc.religion.christian\",\n",
    "    \"talk.politics.guns\",\n",
    "    \"talk.politics.mideast\",\n",
    "    \"talk.politics.misc\",\n",
    "    \"talk.religion.misc\",\n",
    "]\n",
    "newsgroups = fetch_20newsgroups(categories=categories)\n",
    "y_true = newsgroups.target\n",
    "\n",
    "vectorizer = NumberNormalizingVectorizer(stop_words=\"english\", min_df=5)\n",
    "cocluster = SpectralCoclustering(\n",
    "    n_clusters=len(categories), svd_method=\"arpack\", random_state=0\n",
    ")\n",
    "kmeans = MiniBatchKMeans(\n",
    "    n_clusters=len(categories), batch_size=20000, random_state=0, n_init=3\n",
    ")\n",
    "\n",
    "print(\"Vectorizing...\")\n",
    "X = vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "print(\"Coclustering...\")\n",
    "start_time = time()\n",
    "cocluster.fit(X)\n",
    "y_cocluster = cocluster.row_labels_\n",
    "print(\n",
    "    \"Done in {:.2f}s. V-measure: {:.4f}\".format(\n",
    "        time() - start_time, v_measure_score(y_cocluster, y_true)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"MiniBatchKMeans...\")\n",
    "start_time = time()\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "print(\n",
    "    \"Done in {:.2f}s. V-measure: {:.4f}\".format(\n",
    "        time() - start_time, v_measure_score(y_kmeans, y_true)\n",
    "    )\n",
    ")\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "document_names = [newsgroups.target_names[i] for i in newsgroups.target]\n",
    "\n",
    "\n",
    "def bicluster_ncut(i):\n",
    "    rows, cols = cocluster.get_indices(i)\n",
    "    if not (np.any(rows) and np.any(cols)):\n",
    "        import sys\n",
    "\n",
    "        return sys.float_info.max\n",
    "    row_complement = np.nonzero(np.logical_not(cocluster.rows_[i]))[0]\n",
    "    col_complement = np.nonzero(np.logical_not(cocluster.columns_[i]))[0]\n",
    "    # Note: the following is identical to X[rows[:, np.newaxis],\n",
    "    # cols].sum() but much faster in scipy <= 0.16\n",
    "    weight = X[rows][:, cols].sum()\n",
    "    cut = X[row_complement][:, cols].sum() + X[rows][:, col_complement].sum()\n",
    "    return cut / weight\n",
    "\n",
    "\n",
    "def most_common(d):\n",
    "    \"\"\"Items of a defaultdict(int) with the highest values.\n",
    "\n",
    "    Like Counter.most_common in Python >=2.7.\n",
    "    \"\"\"\n",
    "    return sorted(d.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "\n",
    "bicluster_ncuts = [\n",
    "    bicluster_ncut(i) for i in range(len(newsgroups.target_names))\n",
    "]\n",
    "best_idx = np.argsort(bicluster_ncuts)[:5]\n",
    "\n",
    "print()\n",
    "print(\"Best biclusters:\")\n",
    "print(\"----------------\")\n",
    "for idx, cluster in enumerate(best_idx):\n",
    "    n_rows, n_cols = cocluster.get_shape(cluster)\n",
    "    cluster_docs, cluster_words = cocluster.get_indices(cluster)\n",
    "    if not len(cluster_docs) or not len(cluster_words):\n",
    "        continue\n",
    "\n",
    "    # categories\n",
    "    counter = defaultdict(int)\n",
    "    for i in cluster_docs:\n",
    "        counter[document_names[i]] += 1\n",
    "    cat_string = \", \".join(\n",
    "        \"{:.0f}% {}\".format(float(c) / n_rows * 100, name)\n",
    "        for name, c in most_common(counter)[:3]\n",
    "    )\n",
    "\n",
    "    # words\n",
    "    out_of_cluster_docs = cocluster.row_labels_ != cluster\n",
    "    out_of_cluster_docs = np.where(out_of_cluster_docs)[0]\n",
    "    word_col = X[:, cluster_words]\n",
    "    word_scores = np.array(\n",
    "        word_col[cluster_docs, :].sum(axis=0)\n",
    "        - word_col[out_of_cluster_docs, :].sum(axis=0)\n",
    "    )\n",
    "    word_scores = word_scores.ravel()\n",
    "    important_words = [\n",
    "        feature_names[cluster_words[i]] for i in word_scores.argsort()[:-11:-1]\n",
    "    ]\n",
    "\n",
    "    print(f\"bicluster {idx} : {n_rows} documents, {n_cols} words\")\n",
    "    print(f\"categories   : {cat_string}\")\n",
    "    print(f'words        : {\", \".join(important_words)}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
