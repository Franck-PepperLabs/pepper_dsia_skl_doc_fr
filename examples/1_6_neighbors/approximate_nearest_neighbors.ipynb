{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [**Plus proches voisins approchés dans TSNE**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_6_neighbors/approximate_nearest_neighbors.ipynb)<br/>([_Approximate nearest neighbors in TSNE_](https://scikit-learn.org/stable/auto_examples/neighbors/approximate_nearest_neighbors.html))\n",
    "\n",
    "Cet exemple montre comment chaîner [**`KNeighborsTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html#sklearn.neighbors.KNeighborsTransformer) et [**`TSNE`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE) dans un pipeline. Il explique également comment utiliser les packages `nmslib` et `pynndescent` pour remplacer [**`KNeighborsTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html#sklearn.neighbors.KNeighborsTransformer) et effectuer une recherche de plus proches voisins approximative. Ces packages peuvent être installés avec `pip install nmslib pynndescent`.\n",
    "\n",
    "Remarque : Dans [**`KNeighborsTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html#sklearn.neighbors.KNeighborsTransformer), nous utilisons la définition qui inclut chaque point d'entraînement en tant que son propre voisin dans le décompte de `n_neighbors`, et pour des raisons de compatibilité, un voisin supplémentaire est calculé lorsque `mode == 'distance'`. Veuillez noter que nous faisons de même dans l'enveloppe `nmslib` proposée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Tom Dupre la Tour\n",
    "# License: BSD 3 clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord, nous essayons d'importer les packages et avertissons l'utilisateur en cas d'absence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "try:\n",
    "    import nmslib\n",
    "except ImportError:\n",
    "    print(\"The package 'nmslib' is required to run this example.\")\n",
    "    sys.exit()\n",
    "\n",
    "try:\n",
    "    from pynndescent import PyNNDescentTransformer\n",
    "except ImportError:\n",
    "    print(\"The package 'pynndescent' is required to run this example.\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous définissons une classe d'enveloppe pour implémenter l'API scikit-learn dans `nmslib`, ainsi qu'une fonction de chargement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "class NMSlibTransformer(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"Wrapper for using nmslib as sklearn's KNeighborsTransformer\"\"\"\n",
    "\n",
    "    def __init__(self, n_neighbors=5, metric=\"euclidean\", method=\"sw-graph\", n_jobs=-1):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.method = method\n",
    "        self.metric = metric\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.n_samples_fit_ = X.shape[0]\n",
    "\n",
    "        # see more metric in the manual\n",
    "        # https://github.com/nmslib/nmslib/tree/master/manual\n",
    "        space = {\n",
    "            \"euclidean\": \"l2\",\n",
    "            \"cosine\": \"cosinesimil\",\n",
    "            \"l1\": \"l1\",\n",
    "            \"l2\": \"l2\",\n",
    "        }[self.metric]\n",
    "\n",
    "        self.nmslib_ = nmslib.init(method=self.method, space=space)\n",
    "        self.nmslib_.addDataPointBatch(X.copy())\n",
    "        self.nmslib_.createIndex()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        n_samples_transform = X.shape[0]\n",
    "\n",
    "        # For compatibility reasons, as each sample is considered as its own\n",
    "        # neighbor, one extra neighbor will be computed.\n",
    "        n_neighbors = self.n_neighbors + 1\n",
    "\n",
    "        if self.n_jobs < 0:\n",
    "            # Same handling as done in joblib for negative values of n_jobs:\n",
    "            # in particular, `n_jobs == -1` means \"as many threads as CPUs\".\n",
    "            num_threads = joblib.cpu_count() + self.n_jobs + 1\n",
    "        else:\n",
    "            num_threads = self.n_jobs\n",
    "\n",
    "        results = self.nmslib_.knnQueryBatch(\n",
    "            X.copy(), k=n_neighbors, num_threads=num_threads\n",
    "        )\n",
    "        indices, distances = zip(*results)\n",
    "        indices, distances = np.vstack(indices), np.vstack(distances)\n",
    "\n",
    "        indptr = np.arange(0, n_samples_transform * n_neighbors + 1, n_neighbors)\n",
    "        kneighbors_graph = csr_matrix(\n",
    "            (distances.ravel(), indices.ravel(), indptr),\n",
    "            shape=(n_samples_transform, self.n_samples_fit_),\n",
    "        )\n",
    "\n",
    "        return kneighbors_graph\n",
    "\n",
    "\n",
    "def load_mnist(n_samples):\n",
    "    \"\"\"Load MNIST, shuffle the data, and return only n_samples.\"\"\"\n",
    "    mnist = fetch_openml(\"mnist_784\", as_frame=False, parser=\"pandas\")\n",
    "    X, y = shuffle(mnist.data, mnist.target, random_state=2)\n",
    "    return X[:n_samples] / 255, y[:n_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous évaluons les différentes transformateurs de plus proches voisins exacts/approximatifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import KNeighborsTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "datasets = [\n",
    "    (\"MNIST_10000\", load_mnist(n_samples=10_000)),\n",
    "    (\"MNIST_20000\", load_mnist(n_samples=20_000)),\n",
    "]\n",
    "\n",
    "n_iter = 500\n",
    "perplexity = 30\n",
    "metric = \"euclidean\"\n",
    "# TSNE requires a certain number of neighbors which depends on the\n",
    "# perplexity parameter.\n",
    "# Add one since we include each sample as its own neighbor.\n",
    "n_neighbors = int(3.0 * perplexity + 1) + 1\n",
    "\n",
    "tsne_params = dict(\n",
    "    init=\"random\",  # pca not supported for sparse matrices\n",
    "    perplexity=perplexity,\n",
    "    method=\"barnes_hut\",\n",
    "    random_state=42,\n",
    "    n_iter=n_iter,\n",
    "    learning_rate=\"auto\",\n",
    ")\n",
    "\n",
    "transformers = [\n",
    "    (\n",
    "        \"KNeighborsTransformer\",\n",
    "        KNeighborsTransformer(n_neighbors=n_neighbors, mode=\"distance\", metric=metric),\n",
    "    ),\n",
    "    (\n",
    "        \"NMSlibTransformer\",\n",
    "        NMSlibTransformer(n_neighbors=n_neighbors, metric=metric),\n",
    "    ),\n",
    "    (\n",
    "        \"PyNNDescentTransformer\",\n",
    "        PyNNDescentTransformer(\n",
    "            n_neighbors=n_neighbors, metric=metric, parallel_batch_queries=True\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "for dataset_name, (X, y) in datasets:\n",
    "    msg = f\"Benchmarking on {dataset_name}:\"\n",
    "    print(f\"\\n{msg}\\n\" + str(\"-\" * len(msg)))\n",
    "\n",
    "    for transformer_name, transformer in transformers:\n",
    "        longest = np.max([len(name) for name, model in transformers])\n",
    "        start = time.time()\n",
    "        transformer.fit(X)\n",
    "        fit_duration = time.time() - start\n",
    "        print(f\"{transformer_name:<{longest}} {fit_duration:.3f} sec (fit)\")\n",
    "        start = time.time()\n",
    "        Xt = transformer.transform(X)\n",
    "        transform_duration = time.time() - start\n",
    "        print(f\"{transformer_name:<{longest}} {transform_duration:.3f} sec (transform)\")\n",
    "        if transformer_name == \"PyNNDescentTransformer\":\n",
    "            start = time.time()\n",
    "            Xt = transformer.transform(X)\n",
    "            transform_duration = time.time() - start\n",
    "            print(\n",
    "                f\"{transformer_name:<{longest}} {transform_duration:.3f} sec\"\n",
    "                \" (transform)\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sortie d'exemple :\n",
    "\n",
    "```python\n",
    "Benchmarking on MNIST_10000:\n",
    "----------------------------\n",
    "KNeighborsTransformer  0.007 sec (fit)\n",
    "KNeighborsTransformer  1.139 sec (transform)\n",
    "NMSlibTransformer      0.208 sec (fit)\n",
    "NMSlibTransformer      0.315 sec (transform)\n",
    "PyNNDescentTransformer 4.823 sec (fit)\n",
    "PyNNDescentTransformer 4.884 sec (transform)\n",
    "PyNNDescentTransformer 0.744 sec (transform)\n",
    "\n",
    "Benchmarking on MNIST_20000:\n",
    "----------------------------\n",
    "KNeighborsTransformer  0.011 sec (fit)\n",
    "KNeighborsTransformer  5.769 sec (transform)\n",
    "NMSlibTransformer      0.733 sec (fit)\n",
    "NMSlibTransformer      1.077 sec (transform)\n",
    "PyNNDescentTransformer 14.448 sec (fit)\n",
    "PyNNDescentTransformer 7.103 sec (transform)\n",
    "PyNNDescentTransformer 1.759 sec (transform)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarquez que le `PyNNDescentTransformer` prend plus de temps lors du premier `fit` et du premier `transform` en raison du surcoût du compilateur à la volée de numba. Mais après le premier appel, le code Python compilé est conservé dans un cache par numba et les appels ultérieurs ne souffrent pas de ce coût initial. Tant [**`KNeighborsTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html#sklearn.neighbors.KNeighborsTransformer) que `NMSlibTransformer` ne sont exécutés qu'une seule fois ici car ils afficheraient des temps de `fit` et de `transform` plus stables (ils n'ont pas le problème du démarrage à froid du `PyNNDescentTransformer`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "\n",
    "transformers = [\n",
    "    (\"TSNE with internal NearestNeighbors\", TSNE(metric=metric, **tsne_params)),\n",
    "    (\n",
    "        \"TSNE with KNeighborsTransformer\",\n",
    "        make_pipeline(\n",
    "            KNeighborsTransformer(\n",
    "                n_neighbors=n_neighbors, mode=\"distance\", metric=metric\n",
    "            ),\n",
    "            TSNE(metric=\"precomputed\", **tsne_params),\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"TSNE with NMSlibTransformer\",\n",
    "        make_pipeline(\n",
    "            NMSlibTransformer(n_neighbors=n_neighbors, metric=metric),\n",
    "            TSNE(metric=\"precomputed\", **tsne_params),\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "# init the plot\n",
    "nrows = len(datasets)\n",
    "ncols = np.sum([1 for name, model in transformers if \"TSNE\" in name])\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=nrows, ncols=ncols, squeeze=False, figsize=(5 * ncols, 4 * nrows)\n",
    ")\n",
    "axes = axes.ravel()\n",
    "i_ax = 0\n",
    "\n",
    "for dataset_name, (X, y) in datasets:\n",
    "    msg = f\"Benchmarking on {dataset_name}:\"\n",
    "    print(f\"\\n{msg}\\n\" + str(\"-\" * len(msg)))\n",
    "\n",
    "    for transformer_name, transformer in transformers:\n",
    "        longest = np.max([len(name) for name, model in transformers])\n",
    "        start = time.time()\n",
    "        Xt = transformer.fit_transform(X)\n",
    "        transform_duration = time.time() - start\n",
    "        print(\n",
    "            f\"{transformer_name:<{longest}} {transform_duration:.3f} sec\"\n",
    "            \" (fit_transform)\"\n",
    "        )\n",
    "\n",
    "        # plot TSNE embedding which should be very similar across methods\n",
    "        axes[i_ax].set_title(transformer_name + \"\\non \" + dataset_name)\n",
    "        axes[i_ax].scatter(\n",
    "            Xt[:, 0],\n",
    "            Xt[:, 1],\n",
    "            c=y.astype(np.int32),\n",
    "            alpha=0.2,\n",
    "            cmap=plt.cm.viridis,\n",
    "        )\n",
    "        axes[i_ax].xaxis.set_major_formatter(NullFormatter())\n",
    "        axes[i_ax].yaxis.set_major_formatter(NullFormatter())\n",
    "        axes[i_ax].axis(\"tight\")\n",
    "        i_ax += 1\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sortie d'exemple :\n",
    "\n",
    "```python\n",
    "Benchmarking on MNIST_10000:\n",
    "----------------------------\n",
    "TSNE with internal NearestNeighbors 24.828 sec (fit_transform)\n",
    "TSNE with KNeighborsTransformer     20.111 sec (fit_transform)\n",
    "TSNE with NMSlibTransformer         21.757 sec (fit_transform)\n",
    "\n",
    "Benchmarking on MNIST_20000:\n",
    "----------------------------\n",
    "TSNE with internal NearestNeighbors 51.955 sec (fit_transform)\n",
    "TSNE with KNeighborsTransformer     50.994 sec (fit_transform)\n",
    "TSNE with NMSlibTransformer         43.536 sec (fit_transform)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons observer que l'estimateur [**`TSNE`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE) par défaut avec sa mise en œuvre interne de [**`NearestNeighbors`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors) est approximativement équivalent au pipeline avec [**`TSNE`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE) et [**`KNeighborsTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html#sklearn.neighbors.KNeighborsTransformer) en termes de performances. Cela est attendu car les deux pipelines reposent en interne sur la même mise en œuvre de [**`NearestNeighbors`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors) qui effectue une recherche de voisins exacte. Le `NMSlibTransformer` approximatif est déjà légèrement plus rapide que la recherche exacte sur le plus petit ensemble de données, mais il est prévu que cette différence de vitesse devienne plus significative sur des ensembles de données avec un plus grand nombre d'échantillons.\n",
    "\n",
    "Notez cependant que ce ne sont pas tous les méthodes de recherche approximative qui garantissent d'améliorer la vitesse de la méthode de recherche exacte par défaut : en effet, l'implémentation de recherche exacte a significativement progressé depuis scikit-learn 1.1. De plus, l'implémentation de recherche exacte en force brute ne nécessite pas la construction d'un index à `fit` time. Ainsi, pour obtenir une amélioration globale des performances dans le contexte du pipeline [**`TSNE`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE), les avantages de la recherche approximative au moment du `transform` doivent être supérieurs au temps supplémentaire nécessaire pour construire l'index de recherche approximative au moment du `fit`.\n",
    "\n",
    "Enfin, l'algorithme TSNE lui-même est également intensif en calcul, indépendamment de la recherche de voisins les plus proches. Ainsi, accélérer l'étape de recherche des voisins les plus proches d'un facteur de 5 ne se traduirait pas par une accélération du pipeline global d'un facteur de 5."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
