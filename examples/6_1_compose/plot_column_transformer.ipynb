{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [**Transformateur de colonne avec des sources de données hétérogènes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/6_1_compose/plot_column_transformer.ipynb)<br/>([*Column Transformer with Heterogeneous Data Sources*](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer.html))\n",
    "\n",
    "Les jeux de données peuvent souvent contenir des composants qui nécessitent des pipelines d'extraction et de traitement de caractéristiques différentes. Ce scénario peut se produire lorsque:\n",
    "1. votre jeu de données est constitué de types de données hétérogènes (par exemple, des images matricielles et des légendes de texte),\n",
    "2. votre jeu de données est stocké dans un [**`pandas.DataFrame`**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) et différentes colonnes nécessitent des pipelines de traitement différents.\n",
    "\n",
    "Cet exemple montre comment utiliser [**`ColumnTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) sur un jeu de données contenant différents types de caractéristiques. Le choix des caractéristiques n'est pas particulièrement utile, mais sert à illustrer la technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Matt Terry <matt.terry@gmail.com>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jeu de données 20 newsgroups\n",
    "\n",
    "Nous utiliserons le [**jeu de données 20 newsgroups** (7.2.2)](https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset), qui comprend des publications de newsgroups sur 20 sujets. Ce jeu de données est divisé en sous-ensembles d'entraînement et de test en fonction de messages publiés avant et après une date spécifique. Nous n'utiliserons que des messages de 2 catégories pour accélérer le temps d'exécution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"sci.med\", \"sci.space\"]\n",
    "X_train, y_train = fetch_20newsgroups(\n",
    "    random_state=1,\n",
    "    subset=\"train\",\n",
    "    categories=categories,\n",
    "    remove=(\"footers\", \"quotes\"),\n",
    "    return_X_y=True,\n",
    ")\n",
    "X_test, y_test = fetch_20newsgroups(\n",
    "    random_state=1,\n",
    "    subset=\"test\",\n",
    "    categories=categories,\n",
    "    remove=(\"footers\", \"quotes\"),\n",
    "    return_X_y=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaque caractéristique comprend des métadonnées sur cette publication, telles que le sujet et le corps du message d'actualités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: mccall@mksol.dseg.ti.com (fred j mccall 575-3539)\n",
      "Subject: Re: Metric vs English\n",
      "Article-I.D.: mksol.1993Apr6.131900.8407\n",
      "Organization: Texas Instruments Inc\n",
      "Lines: 31\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "American, perhaps, but nothing military about it.  I learned (mostly)\n",
      "slugs when we talked English units in high school physics and while\n",
      "the teacher was an ex-Navy fighter jock the book certainly wasn't\n",
      "produced by the military.\n",
      "\n",
      "[Poundals were just too flinking small and made the math come out\n",
      "funny; sort of the same reason proponents of SI give for using that.] \n",
      "\n",
      "-- \n",
      "\"Insisting on perfect safety is for people who don't have the balls to live\n",
      " in the real world.\"   -- Mary Shafer, NASA Ames Dryden\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création de transformateurs\n",
    "\n",
    "Tout d'abord, nous voudrions un transformateur qui extrait le sujet et le corps de chaque publication. Comme il s'agit d'une transformation sans état (ne nécessite pas d'informations d'état provenant de données d'entraînement), nous pouvons définir une fonction qui effectue la transformation de données, puis utiliser [**`FunctionTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html) pour créer un transformateur scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subject_body_extractor(posts):\n",
    "    # construct object dtype array with two columns\n",
    "    # first column = 'subject' and second column = 'body'\n",
    "    features = np.empty(shape=(len(posts), 2), dtype=object)\n",
    "    for i, text in enumerate(posts):\n",
    "        # temporary variable `_` stores '\\n\\n'\n",
    "        headers, _, body = text.partition(\"\\n\\n\")\n",
    "        # store body text in second column\n",
    "        features[i, 1] = body\n",
    "\n",
    "        prefix = \"Subject:\"\n",
    "        sub = \"\"\n",
    "        # save text after 'Subject:' in first column\n",
    "        for line in headers.split(\"\\n\"):\n",
    "            if line.startswith(prefix):\n",
    "                sub = line[len(prefix) :]\n",
    "                break\n",
    "        features[i, 0] = sub\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "subject_body_transformer = FunctionTransformer(subject_body_extractor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons également créer un transformateur qui extrait la longueur du texte et le nombre de phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_stats(posts):\n",
    "    return [{\"length\": len(text), \"num_sentences\": text.count(\".\")} for text in posts]\n",
    "\n",
    "\n",
    "text_stats_transformer = FunctionTransformer(text_stats)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline de classification\n",
    "\n",
    "Le pipeline ci-dessous extrait le sujet et le corps de chaque publication en utilisant `SubjectBodyExtractor`, produisant un tableau `(n_samples, 2)`. Ce tableau est ensuite utilisé pour calculer des caractéristiques de sac de mots standard pour le sujet et le corps, ainsi que la longueur du texte et le nombre de phrases sur le corps, en utilisant `ColumnTransformer`. Nous les combinons, avec des poids, puis formons un classificateur sur l'ensemble combiné de caractéristiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        # Extract subject & body\n",
    "        (\"subjectbody\", subject_body_transformer),\n",
    "        # Use ColumnTransformer to combine the subject and body features\n",
    "        (\n",
    "            \"union\",\n",
    "            ColumnTransformer(\n",
    "                [\n",
    "                    # bag-of-words for subject (col 0)\n",
    "                    (\"subject\", TfidfVectorizer(min_df=50), 0),\n",
    "                    # bag-of-words with decomposition for body (col 1)\n",
    "                    (\n",
    "                        \"body_bow\",\n",
    "                        Pipeline(\n",
    "                            [\n",
    "                                (\"tfidf\", TfidfVectorizer()),\n",
    "                                (\"best\", TruncatedSVD(n_components=50)),\n",
    "                            ]\n",
    "                        ),\n",
    "                        1,\n",
    "                    ),\n",
    "                    # Pipeline for pulling text stats from post's body\n",
    "                    (\n",
    "                        \"body_stats\",\n",
    "                        Pipeline(\n",
    "                            [\n",
    "                                (\n",
    "                                    \"stats\",\n",
    "                                    text_stats_transformer,\n",
    "                                ),  # returns a list of dicts\n",
    "                                (\n",
    "                                    \"vect\",\n",
    "                                    DictVectorizer(),\n",
    "                                ),  # list of dicts -> feature matrix\n",
    "                            ]\n",
    "                        ),\n",
    "                        1,\n",
    "                    ),\n",
    "                ],\n",
    "                # weight above ColumnTransformer features\n",
    "                transformer_weights={\n",
    "                    \"subject\": 0.8,\n",
    "                    \"body_bow\": 0.5,\n",
    "                    \"body_stats\": 1.0,\n",
    "                },\n",
    "            ),\n",
    "        ),\n",
    "        # Use a SVC classifier on the combined features\n",
    "        (\"svc\", LinearSVC(dual=False)),\n",
    "    ],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, nous ajustons notre pipeline sur les données d'entraînement et l'utilisons pour prédire les sujets de `X_test`. Les métriques de performance de notre pipeline sont alors imprimées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ....... (step 1 of 3) Processing subjectbody, total=   0.0s\n",
      "[Pipeline] ............. (step 2 of 3) Processing union, total=   1.2s\n",
      "[Pipeline] ............... (step 3 of 3) Processing svc, total=   0.0s\n",
      "Classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.87      0.85       396\n",
      "           1       0.87      0.83      0.85       394\n",
      "\n",
      "    accuracy                           0.85       790\n",
      "   macro avg       0.85      0.85      0.85       790\n",
      "weighted avg       0.85      0.85      0.85       790\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"Classification report:\\n\\n{}\".format(classification_report(y_test, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9aff9e50adfaa9e30c910fb3872ffdc72747acb5f50803ca0504f00e980f7c25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
