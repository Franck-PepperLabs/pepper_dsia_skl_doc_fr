{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='unsupervised-learning'></a> 2. [**Apprentissage non supervisé**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_unsupervised_learning.ipynb#model-selection-and-evaluation)</br>([*Unsupervised learning*](https://scikit-learn.org/stable/unsupervised_learning.html#unsupervised-learning))\n",
    "\n",
    "# 2.1. [**Modèles de mélange gaussien**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_1_mixture.ipynb#gaussian-mixture-models)<br/>([_Gaussian mixture models_](https://scikit-learn.org/stable/modules/mixture.html#gaussian-mixture-models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 11 pages, 7 exemples, <mark>0 papiers</mark>\n",
    "- 2.1.1. [**Mélange gaussien**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_1_mixture.ipynb#gaussian-mixture)<br/>([_Gaussian Mixture_](https://scikit-learn.org/stable/modules/mixture.html#gaussian-mixture))\n",
    "- 2.1.2. [**Mélange gaussien bayésien variationnel**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_1_mixture.ipynb#variational-bayesian-gaussian-mixture)<br/>([_Variational Bayesian Gaussian Mixture_](https://scikit-learn.org/stable/modules/mixture.html#variational-bayesian-gaussian-mixture))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='gaussian-mixture-models'></a> 2.1. Modèles de mélange gaussien\n",
    "\n",
    "[**`sklearn.mixture`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.mixture) est un module qui permet d'apprendre des [**modèles de mélange gaussiens**](https://fr.wikipedia.org/wiki/Modèle_de_mélange_gaussien) (matrices de covariance diagonales, sphériques, liées et complètes prises en charge), de les échantillonner et de les estimer à partir des données. Des méthodes permettant de déterminer le nombre approprié de composants sont également fournies.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_gmm_pdf_001.png\"\n",
    "    alt=\"Modèle de mélange gaussien à deux composants\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "    <p><b>Modèle de mélange gaussien à deux composants</b>: <i>points de données et surfaces d'équi-probabilité du modèle.</i></p>\n",
    "</div>\n",
    "\n",
    "Un modèle de mélange gaussien est un modèle probabiliste qui suppose que tous les points de données sont générés à partir d'un mélange d'un nombre fini de distributions gaussiennes avec des paramètres inconnus. On peut considérer les modèles de mélange comme une généralisation de l'algorithme de regroupement k-moyennes, intégrant des informations sur la structure de covariance des données ainsi que sur les centres des gaussiennes latentes.\n",
    "\n",
    "Scikit-learn implémente différentes classes pour estimer les modèles de mélange gaussiens, qui correspondent à différentes stratégies d'estimation, détaillées ci-dessous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='gaussian-mixture-models'></a> 2.1.1. Mélange gaussien\n",
    "\n",
    "L'objet [**`GaussianMixture`**](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) implémente l'algorithme de [**maximisation des attentes** (2.1.1.3)](https://scikit-learn.org/stable/modules/mixture.html#expectation-maximization) (EM) pour ajuster les modèles de mélange gaussiens. Il peut également dessiner des ellipsoïdes de confiance pour les modèles multivariés et calculer le critère d'information bayésien pour évaluer le nombre de grappes dans les données. Une méthode [**`GaussianMixture.fit`**](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) est fournie qui apprend un modèle de mélange gaussien à partir de données d'entraînement. Compte tenu des données de test, il peut attribuer à chaque échantillon le gaussien auquel il appartient le plus probablement en utilisant la méthode [**`GaussianMixture.predict`**](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html).\n",
    "\n",
    "[**`GaussianMixture`**](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) vient avec différentes options pour contraindre la covariance des classes de différence estimées : covariance sphérique, diagonale, liée ou complète.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_gmm_covariances_001.png\"\n",
    "    alt=\"Covariances des mélanges gaussiens\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "    <p>Covariances des mélanges gaussiens</p>\n",
    "</div>\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Covariances des GMM**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_1_mixture/plot_gmm_covariances.ipynb)<br/>([_GMM covariances_](https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_covariances.html))\n",
    "\n",
    "Exemple d'utilisation du mélange gaussien comme méthode de regroupement sur l'ensemble de données Iris.\n",
    "\n",
    "##### [**Estimation de densité pour un mélange gaussien**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_1_mixture/plot_gmm_pdf.ipynb)<br/>([_Density Estimation for a Gaussian mixture_](https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_pdf.html))\n",
    "\n",
    "Exemple de tracé de l'estimation de densité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='pros-and-cons-of-class-gaussianmixture'></a> 2.1.1.1. Avantages et inconvénients de la classe [**`GaussianMixture`**](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)\n",
    "\n",
    "#### Avantages\n",
    "\n",
    "- **Rapidité :** C'est l'algorithme le plus rapide pour apprendre des modèles de mélange.\n",
    "- **Agnostique :** Comme cet algorithme maximise uniquement la vraisemblance, il ne biaisera pas les moyennes vers zéro, ni ne biaisera les tailles de grappes pour avoir des structures spécifiques qui pourraient ou pourraient ne pas s'appliquer.\n",
    "\n",
    "#### Inconvénients\n",
    "\n",
    "- **Singularités :** Lorsqu'on a trop peu de points par mélange, l'estimation des matrices de covariance devient difficile, et l'algorithme est connu pour diverger et trouver des solutions avec une vraisemblance infinie, sauf si on régularise artificiellement les covariances.\n",
    "- **Nombre de composants :** Cet algorithme utilisera toujours tous les composants auxquels il a accès, nécessitant des données de réserve ou des critères théoriques d'information pour décider du nombre de composants à utiliser en l'absence d'indices externes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='selecting-the-number-of-components-in-a-classical-gaussian-mixture-model'></a> 2.1.1.2. Sélection du nombre de composants dans un modèle de mélange gaussien classique\n",
    "\n",
    "Le critère BIC peut être utilisé pour sélectionner de manière efficace le nombre de composants dans un modèle de mélange gaussien. En théorie, il récupère le véritable nombre de composants uniquement dans le régime asymptotique (c'est-à-dire si beaucoup de données sont disponibles et en supposant que les données ont été réellement générées de manière i.i.d. à partir d'un mélange de distributions gaussiennes). Notez que l'utilisation d'un [**mélange gaussien bayésien variationnel** (2.1.2)](https://scikit-learn.org/stable/modules/mixture.html#bgmm) évite la spécification du nombre de composants pour un modèle de mélange gaussien.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_gmm_selection_002.png\"\n",
    "    alt=\"Sélection de modèle de mélange gaussien\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "    <p>Sélection de modèle de mélange gaussien</p>\n",
    "</div>\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Sélection de modèle de mélange gaussien**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_1_mixture/plot_gmm_selection.ipynb)<br/>([_Gaussian Mixture Model Selection_](https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_selection.html))\n",
    "\n",
    "Exemple de sélection de modèle effectuée avec un mélange gaussien classique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='estimation-algorithm-expectation-maximization'></a> 2.1.1.3. Algorithme d'estimation Expectation-Maximization\n",
    "\n",
    "La principale difficulté dans l'apprentissage des modèles de mélange gaussien à partir de données non étiquetées est que l'on ne sait généralement pas quels points proviennent de quel composant latent (si l'on a accès à ces informations, il devient très facile d'ajuster une distribution gaussienne distincte à chaque ensemble de points). L'[**algorithme espérance-maximisation**](https://fr.wikipedia.org/wiki/Algorithme_esp%C3%A9rance-maximisation) est un algorithme statistique bien fondé pour contourner ce problème par un processus itératif. Tout d'abord, on suppose des composants aléatoires (centrés de manière aléatoire sur les points de données, appris à partir des k-moyennes, ou même simplement distribués normalement autour de l'origine) et on calcule pour chaque point une probabilité d'avoir été généré par chaque composant du modèle. Ensuite, on ajuste les paramètres pour maximiser la probabilité des données étant donné ces affectations. Répéter ce processus garantit toujours de converger vers un optimum local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='choice-of-the-initialization-method'></a> 2.1.1.4. Choix de la méthode d'initialisation\n",
    "\n",
    "Il existe quatre méthodes d'initialisation au choix (ainsi que la possibilité de fournir des moyennes initiales définies par l'utilisateur) pour générer les centres initiaux des composants du modèle :\n",
    "\n",
    "- **k-means (par défaut)** : Cette méthode applique un algorithme de regroupement k-moyennes traditionnel. Cela peut être plus coûteux en temps de calcul par rapport aux autres méthodes d'initialisation.\n",
    "- **k-means++** : Cette méthode utilise la méthode d'initialisation de k-moyennes : k-means++. Elle choisit le premier centre au hasard à partir des données. Les centres suivants seront choisis à partir d'une distribution pondérée des données favorisant les points plus éloignés des centres existants. k-means++ est l'initialisation par défaut pour les k-moyennes, il sera donc plus rapide que l'exécution d'un k-moyennes complet, mais cela peut encore prendre un temps significatif pour les grands ensembles de données avec de nombreux composants.\n",
    "- **random_from_data** : Cette méthode choisit des points de données aléatoires à partir des données d'entrée comme centres initiaux. C'est une méthode d'initialisation très rapide, mais elle peut produire des résultats non convergents si les points choisis sont trop proches les uns des autres.\n",
    "- **random** : Les centres sont choisis comme une petite perturbation par rapport à la moyenne de toutes les données. Cette méthode est simple, mais elle peut entraîner un temps de convergence plus long du modèle.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_gmm_init_001.png\"\n",
    "    alt=\"Méthodes d'initialisation du GMM\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "    <p>Méthodes d'initialisation du GMM</p>\n",
    "</div>\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Méthodes d'initialisation du GMM**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_1_mixture/plot_gmm_init.ipynb)<br/>([_GMM Initialization Methods_](https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_init.html))\n",
    "\n",
    "Exemple de l'utilisation de différentes initialisations dans un modèle de mélange gaussien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='variational-bayesian-gaussian-mixture'></a> 2.1.2. Modèle de Mélange Gaussien Bayésien Variationnel\n",
    "\n",
    "L'objet [**`BayesianGaussianMixture`**](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.BayesianGaussianMixture.html) implémente une variante du modèle de mélange gaussien avec des algorithmes d'[**inférence variationnelle**](https://en.wikipedia.org/wiki/Variational_Bayesian_methods). L'API est similaire à celle définie par [**`GaussianMixture`**](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html).\n",
    "\n",
    "### <a id='estimation-algorithm-variational-inference'></a> 2.1.2.1. Algorithme d'estimation : inférence variationnelle\n",
    "\n",
    "L'inférence variationnelle est une extension de l'espérance-maximisation qui maximise une borne inférieure sur la preuve du modèle (y compris les priorités) au lieu de la vraisemblance des données. Le principe derrière les méthodes variationnelles est le même que celui de l'espérance-maximisation (les deux sont des algorithmes itératifs qui alternent entre la recherche des probabilités pour chaque point d'être généré par chaque mélange et l'ajustement du mélange à ces points assignés), mais les méthodes variationnelles ajoutent une régularisation en intégrant des informations provenant de distributions a priori. Cela évite les singularités souvent trouvées dans les solutions d'espérance-maximisation, mais introduit quelques biais subtils dans le modèle. L'inférence est souvent notablement plus lente, mais pas généralement au point de rendre l'utilisation impraticable.\n",
    "\n",
    "En raison de sa nature bayésienne, l'algorithme variationnel nécessite plus d'hyperparamètres que l'espérance-maximisation, le plus important d'entre eux étant le paramètre de concentration `weight_concentration_prior`. Spécifier une faible valeur pour la concentration a priori fera en sorte que le modèle mette la plupart du poids sur quelques composantes et fixe les poids des composantes restantes très proches de zéro. Des valeurs élevées de la concentration a priori permettront à un plus grand nombre de composantes d'être actives dans le mélange.\n",
    "\n",
    "La classe [**`BayesianGaussianMixture`**](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.BayesianGaussianMixture.html) propose deux types de priorités pour la distribution des poids dans les paramètres : un modèle de mélange fini avec une distribution de Dirichlet et un modèle de mélange infini avec le Processus de Dirichlet. En pratique, l'algorithme d'inférence du Processus de Dirichlet est approximé et utilise une distribution tronquée avec un nombre maximal fixe de composantes (appelé représentation Stick-breaking). Le nombre de composantes réellement utilisé dépend presque toujours des données.\n",
    "\n",
    "La figure suivante compare les résultats obtenus pour le type différent de la priorité de concentration des poids (paramètre `weight_concentration_prior_type`) pour différentes valeurs de `weight_concentration_prior`. Ici, nous pouvons voir que la valeur du paramètre `weight_concentration_prior` a un impact important sur le nombre effectif de composantes actives obtenues. Nous pouvons également remarquer que de grandes valeurs pour la priorité de poids de concentration conduisent à des poids plus uniformes lorsque le type de priorité est `'dirichlet_distribution'`, tandis que ce n'est pas nécessairement le cas pour le type `'dirichlet_process'` (utilisé par défaut).\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_concentration_prior_001.png\"\n",
    "    alt=\"Priorité de concentration 1\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_concentration_prior_002.png\"\n",
    "    alt=\"Priorité de concentration 2\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Les exemples ci-dessous comparent les modèles de mélange gaussien avec un nombre fixe de composantes aux modèles de mélange gaussien variationnels avec une priorité de processus de Dirichlet. Ici, un modèle de mélange gaussien classique est ajusté avec 5 composantes sur un ensemble de données composé de 2 grappes. Nous pouvons voir que le modèle de mélange gaussien variationnel avec une priorité de processus de Dirichlet peut se limiter à seulement 2 composantes, tandis que le mélange gaussien ajuste les données avec un nombre fixe de composantes qui doit être fixé a priori par l'utilisateur. Dans ce cas, l'utilisateur a sélectionné `n_components=5`, ce qui ne correspond pas à la distribution générative réelle de cet ensemble de données de démonstration. Notez qu'avec très peu d'observations, les modèles de mélange gaussien variationnels avec une priorité de processus de Dirichlet peuvent adopter une position conservatrice et ajuster une seule composante.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_gmm_001.png\"\n",
    "    alt=\"Comparaison GMM fixe vs. variationnel\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Sur la figure suivante, nous ajustons un ensemble de données qui n'est pas bien décrit par un modèle de mélange gaussien. En ajustant le paramètre `weight_concentration_prior`, du [**`BayesianGaussianMixture`**](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.BayesianGaussianMixture.html), cela contrôle le nombre de composantes utilisées pour ajuster ces données. Nous présentons également sur les deux derniers tracés un échantillonnage aléatoire généré à partir des deux mélanges résultants.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_gmm_sin_001.png\"\n",
    "    alt=\"Ajustement de la méthode variationnelle\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Ellipses de Confiance du modèle de mélange gaussien**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_1_mixture/plot_gmm.ipynb)<br/>([_Gaussian Mixture Model Ellipsoids_](https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm.html))\n",
    "\n",
    "Exemple sur le tracé des ellipses de confiance à la fois pour [**`GaussianMixture`**](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) et [**`BayesianGaussianMixture`**](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.BayesianGaussianMixture.html).\n",
    "\n",
    "##### [**Courbe en Sine d'un modèle de mélange gaussien**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_1_mixture/plot_gmm_sin.ipynb)<br/>([_Gaussian Mixture Model Sine Curve_](https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_sin.html))\n",
    "\n",
    "Montre comment utiliser [**`GaussianMixture`**](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) et [**`BayesianGaussianMixture`**](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.BayesianGaussianMixture.html) pour ajuster une courbe en sine.\n",
    "\n",
    "##### [**Analyse du type de priorité de concentration du modèle de mélange gaussien bayésien variationnel**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_1_mixture/plot_concentration_prior.ipynb)<br/>([_Concentration Prior Type Analysis of Variation Bayesian Gaussian Mixture_](https://scikit-learn.org/stable/auto_examples/mixture/plot_concentration_prior.html))\n",
    "\n",
    "Exemple tracé des ellipses de confiance pour [**`BayesianGaussianMixture`**](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.BayesianGaussianMixture.html) avec différents `weight_concentration_prior_type` pour différentes valeurs du paramètre `weight_concentration_prior`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='estimation-algorithm-variational-inference'></a> 2.1.2.2. Avantages et inconvénients de l'inférence variationnelle avec [**`BayesianGaussianMixture`**](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.BayesianGaussianMixture.html)\n",
    "\n",
    "#### Avantages\n",
    "\n",
    "- **Sélection automatique :** lorsque `weight_concentration_prior` est suffisamment petit et que `n_components` est plus grand que ce que le modèle juge nécessaire, le modèle de mélange bayésien variationnel a tendance à fixer certaines valeurs de poids du mélange très près de zéro. Cela permet de laisser le modèle choisir automatiquement un nombre approprié de composantes effectives. Seule une borne supérieure de ce nombre doit être fournie. Notez cependant que le nombre \"idéal\" de composantes actives est très spécifique à l'application et est généralement mal défini dans un contexte d'exploration de données.\n",
    "- **Moins de sensibilité au nombre de paramètres :** contrairement aux modèles finis, qui utiliseront presque toujours toutes les composantes autant que possible et produiront donc des solutions très différentes pour différents nombres de composantes, l'inférence variationnelle avec une priorité de processus de Dirichlet (`weight_concentration_prior_type='dirichlet_process'`) ne changera pas beaucoup avec les modifications des paramètres, conduisant à plus de stabilité et moins d'ajustement.\n",
    "- **Régularisation :** en raison de l'incorporation d'informations antérieures, les solutions variationnelles ont moins de cas particuliers pathologiques que les solutions de l'espérance-maximisation.\n",
    "\n",
    "#### Inconvénients\n",
    "\n",
    "- **Vitesse :** la paramétrisation supplémentaire nécessaire à l'inférence variationnelle rend l'inférence plus lente, bien que cela ne soit pas très significatif.\n",
    "- **Hyperparamètres :** cet algorithme nécessite un hyperparamètre supplémentaire qui pourrait nécessiter un ajustement expérimental par validation croisée.\n",
    "- **Biais :** il y a de nombreux biais implicites dans les algorithmes d'inférence (et aussi dans le processus de Dirichlet s'il est utilisé), et chaque fois qu'il y a un désaccord entre ces biais et les données, il pourrait être possible d'ajuster de meilleurs modèles en utilisant un mélange fini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='the-dirichlet-process'></a> 2.1.2.3. Le processus de Dirichlet\n",
    "\n",
    "Ici, nous décrivons les algorithmes d'inférence variationnelle sur le mélange de processus de Dirichlet. Le processus de Dirichlet est une distribution de probabilité a priori sur les _groupements avec un nombre infini et non borné de partitions_. Les techniques variationnelles nous permettent d'incorporer cette structure a priori sur les modèles de mélange gaussien sans presque aucune pénalité en termes de temps d'inférence, par rapport à un modèle de mélange gaussien fini.\n",
    "\n",
    "Une question importante est de savoir comment le processus de Dirichlet peut utiliser un nombre infini et non borné de groupes tout en restant cohérent. Bien qu'une explication complète ne puisse pas être donnée ici, on peut penser à son analogie avec le [**processus de cassure de bâton**](https://en.wikipedia.org/wiki/Dirichlet_process#The_stick-breaking_process) pour aider à le comprendre. Le processus de cassure de bâton est une histoire générative pour le processus de Dirichlet. Nous commençons avec un bâton de longueur unitaire et à chaque étape, nous cassons une partie du bâton restant. À chaque fois, nous associons la longueur de la partie du bâton à la proportion de points qui tombent dans un groupe du mélange. À la fin, pour représenter le mélange infini, nous associons la dernière partie restante du bâton à la proportion de points qui ne tombent dans aucun des autres groupes. La longueur de chaque partie est une variable aléatoire avec une probabilité proportionnelle au paramètre de concentration. Les valeurs plus petites du paramètre de concentration diviseront la longueur unitaire en parties plus grandes du bâton (définissant une distribution plus concentrée). Les valeurs de concentration plus grandes créeront des parties plus petites du bâton (augmentant le nombre de composantes avec des poids non nuls).\n",
    "\n",
    "Les techniques d'inférence variationnelle pour le processus de Dirichlet fonctionnent toujours avec une approximation finie de ce modèle de mélange infini, mais au lieu de devoir spécifier a priori combien de composantes on veut utiliser, on spécifie simplement le paramètre de concentration et une borne supérieure pour le nombre de composantes du mélange (cette borne supérieure, en supposant qu'elle soit plus élevée que le nombre \"réel\" de composantes, n'affecte que la complexité algorithmique, pas le nombre réel de composantes utilisées)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
