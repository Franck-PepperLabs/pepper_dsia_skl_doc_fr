{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='unsupervised-learning'></a> 2. [**Apprentissage non supervisé**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_unsupervised_learning.ipynb#model-selection-and-evaluation)</br>([*Unsupervised learning*](https://scikit-learn.org/stable/unsupervised_learning.html#unsupervised-learning))\n",
    "\n",
    "# 2.1. [**Modèles de mélange gaussien**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_1_mixture.ipynb#gaussian-mixture-models)<br/>([_Gaussian mixture models_](https://scikit-learn.org/stable/modules/mixture.html#gaussian-mixture-models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 11 pages, 7 exemples, 0 papiers\n",
    "- 2.1.1. [**Mélange gaussien**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_1_mixture.ipynb#gaussian-mixture)<br/>([_Gaussian Mixture_](https://scikit-learn.org/stable/modules/mixture.html#gaussian-mixture))\n",
    "- 2.1.2. [**Mélange gaussien bayésien variationnel**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_1_mixture.ipynb#variational-bayesian-gaussian-mixture)<br/>([_Variational Bayesian Gaussian Mixture_](https://scikit-learn.org/stable/modules/mixture.html#variational-bayesian-gaussian-mixture))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='gaussian-mixture-models'></a> 2.1. Modèles de mélange gaussien\n",
    "\n",
    "[**`sklearn.mixture`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.mixture) est un module qui permet d'apprendre des [**modèles de mélange gaussiens**](https://fr.wikipedia.org/wiki/Modèle_de_mélange_gaussien) (matrices de covariance diagonales, sphériques, liées et complètes prises en charge), de les échantillonner et de les estimer à partir des données. Des méthodes permettant de déterminer le nombre approprié de composants sont également fournies.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_gmm_pdf_001.png\"\n",
    "    alt=\"Modèle de mélange gaussien à deux composants\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "    <p><b>Modèle de mélange gaussien à deux composants</b>: <i>points de données et surfaces d'équi-probabilité du modèle.</i></p>\n",
    "</div>\n",
    "\n",
    "Un modèle de mélange gaussien est un modèle probabiliste qui suppose que tous les points de données sont générés à partir d'un mélange d'un nombre fini de distributions gaussiennes avec des paramètres inconnus. On peut considérer les modèles de mélange comme une généralisation de l'algorithme de regroupement k-moyennes, intégrant des informations sur la structure de covariance des données ainsi que sur les centres des gaussiennes latentes.\n",
    "\n",
    "Scikit-learn implémente différentes classes pour estimer les modèles de mélange gaussiens, qui correspondent à différentes stratégies d'estimation, détaillées ci-dessous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='gaussian-mixture-models'></a> 2.1.1. Mélange gaussien\n",
    "\n",
    "L'objet [**`GaussianMixture`**](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) implémente l'algorithme de [**maximisation des attentes** (2.1.1.3)](https://scikit-learn.org/stable/modules/mixture.html#expectation-maximization) (EM) pour ajuster les modèles de mélange gaussiens. Il peut également dessiner des ellipsoïdes de confiance pour les modèles multivariés et calculer le critère d'information bayésien pour évaluer le nombre de grappes dans les données. Une méthode [**`GaussianMixture.fit`**](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) est fournie qui apprend un modèle de mélange gaussien à partir de données d'entraînement. Compte tenu des données de test, il peut attribuer à chaque échantillon le gaussien auquel il appartient le plus probablement en utilisant la méthode [**`GaussianMixture.predict`**](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html).\n",
    "\n",
    "[**`GaussianMixture`**](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) vient avec différentes options pour contraindre la covariance des classes de différence estimées : covariance sphérique, diagonale, liée ou complète.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_gmm_covariances_001.png\"\n",
    "    alt=\"Covariances des mélanges gaussiens\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "    <p>Covariances des mélanges gaussiens</p>\n",
    "</div>\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Covariances des GMM**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_1_mixture/plot_gmm_covariances.ipynb)<br/>([_GMM covariances_](https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_covariances.html))\n",
    "\n",
    "Exemple d'utilisation du mélange gaussien comme méthode de regroupement sur l'ensemble de données Iris.\n",
    "\n",
    "##### [**Estimation de densité pour un mélange gaussien**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_1_mixture/plot_gmm_pdf.ipynb)<br/>([_Density Estimation for a Gaussian mixture_](https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_pdf.html))\n",
    "\n",
    "Exemple de tracé de l'estimation de densité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='pros-and-cons-of-class-gaussianmixture'></a> 2.1.1.1. Avantages et inconvénients de la classe [**`GaussianMixture`**](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)\n",
    "\n",
    "#### Avantages\n",
    "\n",
    "- **Rapidité :** C'est l'algorithme le plus rapide pour apprendre des modèles de mélange.\n",
    "- **Agnostique :** Comme cet algorithme maximise uniquement la vraisemblance, il ne biaisera pas les moyennes vers zéro, ni ne biaisera les tailles de grappes pour avoir des structures spécifiques qui pourraient ou pourraient ne pas s'appliquer.\n",
    "\n",
    "#### Inconvénients\n",
    "\n",
    "- **Singularités :** Lorsqu'on a trop peu de points par mélange, l'estimation des matrices de covariance devient difficile, et l'algorithme est connu pour diverger et trouver des solutions avec une vraisemblance infinie, sauf si on régularise artificiellement les covariances.\n",
    "- **Nombre de composants :** Cet algorithme utilisera toujours tous les composants auxquels il a accès, nécessitant des données de réserve ou des critères théoriques d'information pour décider du nombre de composants à utiliser en l'absence d'indices externes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='selecting-the-number-of-components-in-a-classical-gaussian-mixture-model'></a> 2.1.1.2. Sélection du nombre de composants dans un modèle de mélange gaussien classique\n",
    "\n",
    "Le critère BIC peut être utilisé pour sélectionner de manière efficace le nombre de composants dans un modèle de mélange gaussien. En théorie, il récupère le véritable nombre de composants uniquement dans le régime asymptotique (c'est-à-dire si beaucoup de données sont disponibles et en supposant que les données ont été réellement générées de manière i.i.d. à partir d'un mélange de distributions gaussiennes). Notez que l'utilisation d'un [**mélange gaussien bayésien variationnel** (2.1.2)](https://scikit-learn.org/stable/modules/mixture.html#bgmm) évite la spécification du nombre de composants pour un modèle de mélange gaussien.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_gmm_selection_002.png\"\n",
    "    alt=\"Sélection de modèle de mélange gaussien\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "    <p>Sélection de modèle de mélange gaussien</p>\n",
    "</div>\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Sélection de modèle de mélange gaussien**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_1_mixture/plot_gmm_selection.ipynb)<br/>([_Gaussian Mixture Model Selection_](https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_selection.html))\n",
    "\n",
    "Exemple de sélection de modèle effectuée avec un mélange gaussien classique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='estimation-algorithm-expectation-maximization'></a> 2.1.1.3. Algorithme d'estimation Expectation-Maximization\n",
    "\n",
    "La principale difficulté dans l'apprentissage des modèles de mélange gaussien à partir de données non étiquetées est que l'on ne sait généralement pas quels points proviennent de quel composant latent (si l'on a accès à ces informations, il devient très facile d'ajuster une distribution gaussienne distincte à chaque ensemble de points). L'[**algorithme espérance-maximisation**](https://fr.wikipedia.org/wiki/Algorithme_esp%C3%A9rance-maximisation) est un algorithme statistique bien fondé pour contourner ce problème par un processus itératif. Tout d'abord, on suppose des composants aléatoires (centrés de manière aléatoire sur les points de données, appris à partir des k-moyennes, ou même simplement distribués normalement autour de l'origine) et on calcule pour chaque point une probabilité d'avoir été généré par chaque composant du modèle. Ensuite, on ajuste les paramètres pour maximiser la probabilité des données étant donné ces affectations. Répéter ce processus garantit toujours de converger vers un optimum local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='choice-of-the-initialization-method'></a> 2.1.1.4. Choix de la méthode d'initialisation\n",
    "\n",
    "Il existe quatre méthodes d'initialisation au choix (ainsi que la possibilité de fournir des moyennes initiales définies par l'utilisateur) pour générer les centres initiaux des composants du modèle :\n",
    "\n",
    "- **k-means (par défaut)** : Cette méthode applique un algorithme de regroupement k-moyennes traditionnel. Cela peut être plus coûteux en temps de calcul par rapport aux autres méthodes d'initialisation.\n",
    "- **k-means++** : Cette méthode utilise la méthode d'initialisation de k-moyennes : k-means++. Elle choisit le premier centre au hasard à partir des données. Les centres suivants seront choisis à partir d'une distribution pondérée des données favorisant les points plus éloignés des centres existants. k-means++ est l'initialisation par défaut pour les k-moyennes, il sera donc plus rapide que l'exécution d'un k-moyennes complet, mais cela peut encore prendre un temps significatif pour les grands ensembles de données avec de nombreux composants.\n",
    "- **random_from_data** : Cette méthode choisit des points de données aléatoires à partir des données d'entrée comme centres initiaux. C'est une méthode d'initialisation très rapide, mais elle peut produire des résultats non convergents si les points choisis sont trop proches les uns des autres.\n",
    "- **random** : Les centres sont choisis comme une petite perturbation par rapport à la moyenne de toutes les données. Cette méthode est simple, mais elle peut entraîner un temps de convergence plus long du modèle.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_gmm_init_001.png\"\n",
    "    alt=\"Méthodes d'initialisation du GMM\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "    <p>Méthodes d'initialisation du GMM</p>\n",
    "</div>\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Méthodes d'initialisation du GMM**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_1_mixture/plot_gmm_init.ipynb)<br/>([_GMM Initialization Methods_](https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_init.html))\n",
    "\n",
    "Exemple de l'utilisation de différentes initialisations dans un modèle de mélange gaussien.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
