{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='unsupervised-learning'></a> 2. [**Apprentissage non supervisé**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_unsupervised_learning.ipynb#model-selection-and-evaluation)</br>([*Unsupervised learning*](https://scikit-learn.org/stable/unsupervised_learning.html#unsupervised-learning))\n",
    "\n",
    "# 2.2. [**Apprentissage des variétés**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_2_manifold.ipynb#manifold-learning)</br>([*Manifold learning*](https://scikit-learn.org/stable/modules/manifold.html#manifold-learning))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 18 pages, 2 exemples, 13 papiers\n",
    "- 2.2.1. [**Introduction**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_unsupervised_learning.ipynb#introduction)<br/>([_Introduction_](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_2_manifold.ipynb#introduction))\n",
    "- 2.2.2. [**Isomap**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_unsupervised_learning.ipynb#isomap)<br/>([_Isomap_](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_2_manifold.ipynb#isomap))\n",
    "- 2.2.3. [**Plongement localement linéaire (LLE)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_unsupervised_learning.ipynb#locally-linear-embedding)<br/>([_Locally Linear Embedding_](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_2_manifold.ipynb#locally-linear-embedding))\n",
    "- 2.2.4. [**Plongement localement linéaire modifié (MLLE)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_unsupervised_learning.ipynb#modified-locally-linear-embedding)<br/>([_Modified Locally Linear Embedding_](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_2_manifold.ipynb#modified-locally-linear-embedding))\n",
    "- 2.2.5. [**Correspondance hessienne de valeurs propres (HLLE)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_unsupervised_learning.ipynb#hessian-eigenmapping)<br/>([_Hessian Eigenmapping_](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_2_manifold.ipynb#hessian-eigenmapping))\n",
    "- 2.2.6. [**Plongement spectral**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_unsupervised_learning.ipynb#spectral-embedding)<br/>([_Spectral Embedding_](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_2_manifold.ipynb#spectral-embedding))\n",
    "- 2.2.7. [**Alignement local dans l'espace tangent (LTSA)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_unsupervised_learning.ipynb#local-tangent-space-alignment)<br/>([_Local Tangent Space Alignment_](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_2_manifold.ipynb#local-tangent-space-alignment))\n",
    "- 2.2.8. [**Positionnement multidimensionnel (MDS)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_unsupervised_learning.ipynb#multi-dimensional-scaling-mds)<br/>([_Multi-dimensional Scaling (MDS)_](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_2_manifold.ipynb#multi-dimensional-scaling-mds))\n",
    "- 2.2.9. [**Plongement stochastique des voisins distribués en t (t-SNE)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_unsupervised_learning.ipynb#t-distributed-stochastic-neighbor-embedding-t-sne)<br/>([_t-distributed Stochastic Neighbor Embedding (t-SNE)_](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_2_manifold.ipynb#t-distributed-stochastic-neighbor-embedding-t-sne))\n",
    "- 2.2.10. [**Conseils pratiques**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_unsupervised_learning.ipynb#tips-on-practical-use)<br/>([_Tips on practical use_](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_2_manifold.ipynb#tips-on-practical-use))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='manifold-learning'></a> 2.2. Apprentissage des variétés\n",
    "\n",
    "<i>Cherchez l'essentiel<br/>\n",
    "Les simples choses essentielles<br/>\n",
    "Oubliez vos soucis et vos tracas<br/>\n",
    "Je veux dire l'essentiel<br/>\n",
    "Les recettes de Mère Nature<br/>\n",
    "Celles qui apportent les choses essentielles de la vie<br/>\n",
    "<br/>\n",
    "Chanson de Baloo [Le livre de la jungle]</i>\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_compare_methods_001.png\"\n",
    "    alt=\"Original S-curve samples\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_compare_methods_003.png\"\n",
    "    alt=\"Isomap Embedding\"\n",
    "    style=\"width: auto; height: 220px;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_compare_methods_004.png\"\n",
    "    alt=\"Multidimensional scaling\"\n",
    "    style=\"width: auto; height: 220px;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_compare_methods_005.png\"\n",
    "    alt=\"Spectral Embedding\"\n",
    "    style=\"width: auto; height: 220px;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_compare_methods_006.png\"\n",
    "    alt=\"t-SNE\"\n",
    "    style=\"width: auto; height: 220px;\"/>\n",
    "</div>\n",
    "\n",
    "L'apprentissage des variétés est une approche non linéaire de réduction de la dimensionnalité. Les algorithmes pour cette tâche sont basés sur l'idée que la dimensionnalité de nombreux ensembles de données est artificiellement élevée."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='introduction'></a> 2.2.1. Introduction\n",
    "\n",
    "Les ensembles de données de grande dimension peuvent être très difficiles à visualiser. Bien que les données en deux ou trois dimensions puissent être représentées graphiquement pour montrer la structure inhérente des données, les graphiques équivalents en haute dimension sont beaucoup moins intuitifs. Pour faciliter la visualisation de la structure d'un ensemble de données, la dimension doit être réduite d'une manière ou d'une autre.\n",
    "\n",
    "La manière la plus simple de réaliser cette réduction de dimensionnalité est de prendre une projection aléatoire des données. Bien que cela permette une certaine visualisation de la structure des données, le caractère aléatoire du choix laisse beaucoup à désirer. Dans une projection aléatoire, il est probable que la structure la plus intéressante des données sera perdue.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "<img\n",
    "  src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_001.png\"\n",
    "  alt=\"Handwritten Digits\"\n",
    "  style=\"width: auto; height: 240px;\"/>\n",
    "<img\n",
    "  src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_002.png\"\n",
    "  alt=\"Random Projection Embedding\"\n",
    "  style=\"width: auto; height: 240px;\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Pour répondre à cette préoccupation, plusieurs cadres de réduction de la dimensionnalité linéaires supervisés et non supervisés ont été conçus, tels que l'analyse en composantes principales (PCA), l'analyse en composantes indépendantes (ICA), l'analyse discriminante linéaire (LDA) et d'autres encore. Ces algorithmes définissent des rubriques spécifiques pour choisir une projection linéaire \"intéressante\" des données. Si ces méthodes peuvent être puissantes, elles passent souvent à côté d'importantes structures non linéaires dans les données.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "<img\n",
    "  src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_003.png\"\n",
    "  alt=\"Truncated SVD Embedding\"\n",
    "  style=\"width: auto; height: 240px;\"/>\n",
    "<img\n",
    "  src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_003.png\"\n",
    "  alt=\"LDA Embedding\"\n",
    "  style=\"width: auto; height: 240px;\"/>\n",
    "</div>\n",
    "\n",
    "L'apprentissage des variétés (Manifold Learning) peut être considéré comme une tentative de généraliser les cadres linéaires tels que PCA pour être sensibles à la structure non linéaire des données. Bien que des variantes supervisées existent, le problème d'apprentissage de variété typique est non supervisé : il apprend la structure multidimensionnelle des données à partir des données elles-mêmes, sans l'utilisation de classifications prédéterminées.\n",
    "\n",
    "Les implémentations d'apprentissage de variété disponibles dans scikit-learn sont résumées ci-dessous.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Apprentissage des variétés sur des chiffres manuscrits : Plongement localement linéaire, Isomap…**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_2_manifold/plot_lle_digits.ipynb)<br/>([*Manifold learning on handwritten digits: Locally Linear Embedding, Isomap…*](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html))\n",
    "\n",
    "Exemple de réduction de dimensionnalité sur des chiffres manuscrits.\n",
    "\n",
    "#### [**Comparaison des méthodes d'apprentissage de variétés**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_2_manifold/plot_compare_methods.ipynb)<br/>([*Comparison of Manifold Learning methods*](https://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html))\n",
    "\n",
    "Exemple de réduction de dimensionnalité sur un jeu de données de \"S-curve\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='isomap'></a> 2.2.2. Isomap\n",
    "\n",
    "L'une des premières approches de l'apprentissage des variétés est l'algorithme Isomap, abréviation de Isometric Mapping (cartographie isométrique). Isomap peut être considéré comme une extension de la mise à l'échelle multidimensionnelle (MDS) ou de l'ACP à noyau. Isomap cherche un plongement de dimension inférieure qui conserve les distances géodésiques entre tous les points. Isomap peut être réalisé à l'aide de l'objet [**`Isomap`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html).\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "<img\n",
    "  src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_005.png\"\n",
    "  alt=\"Isomap\"\n",
    "  style=\"width: auto; height: 240px;\"/>\n",
    "</div>\n",
    "\n",
    "### 2.2.2.1. Complexité\n",
    "\n",
    "L'algorithme Isomap se compose de trois étapes :\n",
    "\n",
    "1. **Recherche des voisins les plus proches.** Isomap utilise [**`BallTree`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html) pour une recherche efficace des voisins. Le coût est d'environ $\\mathcal{O}[D \\log(k) N \\log(N)]$, pour les $k$ voisins les plus proches de $N$ points en $D$ dimensions.\n",
    "2. **Recherche en graphe des plus courts chemins.** Les algorithmes les plus efficaces connus pour cela sont l'*algorithme de Dijkstra*, qui est d'environ $\\mathcal{O}[N^2(k + \\log(N))]$, ou l'algorithme de Floyd-Warshall, qui est $\\mathcal{O}[N^3]$. L'algorithme peut être sélectionné par l'utilisateur avec le mot-clé `path_method` d'`Isomap`. S'il n'est pas spécifié, le code tente de choisir le meilleur algorithme en fonction des données d'entrée.\n",
    "3. **Décomposition partielle des valeurs propres.** Le plongement des données est encodé dans les vecteurs propres correspondant aux $d$ plus grandes valeurs propres du noyau isomap $N \\times N$. Pour un solveur dense, le coût est d'environ $\\mathcal{O}[d N^2]$. Ce coût peut souvent être amélioré en utilisant le solveur `ARPACK`. Le solveur de valeurs propres peut être spécifié par l'utilisateur avec le mot-clé `eigen_solver` d'`Isomap`. S'il n'est pas spécifié, le code tente de choisir le meilleur algorithme pour les données d'entrée.\n",
    "\n",
    "La complexité globale d'Isomap est $\\mathcal{O}[D \\log(k) N \\log(N)] + \\mathcal{O}[N^2(k + \\log(N))] + \\mathcal{O}[d N^2]$.\n",
    "\n",
    "* $N$ : nombre de points de données d'entraînement\n",
    "* $D$ : dimension d'entrée\n",
    "* $k$ : nombre de voisins les plus proches\n",
    "* $d$ : dimension de sortie\n",
    "\n",
    "#### Références\n",
    "\n",
    "🔬 [**“A Global Geometric Framework for Nonlinear Dimensionality Reduction”**](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=9c2c88edc29c02c71064db3a77e89399f9e197c1) Tenenbaum, J.B.; De Silva, V.; & Langford, J.C. Science 290 (5500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='locally-linear-embedding'></a> 2.2.3. Plongement localement linéaire (LLE)\n",
    "\n",
    "Le plongement localement linéaire (LLE) cherche une projection des données de dimension inférieure qui préserve les distances au sein des voisinages locaux. On peut le considérer comme une série d'analyses en composantes principales locales qui sont comparées globalement pour trouver le meilleur plongement non linéaire.\n",
    "\n",
    "Le plongement localement linéaire peut être effectuée avec la fonction [**`locally_linear_embedding`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.locally_linear_embedding.html) ou son homologue orienté objet [**`LocallyLinearEmbedding`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html).\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "<img\n",
    "  src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_006.png\"\n",
    "  alt=\"LLE\"\n",
    "  style=\"width: auto; height: 240px;\"/>\n",
    "</div>\n",
    "\n",
    "### 2.2.3.1. Complexité\n",
    "\n",
    "L'algorithme LLE standard comporte trois étapes :\n",
    "1. **Recherche des plus proches voisins.** Voir la discussion sous Isomap ci-dessus.\n",
    "2. **Construction de la matrice de pondération.** $\\mathcal{O}[D N k^3]$. La construction de la matrice de pondération LLE implique la solution d'une équation linéaire $k \\times $ pour chacun des $N$ voisinages locaux.\n",
    "3. **Décomposition partielle des valeurs propres.** Voir la discussion sous Isomap ci-dessus.\n",
    "\n",
    "La complexité globale de la LLE standard est $\\mathcal{O}[D \\log(k) N \\log(N)] + \\mathcal{O}[D N k^3] + \\mathcal{O}[d N^2]$.\n",
    "* $N$ : nombre de points de données d'entraînement\n",
    "* $D$ : dimension d'entrée\n",
    "* $k$ : nombre de plus proches voisins\n",
    "* $d$ : dimension de sortie\n",
    "\n",
    "#### Références\n",
    "\n",
    "🔬 [**“Nonlinear dimensionality reduction by locally linear embedding”**](https://www.science.org/doi/10.1126/science.290.5500.2323) Roweis, S. & Saul, L. Science 290:2323 (2000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='modified-locally-linear-embedding'></a> 2.2.4. Plongement localement linéaire modifiée (MLLE)\n",
    "\n",
    "Un problème bien connu avec LLE est le problème de régularisation. Lorsque le nombre de voisins est supérieur au nombre de dimensions d'entrée, la matrice définissant chaque voisinage local a un rang déficient. Pour résoudre ce problème, le LLE standard applique un paramètre de régularisation arbitraire $r$, qui est choisi par rapport à la trace de la matrice de pondération locale. Bien qu'il puisse être démontré formellement qu'à mesure que $r \\to 0$, la solution converge vers le plongement souhaitée, il n'y a aucune garantie que la solution optimale sera trouvée pour $r > 0$. Ce problème se manifeste dans les plongements qui déforment la géométrie sous-jacente de la variété.\n",
    "\n",
    "Une méthode pour résoudre le problème de régularisation consiste à utiliser plusieurs vecteurs de poids dans chaque voisinage. C'est l'essence du *plongement localement linéaire modifiée* (MLLE). MLLE peut être effectué avec la fonction [**`locally_linear_embedding`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html) ou son homologue orienté objet [**`LocallyLinearEmbedding`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html), avec le mot-clé  `method='modified'`. Cela nécessite `n_neighbors > n_components`.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "<img\n",
    "  src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_007.png\"\n",
    "  alt=\"MLLE\"\n",
    "  style=\"width: auto; height: 240px;\"/>\n",
    "</div>\n",
    "\n",
    "### 2.2.4.1. Complexité\n",
    "\n",
    "L'algorithme MLLE comprend trois étapes :\n",
    "1. **Recherche des plus proches voisins.** Identique à celle de la LLE standard.\n",
    "2. **Construction de la matrice de pondération.** Environ $\\mathcal{O}[D N k^3] + \\mathcal{O}[N (k-D) k^2]$. Le premier terme est exactement équivalent à celui du LLE standard. Le deuxième terme a à voir avec la construction de la matrice de pondération à partir de plusieurs poids. En pratique, le coût supplémentaire de la construction de la matrice de pondération du MLLE est négligeable par rapport au coût des étapes 1 et 3.\n",
    "3. **Décomposition partielle des valeurs propres.** Identique à celle du LLE standard.\n",
    "\n",
    "La complexité globale de MLLE est $\\mathcal{O}[D \\log(k) N \\log(N)] + \\mathcal{O}[D N k^3] + \\mathcal{O}[N (k-D) k^2] + \\mathcal{O}[d N^2]$.\n",
    "* $N$ : nombre de points de données d'entraînement\n",
    "* $D$ : dimension d'entrée\n",
    "* $k$ : nombre de plus proches voisins\n",
    "* $d$ : dimension de sortie\n",
    "\n",
    "#### Références\n",
    "\n",
    "Z 🔬 [**“MLLE: Modified Locally Linear Embedding Using Multiple Weights”**](https://proceedings.neurips.cc/paper_files/paper/2006/file/fb2606a5068901da92473666256e6e5b-Paper.pdf) Zhang, Z. & Wang, J."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='hessian-eigenmapping'></a> 2.2.5. Correspondance hessienne de valeurs propres (HLLE)\n",
    "\n",
    "La correspondance hessienne de valeurs propres (également connue sous le nom de LLE basée sur la hessienne : HLLE) est une autre méthode de résolution du problème de régularisation de LLE. Elle repose sur une forme quadratique basée sur la hessienne dans chaque voisinage, qui est utilisée pour récupérer la structure localement linéaire. Bien que d'autres implémentations notent sa faible mise à l'échelle avec la taille des données, scikit-learn met en œuvre certaines améliorations algorithmiques qui rendent son coût comparable à celui d'autres variantes de LLE pour une petite dimension de sortie. Le HLLE peut être effectué avec la fonction [**`locally_linear_embedding`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.locally_linear_embedding.html) ou sa version orientée objet [**`LocallyLinearEmbedding`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html), avec le mot-clé `method='hessian'`. Il nécessite `n_neighbors > n_components * (n_components + 3) / 2`.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "<img\n",
    "  src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_008.png\"\n",
    "  alt=\"HLLE\"\n",
    "  style=\"width: auto; height: 240px;\"/>\n",
    "</div>\n",
    "\n",
    "### 2.2.5.1. Complexité\n",
    "\n",
    "L'algorithme HLLE comprend trois étapes :\n",
    "1. **Recherche des plus proches voisins.** Identique à celle du LLE standard.\n",
    "2. **Construction de la matrice de poids.** Environ $\\mathcal{O}[D N k^3] + \\mathcal{O}[N d^6]$. Le premier terme est exactement équivalent à celui du LLE standard. Le deuxième terme provient d'une décomposition QR de l'estimateur hessien local.\n",
    "3. **Décomposition partielle des valeurs propres.** Identique à celle du LLE standard.\n",
    "\n",
    "La complexité globale d HLLE est $\\mathcal{O}[D \\log(k) N \\log(N)] + \\mathcal{O}[D N k^3] + \\mathcal{O}[N d^6] + \\mathcal{O}[d N^2]$.\n",
    "* $N$ : nombre de points de données d'entraînement\n",
    "* $D$ : dimension d'entrée\n",
    "* $k$ : nombre de plus proches voisins\n",
    "* $d$ : dimension de sortie\n",
    "\n",
    "#### Références\n",
    "\n",
    "🔬 [**“Hessian Eigenmaps: Locally linear embedding techniques for high-dimensional data”**](https://www.pnas.org/doi/10.1073/pnas.1031596100) Donoho, D. & Grimes, C. Proc Natl Acad Sci USA. 100:5591 (2003)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='spectral-embedding'></a> 2.2.6. Plongement spectral\n",
    "\n",
    "Le plongement spectral est une approche de calcul d'un plongement non linéaire. Scikit-learn implémente Laplacian Eigenmaps, qui trouve une représentation de basse dimension des données en utilisant une décomposition spectrale du Laplacien du graphe. Le graphe généré peut être considéré comme une approximation discrète de la variété de basse dimension dans l'espace de haute dimension. La minimisation d'une fonction de coût basée sur le graphe garantit que les points proches les uns des autres sur la variété sont projetés à proximité les uns des autres dans l'espace de basse dimension, préservant ainsi les distances locales. Le plongement spectral peut être effectué avec la fonction [**`spectral_embedding`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.spectral_embedding.html) ou son homologue orienté objet [**`SpectralEmbedding`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.SpectralEmbedding.html).\n",
    "\n",
    "### 2.2.6.1. Complexité\n",
    "\n",
    "L'algorithme de plongement spectral (Laplacian Eigenmaps) comprend trois étapes :\n",
    "* **Construction du graphe pondéré.** Transformer les données brutes d'entrée en une représentation de graphe à l'aide d'une matrice d'affinité (d'adjacence).\n",
    "* **Construction du Laplacien du graphe.** Le Laplacien du graphe non normalisé est construit comme $L = D - A$, et le Laplacien normalisé comme $L = D^{-\\frac{1}{2}} (D - A) D^{-\\frac{1}{2}}$.\n",
    "* **Décomposition partielle des valeurs propres.** Une décomposition des valeurs propres est effectuée sur le Laplacien du graphe.\n",
    "\n",
    "La complexité globale de l'intégration spectrale est $\\mathcal{O}[D \\log(k) N \\log(N)] + \\mathcal{O}[D N k^3] + \\mathcal{O}[d N^2]$.\n",
    "* $N$ : nombre de points de données d'entraînement\n",
    "* $D$ : dimension d'entrée\n",
    "* $k$ : nombre de plus proches voisins\n",
    "* $d$ : dimension de sortie\n",
    "\n",
    "#### Références\n",
    "\n",
    "🔬 [**“Laplacian Eigenmaps for Dimensionality Reduction and Data Representation”**](https://web.cse.ohio-state.edu/~belkin.8/papers/LEM_NC_03.pdf) M. Belkin, P. Niyogi, Neural Computation, June 2003; 15 (6):1373-1396"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='local-tangent-space-alignment'></a> 2.2.7. Alignement local dans l'espace tangent (LTSA)\n",
    "\n",
    "Bien que techniquement pas une variante de LLE, l'alignement local dans l'espace tangent (LTSA) est suffisamment similaire sur le plan algorithmique à LLE pour qu'il puisse être inclus dans cette catégorie. Au lieu de se concentrer sur la préservation des distances entre les voisins comme dans LLE, LTSA cherche à caractériser la géométrie locale à chaque voisinage via son espace tangent, et effectue une optimisation globale pour aligner ces espaces tangents locaux afin d'apprendre l'incorporation. LTSA peut être effectué avec la fonction [**`locally_linear_embedding`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.locally_linear_embedding.html) ou son homologue orienté objet [**`LocallyLinearEmbedding`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html), avec le mot clé `method = 'ltsa'`.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "<img\n",
    "  src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_009.png\"\n",
    "  alt=\"LTSA\"\n",
    "  style=\"width: auto; height: 240px;\"/>\n",
    "</div>\n",
    "\n",
    "### 2.2.7.1. Complexité\n",
    "\n",
    "L'algorithme LTSA comprend trois étapes :\n",
    "* **Recherche des voisins les plus proches.** Identique à LLE standard.\n",
    "* **Construction de la matrice de pondération.** Environ $\\mathcal{O}[D N k^3] + \\mathcal{O}[k^2 d]$. Le premier terme reflète un coût similaire à celui de LLE standard.\n",
    "* **Décomposition partielle des valeurs propres.** Identique à LLE standard.\n",
    "\n",
    "La complexité globale du LTSA standard est $\\mathcal{O}[D \\log(k) N \\log(N)] + \\mathcal{O}[D N k^3] + \\mathcal{O}[k^2 d] + \\mathcal{O}[d N^2]$.\n",
    "* $N$ : nombre de points de données d'entraînement\n",
    "* $D$ : dimension d'entrée\n",
    "* $k$ : nombre de plus proches voisins\n",
    "* $d$ : dimension de sortie\n",
    "\n",
    "#### Références\n",
    "\n",
    "🔬 [“**Principal Manifolds and Nonlinear Dimension Reduction via Local Tangent Space Alignment”**](https://arxiv.org/abs/cs/0212008) Zhang, Z. & Zha, H. Journal of Shanghai Univ. 8:406 (2004)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='multi-dimensional-scaling-mds'></a> 2.2.8. Positionnement multidimensionnel (MDS)\n",
    "\n",
    "Le [**positionnement multidimensionnel**](https://en.wikipedia.org/wiki/Multidimensional_scaling) (*Multi-dimensional Scaling (MDS)* en anglais) implémentée par la classe [**`MDS`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html) vise à trouver une représentation en basse dimension des données dans laquelle les distances respectent bien les distances de l'espace d'origine de haute dimension.\n",
    "\n",
    "En général, [**`MDS`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html) est une technique utilisée pour analyser des données de similarité ou de dissimilarité. Elle tente de modéliser ces données comme des distances dans un espace géométrique. Les données peuvent être des évaluations de similarité entre des objets, des fréquences d'interaction entre des molécules ou des indices commerciaux entre des pays.\n",
    "\n",
    "Il existe deux types d'algorithmes MDS : métriques et non métriques. Dans scikit-learn, la classe [**`MDS`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html) implémente les deux. Dans MDS métrique, la matrice de similarité d'entrée provient d'une métrique (et respecte donc l'inégalité triangulaire), les distances entre les deux points de sortie sont alors définies pour être aussi proches que possible des données de similarité ou de dissimilarité. Dans la version non métrique, les algorithmes essaieront de préserver l'ordre des distances et chercheront donc une relation monotone entre les distances dans l'espace intégré et les similarités/dissimilarités.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "<img\n",
    "  src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_010.png\"\n",
    "  alt=\"MDS\"\n",
    "  style=\"width: auto; height: 240px;\"/>\n",
    "</div>\n",
    "\n",
    "Soit $S$ la matrice de similarité et $X$ les coordonnées des $n$ points d'entrée. Les disparités $\\hat{d}_{ij}$ sont des transformations des similarités choisies de manière optimale. L'objectif, appelé stress, est alors défini par $\\sum_{i < j} d_{ij}(X) - \\hat{d}_{ij}(X)$.\n",
    "\n",
    "### <a id='metric-mds'></a> 2.2.8.1. MDS métrique\n",
    "\n",
    "Dans le modèle MDS métrique le plus simple, appelé _MDS absolu_, les disparités sont définies par $\\hat{d}_{ij}=S_{ij}$. Avec le MDS absolu, la valeur $S_{ij}$ devrait alors correspondre exactement à la distance entre les points $i$ et $j$ dans le point plongé.\n",
    "\n",
    "Le plus souvent, les disparités sont fixées à $\\hat{d}_{ij}=bS_{ij}$.\n",
    "\n",
    "### <a id='nonmetric-mds'></a> 2.2.8.2. MDS non métrique\n",
    "\n",
    "Le [**`MDS`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html) non métrique se concentre sur l'ordination des données. Si $S_{ij} > S_{jk}$, alors le plongement doit assurer $d_{ij} <> d_{jk}$. Pour cette raison, nous en parlons en termes de dissimilarités ($\\delta_{ij}$) au lieu de similarités ($S_{ij}$). Notez que les dissimilarités peuvent facilement être obtenues à partir de similarités par une transformation simple, par ex. $\\delta_{ij}=c_1-c_2 S_{ij}$ pour certaines constantes réelles $c_1, c_2$. Un algorithme simple pour pour imposer une ordination correcte consiste à utiliser une régression monotone de $d_{ij}$ sur $\\delta_{ij}$, produisant des disparités $\\hat{d}_{ij}$ dans le même ordre que $\\delta_{ij}$.\n",
    "\n",
    "Une solution triviale à ce problème consiste à définir tous les points sur l'origine. Pour éviter cela, les disparités $\\hat{d}_{ij}$ sont normalisés. Notez que comme nous nous soucions seulement de l'ordre relatif, notre objectif devrait être invariant à une simple translation et mise à l'échelle. Cependant, le stress utilisé dans la MDS métrique est sensible à la mise à l'échelle. Pour remédier à cela, la MDS non métrique peut utiliser un stress normalisé, appelé Stress-1, défini comme\n",
    "\n",
    "$$\n",
    "\\sqrt{\\frac{\\sum_{i < j} (d_{ij} - \\hat{d}_{ij})^2}{\\sum_{i < j} d_{ij}^2}}.\n",
    "$$\n",
    "\n",
    "L'utilisation de Stress-1 normalisé peut être activée en définissant `normalized_stress=True`, mais elle n'est compatible qu'avec le problème MDS non métrique et sera ignorée dans le cas métrique.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "<img\n",
    "  src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_mds_001.png\"\n",
    "  alt=\"Nonmetric MDS\"\n",
    "  style=\"width: auto; height: 240px;\"/>\n",
    "</div>\n",
    "\n",
    "#### Références\n",
    "\n",
    "📚 [150 €] [**“Modern Multidimensional Scaling - Theory and Applications”**](https://link.springer.com/content/pdf/bfm:978-0-387-28981-6/1?pdf=chapter%20toc) Borg, I.; Groenen P. Springer Series in Statistics (1997)\n",
    "\n",
    "🔬 [**“Nonmetric multidimensional scaling: a numerical method”**](http://cda.psych.uiuc.edu/psychometrika_highly_cited_articles/kruskal_1964b.pdf) Kruskal, J. Psychometrika, 29 (1964)\n",
    "\n",
    "🔬 [**“Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis”**](http://cda.psych.uiuc.edu/psychometrika_highly_cited_articles/kruskal_1964a.pdf) Kruskal, J. Psychometrika, 29, (1964)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='t-distributed-stochastic-neighbor-embedding-t-sne'></a> 2.2.9. Plongement stochastique des voisins distribués en t (t-SNE)\n",
    "\n",
    "([**`TSNE`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)) (_T-distributed Stochastic Neighbor Embedding_ en anglais) convertit les affinités des points de données en probabilités. Les affinités dans l'espace d'origine sont représentées par des probabilités conjointes gaussiennes et les affinités dans l'espace de plongement sont représentées par des distributions t de Student. Cela permet à t-SNE d'être particulièrement sensible à la structure locale et présente quelques avantages par rapport aux techniques existantes :\n",
    "* Révéler la structure à de nombreuses échelles sur une seule carte\n",
    "* Révéler les données qui se trouvent dans plusieurs variétés ou clusters différents\n",
    "* Réduire la tendance à regrouper les points au centre\n",
    "\n",
    "Alors qu'Isomap, LLE et ses variantes sont les mieux adaptées pour déployer une seule variété continue de basse dimension, t-SNE se concentrera sur la structure locale des données et aura tendance à extraire des groupes locaux de points regroupés comme cela est mis en évidence sur l'exemple de la courbe en S. Cette capacité à regrouper des échantillons en fonction de la structure locale peut être bénéfique pour démêler visuellement un ensemble de données qui comprend plusieurs variétés simultanément, comme c'est le cas dans le jeu de données des chiffres manuscrits.\n",
    "\n",
    "La divergence de Kullback-Leibler (KL) des probabilités conjointes dans l'espace d'origine et l'espace de plongement sera minimisée par descente de gradient. Notez que la divergence KL n'est pas convexe, c'est-à-dire que plusieurs redémarrages avec des initialisations différentes aboutiront à des minima locaux de la divergence KL. Il est donc parfois utile d'essayer différentes graines et de sélectionner le plongement avec la plus faible divergence KL.\n",
    "\n",
    "Les inconvénients de l'utilisation de t-SNE sont globalement les suivants :\n",
    "* t-SNE est coûteux en termes de calcul et peut prendre plusieurs heures sur des ensembles de données de plusieurs millions d'échantillons, là où PCA se termine en quelques secondes ou minutes.\n",
    "* La méthode de Barnes-Hut t-SNE est limitée aux plongements bidimensionnels ou tridimensionnels.\n",
    "* L'algorithme est stochastique et plusieurs redémarrages avec des graines différentes peuvent donner des plongements différents. Cependant, il est tout à fait légitime de choisir le plongement avec l'erreur la plus faible.\n",
    "* La structure globale n'est pas explicitement préservée. Ce problème est atténué en initialisant les points avec PCA (en utilisant `init='pca'`).\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "<img\n",
    "  src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_013.png\"\n",
    "  alt=\"Nonmetric MDS\"\n",
    "  style=\"width: auto; height: 240px;\"/>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='optimizing-t-sne'></a> 2.2.9.1. Optimisation de t-SNE\n",
    "\n",
    "Le principal objectif de t-SNE est la visualisation de données de haute dimension. Par conséquent, il fonctionne mieux lorsque les données sont intégrées sur deux ou trois dimensions.\n",
    "\n",
    "L'optimisation de la divergence KL peut parfois être un peu délicate. Il existe cinq paramètres qui contrôlent l'optimisation de t-SNE et donc potentiellement la qualité du plongement résultant :\n",
    "\n",
    "- La perplexité\n",
    "- Le facteur d'exagération précoce (_early exaggeration factor_)\n",
    "- Le taux d'apprentissage (_learning rate_)\n",
    "- Le nombre maximum d'itérations\n",
    "- L'angle (non utilisé dans la méthode exacte)\n",
    "\n",
    "La perplexité est définie comme $k = 2^{(S)}$ où $S$ est l'entropie de Shannon de la distribution de probabilité conditionnelle. Des perplexités plus élevées conduisent à plus de voisins proches et sont moins sensibles aux petites structures. À l'inverse, une perplexité plus faible considère un nombre plus petit de voisins, ignorant ainsi plus d'informations globales au profit du voisinage local. À mesure que les tailles des ensembles de données augmentent, plus de points seront nécessaires pour obtenir un échantillon raisonnable du voisinage local, d'où la nécessité de perplexités plus grandes. De même, des ensembles de données plus bruyants nécessiteront des valeurs de perplexité plus élevées pour englober suffisamment de voisins locaux et voir au-delà du bruit de fond.\n",
    "\n",
    "Le nombre maximum d'itérations est généralement assez élevé et n'a pas besoin d'être réglé. L'optimisation se compose de deux phases : la phase d'exagération précoce et l'optimisation finale. Pendant l'exagération précoce, les probabilités conjointes dans l'espace d'origine seront artificiellement augmentées par multiplication par un facteur donné. Des facteurs plus importants entraînent des écarts plus importants entre les amas naturels dans les données. Si le facteur est trop élevé, la divergence KL pourrait augmenter pendant cette phase. En général, il n'est pas nécessaire de le régler. Un paramètre critique est le taux d'apprentissage. S'il est trop faible, la descente de gradient restera bloquée dans un mauvais minimum local. S'il est trop élevé, la divergence KL augmentera pendant l'optimisation. Une heuristique suggérée par Belkina et al. (2019) est de définir le taux d'apprentissage comme étant la taille de l'échantillon divisée par le facteur d'exagération précoce. Nous mettons en œuvre cette heuristique sous forme d'argument learning_rate='auto'. D'autres conseils peuvent être trouvés dans la FAQ de Laurens van der Maaten (voir les références). Le dernier paramètre, l'angle, est un compromis entre les performances et la précision. Des angles plus importants impliquent que nous pouvons approximer de plus grandes régions par un seul point, ce qui accélère la vitesse mais donne des résultats moins précis.\n",
    "\n",
    "[“**How to Use t-SNE Effectively**”](https://distill.pub/2016/misread-tsne/) offre une bonne discussion sur les effets des différents paramètres, ainsi que des graphiques interactifs pour explorer les effets de différents paramètres."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='optimizing-t-sne'></a> 2.2.9.2. Barnes-Hut t-SNE\n",
    "\n",
    "La version de t-SNE de Barnes-Hut implémentée ici est généralement beaucoup plus lente que d'autres algorithmes d'apprentissage de variétés. L'optimisation est assez difficile et le calcul du gradient est en $\\mathcal{O}[d N \\log(N)]$, où $d$ est le nombre de dimensions de sortie et $N$ est le nombre d'échantillons. La méthode Barnes-Hut améliore la méthode exacte, où la complexité de t-SNE est en $\\mathcal{O}[d N^2]$, mais présente plusieurs autres différences notables :\n",
    "\n",
    "- L'implémentation de Barnes-Hut ne fonctionne que lorsque la dimensionnalité cible est de 3 ou moins. Le cas en 2D est typique lors de la construction de visualisations.\n",
    "- Barnes-Hut ne fonctionne qu'avec des données d'entrée denses. Les matrices de données creuses ne peuvent être plongées qu'avec la méthode exacte ou peuvent être approchées par une projection dense à faible rang, par exemple en utilisant [**`TruncatedSVD`**](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html).\n",
    "- Barnes-Hut est une approximation de la méthode exacte. L'approximation est paramétrée avec le paramètre d'angle, qui n'est donc pas utilisé lorsque `method=\"exact\"`.\n",
    "- Barnes-Hut est nettement plus scalable. Barnes-Hut peut être utilisé pour plonger des centaines de milliers de points de données alors que la méthode exacte ne peut traiter que des milliers d'échantillons avant de devenir rédhibitoire en termes de coût de calcul.\n",
    "\n",
    "Pour la visualisation (qui est le principal cas d'utilisation de t-SNE), l'utilisation de la méthode Barnes-Hut est fortement recommandée. La méthode t-SNE exacte est utile pour vérifier les propriétés théoriques du plongement, éventuellement dans un espace de dimension supérieure, mais elle est limitée aux petits ensembles de données en raison de la contrainte de calcul.\n",
    "\n",
    "Il convient également de noter que les étiquettes de chiffres correspondent approximativement au regroupement naturel trouvé par t-SNE, tandis que la projection linéaire en 2D du modèle PCA donne une représentation où les régions d'étiquettes se chevauchent largement. C'est un indice fort que ces données peuvent être bien séparées par des méthodes non linéaires qui se concentrent sur la structure locale (par exemple, un SVM avec un noyau RBF gaussien). Cependant, l'incapacité à visualiser des groupes étiquetés homogènes bien séparés avec t-SNE en 2D n'implique pas nécessairement que les données ne peuvent pas être correctement classées par un modèle supervisé. Il se peut que 2 dimensions ne soient pas suffisantes pour représenter avec précision la structure interne des données.\n",
    "\n",
    "#### Références\n",
    "\n",
    "🔬 [**“Visualizing High-Dimensional Data Using t-SNE”**](https://jmlr.org/papers/v9/vandermaaten08a.html) van der Maaten, L.J.P.; Hinton, G. Journal of Machine Learning Research (2008)\n",
    "\n",
    "🔬🔬 [**t-SNE@Laurens van der Maaten**](https://lvdmaaten.github.io/tsne/) - papiers et implémentations.\n",
    "\n",
    "🔬 [**“Accelerating t-SNE using Tree-Based Algorithms”**](https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf) van der Maaten, L.J.P.; Journal of Machine Learning Research 15(Oct):3221-3245, 2014.\n",
    "\n",
    "🔬 [**“Automated optimized parameters for T-distributed stochastic neighbor embedding improve visualization and analysis of large datasets”**](https://www.nature.com/articles/s41467-019-13055-y.pdf) Belkina, A.C., Ciccolella, C.O., Anno, R., Halpert, R., Spidlen, J., Snyder-Cappione, J.E., Nature Communications 10, 5415 (2019)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='tips-on-practical-use'></a> 2.2.10. Conseils pratiques\n",
    "\n",
    "- Assurez-vous d'utiliser la même échelle pour toutes les caractéristiques. Étant donné que les méthodes d'apprentissage de variétés sont basées sur une recherche de voisins les plus proches, l'algorithme peut mal se comporter autrement. Voir [**`StandardScaler`**](https://scikit-learn.org/stable/modules/preprocessing.html) pour des moyens pratiques de mettre à l'échelle des données hétérogènes.\n",
    "\n",
    "- L'erreur de reconstruction calculée par chaque routine peut être utilisée pour choisir la dimension de sortie optimale. Pour une variété de $d$ dimensions plongée dans un espace de paramètres de $D$ dimensions, l'erreur de reconstruction diminuera à mesure que `n_components` est augmenté jusqu'à ce que `n_components == d`.\n",
    "\n",
    "- Notez que des données bruitées peuvent \"court-circuiter\" la variété, agissant essentiellement comme un pont entre les parties de la variété qui seraient autrement bien séparées. L'apprentissage de variétés sur des données bruitées et/ou incomplètes est un domaine de recherche actif.\n",
    "\n",
    "- Certaines configurations d'entrée peuvent conduire à des matrices de poids singulières, par exemple lorsque plus de deux points dans l'ensemble de données sont identiques, ou lorsque les données sont divisées en groupes disjoints. Dans ce cas, `solver='arpack'` ne parviendra pas à trouver l'espace nul. Le moyen le plus simple de résoudre ce problème est d'utiliser `solver='dense'`, qui fonctionnera sur une matrice singulière, bien qu'il puisse être très lent selon le nombre de points d'entrée. Alternativement, on peut essayer de comprendre la source de la singularité : si elle est due à des ensembles disjoints, augmenter `n_neighbors` peut aider. Si elle est due à des points identiques dans l'ensemble de données, les supprimer peut aider.\n",
    "\n",
    "### Voir également\n",
    "\n",
    "[**Le plongement par arbres totalement aléatoires** (1.11.2.6)](https://scikit-learn.org/stable/modules/ensemble.html#random-trees-embedding) peut également être utile pour dériver des représentations non linéaires de l'espace de caractéristiques, bien qu'elle ne réduise pas la dimensionnalité."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9aff9e50adfaa9e30c910fb3872ffdc72747acb5f50803ca0504f00e980f7c25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
