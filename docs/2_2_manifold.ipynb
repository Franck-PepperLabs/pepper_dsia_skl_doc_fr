{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='unsupervised-learning'></a> 2. [**Apprentissage non supervis√©**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_unsupervised_learning.ipynb#model-selection-and-evaluation)</br>([*Unsupervised learning*](https://scikit-learn.org/stable/unsupervised_learning.html#unsupervised-learning))\n",
    "\n",
    "# 2.2. [**Apprentissage des vari√©t√©s**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_2_manifold.ipynb#manifold-learning)</br>([*Manifold learning*](https://scikit-learn.org/stable/modules/manifold.html#manifold-learning))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 18 pages, 2 exemples, 13 papiers\n",
    "- 2.2.1. [**Introduction**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_unsupervised_learning.ipynb#introduction)<br/>([_Introduction_](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_2_manifold.ipynb#introduction))\n",
    "- 2.2.2. [**Isomap**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_unsupervised_learning.ipynb#isomap)<br/>([_Isomap_](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_2_manifold.ipynb#isomap))\n",
    "- 2.2.3. [**Plongement localement lin√©aire (LLE)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_unsupervised_learning.ipynb#locally-linear-embedding)<br/>([_Locally Linear Embedding_](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_2_manifold.ipynb#locally-linear-embedding))\n",
    "- 2.2.4. [**Plongement localement lin√©aire modifi√© (MLLE)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_unsupervised_learning.ipynb#modified-locally-linear-embedding)<br/>([_Modified Locally Linear Embedding_](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_2_manifold.ipynb#modified-locally-linear-embedding))\n",
    "- 2.2.5. [**Correspondance hessienne de valeurs propres (HLLE)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_unsupervised_learning.ipynb#hessian-eigenmapping)<br/>([_Hessian Eigenmapping_](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_2_manifold.ipynb#hessian-eigenmapping))\n",
    "- 2.2.6. [**Plongement spectral**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_unsupervised_learning.ipynb#spectral-embedding)<br/>([_Spectral Embedding_](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_2_manifold.ipynb#spectral-embedding))\n",
    "- 2.2.7. [**Alignement local dans l'espace tangent (LTSA)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_unsupervised_learning.ipynb#local-tangent-space-alignment)<br/>([_Local Tangent Space Alignment_](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_2_manifold.ipynb#local-tangent-space-alignment))\n",
    "- 2.2.8. [**Positionnement multidimensionnel (MDS)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_unsupervised_learning.ipynb#multi-dimensional-scaling-mds)<br/>([_Multi-dimensional Scaling (MDS)_](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_2_manifold.ipynb#multi-dimensional-scaling-mds))\n",
    "- 2.2.9. [**Plongement stochastique des voisins distribu√©s en t (t-SNE)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_unsupervised_learning.ipynb#t-distributed-stochastic-neighbor-embedding-t-sne)<br/>([_t-distributed Stochastic Neighbor Embedding (t-SNE)_](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_2_manifold.ipynb#t-distributed-stochastic-neighbor-embedding-t-sne))\n",
    "- 2.2.10. [**Conseils pratiques**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_unsupervised_learning.ipynb#tips-on-practical-use)<br/>([_Tips on practical use_](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_2_manifold.ipynb#tips-on-practical-use))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='manifold-learning'></a> 2.2. Apprentissage des vari√©t√©s\n",
    "\n",
    "<i>Cherchez l'essentiel<br/>\n",
    "Les simples choses essentielles<br/>\n",
    "Oubliez vos soucis et vos tracas<br/>\n",
    "Je veux dire l'essentiel<br/>\n",
    "Les recettes de M√®re Nature<br/>\n",
    "Celles qui apportent les choses essentielles de la vie<br/>\n",
    "<br/>\n",
    "Chanson de Baloo [Le livre de la jungle]</i>\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_compare_methods_001.png\"\n",
    "    alt=\"Original S-curve samples\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_compare_methods_003.png\"\n",
    "    alt=\"Isomap Embedding\"\n",
    "    style=\"width: auto; height: 220px;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_compare_methods_004.png\"\n",
    "    alt=\"Multidimensional scaling\"\n",
    "    style=\"width: auto; height: 220px;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_compare_methods_005.png\"\n",
    "    alt=\"Spectral Embedding\"\n",
    "    style=\"width: auto; height: 220px;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_compare_methods_006.png\"\n",
    "    alt=\"t-SNE\"\n",
    "    style=\"width: auto; height: 220px;\"/>\n",
    "</div>\n",
    "\n",
    "L'apprentissage des vari√©t√©s est une approche non lin√©aire de r√©duction de la dimensionnalit√©. Les algorithmes pour cette t√¢che sont bas√©s sur l'id√©e que la dimensionnalit√© de nombreux ensembles de donn√©es est artificiellement √©lev√©e."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='introduction'></a> 2.2.1. Introduction\n",
    "\n",
    "Les ensembles de donn√©es de grande dimension peuvent √™tre tr√®s difficiles √† visualiser. Bien que les donn√©es en deux ou trois dimensions puissent √™tre repr√©sent√©es graphiquement pour montrer la structure inh√©rente des donn√©es, les graphiques √©quivalents en haute dimension sont beaucoup moins intuitifs. Pour faciliter la visualisation de la structure d'un ensemble de donn√©es, la dimension doit √™tre r√©duite d'une mani√®re ou d'une autre.\n",
    "\n",
    "La mani√®re la plus simple de r√©aliser cette r√©duction de dimensionnalit√© est de prendre une projection al√©atoire des donn√©es. Bien que cela permette une certaine visualisation de la structure des donn√©es, le caract√®re al√©atoire du choix laisse beaucoup √† d√©sirer. Dans une projection al√©atoire, il est probable que la structure la plus int√©ressante des donn√©es sera perdue.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "<img\n",
    "  src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_001.png\"\n",
    "  alt=\"Handwritten Digits\"\n",
    "  style=\"width: auto; height: 240px;\"/>\n",
    "<img\n",
    "  src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_002.png\"\n",
    "  alt=\"Random Projection Embedding\"\n",
    "  style=\"width: auto; height: 240px;\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Pour r√©pondre √† cette pr√©occupation, plusieurs cadres de r√©duction de la dimensionnalit√© lin√©aires supervis√©s et non supervis√©s ont √©t√© con√ßus, tels que l'analyse en composantes principales (PCA), l'analyse en composantes ind√©pendantes (ICA), l'analyse discriminante lin√©aire (LDA) et d'autres encore. Ces algorithmes d√©finissent des rubriques sp√©cifiques pour choisir une projection lin√©aire \"int√©ressante\" des donn√©es. Si ces m√©thodes peuvent √™tre puissantes, elles passent souvent √† c√¥t√© d'importantes structures non lin√©aires dans les donn√©es.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "<img\n",
    "  src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_003.png\"\n",
    "  alt=\"Truncated SVD Embedding\"\n",
    "  style=\"width: auto; height: 240px;\"/>\n",
    "<img\n",
    "  src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_003.png\"\n",
    "  alt=\"LDA Embedding\"\n",
    "  style=\"width: auto; height: 240px;\"/>\n",
    "</div>\n",
    "\n",
    "L'apprentissage des vari√©t√©s (Manifold Learning) peut √™tre consid√©r√© comme une tentative de g√©n√©raliser les cadres lin√©aires tels que PCA pour √™tre sensibles √† la structure non lin√©aire des donn√©es. Bien que des variantes supervis√©es existent, le probl√®me d'apprentissage de vari√©t√© typique est non supervis√© : il apprend la structure multidimensionnelle des donn√©es √† partir des donn√©es elles-m√™mes, sans l'utilisation de classifications pr√©d√©termin√©es.\n",
    "\n",
    "Les impl√©mentations d'apprentissage de vari√©t√© disponibles dans scikit-learn sont r√©sum√©es ci-dessous.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Apprentissage des vari√©t√©s sur des chiffres manuscrits : Plongement localement lin√©aire, Isomap‚Ä¶**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_2_manifold/plot_lle_digits.ipynb)<br/>([*Manifold learning on handwritten digits: Locally Linear Embedding, Isomap‚Ä¶*](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html))\n",
    "\n",
    "Exemple de r√©duction de dimensionnalit√© sur des chiffres manuscrits.\n",
    "\n",
    "#### [**Comparaison des m√©thodes d'apprentissage de vari√©t√©s**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_2_manifold/plot_compare_methods.ipynb)<br/>([*Comparison of Manifold Learning methods*](https://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html))\n",
    "\n",
    "Exemple de r√©duction de dimensionnalit√© sur un jeu de donn√©es de \"S-curve\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='isomap'></a> 2.2.2. Isomap\n",
    "\n",
    "L'une des premi√®res approches de l'apprentissage des vari√©t√©s est l'algorithme Isomap, abr√©viation de Isometric Mapping (cartographie isom√©trique). Isomap peut √™tre consid√©r√© comme une extension de la mise √† l'√©chelle multidimensionnelle (MDS) ou de l'ACP √† noyau. Isomap cherche un plongement de dimension inf√©rieure qui conserve les distances g√©od√©siques entre tous les points. Isomap peut √™tre r√©alis√© √† l'aide de l'objet [**`Isomap`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html).\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "<img\n",
    "  src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_005.png\"\n",
    "  alt=\"Isomap\"\n",
    "  style=\"width: auto; height: 240px;\"/>\n",
    "</div>\n",
    "\n",
    "### 2.2.2.1. Complexit√©\n",
    "\n",
    "L'algorithme Isomap se compose de trois √©tapes :\n",
    "\n",
    "1. **Recherche des voisins les plus proches.** Isomap utilise [**`BallTree`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html) pour une recherche efficace des voisins. Le co√ªt est d'environ $\\mathcal{O}[D \\log(k) N \\log(N)]$, pour les $k$ voisins les plus proches de $N$ points en $D$ dimensions.\n",
    "2. **Recherche en graphe des plus courts chemins.** Les algorithmes les plus efficaces connus pour cela sont l'*algorithme de Dijkstra*, qui est d'environ $\\mathcal{O}[N^2(k + \\log(N))]$, ou l'algorithme de Floyd-Warshall, qui est $\\mathcal{O}[N^3]$. L'algorithme peut √™tre s√©lectionn√© par l'utilisateur avec le mot-cl√© `path_method` d'`Isomap`. S'il n'est pas sp√©cifi√©, le code tente de choisir le meilleur algorithme en fonction des donn√©es d'entr√©e.\n",
    "3. **D√©composition partielle des valeurs propres.** Le plongement des donn√©es est encod√© dans les vecteurs propres correspondant aux $d$ plus grandes valeurs propres du noyau isomap $N \\times N$. Pour un solveur dense, le co√ªt est d'environ $\\mathcal{O}[d N^2]$. Ce co√ªt peut souvent √™tre am√©lior√© en utilisant le solveur `ARPACK`. Le solveur de valeurs propres peut √™tre sp√©cifi√© par l'utilisateur avec le mot-cl√© `eigen_solver` d'`Isomap`. S'il n'est pas sp√©cifi√©, le code tente de choisir le meilleur algorithme pour les donn√©es d'entr√©e.\n",
    "\n",
    "La complexit√© globale d'Isomap est $\\mathcal{O}[D \\log(k) N \\log(N)] + \\mathcal{O}[N^2(k + \\log(N))] + \\mathcal{O}[d N^2]$.\n",
    "\n",
    "* $N$ : nombre de points de donn√©es d'entra√Ænement\n",
    "* $D$ : dimension d'entr√©e\n",
    "* $k$ : nombre de voisins les plus proches\n",
    "* $d$ : dimension de sortie\n",
    "\n",
    "#### R√©f√©rences\n",
    "\n",
    "üî¨ [**‚ÄúA Global Geometric Framework for Nonlinear Dimensionality Reduction‚Äù**](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=9c2c88edc29c02c71064db3a77e89399f9e197c1) Tenenbaum, J.B.; De Silva, V.; & Langford, J.C. Science 290 (5500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='locally-linear-embedding'></a> 2.2.3. Plongement localement lin√©aire (LLE)\n",
    "\n",
    "Le plongement localement lin√©aire (LLE) cherche une projection des donn√©es de dimension inf√©rieure qui pr√©serve les distances au sein des voisinages locaux. On peut le consid√©rer comme une s√©rie d'analyses en composantes principales locales qui sont compar√©es globalement pour trouver le meilleur plongement non lin√©aire.\n",
    "\n",
    "Le plongement localement lin√©aire peut √™tre effectu√©e avec la fonction [**`locally_linear_embedding`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.locally_linear_embedding.html) ou son homologue orient√© objet [**`LocallyLinearEmbedding`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html).\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "<img\n",
    "  src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_006.png\"\n",
    "  alt=\"LLE\"\n",
    "  style=\"width: auto; height: 240px;\"/>\n",
    "</div>\n",
    "\n",
    "### 2.2.3.1. Complexit√©\n",
    "\n",
    "L'algorithme LLE standard comporte trois √©tapes :\n",
    "1. **Recherche des plus proches voisins.** Voir la discussion sous Isomap ci-dessus.\n",
    "2. **Construction de la matrice de pond√©ration.** $\\mathcal{O}[D N k^3]$. La construction de la matrice de pond√©ration LLE implique la solution d'une √©quation lin√©aire $k \\times $ pour chacun des $N$ voisinages locaux.\n",
    "3. **D√©composition partielle des valeurs propres.** Voir la discussion sous Isomap ci-dessus.\n",
    "\n",
    "La complexit√© globale de la LLE standard est $\\mathcal{O}[D \\log(k) N \\log(N)] + \\mathcal{O}[D N k^3] + \\mathcal{O}[d N^2]$.\n",
    "* $N$ : nombre de points de donn√©es d'entra√Ænement\n",
    "* $D$ : dimension d'entr√©e\n",
    "* $k$ : nombre de plus proches voisins\n",
    "* $d$ : dimension de sortie\n",
    "\n",
    "#### R√©f√©rences\n",
    "\n",
    "üî¨ [**‚ÄúNonlinear dimensionality reduction by locally linear embedding‚Äù**](https://www.science.org/doi/10.1126/science.290.5500.2323) Roweis, S. & Saul, L. Science 290:2323 (2000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='modified-locally-linear-embedding'></a> 2.2.4. Plongement localement lin√©aire modifi√©e (MLLE)\n",
    "\n",
    "Un probl√®me bien connu avec LLE est le probl√®me de r√©gularisation. Lorsque le nombre de voisins est sup√©rieur au nombre de dimensions d'entr√©e, la matrice d√©finissant chaque voisinage local a un rang d√©ficient. Pour r√©soudre ce probl√®me, le LLE standard applique un param√®tre de r√©gularisation arbitraire $r$, qui est choisi par rapport √† la trace de la matrice de pond√©ration locale. Bien qu'il puisse √™tre d√©montr√© formellement qu'√† mesure que $r \\to 0$, la solution converge vers le plongement souhait√©e, il n'y a aucune garantie que la solution optimale sera trouv√©e pour $r > 0$. Ce probl√®me se manifeste dans les plongements qui d√©forment la g√©om√©trie sous-jacente de la vari√©t√©.\n",
    "\n",
    "Une m√©thode pour r√©soudre le probl√®me de r√©gularisation consiste √† utiliser plusieurs vecteurs de poids dans chaque voisinage. C'est l'essence du *plongement localement lin√©aire modifi√©e* (MLLE). MLLE peut √™tre effectu√© avec la fonction [**`locally_linear_embedding`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html) ou son homologue orient√© objet [**`LocallyLinearEmbedding`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html), avec le mot-cl√©  `method='modified'`. Cela n√©cessite `n_neighbors > n_components`.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "<img\n",
    "  src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_007.png\"\n",
    "  alt=\"MLLE\"\n",
    "  style=\"width: auto; height: 240px;\"/>\n",
    "</div>\n",
    "\n",
    "### 2.2.4.1. Complexit√©\n",
    "\n",
    "L'algorithme MLLE comprend trois √©tapes :\n",
    "1. **Recherche des plus proches voisins.** Identique √† celle de la LLE standard.\n",
    "2. **Construction de la matrice de pond√©ration.** Environ $\\mathcal{O}[D N k^3] + \\mathcal{O}[N (k-D) k^2]$. Le premier terme est exactement √©quivalent √† celui du LLE standard. Le deuxi√®me terme a √† voir avec la construction de la matrice de pond√©ration √† partir de plusieurs poids. En pratique, le co√ªt suppl√©mentaire de la construction de la matrice de pond√©ration du MLLE est n√©gligeable par rapport au co√ªt des √©tapes 1 et 3.\n",
    "3. **D√©composition partielle des valeurs propres.** Identique √† celle du LLE standard.\n",
    "\n",
    "La complexit√© globale de MLLE est $\\mathcal{O}[D \\log(k) N \\log(N)] + \\mathcal{O}[D N k^3] + \\mathcal{O}[N (k-D) k^2] + \\mathcal{O}[d N^2]$.\n",
    "* $N$ : nombre de points de donn√©es d'entra√Ænement\n",
    "* $D$ : dimension d'entr√©e\n",
    "* $k$ : nombre de plus proches voisins\n",
    "* $d$ : dimension de sortie\n",
    "\n",
    "#### R√©f√©rences\n",
    "\n",
    "Z üî¨ [**‚ÄúMLLE: Modified Locally Linear Embedding Using Multiple Weights‚Äù**](https://proceedings.neurips.cc/paper_files/paper/2006/file/fb2606a5068901da92473666256e6e5b-Paper.pdf) Zhang, Z. & Wang, J."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='hessian-eigenmapping'></a> 2.2.5. Correspondance hessienne de valeurs propres (HLLE)\n",
    "\n",
    "La correspondance hessienne de valeurs propres (√©galement connue sous le nom de LLE bas√©e sur la hessienne : HLLE) est une autre m√©thode de r√©solution du probl√®me de r√©gularisation de LLE. Elle repose sur une forme quadratique bas√©e sur la hessienne dans chaque voisinage, qui est utilis√©e pour r√©cup√©rer la structure localement lin√©aire. Bien que d'autres impl√©mentations notent sa faible mise √† l'√©chelle avec la taille des donn√©es, scikit-learn met en ≈ìuvre certaines am√©liorations algorithmiques qui rendent son co√ªt comparable √† celui d'autres variantes de LLE pour une petite dimension de sortie. Le HLLE peut √™tre effectu√© avec la fonction [**`locally_linear_embedding`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.locally_linear_embedding.html) ou sa version orient√©e objet [**`LocallyLinearEmbedding`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html), avec le mot-cl√© `method='hessian'`. Il n√©cessite `n_neighbors > n_components * (n_components + 3) / 2`.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "<img\n",
    "  src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_008.png\"\n",
    "  alt=\"HLLE\"\n",
    "  style=\"width: auto; height: 240px;\"/>\n",
    "</div>\n",
    "\n",
    "### 2.2.5.1. Complexit√©\n",
    "\n",
    "L'algorithme HLLE comprend trois √©tapes :\n",
    "1. **Recherche des plus proches voisins.** Identique √† celle du LLE standard.\n",
    "2. **Construction de la matrice de poids.** Environ $\\mathcal{O}[D N k^3] + \\mathcal{O}[N d^6]$. Le premier terme est exactement √©quivalent √† celui du LLE standard. Le deuxi√®me terme provient d'une d√©composition QR de l'estimateur hessien local.\n",
    "3. **D√©composition partielle des valeurs propres.** Identique √† celle du LLE standard.\n",
    "\n",
    "La complexit√© globale d HLLE est $\\mathcal{O}[D \\log(k) N \\log(N)] + \\mathcal{O}[D N k^3] + \\mathcal{O}[N d^6] + \\mathcal{O}[d N^2]$.\n",
    "* $N$ : nombre de points de donn√©es d'entra√Ænement\n",
    "* $D$ : dimension d'entr√©e\n",
    "* $k$ : nombre de plus proches voisins\n",
    "* $d$ : dimension de sortie\n",
    "\n",
    "#### R√©f√©rences\n",
    "\n",
    "üî¨ [**‚ÄúHessian Eigenmaps: Locally linear embedding techniques for high-dimensional data‚Äù**](https://www.pnas.org/doi/10.1073/pnas.1031596100) Donoho, D. & Grimes, C. Proc Natl Acad Sci USA. 100:5591 (2003)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='spectral-embedding'></a> 2.2.6. Plongement spectral\n",
    "\n",
    "Le plongement spectral est une approche de calcul d'un plongement non lin√©aire. Scikit-learn impl√©mente Laplacian Eigenmaps, qui trouve une repr√©sentation de basse dimension des donn√©es en utilisant une d√©composition spectrale du Laplacien du graphe. Le graphe g√©n√©r√© peut √™tre consid√©r√© comme une approximation discr√®te de la vari√©t√© de basse dimension dans l'espace de haute dimension. La minimisation d'une fonction de co√ªt bas√©e sur le graphe garantit que les points proches les uns des autres sur la vari√©t√© sont projet√©s √† proximit√© les uns des autres dans l'espace de basse dimension, pr√©servant ainsi les distances locales. Le plongement spectral peut √™tre effectu√© avec la fonction [**`spectral_embedding`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.spectral_embedding.html) ou son homologue orient√© objet [**`SpectralEmbedding`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.SpectralEmbedding.html).\n",
    "\n",
    "### 2.2.6.1. Complexit√©\n",
    "\n",
    "L'algorithme de plongement spectral (Laplacian Eigenmaps) comprend trois √©tapes :\n",
    "* **Construction du graphe pond√©r√©.** Transformer les donn√©es brutes d'entr√©e en une repr√©sentation de graphe √† l'aide d'une matrice d'affinit√© (d'adjacence).\n",
    "* **Construction du Laplacien du graphe.** Le Laplacien du graphe non normalis√© est construit comme $L = D - A$, et le Laplacien normalis√© comme $L = D^{-\\frac{1}{2}} (D - A) D^{-\\frac{1}{2}}$.\n",
    "* **D√©composition partielle des valeurs propres.** Une d√©composition des valeurs propres est effectu√©e sur le Laplacien du graphe.\n",
    "\n",
    "La complexit√© globale de l'int√©gration spectrale est $\\mathcal{O}[D \\log(k) N \\log(N)] + \\mathcal{O}[D N k^3] + \\mathcal{O}[d N^2]$.\n",
    "* $N$ : nombre de points de donn√©es d'entra√Ænement\n",
    "* $D$ : dimension d'entr√©e\n",
    "* $k$ : nombre de plus proches voisins\n",
    "* $d$ : dimension de sortie\n",
    "\n",
    "#### R√©f√©rences\n",
    "\n",
    "üî¨ [**‚ÄúLaplacian Eigenmaps for Dimensionality Reduction and Data Representation‚Äù**](https://web.cse.ohio-state.edu/~belkin.8/papers/LEM_NC_03.pdf) M. Belkin, P. Niyogi, Neural Computation, June 2003; 15 (6):1373-1396"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='local-tangent-space-alignment'></a> 2.2.7. Alignement local dans l'espace tangent (LTSA)\n",
    "\n",
    "Bien que techniquement pas une variante de LLE, l'alignement local dans l'espace tangent (LTSA) est suffisamment similaire sur le plan algorithmique √† LLE pour qu'il puisse √™tre inclus dans cette cat√©gorie. Au lieu de se concentrer sur la pr√©servation des distances entre les voisins comme dans LLE, LTSA cherche √† caract√©riser la g√©om√©trie locale √† chaque voisinage via son espace tangent, et effectue une optimisation globale pour aligner ces espaces tangents locaux afin d'apprendre l'incorporation. LTSA peut √™tre effectu√© avec la fonction [**`locally_linear_embedding`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.locally_linear_embedding.html) ou son homologue orient√© objet [**`LocallyLinearEmbedding`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html), avec le mot cl√© `method = 'ltsa'`.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "<img\n",
    "  src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_009.png\"\n",
    "  alt=\"LTSA\"\n",
    "  style=\"width: auto; height: 240px;\"/>\n",
    "</div>\n",
    "\n",
    "### 2.2.7.1. Complexit√©\n",
    "\n",
    "L'algorithme LTSA comprend trois √©tapes :\n",
    "* **Recherche des voisins les plus proches.** Identique √† LLE standard.\n",
    "* **Construction de la matrice de pond√©ration.** Environ $\\mathcal{O}[D N k^3] + \\mathcal{O}[k^2 d]$. Le premier terme refl√®te un co√ªt similaire √† celui de LLE standard.\n",
    "* **D√©composition partielle des valeurs propres.** Identique √† LLE standard.\n",
    "\n",
    "La complexit√© globale du LTSA standard est $\\mathcal{O}[D \\log(k) N \\log(N)] + \\mathcal{O}[D N k^3] + \\mathcal{O}[k^2 d] + \\mathcal{O}[d N^2]$.\n",
    "* $N$ : nombre de points de donn√©es d'entra√Ænement\n",
    "* $D$ : dimension d'entr√©e\n",
    "* $k$ : nombre de plus proches voisins\n",
    "* $d$ : dimension de sortie\n",
    "\n",
    "#### R√©f√©rences\n",
    "\n",
    "üî¨ [‚Äú**Principal Manifolds and Nonlinear Dimension Reduction via Local Tangent Space Alignment‚Äù**](https://arxiv.org/abs/cs/0212008) Zhang, Z. & Zha, H. Journal of Shanghai Univ. 8:406 (2004)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='multi-dimensional-scaling-mds'></a> 2.2.8. Positionnement multidimensionnel (MDS)\n",
    "\n",
    "Le [**positionnement multidimensionnel**](https://en.wikipedia.org/wiki/Multidimensional_scaling) (*Multi-dimensional Scaling (MDS)* en anglais) impl√©ment√©e par la classe [**`MDS`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html) vise √† trouver une repr√©sentation en basse dimension des donn√©es dans laquelle les distances respectent bien les distances de l'espace d'origine de haute dimension.\n",
    "\n",
    "En g√©n√©ral, [**`MDS`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html) est une technique utilis√©e pour analyser des donn√©es de similarit√© ou de dissimilarit√©. Elle tente de mod√©liser ces donn√©es comme des distances dans un espace g√©om√©trique. Les donn√©es peuvent √™tre des √©valuations de similarit√© entre des objets, des fr√©quences d'interaction entre des mol√©cules ou des indices commerciaux entre des pays.\n",
    "\n",
    "Il existe deux types d'algorithmes MDS : m√©triques et non m√©triques. Dans scikit-learn, la classe [**`MDS`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html) impl√©mente les deux. Dans MDS m√©trique, la matrice de similarit√© d'entr√©e provient d'une m√©trique (et respecte donc l'in√©galit√© triangulaire), les distances entre les deux points de sortie sont alors d√©finies pour √™tre aussi proches que possible des donn√©es de similarit√© ou de dissimilarit√©. Dans la version non m√©trique, les algorithmes essaieront de pr√©server l'ordre des distances et chercheront donc une relation monotone entre les distances dans l'espace int√©gr√© et les similarit√©s/dissimilarit√©s.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "<img\n",
    "  src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_010.png\"\n",
    "  alt=\"MDS\"\n",
    "  style=\"width: auto; height: 240px;\"/>\n",
    "</div>\n",
    "\n",
    "Soit $S$ la matrice de similarit√© et $X$ les coordonn√©es des $n$ points d'entr√©e. Les disparit√©s $\\hat{d}_{ij}$ sont des transformations des similarit√©s choisies de mani√®re optimale. L'objectif, appel√© stress, est alors d√©fini par $\\sum_{i < j} d_{ij}(X) - \\hat{d}_{ij}(X)$.\n",
    "\n",
    "### <a id='metric-mds'></a> 2.2.8.1. MDS m√©trique\n",
    "\n",
    "Dans le mod√®le MDS m√©trique le plus simple, appel√© _MDS absolu_, les disparit√©s sont d√©finies par $\\hat{d}_{ij}=S_{ij}$. Avec le MDS absolu, la valeur $S_{ij}$ devrait alors correspondre exactement √† la distance entre les points $i$ et $j$ dans le point plong√©.\n",
    "\n",
    "Le plus souvent, les disparit√©s sont fix√©es √† $\\hat{d}_{ij}=bS_{ij}$.\n",
    "\n",
    "### <a id='nonmetric-mds'></a> 2.2.8.2. MDS non m√©trique\n",
    "\n",
    "Le [**`MDS`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html) non m√©trique se concentre sur l'ordination des donn√©es. Si $S_{ij} > S_{jk}$, alors le plongement doit assurer $d_{ij} <> d_{jk}$. Pour cette raison, nous en parlons en termes de dissimilarit√©s ($\\delta_{ij}$) au lieu de similarit√©s ($S_{ij}$). Notez que les dissimilarit√©s peuvent facilement √™tre obtenues √† partir de similarit√©s par une transformation simple, par ex. $\\delta_{ij}=c_1-c_2 S_{ij}$ pour certaines constantes r√©elles $c_1, c_2$. Un algorithme simple pour pour imposer une ordination correcte consiste √† utiliser une r√©gression monotone de $d_{ij}$ sur $\\delta_{ij}$, produisant des disparit√©s $\\hat{d}_{ij}$ dans le m√™me ordre que $\\delta_{ij}$.\n",
    "\n",
    "Une solution triviale √† ce probl√®me consiste √† d√©finir tous les points sur l'origine. Pour √©viter cela, les disparit√©s $\\hat{d}_{ij}$ sont normalis√©s. Notez que comme nous nous soucions seulement de l'ordre relatif, notre objectif devrait √™tre invariant √† une simple translation et mise √† l'√©chelle. Cependant, le stress utilis√© dans la MDS m√©trique est sensible √† la mise √† l'√©chelle. Pour rem√©dier √† cela, la MDS non m√©trique peut utiliser un stress normalis√©, appel√© Stress-1, d√©fini comme\n",
    "\n",
    "$$\n",
    "\\sqrt{\\frac{\\sum_{i < j} (d_{ij} - \\hat{d}_{ij})^2}{\\sum_{i < j} d_{ij}^2}}.\n",
    "$$\n",
    "\n",
    "L'utilisation de Stress-1 normalis√© peut √™tre activ√©e en d√©finissant `normalized_stress=True`, mais elle n'est compatible qu'avec le probl√®me MDS non m√©trique et sera ignor√©e dans le cas m√©trique.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "<img\n",
    "  src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_mds_001.png\"\n",
    "  alt=\"Nonmetric MDS\"\n",
    "  style=\"width: auto; height: 240px;\"/>\n",
    "</div>\n",
    "\n",
    "#### R√©f√©rences\n",
    "\n",
    "üìö [150 ‚Ç¨] [**‚ÄúModern Multidimensional Scaling - Theory and Applications‚Äù**](https://link.springer.com/content/pdf/bfm:978-0-387-28981-6/1?pdf=chapter%20toc) Borg, I.; Groenen P. Springer Series in Statistics (1997)\n",
    "\n",
    "üî¨ [**‚ÄúNonmetric multidimensional scaling: a numerical method‚Äù**](http://cda.psych.uiuc.edu/psychometrika_highly_cited_articles/kruskal_1964b.pdf) Kruskal, J. Psychometrika, 29 (1964)\n",
    "\n",
    "üî¨ [**‚ÄúMultidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis‚Äù**](http://cda.psych.uiuc.edu/psychometrika_highly_cited_articles/kruskal_1964a.pdf) Kruskal, J. Psychometrika, 29, (1964)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='t-distributed-stochastic-neighbor-embedding-t-sne'></a> 2.2.9. Plongement stochastique des voisins distribu√©s en t (t-SNE)\n",
    "\n",
    "([**`TSNE`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)) (_T-distributed Stochastic Neighbor Embedding_ en anglais) convertit les affinit√©s des points de donn√©es en probabilit√©s. Les affinit√©s dans l'espace d'origine sont repr√©sent√©es par des probabilit√©s conjointes gaussiennes et les affinit√©s dans l'espace de plongement sont repr√©sent√©es par des distributions t de Student. Cela permet √† t-SNE d'√™tre particuli√®rement sensible √† la structure locale et pr√©sente quelques avantages par rapport aux techniques existantes :\n",
    "* R√©v√©ler la structure √† de nombreuses √©chelles sur une seule carte\n",
    "* R√©v√©ler les donn√©es qui se trouvent dans plusieurs vari√©t√©s ou clusters diff√©rents\n",
    "* R√©duire la tendance √† regrouper les points au centre\n",
    "\n",
    "Alors qu'Isomap, LLE et ses variantes sont les mieux adapt√©es pour d√©ployer une seule vari√©t√© continue de basse dimension, t-SNE se concentrera sur la structure locale des donn√©es et aura tendance √† extraire des groupes locaux de points regroup√©s comme cela est mis en √©vidence sur l'exemple de la courbe en S. Cette capacit√© √† regrouper des √©chantillons en fonction de la structure locale peut √™tre b√©n√©fique pour d√©m√™ler visuellement un ensemble de donn√©es qui comprend plusieurs vari√©t√©s simultan√©ment, comme c'est le cas dans le jeu de donn√©es des chiffres manuscrits.\n",
    "\n",
    "La divergence de Kullback-Leibler (KL) des probabilit√©s conjointes dans l'espace d'origine et l'espace de plongement sera minimis√©e par descente de gradient. Notez que la divergence KL n'est pas convexe, c'est-√†-dire que plusieurs red√©marrages avec des initialisations diff√©rentes aboutiront √† des minima locaux de la divergence KL. Il est donc parfois utile d'essayer diff√©rentes graines et de s√©lectionner le plongement avec la plus faible divergence KL.\n",
    "\n",
    "Les inconv√©nients de l'utilisation de t-SNE sont globalement les suivants :\n",
    "* t-SNE est co√ªteux en termes de calcul et peut prendre plusieurs heures sur des ensembles de donn√©es de plusieurs millions d'√©chantillons, l√† o√π PCA se termine en quelques secondes ou minutes.\n",
    "* La m√©thode de Barnes-Hut t-SNE est limit√©e aux plongements bidimensionnels ou tridimensionnels.\n",
    "* L'algorithme est stochastique et plusieurs red√©marrages avec des graines diff√©rentes peuvent donner des plongements diff√©rents. Cependant, il est tout √† fait l√©gitime de choisir le plongement avec l'erreur la plus faible.\n",
    "* La structure globale n'est pas explicitement pr√©serv√©e. Ce probl√®me est att√©nu√© en initialisant les points avec PCA (en utilisant `init='pca'`).\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "<img\n",
    "  src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_013.png\"\n",
    "  alt=\"Nonmetric MDS\"\n",
    "  style=\"width: auto; height: 240px;\"/>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='optimizing-t-sne'></a> 2.2.9.1. Optimisation de t-SNE\n",
    "\n",
    "Le principal objectif de t-SNE est la visualisation de donn√©es de haute dimension. Par cons√©quent, il fonctionne mieux lorsque les donn√©es sont int√©gr√©es sur deux ou trois dimensions.\n",
    "\n",
    "L'optimisation de la divergence KL peut parfois √™tre un peu d√©licate. Il existe cinq param√®tres qui contr√¥lent l'optimisation de t-SNE et donc potentiellement la qualit√© du plongement r√©sultant :\n",
    "\n",
    "- La perplexit√©\n",
    "- Le facteur d'exag√©ration pr√©coce (_early exaggeration factor_)\n",
    "- Le taux d'apprentissage (_learning rate_)\n",
    "- Le nombre maximum d'it√©rations\n",
    "- L'angle (non utilis√© dans la m√©thode exacte)\n",
    "\n",
    "La perplexit√© est d√©finie comme $k = 2^{(S)}$ o√π $S$ est l'entropie de Shannon de la distribution de probabilit√© conditionnelle. Des perplexit√©s plus √©lev√©es conduisent √† plus de voisins proches et sont moins sensibles aux petites structures. √Ä l'inverse, une perplexit√© plus faible consid√®re un nombre plus petit de voisins, ignorant ainsi plus d'informations globales au profit du voisinage local. √Ä mesure que les tailles des ensembles de donn√©es augmentent, plus de points seront n√©cessaires pour obtenir un √©chantillon raisonnable du voisinage local, d'o√π la n√©cessit√© de perplexit√©s plus grandes. De m√™me, des ensembles de donn√©es plus bruyants n√©cessiteront des valeurs de perplexit√© plus √©lev√©es pour englober suffisamment de voisins locaux et voir au-del√† du bruit de fond.\n",
    "\n",
    "Le nombre maximum d'it√©rations est g√©n√©ralement assez √©lev√© et n'a pas besoin d'√™tre r√©gl√©. L'optimisation se compose de deux phases : la phase d'exag√©ration pr√©coce et l'optimisation finale. Pendant l'exag√©ration pr√©coce, les probabilit√©s conjointes dans l'espace d'origine seront artificiellement augment√©es par multiplication par un facteur donn√©. Des facteurs plus importants entra√Ænent des √©carts plus importants entre les amas naturels dans les donn√©es. Si le facteur est trop √©lev√©, la divergence KL pourrait augmenter pendant cette phase. En g√©n√©ral, il n'est pas n√©cessaire de le r√©gler. Un param√®tre critique est le taux d'apprentissage. S'il est trop faible, la descente de gradient restera bloqu√©e dans un mauvais minimum local. S'il est trop √©lev√©, la divergence KL augmentera pendant l'optimisation. Une heuristique sugg√©r√©e par Belkina et al. (2019) est de d√©finir le taux d'apprentissage comme √©tant la taille de l'√©chantillon divis√©e par le facteur d'exag√©ration pr√©coce. Nous mettons en ≈ìuvre cette heuristique sous forme d'argument learning_rate='auto'. D'autres conseils peuvent √™tre trouv√©s dans la FAQ de Laurens van der Maaten (voir les r√©f√©rences). Le dernier param√®tre, l'angle, est un compromis entre les performances et la pr√©cision. Des angles plus importants impliquent que nous pouvons approximer de plus grandes r√©gions par un seul point, ce qui acc√©l√®re la vitesse mais donne des r√©sultats moins pr√©cis.\n",
    "\n",
    "[‚Äú**How to Use t-SNE Effectively**‚Äù](https://distill.pub/2016/misread-tsne/) offre une bonne discussion sur les effets des diff√©rents param√®tres, ainsi que des graphiques interactifs pour explorer les effets de diff√©rents param√®tres."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='optimizing-t-sne'></a> 2.2.9.2. Barnes-Hut t-SNE\n",
    "\n",
    "La version de t-SNE de Barnes-Hut impl√©ment√©e ici est g√©n√©ralement beaucoup plus lente que d'autres algorithmes d'apprentissage de vari√©t√©s. L'optimisation est assez difficile et le calcul du gradient est en $\\mathcal{O}[d N \\log(N)]$, o√π $d$ est le nombre de dimensions de sortie et $N$ est le nombre d'√©chantillons. La m√©thode Barnes-Hut am√©liore la m√©thode exacte, o√π la complexit√© de t-SNE est en $\\mathcal{O}[d N^2]$, mais pr√©sente plusieurs autres diff√©rences notables :\n",
    "\n",
    "- L'impl√©mentation de Barnes-Hut ne fonctionne que lorsque la dimensionnalit√© cible est de 3 ou moins. Le cas en 2D est typique lors de la construction de visualisations.\n",
    "- Barnes-Hut ne fonctionne qu'avec des donn√©es d'entr√©e denses. Les matrices de donn√©es creuses ne peuvent √™tre plong√©es qu'avec la m√©thode exacte ou peuvent √™tre approch√©es par une projection dense √† faible rang, par exemple en utilisant [**`TruncatedSVD`**](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html).\n",
    "- Barnes-Hut est une approximation de la m√©thode exacte. L'approximation est param√©tr√©e avec le param√®tre d'angle, qui n'est donc pas utilis√© lorsque `method=\"exact\"`.\n",
    "- Barnes-Hut est nettement plus scalable. Barnes-Hut peut √™tre utilis√© pour plonger des centaines de milliers de points de donn√©es alors que la m√©thode exacte ne peut traiter que des milliers d'√©chantillons avant de devenir r√©dhibitoire en termes de co√ªt de calcul.\n",
    "\n",
    "Pour la visualisation (qui est le principal cas d'utilisation de t-SNE), l'utilisation de la m√©thode Barnes-Hut est fortement recommand√©e. La m√©thode t-SNE exacte est utile pour v√©rifier les propri√©t√©s th√©oriques du plongement, √©ventuellement dans un espace de dimension sup√©rieure, mais elle est limit√©e aux petits ensembles de donn√©es en raison de la contrainte de calcul.\n",
    "\n",
    "Il convient √©galement de noter que les √©tiquettes de chiffres correspondent approximativement au regroupement naturel trouv√© par t-SNE, tandis que la projection lin√©aire en 2D du mod√®le PCA donne une repr√©sentation o√π les r√©gions d'√©tiquettes se chevauchent largement. C'est un indice fort que ces donn√©es peuvent √™tre bien s√©par√©es par des m√©thodes non lin√©aires qui se concentrent sur la structure locale (par exemple, un SVM avec un noyau RBF gaussien). Cependant, l'incapacit√© √† visualiser des groupes √©tiquet√©s homog√®nes bien s√©par√©s avec t-SNE en 2D n'implique pas n√©cessairement que les donn√©es ne peuvent pas √™tre correctement class√©es par un mod√®le supervis√©. Il se peut que 2 dimensions ne soient pas suffisantes pour repr√©senter avec pr√©cision la structure interne des donn√©es.\n",
    "\n",
    "#### R√©f√©rences\n",
    "\n",
    "üî¨ [**‚ÄúVisualizing High-Dimensional Data Using t-SNE‚Äù**](https://jmlr.org/papers/v9/vandermaaten08a.html) van der Maaten, L.J.P.; Hinton, G. Journal of Machine Learning Research (2008)\n",
    "\n",
    "üî¨üî¨ [**t-SNE@Laurens van der Maaten**](https://lvdmaaten.github.io/tsne/) - papiers et impl√©mentations.\n",
    "\n",
    "üî¨ [**‚ÄúAccelerating t-SNE using Tree-Based Algorithms‚Äù**](https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf) van der Maaten, L.J.P.; Journal of Machine Learning Research 15(Oct):3221-3245, 2014.\n",
    "\n",
    "üî¨ [**‚ÄúAutomated optimized parameters for T-distributed stochastic neighbor embedding improve visualization and analysis of large datasets‚Äù**](https://www.nature.com/articles/s41467-019-13055-y.pdf) Belkina, A.C., Ciccolella, C.O., Anno, R., Halpert, R., Spidlen, J., Snyder-Cappione, J.E., Nature Communications 10, 5415 (2019)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='tips-on-practical-use'></a> 2.2.10. Conseils pratiques\n",
    "\n",
    "- Assurez-vous d'utiliser la m√™me √©chelle pour toutes les caract√©ristiques. √âtant donn√© que les m√©thodes d'apprentissage de vari√©t√©s sont bas√©es sur une recherche de voisins les plus proches, l'algorithme peut mal se comporter autrement. Voir [**`StandardScaler`**](https://scikit-learn.org/stable/modules/preprocessing.html) pour des moyens pratiques de mettre √† l'√©chelle des donn√©es h√©t√©rog√®nes.\n",
    "\n",
    "- L'erreur de reconstruction calcul√©e par chaque routine peut √™tre utilis√©e pour choisir la dimension de sortie optimale. Pour une vari√©t√© de $d$ dimensions plong√©e dans un espace de param√®tres de $D$ dimensions, l'erreur de reconstruction diminuera √† mesure que `n_components` est augment√© jusqu'√† ce que `n_components == d`.\n",
    "\n",
    "- Notez que des donn√©es bruit√©es peuvent \"court-circuiter\" la vari√©t√©, agissant essentiellement comme un pont entre les parties de la vari√©t√© qui seraient autrement bien s√©par√©es. L'apprentissage de vari√©t√©s sur des donn√©es bruit√©es et/ou incompl√®tes est un domaine de recherche actif.\n",
    "\n",
    "- Certaines configurations d'entr√©e peuvent conduire √† des matrices de poids singuli√®res, par exemple lorsque plus de deux points dans l'ensemble de donn√©es sont identiques, ou lorsque les donn√©es sont divis√©es en groupes disjoints. Dans ce cas, `solver='arpack'` ne parviendra pas √† trouver l'espace nul. Le moyen le plus simple de r√©soudre ce probl√®me est d'utiliser `solver='dense'`, qui fonctionnera sur une matrice singuli√®re, bien qu'il puisse √™tre tr√®s lent selon le nombre de points d'entr√©e. Alternativement, on peut essayer de comprendre la source de la singularit√© : si elle est due √† des ensembles disjoints, augmenter `n_neighbors` peut aider. Si elle est due √† des points identiques dans l'ensemble de donn√©es, les supprimer peut aider.\n",
    "\n",
    "### Voir √©galement\n",
    "\n",
    "[**Le plongement par arbres totalement al√©atoires** (1.11.2.6)](https://scikit-learn.org/stable/modules/ensemble.html#random-trees-embedding) peut √©galement √™tre utile pour d√©river des repr√©sentations non lin√©aires de l'espace de caract√©ristiques, bien qu'elle ne r√©duise pas la dimensionnalit√©."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9aff9e50adfaa9e30c910fb3872ffdc72747acb5f50803ca0504f00e980f7c25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
