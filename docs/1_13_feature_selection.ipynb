{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='supervised-learning'></a> 1. [**Apprentissage supervisé**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_supervised_learning.ipynb#supervised-learning)</br>([*Supervised learning*](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning))\n",
    "\n",
    "# <a id='feature-selection'></a> 1.13. [**Sélection de caractéristiques**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_13_feature_selection.ipynb)<br/>([_Feature selection_](https://scikit-learn.org/stable/modules/feature_selection.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 6 pages, 9 exemples, 2 papiers\n",
    "- 1.13.1. [**Suppression de caractéristiques à faible variance**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_13_feature_selection.ipynb#removing-features-with-low-variance)<br/>([_Removing features with low variance_](https://scikit-learn.org/stable/modules/feature_selection.html#removing-features-with-low-variance))\n",
    "- 1.13.2. [**Sélection de caractéristiques univariées**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_13_feature_selection.ipynb#univariate-feature-selection)<br/>([_Univariate feature selection_](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection))\n",
    "- 1.13.3. [**Élimination récursive de caractéristiques**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_13_feature_selection.ipynb#recursive-feature-elimination)<br/>([_Recursive feature elimination_](https://scikit-learn.org/stable/modules/feature_selection.html#recursive-feature-elimination))\n",
    "- 1.13.4. [**Sélection de caractéristiques avec `SelectFromModel`**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_13_feature_selection.ipynb#feature-selection-using-selectfrommodel)<br/>([_Feature selection using `SelectFromModel`_](https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection-using-selectfrommodel))\n",
    "    - 1.13.4.1. [**Sélection de caractéristiques basées sur $\\ell_1$**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_13_feature_selection.ipynb#l1-based-feature-selection)<br/>([_L1-based feature selection_](https://scikit-learn.org/stable/modules/feature_selection.html#l1-based-feature-selection))\n",
    "    - 1.13.4.2. [**Sélection de caractéristiques basées sur les arbres**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_13_feature_selection.ipynb#tree-based-feature-selection)<br/>([_Tree-based feature selection_](https://scikit-learn.org/stable/modules/feature_selection.html#tree-based-feature-selection))\n",
    "- 1.13.5. [**Sélection séquentielle de caractéristiques**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_13_feature_selection.ipynb#sequential-feature-selection)<br/>([_Sequential Feature Selection_](https://scikit-learn.org/stable/modules/feature_selection.html#sequential-feature-selection))\n",
    "- 1.13.6. [**Sélection de caractéristiques dans le cadre d'un pipeline**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_13_feature_selection.ipynb#feature-selection-as-part-of-a-pipeline)<br/>([_Feature selection as part of a pipeline_](https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection-as-part-of-a-pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='feature-selection'></a> 1.13. **Sélection de caractéristiques**<br/>([_Feature selection_](https://scikit-learn.org/stable/modules/feature_selection.html))\n",
    "\n",
    "Les classes du module [**`sklearn.feature_selection`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) peuvent être utilisées pour la sélection de caractéristiques/réduction de dimensionnalité sur des ensembles d'échantillons, que ce soit pour améliorer les scores de exactitude des estimateurs ou pour renforcer leurs performances sur des ensembles de données de très grande dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='removing-features-with-low-variance'></a> 1.13.1. **Suppression de caractéristiques à faible variance**<br/>([_Removing features with low variance_](https://scikit-learn.org/stable/modules/feature_selection.html#removing-features-with-low-variance))\n",
    "\n",
    "[**`VarianceThreshold`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold) est une base de référence pour la sélection de caractéristiques. Elle supprime toutes les caractéristiques dont la variance ne satisfait pas un certain seuil. Par défaut, elle supprime toutes les caractéristiques de variance nulle, c'est-à-dire les caractéristiques ayant la même valeur dans tous les échantillons.\n",
    "\n",
    "À titre d'exemple, supposons que nous ayons un jeu de données avec des caractéristiques booléennes, et nous souhaitons supprimer toutes les caractéristiques qui sont soit un ou zéro (activées ou désactivées) dans plus de 80% des échantillons. Les caractéristiques booléennes sont des variables aléatoires de Bernoulli, et la variance de telles variables est donnée par\n",
    "\n",
    "$$\\mathrm{Var}[X] = p(1 - p)$$\n",
    "\n",
    "nous pouvons donc effectuer la sélection en utilisant le seuil `.8 * (1 - .8)` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 0],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [1, 0],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "sel.fit_transform(X)\n",
    "# array([[0, 1],\n",
    "#        [1, 0],\n",
    "#        [0, 0],\n",
    "#        [1, 1],\n",
    "#        [1, 0],\n",
    "#        [1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme prévu, [**`VarianceThreshold`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold) a supprimé la première colonne, qui a une probabilité $p = 5/6 > .8$ de contenir un zéro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='univariate-feature-selection'></a> 1.13.2. **Sélection de caractéristiques univariées**<br/>([_Univariate feature selection_](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection))\n",
    "\n",
    "La sélection de caractéristiques univariées fonctionne en sélectionnant les meilleures caractéristiques en fonction de tests statistiques univariés. Elle peut être considérée comme une étape de prétraitement pour un estimateur. Scikit-learn expose les routines de sélection de caractéristiques en tant qu'objets qui implémentent la méthode `transform` :\n",
    "- [**`SelectKBest`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) supprime toutes les caractéristiques sauf les $k$ caractéristiques ayant les scores les plus élevés.\n",
    "- [**`SelectPercentile`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile) supprime toutes les caractéristiques sauf un pourcentage de caractéristiques ayant les scores les plus élevés, spécifié par l'utilisateur.\n",
    "- en utilisant des tests statistiques univariés courants pour chaque caractéristique : taux de faux positifs [**`SelectFpr`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFpr.html#sklearn.feature_selection.SelectFpr), taux de fausses découvertes [**`SelectFdr`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFdr.html#sklearn.feature_selection.SelectFdr), ou erreur globale [**`SelectFwe`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFwe.html#sklearn.feature_selection.SelectFwe).\n",
    "- [**`GenericUnivariateSelect`**]() permet d'effectuer la sélection de caractéristiques univariées avec une stratégie configurable. Cela permet de sélectionner la meilleure stratégie de sélection univariée avec un estimateur de recherche d'hyperparamètres.\n",
    "\n",
    "Par exemple, nous pouvons utiliser un test F pour récupérer les deux meilleures caractéristiques pour un jeu de données de la manière suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X.shape\n",
    "# (150, 4)\n",
    "X_new = SelectKBest(f_classif, k=2).fit_transform(X, y)\n",
    "X_new.shape\n",
    "# (150, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces objets prennent en entrée une fonction d'évaluation qui renvoie des scores univariés et des p-valeurs (ou seulement des scores pour [**`SelectKBest`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) et [**`SelectPercentile`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile)) :\n",
    "- Pour la régression : [**`r_regression`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.r_regression.html#sklearn.feature_selection.r_regression), [**`f_regression`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html#sklearn.feature_selection.f_regression), [**`mutual_info_regression`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression)\n",
    "- Pour la classification : [**`chi2`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2), [**`f_classif`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif), [**`mutual_info_classif`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif)\n",
    "\n",
    "Les méthodes basées sur le test F estiment le degré de dépendance linéaire entre deux variables aléatoires. En revanche, les méthodes d'information mutuelle peuvent capturer tout type de dépendance statistique, mais comme méthodes non paramétriques, elles nécessitent plus d'échantillons pour une estimation précise. Notez que le test du $\\chi^2$ ne doit être appliqué qu'aux caractéristiques non négatives, telles que les fréquences.\n",
    "\n",
    "**Sélection de caractéristiques avec des données éparses**\n",
    "\n",
    "Si vous utilisez des données creuses (c'est-à-dire des données représentées sous forme de matrices creuses), [**`chi2`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2), [**`mutual_info_regression`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression), [**`mutual_info_classif`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif) géreront les données sans les rendre denses.\n",
    "\n",
    "> **Attention :** Veillez à ne pas utiliser une fonction d'évaluation de régression avec un problème de classification, vous obtiendriez des résultats inutiles.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Sélection de caractéristiques univariées**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_13_feature_selection/plot_feature_selection.ipynb)<br/>([_Univariate Feature Selection_](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_feature_selection.html))\n",
    "\n",
    "#### [**Comparaison entre le test F et l'information mutuelle**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_13_feature_selection/plot_f_test_vs_mi.ipynb)<br/>([_Comparison of F-test and mutual information_](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_f_test_vs_mi.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='recursive-feature-elimination'></a> 1.13.3. **Élimination récursive de caractéristiques**<br/>([_Recursive feature elimination_](https://scikit-learn.org/stable/modules/feature_selection.html#recursive-feature-elimination))\n",
    "\n",
    "Étant donné un estimateur externe qui attribue des poids aux caractéristiques (par exemple, les coefficients d'un modèle linéaire), l'objectif de l'élimination récursive de caractéristiques ([**`RFE`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE)) est de sélectionner les caractéristiques en considérant de manière récursive des ensembles de caractéristiques de plus en plus petits. Tout d'abord, l'estimateur est entraîné sur l'ensemble initial de caractéristiques et l'importance de chaque caractéristique est obtenue soit par le biais d'un attribut spécifique (tel que `coef_`, `feature_importances_`) ou d'un `callable`. Ensuite, les caractéristiques les moins importantes sont élaguées de l'ensemble actuel de caractéristiques. Cette procédure est répétée de manière récursive sur l'ensemble élagué jusqu'à ce que le nombre souhaité de caractéristiques à sélectionner soit finalement atteint.\n",
    "\n",
    "[**`RFECV`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV) effectue une élimination récursive de caractéristiques dans une boucle de validation croisée pour trouver le nombre optimal de caractéristiques.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Élimination récursive de caractéristiques (RFE)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_13_feature_selection/plot_rfe_digits.ipynb)<br/>([_Recursive feature elimination_](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_digits.html))\n",
    "\n",
    "Un exemple d'élimination récursive de caractéristiques montrant la pertinence des pixels dans une tâche de classification de chiffres.\n",
    "\n",
    "#### [**Élimination récursive des caractéristiques (RFE) avec validation croisée (CV)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_13_feature_selection/plot_rfe_with_cross_validation.ipynb)<br/>([_Recursive Feature Elimination (RFE) with cross-validation_](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html))\n",
    "\n",
    "Un exemple d'élimination récursive de caractéristiques avec un réglage automatique du nombre de caractéristiques sélectionnées avec validation croisée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='feature-selection-using-selectfrommodel'></a> 1.13.4. **Sélection de caractéristiques avec `SelectFromModel`**<br/>([_Feature selection using `SelectFromModel`_](https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection-using-selectfrommodel))\n",
    "\n",
    "[**`SelectFromModel`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel) est un méta-transformateur qui peut être utilisé aux côtés de n'importe quel estimateur attribuant de l'importance à chaque caractéristique grâce à un attribut spécifique (tel que `coef_`, `feature_importances_`) ou via un appel à `importance_getter` après l'ajustement. Les caractéristiques sont considérées comme non importantes et supprimées si l'importance correspondante des valeurs des caractéristiques est inférieure au paramètre `threshold` fourni. En plus de spécifier le seuil numériquement, il existe des heuristiques intégrées pour trouver un seuil en utilisant un argument de type chaîne. Les heuristiques disponibles sont `\"mean\"`, `\"median\"` et des multiples flottants de celles-ci comme `\"0.1*mean\"`. En combinaison avec les critères du paramètre `threshold`, on peut utiliser le paramètre `max_features` pour limiter le nombre de caractéristiques à sélectionner.\n",
    "\n",
    "Pour des exemples sur la manière de l'utiliser, veuillez vous référer aux sections ci-dessous.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Sélection de caractéristiques séquentielle basée sur le modèle**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_13_feature_selection/plot_select_from_model_diabetes.ipynb)<br/>([_Model-based and sequential feature selection_](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_select_from_model_diabetes.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='l1-based-feature-selection'></a> 1.13.4.1. **Sélection de caractéristiques basées sur $\\ell_1$**<br/>([_L1-based feature selection_](https://scikit-learn.org/stable/modules/feature_selection.html#l1-based-feature-selection))\n",
    "\n",
    "Les [**modèles linéaires** (1.1)](https://scikit-learn.org/stable/modules/linear_model.html#linear-model) pénalisés avec la norme $\\ell_1$ ont des solutions parcimonieuses : beaucoup de leurs coefficients estimés sont nuls. Lorsque l'objectif est de réduire la dimensionnalité des données à utiliser avec un autre classifieur, ils peuvent être utilisés en conjonction avec [**`SelectFromModel`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel) pour sélectionner les coefficients non nuls. En particulier, les estimateurs parcimonieux utiles à cette fin sont [**`Lasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso) pour la régression, et [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) et [**`LinearSVC`**](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) pour la classification :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X.shape\n",
    "# (150, 4)\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "X_new.shape\n",
    "# (150, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec les SVM et la régression logistique, le paramètre `C` contrôle la parcimonie : plus `C` est petit, moins de caractéristiques sont sélectionnées. Avec le Lasso, plus le paramètre `alpha` est élevé, moins de caractéristiques sont sélectionnées.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### Lasso sur des données denses et creuses\n",
    "\n",
    "#### Récupération $\\ell_1$ et compression de données\n",
    "\n",
    "Pour un bon choix d'`alpha`, le [**Lasso** (1.1.3)](https://scikit-learn.org/stable/modules/linear_model.html#lasso) peut récupérer entièrement l'ensemble exact des variables non nulles en utilisant seulement quelques observations, à condition que certaines conditions spécifiques soient remplies. En particulier, le nombre d'échantillons doit être \"suffisamment grand\", sinon les modèles $\\ell_1$ auront un comportement aléatoire, où \"suffisamment grand\" dépend du nombre de coefficients non nuls, du logarithme du nombre de caractéristiques, de la quantité de bruit, de la plus petite valeur absolue des coefficients non nuls, et de la structure de la matrice de conception `X`. De plus, la matrice de conception doit afficher certaines propriétés spécifiques, telles que le fait de ne pas être trop corrélée.\n",
    "\n",
    "Il n'y a pas de règle générale pour sélectionner un paramètre `alpha` pour la récupération des coefficients non nuls. Il peut être défini par validation croisée ([**`LassoCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV) ou [**`LassoLarsCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsCV.html#sklearn.linear_model.LassoLarsCV)), bien que cela puisse conduire à des modèles sous-pénalisés : inclure un petit nombre de variables non pertinentes n'est pas préjudiciable au score de prédiction. À l'opposé, le BIC ([**`LassoLarsIC`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsIC.html#sklearn.linear_model.LassoLarsIC)) tend à fixer des valeurs élevées pour `alpha`.\n",
    "\n",
    "##### Reference\n",
    "\n",
    "🔬 Richard G. Baraniuk [**“Compressive Sensing”**](http://users.isr.ist.utl.pt/~aguiar/CS_notes.pdf), IEEE Signal Processing Magazine [120] July 2007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='tree-based-feature-selection'></a> 1.13.4.2. **Sélection de caractéristiques basées sur les arbres**<br/>([_Tree-based feature selection_](https://scikit-learn.org/stable/modules/feature_selection.html#tree-based-feature-selection))\n",
    "\n",
    "Les estimateurs basés sur les arbres (consultez le module [**`sklearn.tree`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree) et la forêt d'arbres dans le module [**`sklearn.ensemble`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)) peuvent être utilisés pour calculer les importances des caractéristiques basées sur l'impureté, ce qui peut ensuite être utilisé pour éliminer les caractéristiques non pertinentes (lorsqu'elles sont associées au méta-transformateur [**`SelectFromModel`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X.shape\n",
    "# (150, 4)\n",
    "clf = ExtraTreesClassifier(n_estimators=50)\n",
    "clf = clf.fit(X, y)\n",
    "clf.feature_importances_  \n",
    "# array([ 0.04...,  0.05...,  0.4...,  0.4...])\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "X_new.shape               \n",
    "# (150, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemples\n",
    "\n",
    "##### [**Importances des caractéristiques avec une forêt d'arbres**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_forest_importances.ipynb)<br/>([*Feature importances with a forest of trees*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html))\n",
    "\n",
    "##### [**Importances des pixels avec une forêt parallèle d'arbres**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_forest_importances_faces.ipynb)<br/>([*Pixel importances with a parallel forest of trees*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances_faces.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='sequential-feature-selection'></a> 1.13.5. **Sélection séquentielle de caractéristiques**<br/>([_Sequential Feature Selection_](https://scikit-learn.org/stable/modules/feature_selection.html#sequential-feature-selection))\n",
    "\n",
    "La Sélection Séquentielle de Caractéristiques [SFS] est disponible dans le transformateur [**`SequentialFeatureSelector`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html#sklearn.feature_selection.SequentialFeatureSelector). La SFS peut être effectué soit vers l'avant, soit vers l'arrière.\n",
    "\n",
    "La SFS avant est une procédure gourmande qui recherche de manière itérative la meilleure nouvelle caractéristique à ajouter à l'ensemble des caractéristiques sélectionnées. Concrètement, nous démarrons avec zéro caractéristiques et trouvons la caractéristique qui maximise un score en validation croisée lorsqu'un estimateur est formé sur cette unique caractéristique. Une fois que cette première caractéristique est sélectionnée, nous répétons la procédure en ajoutant une nouvelle caractéristique à l'ensemble des caractéristiques sélectionnées. La procédure s'arrête lorsque le nombre souhaité de caractéristiques sélectionnées est atteint, comme fixé par le paramètre `n_features_to_select`.\n",
    "\n",
    "La SFS arrière suit la même idée mais fonctionne dans la direction opposée : au lieu de commencer sans aucune caractéristique et d'ajouter _goulûment_ des caractéristiques, nous commençons avec _toutes_ les caractéristiques et retirons goulûment des caractéristiques de l'ensemble. Le paramètre `direction` contrôle si la SFS en avant ou en arrière est utilisée.\n",
    "\n",
    "### Détails sur la Sélection Séquentielle de Caractéristiques\n",
    "\n",
    "En général, la sélection avant et arrière ne donnent pas des résultats équivalents. De plus, l'une peut être beaucoup plus rapide que l'autre en fonction du nombre demandé de caractéristiques sélectionnées : si nous avons 10 caractéristiques et que nous demandons 7 caractéristiques sélectionnées, la sélection avant nécessiterait 7 itérations, tandis que 3 suffisent à la sélection arrière.\n",
    "\n",
    "La SFS diffère de [**`RFE`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE) et [**`SelectFromModel`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel) en ce qu'elle n'exige pas que le modèle sous-jacent expose un attribut `coef_` ou `feature_importances_`. Elle peut cependant être plus lente compte tenu du fait que plus de modèles doivent être évalués, par rapport aux autres approches. Par exemple, dans la sélection arrière, l'itération allant de `m` caractéristiques à `m - 1` caractéristiques en utilisant une validation croisée k-fold nécessite le montage de `m * k` modèles, tandis que [**`RFE`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE) n'aurait besoin que d'un seul ajustement, et [**`SelectFromModel`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel) ne nécessite qu'un seul ajustement et ne nécessite aucune itération.\n",
    "\n",
    "#### Référence\n",
    "\n",
    "[sfs] Ferri et al, [**“Comparative study of techniques for large-scale feature selection”**](http://www.cse.msu.edu/~rossarun/courses/sp15/cse802/papers/FerriFeatureSelection_PR1994.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='feature-selection-as-part-of-a-pipeline'></a> 1.13.6. **Sélection de caractéristiques dans le cadre d'un pipeline**<br/>([_Feature selection as part of a pipeline_](https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection-as-part-of-a-pipeline))\n",
    "\n",
    "La sélection de caractéristiques est généralement utilisée comme une étape de prétraitement avant l'apprentissage proprement dit. Pour le faire avec scikit-learn, il est recommandé d'utiliser un [**`Pipeline`**](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;feature_selection&#x27;,\n",
       "                 SelectFromModel(estimator=LinearSVC(dual=&#x27;auto&#x27;,\n",
       "                                                     penalty=&#x27;l1&#x27;))),\n",
       "                (&#x27;classification&#x27;, RandomForestClassifier())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;feature_selection&#x27;,\n",
       "                 SelectFromModel(estimator=LinearSVC(dual=&#x27;auto&#x27;,\n",
       "                                                     penalty=&#x27;l1&#x27;))),\n",
       "                (&#x27;classification&#x27;, RandomForestClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">feature_selection: SelectFromModel</label><div class=\"sk-toggleable__content\"><pre>SelectFromModel(estimator=LinearSVC(dual=&#x27;auto&#x27;, penalty=&#x27;l1&#x27;))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(dual=&#x27;auto&#x27;, penalty=&#x27;l1&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(dual=&#x27;auto&#x27;, penalty=&#x27;l1&#x27;)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('feature_selection',\n",
       "                 SelectFromModel(estimator=LinearSVC(dual='auto',\n",
       "                                                     penalty='l1'))),\n",
       "                ('classification', RandomForestClassifier())])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = Pipeline([\n",
    "    ('feature_selection', SelectFromModel(LinearSVC(dual=\"auto\", penalty=\"l1\"))),\n",
    "    ('classification', RandomForestClassifier())\n",
    "])\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cet extrait de code, nous utilisons un [**`LinearSVC`**](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) couplé à [**`SelectFromModel`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel) pour évaluer l'importance des caractéristiques et sélectionner les caractéristiques les plus pertinentes. Ensuite, un [**`RandomForestClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) est entraîné sur la sortie transformée, c'est-à-dire en n'utilisant que les caractéristiques pertinentes. Vous pouvez effectuer des opérations similaires avec les autres méthodes de sélection de caractéristiques et également avec des classifieurs qui fournissent une manière d'évaluer l'importance des caractéristiques. Consultez les exemples de [**`Pipeline`**](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) pour plus de détails."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
