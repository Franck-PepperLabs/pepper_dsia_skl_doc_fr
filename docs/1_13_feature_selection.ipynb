{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='supervised-learning'></a> 1. [**Apprentissage supervis√©**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_supervised_learning.ipynb#supervised-learning)</br>([*Supervised learning*](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning))\n",
    "\n",
    "# <a id='feature-selection'></a> 1.13. [**S√©lection de caract√©ristiques**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_13_feature_selection.ipynb)<br/>([_Feature selection_](https://scikit-learn.org/stable/modules/feature_selection.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 6 pages, 9 exemples, 2 papiers\n",
    "- 1.13.1. [**Suppression de caract√©ristiques √† faible variance**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_13_feature_selection.ipynb#removing-features-with-low-variance)<br/>([_Removing features with low variance_](https://scikit-learn.org/stable/modules/feature_selection.html#removing-features-with-low-variance))\n",
    "- 1.13.2. [**S√©lection de caract√©ristiques univari√©es**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_13_feature_selection.ipynb#univariate-feature-selection)<br/>([_Univariate feature selection_](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection))\n",
    "- 1.13.3. [**√âlimination r√©cursive de caract√©ristiques**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_13_feature_selection.ipynb#recursive-feature-elimination)<br/>([_Recursive feature elimination_](https://scikit-learn.org/stable/modules/feature_selection.html#recursive-feature-elimination))\n",
    "- 1.13.4. [**S√©lection de caract√©ristiques avec `SelectFromModel`**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_13_feature_selection.ipynb#feature-selection-using-selectfrommodel)<br/>([_Feature selection using `SelectFromModel`_](https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection-using-selectfrommodel))\n",
    "    - 1.13.4.1. [**S√©lection de caract√©ristiques bas√©es sur $\\ell_1$**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_13_feature_selection.ipynb#l1-based-feature-selection)<br/>([_L1-based feature selection_](https://scikit-learn.org/stable/modules/feature_selection.html#l1-based-feature-selection))\n",
    "    - 1.13.4.2. [**S√©lection de caract√©ristiques bas√©es sur les arbres**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_13_feature_selection.ipynb#tree-based-feature-selection)<br/>([_Tree-based feature selection_](https://scikit-learn.org/stable/modules/feature_selection.html#tree-based-feature-selection))\n",
    "- 1.13.5. [**S√©lection s√©quentielle de caract√©ristiques**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_13_feature_selection.ipynb#sequential-feature-selection)<br/>([_Sequential Feature Selection_](https://scikit-learn.org/stable/modules/feature_selection.html#sequential-feature-selection))\n",
    "- 1.13.6. [**S√©lection de caract√©ristiques dans le cadre d'un pipeline**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_13_feature_selection.ipynb#feature-selection-as-part-of-a-pipeline)<br/>([_Feature selection as part of a pipeline_](https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection-as-part-of-a-pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='feature-selection'></a> 1.13. **S√©lection de caract√©ristiques**<br/>([_Feature selection_](https://scikit-learn.org/stable/modules/feature_selection.html))\n",
    "\n",
    "Les classes du module [**`sklearn.feature_selection`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) peuvent √™tre utilis√©es pour la s√©lection de caract√©ristiques/r√©duction de dimensionnalit√© sur des ensembles d'√©chantillons, que ce soit pour am√©liorer les scores de exactitude des estimateurs ou pour renforcer leurs performances sur des ensembles de donn√©es de tr√®s grande dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='removing-features-with-low-variance'></a> 1.13.1. **Suppression de caract√©ristiques √† faible variance**<br/>([_Removing features with low variance_](https://scikit-learn.org/stable/modules/feature_selection.html#removing-features-with-low-variance))\n",
    "\n",
    "[**`VarianceThreshold`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold) est une base de r√©f√©rence pour la s√©lection de caract√©ristiques. Elle supprime toutes les caract√©ristiques dont la variance ne satisfait pas un certain seuil. Par d√©faut, elle supprime toutes les caract√©ristiques de variance nulle, c'est-√†-dire les caract√©ristiques ayant la m√™me valeur dans tous les √©chantillons.\n",
    "\n",
    "√Ä titre d'exemple, supposons que nous ayons un jeu de donn√©es avec des caract√©ristiques bool√©ennes, et nous souhaitons supprimer toutes les caract√©ristiques qui sont soit un ou z√©ro (activ√©es ou d√©sactiv√©es) dans plus de 80% des √©chantillons. Les caract√©ristiques bool√©ennes sont des variables al√©atoires de Bernoulli, et la variance de telles variables est donn√©e par\n",
    "\n",
    "$$\\mathrm{Var}[X] = p(1 - p)$$\n",
    "\n",
    "nous pouvons donc effectuer la s√©lection en utilisant le seuil `.8 * (1 - .8)` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 0],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [1, 0],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "sel.fit_transform(X)\n",
    "# array([[0, 1],\n",
    "#        [1, 0],\n",
    "#        [0, 0],\n",
    "#        [1, 1],\n",
    "#        [1, 0],\n",
    "#        [1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme pr√©vu, [**`VarianceThreshold`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold) a supprim√© la premi√®re colonne, qui a une probabilit√© $p = 5/6 > .8$ de contenir un z√©ro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='univariate-feature-selection'></a> 1.13.2. **S√©lection de caract√©ristiques univari√©es**<br/>([_Univariate feature selection_](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection))\n",
    "\n",
    "La s√©lection de caract√©ristiques univari√©es fonctionne en s√©lectionnant les meilleures caract√©ristiques en fonction de tests statistiques univari√©s. Elle peut √™tre consid√©r√©e comme une √©tape de pr√©traitement pour un estimateur. Scikit-learn expose les routines de s√©lection de caract√©ristiques en tant qu'objets qui impl√©mentent la m√©thode `transform` :\n",
    "- [**`SelectKBest`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) supprime toutes les caract√©ristiques sauf les $k$ caract√©ristiques ayant les scores les plus √©lev√©s.\n",
    "- [**`SelectPercentile`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile) supprime toutes les caract√©ristiques sauf un pourcentage de caract√©ristiques ayant les scores les plus √©lev√©s, sp√©cifi√© par l'utilisateur.\n",
    "- en utilisant des tests statistiques univari√©s courants pour chaque caract√©ristique : taux de faux positifs [**`SelectFpr`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFpr.html#sklearn.feature_selection.SelectFpr), taux de fausses d√©couvertes [**`SelectFdr`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFdr.html#sklearn.feature_selection.SelectFdr), ou erreur globale [**`SelectFwe`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFwe.html#sklearn.feature_selection.SelectFwe).\n",
    "- [**`GenericUnivariateSelect`**]() permet d'effectuer la s√©lection de caract√©ristiques univari√©es avec une strat√©gie configurable. Cela permet de s√©lectionner la meilleure strat√©gie de s√©lection univari√©e avec un estimateur de recherche d'hyperparam√®tres.\n",
    "\n",
    "Par exemple, nous pouvons utiliser un test F pour r√©cup√©rer les deux meilleures caract√©ristiques pour un jeu de donn√©es de la mani√®re suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X.shape\n",
    "# (150, 4)\n",
    "X_new = SelectKBest(f_classif, k=2).fit_transform(X, y)\n",
    "X_new.shape\n",
    "# (150, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces objets prennent en entr√©e une fonction d'√©valuation qui renvoie des scores univari√©s et des p-valeurs (ou seulement des scores pour [**`SelectKBest`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) et [**`SelectPercentile`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile)) :\n",
    "- Pour la r√©gression : [**`r_regression`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.r_regression.html#sklearn.feature_selection.r_regression), [**`f_regression`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html#sklearn.feature_selection.f_regression), [**`mutual_info_regression`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression)\n",
    "- Pour la classification : [**`chi2`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2), [**`f_classif`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif), [**`mutual_info_classif`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif)\n",
    "\n",
    "Les m√©thodes bas√©es sur le test F estiment le degr√© de d√©pendance lin√©aire entre deux variables al√©atoires. En revanche, les m√©thodes d'information mutuelle peuvent capturer tout type de d√©pendance statistique, mais comme m√©thodes non param√©triques, elles n√©cessitent plus d'√©chantillons pour une estimation pr√©cise. Notez que le test du $\\chi^2$ ne doit √™tre appliqu√© qu'aux caract√©ristiques non n√©gatives, telles que les fr√©quences.\n",
    "\n",
    "**S√©lection de caract√©ristiques avec des donn√©es √©parses**\n",
    "\n",
    "Si vous utilisez des donn√©es creuses (c'est-√†-dire des donn√©es repr√©sent√©es sous forme de matrices creuses), [**`chi2`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2), [**`mutual_info_regression`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression), [**`mutual_info_classif`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif) g√©reront les donn√©es sans les rendre denses.\n",
    "\n",
    "> **Attention :** Veillez √† ne pas utiliser une fonction d'√©valuation de r√©gression avec un probl√®me de classification, vous obtiendriez des r√©sultats inutiles.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**S√©lection de caract√©ristiques univari√©es**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_13_feature_selection/plot_feature_selection.ipynb)<br/>([_Univariate Feature Selection_](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_feature_selection.html))\n",
    "\n",
    "#### [**Comparaison entre le test F et l'information mutuelle**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_13_feature_selection/plot_f_test_vs_mi.ipynb)<br/>([_Comparison of F-test and mutual information_](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_f_test_vs_mi.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='recursive-feature-elimination'></a> 1.13.3. **√âlimination r√©cursive de caract√©ristiques**<br/>([_Recursive feature elimination_](https://scikit-learn.org/stable/modules/feature_selection.html#recursive-feature-elimination))\n",
    "\n",
    "√âtant donn√© un estimateur externe qui attribue des poids aux caract√©ristiques (par exemple, les coefficients d'un mod√®le lin√©aire), l'objectif de l'√©limination r√©cursive de caract√©ristiques ([**`RFE`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE)) est de s√©lectionner les caract√©ristiques en consid√©rant de mani√®re r√©cursive des ensembles de caract√©ristiques de plus en plus petits. Tout d'abord, l'estimateur est entra√Æn√© sur l'ensemble initial de caract√©ristiques et l'importance de chaque caract√©ristique est obtenue soit par le biais d'un attribut sp√©cifique (tel que `coef_`, `feature_importances_`) ou d'un `callable`. Ensuite, les caract√©ristiques les moins importantes sont √©lagu√©es de l'ensemble actuel de caract√©ristiques. Cette proc√©dure est r√©p√©t√©e de mani√®re r√©cursive sur l'ensemble √©lagu√© jusqu'√† ce que le nombre souhait√© de caract√©ristiques √† s√©lectionner soit finalement atteint.\n",
    "\n",
    "[**`RFECV`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV) effectue une √©limination r√©cursive de caract√©ristiques dans une boucle de validation crois√©e pour trouver le nombre optimal de caract√©ristiques.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**√âlimination r√©cursive de caract√©ristiques (RFE)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_13_feature_selection/plot_rfe_digits.ipynb)<br/>([_Recursive feature elimination_](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_digits.html))\n",
    "\n",
    "Un exemple d'√©limination r√©cursive de caract√©ristiques montrant la pertinence des pixels dans une t√¢che de classification de chiffres.\n",
    "\n",
    "#### [**√âlimination r√©cursive des caract√©ristiques (RFE) avec validation crois√©e (CV)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_13_feature_selection/plot_rfe_with_cross_validation.ipynb)<br/>([_Recursive Feature Elimination (RFE) with cross-validation_](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html))\n",
    "\n",
    "Un exemple d'√©limination r√©cursive de caract√©ristiques avec un r√©glage automatique du nombre de caract√©ristiques s√©lectionn√©es avec validation crois√©e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='feature-selection-using-selectfrommodel'></a> 1.13.4. **S√©lection de caract√©ristiques avec `SelectFromModel`**<br/>([_Feature selection using `SelectFromModel`_](https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection-using-selectfrommodel))\n",
    "\n",
    "[**`SelectFromModel`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel) est un m√©ta-transformateur qui peut √™tre utilis√© aux c√¥t√©s de n'importe quel estimateur attribuant de l'importance √† chaque caract√©ristique gr√¢ce √† un attribut sp√©cifique (tel que `coef_`, `feature_importances_`) ou via un appel √† `importance_getter` apr√®s l'ajustement. Les caract√©ristiques sont consid√©r√©es comme non importantes et supprim√©es si l'importance correspondante des valeurs des caract√©ristiques est inf√©rieure au param√®tre `threshold` fourni. En plus de sp√©cifier le seuil num√©riquement, il existe des heuristiques int√©gr√©es pour trouver un seuil en utilisant un argument de type cha√Æne. Les heuristiques disponibles sont `\"mean\"`, `\"median\"` et des multiples flottants de celles-ci comme `\"0.1*mean\"`. En combinaison avec les crit√®res du param√®tre `threshold`, on peut utiliser le param√®tre `max_features` pour limiter le nombre de caract√©ristiques √† s√©lectionner.\n",
    "\n",
    "Pour des exemples sur la mani√®re de l'utiliser, veuillez vous r√©f√©rer aux sections ci-dessous.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**S√©lection de caract√©ristiques s√©quentielle bas√©e sur le mod√®le**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_13_feature_selection/plot_select_from_model_diabetes.ipynb)<br/>([_Model-based and sequential feature selection_](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_select_from_model_diabetes.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='l1-based-feature-selection'></a> 1.13.4.1. **S√©lection de caract√©ristiques bas√©es sur $\\ell_1$**<br/>([_L1-based feature selection_](https://scikit-learn.org/stable/modules/feature_selection.html#l1-based-feature-selection))\n",
    "\n",
    "Les [**mod√®les lin√©aires** (1.1)](https://scikit-learn.org/stable/modules/linear_model.html#linear-model) p√©nalis√©s avec la norme $\\ell_1$ ont des solutions parcimonieuses : beaucoup de leurs coefficients estim√©s sont nuls. Lorsque l'objectif est de r√©duire la dimensionnalit√© des donn√©es √† utiliser avec un autre classifieur, ils peuvent √™tre utilis√©s en conjonction avec [**`SelectFromModel`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel) pour s√©lectionner les coefficients non nuls. En particulier, les estimateurs parcimonieux utiles √† cette fin sont [**`Lasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso) pour la r√©gression, et [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) et [**`LinearSVC`**](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) pour la classification :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X.shape\n",
    "# (150, 4)\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "X_new.shape\n",
    "# (150, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec les SVM et la r√©gression logistique, le param√®tre `C` contr√¥le la parcimonie : plus `C` est petit, moins de caract√©ristiques sont s√©lectionn√©es. Avec le Lasso, plus le param√®tre `alpha` est √©lev√©, moins de caract√©ristiques sont s√©lectionn√©es.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### Lasso sur des donn√©es denses et creuses\n",
    "\n",
    "#### R√©cup√©ration $\\ell_1$ et compression de donn√©es\n",
    "\n",
    "Pour un bon choix d'`alpha`, le [**Lasso** (1.1.3)](https://scikit-learn.org/stable/modules/linear_model.html#lasso) peut r√©cup√©rer enti√®rement l'ensemble exact des variables non nulles en utilisant seulement quelques observations, √† condition que certaines conditions sp√©cifiques soient remplies. En particulier, le nombre d'√©chantillons doit √™tre \"suffisamment grand\", sinon les mod√®les $\\ell_1$ auront un comportement al√©atoire, o√π \"suffisamment grand\" d√©pend du nombre de coefficients non nuls, du logarithme du nombre de caract√©ristiques, de la quantit√© de bruit, de la plus petite valeur absolue des coefficients non nuls, et de la structure de la matrice de conception `X`. De plus, la matrice de conception doit afficher certaines propri√©t√©s sp√©cifiques, telles que le fait de ne pas √™tre trop corr√©l√©e.\n",
    "\n",
    "Il n'y a pas de r√®gle g√©n√©rale pour s√©lectionner un param√®tre `alpha` pour la r√©cup√©ration des coefficients non nuls. Il peut √™tre d√©fini par validation crois√©e ([**`LassoCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV) ou [**`LassoLarsCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsCV.html#sklearn.linear_model.LassoLarsCV)), bien que cela puisse conduire √† des mod√®les sous-p√©nalis√©s : inclure un petit nombre de variables non pertinentes n'est pas pr√©judiciable au score de pr√©diction. √Ä l'oppos√©, le BIC ([**`LassoLarsIC`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsIC.html#sklearn.linear_model.LassoLarsIC)) tend √† fixer des valeurs √©lev√©es pour `alpha`.\n",
    "\n",
    "##### Reference\n",
    "\n",
    "üî¨ Richard G. Baraniuk [**‚ÄúCompressive Sensing‚Äù**](http://users.isr.ist.utl.pt/~aguiar/CS_notes.pdf), IEEE Signal Processing Magazine [120] July 2007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='tree-based-feature-selection'></a> 1.13.4.2. **S√©lection de caract√©ristiques bas√©es sur les arbres**<br/>([_Tree-based feature selection_](https://scikit-learn.org/stable/modules/feature_selection.html#tree-based-feature-selection))\n",
    "\n",
    "Les estimateurs bas√©s sur les arbres (consultez le module [**`sklearn.tree`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree) et la for√™t d'arbres dans le module [**`sklearn.ensemble`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)) peuvent √™tre utilis√©s pour calculer les importances des caract√©ristiques bas√©es sur l'impuret√©, ce qui peut ensuite √™tre utilis√© pour √©liminer les caract√©ristiques non pertinentes (lorsqu'elles sont associ√©es au m√©ta-transformateur [**`SelectFromModel`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X.shape\n",
    "# (150, 4)\n",
    "clf = ExtraTreesClassifier(n_estimators=50)\n",
    "clf = clf.fit(X, y)\n",
    "clf.feature_importances_  \n",
    "# array([ 0.04...,  0.05...,  0.4...,  0.4...])\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "X_new.shape               \n",
    "# (150, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemples\n",
    "\n",
    "##### [**Importances des caract√©ristiques avec une for√™t d'arbres**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_forest_importances.ipynb)<br/>([*Feature importances with a forest of trees*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html))\n",
    "\n",
    "##### [**Importances des pixels avec une for√™t parall√®le d'arbres**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_forest_importances_faces.ipynb)<br/>([*Pixel importances with a parallel forest of trees*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances_faces.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='sequential-feature-selection'></a> 1.13.5. **S√©lection s√©quentielle de caract√©ristiques**<br/>([_Sequential Feature Selection_](https://scikit-learn.org/stable/modules/feature_selection.html#sequential-feature-selection))\n",
    "\n",
    "La S√©lection S√©quentielle de Caract√©ristiques [SFS] est disponible dans le transformateur [**`SequentialFeatureSelector`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html#sklearn.feature_selection.SequentialFeatureSelector). La SFS peut √™tre effectu√© soit vers l'avant, soit vers l'arri√®re.\n",
    "\n",
    "La SFS avant est une proc√©dure gourmande qui recherche de mani√®re it√©rative la meilleure nouvelle caract√©ristique √† ajouter √† l'ensemble des caract√©ristiques s√©lectionn√©es. Concr√®tement, nous d√©marrons avec z√©ro caract√©ristiques et trouvons la caract√©ristique qui maximise un score en validation crois√©e lorsqu'un estimateur est form√© sur cette unique caract√©ristique. Une fois que cette premi√®re caract√©ristique est s√©lectionn√©e, nous r√©p√©tons la proc√©dure en ajoutant une nouvelle caract√©ristique √† l'ensemble des caract√©ristiques s√©lectionn√©es. La proc√©dure s'arr√™te lorsque le nombre souhait√© de caract√©ristiques s√©lectionn√©es est atteint, comme fix√© par le param√®tre `n_features_to_select`.\n",
    "\n",
    "La SFS arri√®re suit la m√™me id√©e mais fonctionne dans la direction oppos√©e : au lieu de commencer sans aucune caract√©ristique et d'ajouter _goul√ªment_ des caract√©ristiques, nous commen√ßons avec _toutes_ les caract√©ristiques et retirons goul√ªment des caract√©ristiques de l'ensemble. Le param√®tre `direction` contr√¥le si la SFS en avant ou en arri√®re est utilis√©e.\n",
    "\n",
    "### D√©tails sur la S√©lection S√©quentielle de Caract√©ristiques\n",
    "\n",
    "En g√©n√©ral, la s√©lection avant et arri√®re ne donnent pas des r√©sultats √©quivalents. De plus, l'une peut √™tre beaucoup plus rapide que l'autre en fonction du nombre demand√© de caract√©ristiques s√©lectionn√©es : si nous avons 10 caract√©ristiques et que nous demandons 7 caract√©ristiques s√©lectionn√©es, la s√©lection avant n√©cessiterait 7 it√©rations, tandis que 3 suffisent √† la s√©lection arri√®re.\n",
    "\n",
    "La SFS diff√®re de [**`RFE`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE) et [**`SelectFromModel`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel) en ce qu'elle n'exige pas que le mod√®le sous-jacent expose un attribut `coef_` ou `feature_importances_`. Elle peut cependant √™tre plus lente compte tenu du fait que plus de mod√®les doivent √™tre √©valu√©s, par rapport aux autres approches. Par exemple, dans la s√©lection arri√®re, l'it√©ration allant de `m` caract√©ristiques √† `m - 1` caract√©ristiques en utilisant une validation crois√©e k-fold n√©cessite le montage de `m * k` mod√®les, tandis que [**`RFE`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE) n'aurait besoin que d'un seul ajustement, et [**`SelectFromModel`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel) ne n√©cessite qu'un seul ajustement et ne n√©cessite aucune it√©ration.\n",
    "\n",
    "#### R√©f√©rence\n",
    "\n",
    "[sfs] Ferri et al, [**‚ÄúComparative study of techniques for large-scale feature selection‚Äù**](http://www.cse.msu.edu/~rossarun/courses/sp15/cse802/papers/FerriFeatureSelection_PR1994.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='feature-selection-as-part-of-a-pipeline'></a> 1.13.6. **S√©lection de caract√©ristiques dans le cadre d'un pipeline**<br/>([_Feature selection as part of a pipeline_](https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection-as-part-of-a-pipeline))\n",
    "\n",
    "La s√©lection de caract√©ristiques est g√©n√©ralement utilis√©e comme une √©tape de pr√©traitement avant l'apprentissage proprement dit. Pour le faire avec scikit-learn, il est recommand√© d'utiliser un [**`Pipeline`**](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;feature_selection&#x27;,\n",
       "                 SelectFromModel(estimator=LinearSVC(dual=&#x27;auto&#x27;,\n",
       "                                                     penalty=&#x27;l1&#x27;))),\n",
       "                (&#x27;classification&#x27;, RandomForestClassifier())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;feature_selection&#x27;,\n",
       "                 SelectFromModel(estimator=LinearSVC(dual=&#x27;auto&#x27;,\n",
       "                                                     penalty=&#x27;l1&#x27;))),\n",
       "                (&#x27;classification&#x27;, RandomForestClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">feature_selection: SelectFromModel</label><div class=\"sk-toggleable__content\"><pre>SelectFromModel(estimator=LinearSVC(dual=&#x27;auto&#x27;, penalty=&#x27;l1&#x27;))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(dual=&#x27;auto&#x27;, penalty=&#x27;l1&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(dual=&#x27;auto&#x27;, penalty=&#x27;l1&#x27;)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('feature_selection',\n",
       "                 SelectFromModel(estimator=LinearSVC(dual='auto',\n",
       "                                                     penalty='l1'))),\n",
       "                ('classification', RandomForestClassifier())])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = Pipeline([\n",
    "    ('feature_selection', SelectFromModel(LinearSVC(dual=\"auto\", penalty=\"l1\"))),\n",
    "    ('classification', RandomForestClassifier())\n",
    "])\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cet extrait de code, nous utilisons un [**`LinearSVC`**](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) coupl√© √† [**`SelectFromModel`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel) pour √©valuer l'importance des caract√©ristiques et s√©lectionner les caract√©ristiques les plus pertinentes. Ensuite, un [**`RandomForestClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) est entra√Æn√© sur la sortie transform√©e, c'est-√†-dire en n'utilisant que les caract√©ristiques pertinentes. Vous pouvez effectuer des op√©rations similaires avec les autres m√©thodes de s√©lection de caract√©ristiques et √©galement avec des classifieurs qui fournissent une mani√®re d'√©valuer l'importance des caract√©ristiques. Consultez les exemples de [**`Pipeline`**](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) pour plus de d√©tails."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
