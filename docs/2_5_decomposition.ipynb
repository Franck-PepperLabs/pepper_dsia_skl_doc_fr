{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5. [**Décomposer des signaux en composantes (problèmes de factorisation matricielle)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_5_decomposition.ipynb)</br>([*Decomposing signals in components (matrix factorization problems)*](https://scikit-learn.org/stable/modules/decomposition.html))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='principal-component-analysis-pca'></a> 2.5.1 Analyse en composantes principales (ACP) (*Principal component analysis (PCA)*)\n",
    "\n",
    "## <a id='exact-pca-and-probabilistic-interpretation'></a> 2.5.1.1. ACP exacte et interprétation probabilistique\n",
    "\n",
    "[PCA]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "L'analyse en composantes principales (ACP) est utilisée pour décomposer un jeu de données multivarié en un ensemble de composantes orthogonales successives qui expliquent un maximum de variance. Dans scikit-learn, la [**`PCA`**][PCA] est implémentée sous forme d'objet *transformeur* qui apprend $n$ composantes dans sa méthode `fit` d'ajustement, et peut être utilisée sur de nouvelles données pour les projeter sur ces composantes.\n",
    "\n",
    "L'ACP centre les données d'entrée, mais ne les met pas à l'échelle pour chaque caractéristique avant d'appliquer la SVD. Le paramètre optionnel `whiten=True` permet de projeter les données sur l'espace singulier tout en mettant à l'échelle chaque composante à la variance unitaire. Cela est souvent utile si les modèles en aval font des hypothèses fortes sur l'isotropie du signal : c'est par exemple le cas pour les machines à vecteurs de support avec le noyau RBF et l'algorithme de clustering K-Means.\n",
    "\n",
    "Ci-dessous, un exemple de l'ensemble de données iris, qui comprend 4 caractéristiques, projeté sur les 2 dimensions qui expliquent la plus grande variance :\n",
    "\n",
    "<figure class=\"align-center\">\n",
    "<img alt=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_pca_vs_lda_001.png\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_pca_vs_lda_001.png\" style=\"width: 480.0px; height: 360.0px;\" />\n",
    "</figure>\n",
    "\n",
    "L'objet [**`PCA`**][PCA] fournit également une interprétation probabiliste de l'ACP qui peut donner une vraissemblance des données en fonction de la quantité de variance qu'elle explique. En tant que tel, il implémente une méthode de [score](https://scikit-learn.org/stable/glossary.html#term-score) qui peut être utilisée dans la validation croisée :\n",
    "\n",
    "<figure class=\"align-center\">\n",
    "<img alt=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_pca_vs_fa_model_selection_001.png\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_pca_vs_fa_model_selection_001.png\" style=\"width: 480.0px; height: 360.0px;\" />\n",
    "</figure>\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Comparaison des projections 2D LDA et PCA de l'ensemble de données Iris**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_5_decomposition/plot_pca_vs_lda.ipynb)<br/>([*Comparison of LDA and PCA 2D projection of Iris dataset*](https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_lda.html))\n",
    "\n",
    "#### [**Sélection du modèle avec PCA probabiliste et analyse factorielle (FA)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_5_decomposition/plot_pca_vs_fa_model_selection.ipynb)<br/>([*Model selection with Probabilistic PCA and Factor Analysis (FA)*](https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_fa_model_selection.html))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='incremental-pca'></a> 2.5.1.2 Analyse en Composantes Principales Incrémentale\n",
    "\n",
    "[PCA]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "[IncrementalPCA]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html\n",
    "\n",
    "L'objet [**`PCA`**][PCA] est très utile, mais présente certaines limitations pour les grandes ensembles de données. La plus grande limitation est que [**`PCA`**][PCA] ne prend en charge que le traitement par lots, ce qui signifie que toutes les données à traiter doivent tenir dans la mémoire principale. L'objet [**`IncrementalPCA`**][IncrementalPCA] utilise une forme de traitement différente et permet des calculs partiels qui correspondent presque exactement aux résultats de [**`PCA`**][PCA] tout en traitant les données en mini-lots. [**`IncrementalPCA`**][IncrementalPCA] rend possible la mise en œuvre de l'analyse en composantes principales en dehors de la mémoire centrale soit en:\n",
    "\n",
    "* utilisant sa méthode `partial_fit` sur des blocs de données récupérés séquentiellement depuis le disque dur local ou une base de données réseau.\n",
    "* en appelant sa méthode `fit` sur une matrice creuse ou un fichier mémoire mappé en utilisant `numpy.memmap`.\n",
    "\n",
    "[**`IncrementalPCA`**][IncrementalPCA] ne stocke que les estimations des variances des composantes et du bruit, pour mettre à jour de manière incrémentale `explained_variance_ratio_`. C'est pourquoi l'utilisation de la mémoire dépend du nombre d'échantillons par lot, plutôt que du nombre d'échantillons à traiter dans l'ensemble de données.\n",
    "\n",
    "Comme dans [**`PCA`**][PCA], [**`IncrementalPCA`**][IncrementalPCA] centre mais ne met pas à l'échelle les données d'entrée pour chaque caractéristique avant d'appliquer la SVD.\n",
    "\n",
    "<figure class=\"align-center\">\n",
    "<img alt=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_incremental_pca_001.png\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_incremental_pca_001.png\" style=\"width: 600.0px; height: 600.0px;\" />\n",
    "</figure>\n",
    "<figure class=\"align-center\">\n",
    "<img alt=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_incremental_pca_002.png\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_incremental_pca_002.png\" style=\"width: 600.0px; height: 600.0px;\" />\n",
    "</figure>\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**ACP incrémentale**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_5_decomposition/plot_incremental_pca.ipynb)<br/>([*Incremental PCA*](https://scikit-learn.org/stable/auto_examples/decomposition/plot_incremental_pca.html))\n",
    "\n",
    "\n",
    "#### [**Décompositions de jeux de données de visages**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_5_decomposition/plot_faces_decomposition.ipynb)<br/>([*Faces dataset decompositions*](https://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='pca-using-randomized-svd'></a>  2.5.1.3 Analyse en Composantes Principales utilisant SVD aléatoire\n",
    "\n",
    "[PCA]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "Il est souvent intéressant de projeter les données sur un espace de dimension inférieure qui préserve l'essentiel de la variance, en éliminant les vecteurs singuliers des composantes associées aux valeurs singulières inférieures.\n",
    "\n",
    "Par exemple, si nous travaillons avec des images en niveaux de gris de 64x64 pixels pour la reconnaissance faciale, la dimensionnalité des données est de 4096 et il est lent d'entraîner une machine à vecteurs de support RBF sur des données aussi larges. De plus, nous savons que la dimensionnalité intrinsèque des données est beaucoup plus faible que 4096, car toutes les images de visages humains se ressemblent d'une certaine manière. Les échantillons se trouvent sur une variété de dimension beaucoup plus faible (disons environ 200 par exemple). L'algorithme PCA peut être utilisé pour transformer linéairement les données tout en réduisant la dimensionnalité et en préservant la plupart de la variance expliquée en même temps.\n",
    "\n",
    "La classe [**`PCA`**][PCA] utilisée avec le paramètre optionnel `svd_solver='randomized'` est très utile dans ce cas: étant donné que nous allons éliminer la plupart des vecteurs singuliers, il est beaucoup plus efficace de limiter le calcul à une estimation approchée des vecteurs singuliers que nous allons conserver pour effectuer réellement la transformation.\n",
    "\n",
    "Par exemple, l'image suivante montre 16 portraits d'échantillons (centrés autour de 0,0) provenant de la base de données Olivetti. A droite se trouvent les 16 premiers vecteurs singuliers réorganisés en portraits. Comme nous ne nécessitons que les 16 premiers vecteurs singuliers d'un ensemble de données de taille $n_{\\text{samples}}=400$ et $n_{\\text{features}}=64 \\times 64 = 4096$, le temps de calcul est inférieur à 1s\n",
    "\n",
    "<img alt=\"orig_img\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_001.png\" style=\"width: 360.0px; height: 275.4px;\" />\n",
    "\n",
    "<img alt=\"pca_img\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_002.png\" style=\"width: 360.0px; height: 275.4px;\" />\n",
    "\n",
    "Si nous notons $n_{\\max} = \\max(n_{\\mathrm{samples}}, n_{\\mathrm{features}})$ et $n_{\\min} = \\min(n_{\\mathrm{samples}}, n_{\\mathrm{features}})$, la complexité temporelle de la [**`PCA`**][PCA] aléatoire est $O(n_{\\max}^2 \\cdot n_{\\mathrm{components}})$ au lieu de $O(n_{\\max}^2 \\cdot n_{\\min})$ pour la méthode exacte implémentée dans [**`PCA`**][PCA].\n",
    "\n",
    "L'empreinte mémoire de la [**`PCA`**][PCA] aléatoire est également proportionnelle à $2 \\cdot n_{\\max} \\cdot n_{\\mathrm{components}}$ au lieu de $n_{\\max} \\cdot n_{\\min}$ pour la méthode exacte.\n",
    "\n",
    "**Note** : l'implémentation de `inverse_transform` dans [**`PCA`**][PCA] avec `svd_solver='randomized'` n'est pas la transformation inverse exacte de `transform` même lorsque `whiten=False` (par défaut).\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Exemple de reconnaissance faciale utilisant des eigenfaces et des SVMs**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/applications/plot_face_recognition.ipynb)<br/>([*Faces recognition example using eigenfaces and SVMs*](https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html))\n",
    "    \n",
    "#### [**Décompositions de jeux de données de visages**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_5_decomposition/plot_faces_decomposition.ipynb)<br/>([*Faces dataset decompositions*](https://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html))\n",
    "\n",
    "### Références\n",
    "\n",
    "Algorithm 4.3 in [“**Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions**](https://arxiv.org/abs/0909.4061)[”](https://drive.google.com/file/d/1O9-E0VIScKwDGpK-TMI9C--xscgOfJMY/view?usp=share_link) Halko, et al., 2009\n",
    "\n",
    "[“**An implementation of a randomized algorithm for principal component analysis**](https://arxiv.org/abs/1412.3510)[”](https://drive.google.com/file/d/133PgG41En86q1mYcFUfvMj8DK1wCT-0k/view?usp=share_link) A. Szlam et al. 2014"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a id='pca-using-randomized-svd'></a> 2.5.1.4. ACP creuse (SparsePCA et MiniBatchSparsePCA)\n",
    "\n",
    "[PCA]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "[SparsePCA]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparsePCA.html\n",
    "[MiniBatchSparsePCA]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.MiniBatchSparsePCA.html\n",
    "\n",
    "[**`SparcePCA`**][SparcePCA] est une variante de l'analyse en composantes principales (PCA), qui a pour objectif d'extraire l'ensemble des composantes creuses qui reconstituent le mieux les données.\n",
    "\n",
    "Mini-batch sparse PCA ([**`MiniBatchSparsePCA`**][MiniBatchSparsePCA]) est une variante de [**`SparcePCA`**][SparcePCA] qui est plus rapide mais moins précise. La vitesse accrue est obtenue en parcourant des petits morceaux de l'ensemble des caractéristiques, pour un certain nombre d'itérations.\n",
    "\n",
    "L'analyse en composantes principales ([**`PCA`**][PCA]) a l'inconvénient que les composants extraits par cette méthode ont des expressions uniquement denses, c'est-à-dire qu'ils ont des coefficients non nuls lorsqu'ils sont exprimés sous forme de combinaisons linéaires des variables originales. Cela peut rendre l'interprétation difficile. Dans de nombreux cas, les véritables composants sous-jacents peuvent être plus facilement imaginés comme des vecteurs creux; par exemple, dans la reconnaissance de visages, les composants peuvent naturellement être liés à des parties du visage.\n",
    "\n",
    "Les composantes principales creuses donnent une représentation plus économe et plus interprétable, mettant clairement en évidence les caractéristiques originales qui contribuent aux différences entre les échantillons.\n",
    "\n",
    "L'exemple suivant illustre 16 composants extraits à l'aide de l'analyse en composantes principales creuses (sparse PCA) à partir de l'ensemble de données de visages Olivetti. On peut voir comment le terme de régularisation introduit de nombreux zéros. De plus, la structure naturelle des données fait que les coefficients non nuls sont adjacents verticalement. Le modèle n'impose pas cela mathématiquement : chaque composant est un vecteur $h \\in \\mathbf{R}^{4096}$, et il n'y a pas de notion d'adjacence verticale sauf pendant la visualisation conviviale pour l'homme en tant qu'images de pixels 64x64. Le fait que les composants ci-dessous apparaissent localement est l'effet de la structure inhérente des données, ce qui fait que de tels motifs locaux minimisent l'erreur de reconstruction. Il existe des normes induisant la sparsité qui tiennent compte de l'adjacence et de différents types de structures; voir [Jen09] pour un examen de ces méthodes. Pour plus de détails sur l'utilisation de l'analyse en composantes principales creuses, voir la section Exemples ci-dessous.\n",
    "\n",
    "<img alt=\"pca_img\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_002.png\" style=\"width: 360.0px; height: 275.4px;\" />\n",
    "\n",
    "<img alt=\"spca_img\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_005.png\" style=\"width: 360.0px; height: 275.4px;\" />\n",
    "\n",
    "Notez qu'il existe de nombreuses formulations différentes pour le problème de l'analyse en composantes principales creuses. Celle qui est implémentée ici est basée sur [Mrl09]. Le problème d'optimisation résolu est un problème d'analyse en composantes principales (apprentissage de dictionnaire) avec une pénalisation $\\ell_1$ sur les composantes :\n",
    "\n",
    "$$\n",
    "\\begin{split}(U^*, V^*) =\n",
    "    \\underset{U, V}{\\operatorname{arg\\,min\\,}} & \\frac{1}{2}\n",
    "    ||X-UV||_{\\text{Fro}}^2+\\alpha||V||_{1,1} \\\\\n",
    "    \\text{subject to } & ||U_k||_2 \\le 1 \\text{ for all }\n",
    "    0 \\leq k < n_{components}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "où $||.||_{\\text{Fro}}$ représente la norme de Frobenius et $||.||_{1,1}$ représente la norme matricielle par entrée qui est la somme des valeurs absolues de toutes les entrées dans la matrice. La norme matricielle $||.||_{1,1}$  inductrice d'éparsité empêche également l'apprentissage de composantes à partir de bruit lorsque peu d'échantillons d'entraînement sont disponibles. Le degré de pénalisation (et donc de sparsité) peut être ajusté à travers l'hyperparamètre `alpha`. De petites valeurs conduisent à une factorisation doucement régularisée, tandis que des valeurs plus grandes réduisent beaucoup de coefficients à zéro.\n",
    "\n",
    "**Note** : Bien que dans l'esprit d'un algorithme en ligne, la classe [**`MiniBatchSparsePCA`**][MiniBatchSparsePCA] ne met pas en oeuvre `partial_fit` car l'algorithme est en ligne le long de la direction des caractéristiques, pas le long de la direction des échantillons.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Décompositions de jeux de données de visages**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_5_decomposition/plot_faces_decomposition.ipynb)<br/>([*Faces dataset decompositions*](https://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html))\n",
    "\n",
    "### Références\n",
    "\n",
    "[Mrl09] [“**Online Dictionary Learning for Sparse Coding**](https://www.di.ens.fr/sierra/pdfs/icml09.pdf)[”](https://drive.google.com/file/d/1Hx7MG2TRAuZ9p23Nm0EGSqDfOy3OTotL/view?usp=share_link) J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009\n",
    "\n",
    "[Jen09] [“**Structured Sparse Principal Component Analysis**](https://www.di.ens.fr/~fbach/sspca_AISTATS2010.pdf)[”](https://drive.google.com/file/d/1tSTLTpmjKI2scOWQT8o_onwLI1JEBCo5/view?usp=share_link) R. Jenatton, G. Obozinski, F. Bach, 2009"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='kernel-principal-component-analysis-kpca'></a> 2.5.2. Analyse en composantes principales à noyau (kACP) (*Kernel Principal Component Analysis (kPCA)*)\n",
    "\n",
    "##  <a id='exact-kernel-pca'></a> 2.5.2.1. Kernel PCA exact (*Exact Kernel PCA*)\n",
    "\n",
    "[KernelPCA]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html\n",
    "[KernelPCA.inverse_transform]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA.inverse_transform\n",
    "\n",
    "[**`KernelPCA`**][KernelPCA] est une extension de PCA qui permet la réduction de dimensionnalité non linéaire grâce à l'utilisation de noyaux (voir [**Métriques par paires, affinités et noyaux** (6.8)](https://scikit-learn.org/stable/modules/metrics.html#metrics)) [Scholkopf1997]. Elle possède de nombreuses applications, y compris le débruitage, la compression et la prédiction structurée (estimation de dépendance de noyau). [**`KernelPCA`**][KernelPCA] prend en charge `transform` et `inverse_transform`.\n",
    "\n",
    "<figure class=\"align-center\">\n",
    "<img alt=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_kernel_pca_002.png\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_kernel_pca_002.png\" style=\"width: 1050.0px; height: 300.0px;\" />\n",
    "</figure>\n",
    "\n",
    "### Remarque\n",
    "\n",
    "[**`KernelPCA.inverse_transform`**][KernelPCA.inverse_transform] repose sur un ridge kernel pour apprendre la fonction qui relie les échantillons de la base PCA à l'espace de caractéristiques d'origine [Bakir2003]. Ainsi, la reconstruction obtenue avec [**`KernelPCA.inverse_transform`**][KernelPCA.inverse_transform] est-elle une approximation. Voir l'exemple ci-dessous pour plus de détails.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**ACP à noyau**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_5_decomposition/plot_kernel_pca.ipynb)<br/>([*Kernel PCA*](https://scikit-learn.org/stable/auto_examples/decomposition/plot_kernel_pca.html))\n",
    "\n",
    "### Références\n",
    "\n",
    "[Scholkopf1997] Schölkopf, Bernhard, Alexander Smola, and Klaus-Robert Müller. [“**Kernel principal component analysis.**](https://people.eecs.berkeley.edu/~wainwrig/stat241b/scholkopf_kernel.pdf)[”](https://drive.google.com/file/d/1s20gTFGRkBDYJluOoMkH56m9cYB4S7Uf/view?usp=share_link) International conference on artificial neural networks. Springer, Berlin, Heidelberg, 1997.\n",
    "\n",
    "[Bakir2003] Bakır, Gökhan H., Jason Weston, and Bernhard Schölkopf. [“**Learning to find pre-images.**](https://papers.nips.cc/paper/2003/file/ac1ad983e08ad3304a97e147f522747e-Paper.pdf)[”](https://drive.google.com/file/d/1zCHu0fxYROQYJoMHvPX4HFctlG1IMOhO/view?usp=share_link) Advances in neural information processing systems 16 (2003): 449-456."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a id='choice-of-solver-for-kernel-pca'></a> 2.5.2.2. Choix de solveur pour Kernel PCA (*Choice of solver for Kernel PCA*)\n",
    "\n",
    "[KernelPCA]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html\n",
    "\n",
    "[PCA]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "Alors que dans [**`PCA`**][PCA] le nombre de composants est limité par le nombre de caractéristiques, dans [**`KernelPCA`**][KernelPCA] le nombre de composants est limité par le nombre d'échantillons. De nombreux ensembles de données du monde réel comportent un grand nombre d'échantillons ! Dans ces cas, trouver tous les composants avec un kPCA complet est une perte de temps de calcul, car les données sont principalement décrites par les premiers composants (par exemple, `n_components <= 100`). En d'autres termes, la matrice Gram centrée qui est décomposée en valeurs propres dans le processus d'ajustement de Kernel PCA a un rang effectif beaucoup plus petit que sa taille. C'est une situation où les solveurs d'approximation peuvent fournir une accélération avec une perte de précision très faible.\n",
    "\n",
    "Le paramètre optionnel `eigen_solver='randomized'` peut être utilisé pour réduire considérablement le temps de calcul lorsque le nombre de `n_components` demandé est petit par rapport au nombre d'échantillons. Il repose sur des méthodes de décomposition aléatoires pour trouver une solution approximative en moins de temps.\n",
    "\n",
    "La complexité temporelle de [**`KernelPCA`**][KernelPCA] aléatoire est $\\mathcal{O}(n_{\\mathrm{samples}}^2 \\cdot n_{\\mathrm{components}})$ au lieu de $\\mathcal{O}(n_{\\mathrm{samples}}^3)$ pour la méthode exacte implémentée avec `eigen_solver='dense'`.\n",
    "\n",
    "L'empreinte mémoire de [**`KernelPCA`**][KernelPCA] aléatoire est également proportionnelle à $2 \\cdot n_{\\mathrm{samples}} \\cdot n_{\\mathrm{components}}$ au lieu de $n_{\\mathrm{samples}}^2$ pour la méthode exacte.\n",
    "\n",
    "Remarque : cette technique est la même que dans [**PCA en utilisant SVD aléatoire** (2.5.1.3)](https://scikit-learn.org/stable/modules/decomposition.html#randomizedpca).\n",
    "\n",
    "En plus des deux solveurs ci-dessus, `eigen_solver='arpack'` peut être utilisé comme une alternative pour obtenir une décomposition approximative. En pratique, cette méthode ne fournit des temps d'exécution raisonnables que lorsque le nombre de composants à trouver est extrêmement faible. Elle est activée par défaut lorsque le nombre de composants souhaités est inférieur à 10 (strict) et que le nombre d'échantillons est supérieur à 200 (strict). Voir [**`KernelPCA`**][KernelPCA] pour plus de détails.\n",
    "\n",
    "### Références\n",
    "\n",
    "* *dense solver* : [scipy.linalg.eigh documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eigh.html)\n",
    "* *randomized solver* :\n",
    "        * Algorithm 4.3 in [“**Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions**](https://arxiv.org/abs/0909.4061)[”](https://drive.google.com/file/d/1O9-E0VIScKwDGpK-TMI9C--xscgOfJMY/view?usp=share_link) Halko, et al. (2009)\n",
    "        [“**An implementation of a randomized algorithm for principal component analysis**](https://arxiv.org/abs/1412.3510)[”](https://drive.google.com/file/d/133PgG41En86q1mYcFUfvMj8DK1wCT-0k/view?usp=share_link) A. Szlam et al. (2014)\n",
    "* *arpack solver* : [scipy.sparse.linalg.eigsh documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.eigsh.html) R. B. Lehoucq, D. C. Sorensen, and C. Yang, (1998)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <a id='truncated-singular-value-decomposition-and-latent-semantic-analysis'></a> 2.5.3. Décomposition en valeurs singulières tronquée et analyse sémantique latente (*Truncated singular value decomposition and latent semantic analysis*)\n",
    "\n",
    "[TruncatedSVD]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "\n",
    "[PCA]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "[**`TruncatedSVD`**][TruncatedSVD] implémente une variante de la décomposition en valeurs singulières (SVD) qui ne calcule que les $k$ plus grandes valeurs singulières, où $k$ est un paramètre spécifié par l'utilisateur.\n",
    "\n",
    "Lorsque la SVD tronquée est appliquée à des matrices terme-document (telles que renvoyées par [**`CountVectorizer`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) ou [**`TfidfVectorizer`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)), cette transformation est appelée [**analyse sémantique latente (LSA)**](https://nlp.stanford.edu/IR-book/pdf/18lsi.pdf), car elle transforme de telles matrices en un espace \"sémantique\" de faible dimensionnalité. En particulier, on sait que la LSA lutte contre les effets de synonymie et de polysémie (qui signifient tous deux qu'il y a plusieurs significations par mot), qui rendent les matrices terme-document trop creuses et qui présentent une similarité médiocre selon des mesures telles que la similarité cosinus.\n",
    "\n",
    "**Note** La LSA est également connue sous le nom d'indexation sémantique latente (LSI), bien que cela se réfère strictement à son utilisation dans les index persistants à des fins de recherche d'informations.\n",
    "\n",
    "Mathématiquement, la SVD tronquée appliquée aux échantillons d'entraînement $X$ produit une approximation de bas-rang $X$ :\n",
    "\n",
    "$$X \\approx X_k = U_k \\Sigma_k V_k^\\top$$\n",
    "\n",
    "Après cette opération, $U_k \\Sigma_k$ est l'ensemble d'entraînement transformé avec $k$ caractéristiques (appelées `n_components` dans l'API).\n",
    "\n",
    "Pour transformer également un ensemble de test $X$, nous le multiplions par $V_k$ :\n",
    "\n",
    "$$X' = X V_k$$\n",
    "\n",
    "**Note** La plupart des traitements de la LSA dans la littérature de traitement du langage naturel (NLP) et de recherche d'informations (IR) échangent les axes de la matrice $X$ de sorte qu'elle ait une forme `n_features × n_samples`. Nous présentons la LSA d'une manière différente qui correspond mieux à l'API scikit-learn, mais les valeurs singulières trouvées sont les mêmes.\n",
    "\n",
    "[**`TruncatedSVD`**][TruncatedSVD] est très similaire à [**`PCA`**][PCA], mais diffère en ce que la matrice $X$ n'a pas besoin d'être centrée. Lorsque les moyennes par colonne (par caractéristique) de $X$ sont soustraites des valeurs de caractéristiques, la SVD tronquée sur la matrice résultante est équivalente à la PCA. En termes pratiques, cela signifie que le transformateur [**`TruncatedSVD`**][TruncatedSVD] accepte des matrices `scipy.sparse` sans avoir besoin de les densifier, car la densification peut remplir la mémoire même pour des collections de documents de taille moyenne.\n",
    "\n",
    "Bien que le transformateur [**`TruncatedSVD`**][TruncatedSVD] fonctionne avec n'importe quelle matrice de caractéristiques, il est recommandé de l'utiliser sur des matrices tf-idf plutôt que sur des comptes de fréquence bruts dans un contexte de LSA/traitement de documents. En particulier, une mise à l'échelle sous-linéaire et une fréquence de document inverse doivent être activées (`sublinear_tf=True, use_idf=True`) pour rapprocher les valeurs de caractéristique d'une distribution gaussienne, compensant les hypothèses erronées de la LSA sur les données textuelles.\n",
    "\n",
    "## Exemples\n",
    "\n",
    "### [**Classification de documents textuels à l'aide de k-means**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/text/plot_document_clustering.ipynb)<br/>([*Clustering text documents using k-means*](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html))\n",
    "\n",
    "## Références\n",
    "\n",
    "Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze (2008), [“**Introduction to Information Retrieval**](https://nlp.stanford.edu/IR-book/pdf/18lsi.pdf)[”](https://drive.google.com/file/d/1H_KmwRi9zLlibApB1uNqr3c7ikaP4BoV/view?usp=share_link), Cambridge University Press, chapter 18: Matrix decompositions & latent semantic indexing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <a id='dictionary-learning'></a> 2.5.4. Apprentissage de dictionnaire (*Dictionary Learning*)\n",
    "\n",
    "##  <a id='sparse-coding-with-a-precomputed-dictionary'></a> 2.5.4.1. Codage creux avec un dictionnaire précalculé (*Sparse coding with a precomputed dictionary*)\n",
    "\n",
    "L'objet [**`SparseCoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparseCoder.html) est un estimateur qui peut être utilisé pour transformer des signaux en combinaisons linéaires creuses d'atomes à partir d'un dictionnaire fixe précalculé tel qu'une base d'ondelettes discrètes. Cet objet n'implémente donc pas de méthode `fit`. La transformation correspond à un problème de codage creux : trouver une représentation des données sous forme d'une combinaison linéaire d'un nombre aussi faible que possible d'atomes du dictionnaire. Toutes les variantes de l'apprentissage de dictionnaires implémentent les méthodes de transformation suivantes, contrôlables via le paramètre d'initialisation `transform_method` :\n",
    "* Orthogonal matching pursuit ([**Orthogonal Matching Pursuit (OMP)** (1.1.9)](https://scikit-learn.org/stable/modules/linear_model.html#omp))\n",
    "* Least-angle regression ([**Least Angle Regression** (1.1.7)](https://scikit-learn.org/stable/modules/linear_model.html#least-angle-regression))\n",
    "* Lasso calculé par régression à angles restreints\n",
    "* Lasso utilisant la descente de coordonnées ([**Lasso** (1.1.3)](https://scikit-learn.org/stable/modules/linear_model.html#lasso))\n",
    "* Seuillage\n",
    "\n",
    "Le seuillage est très rapide mais il ne permet pas d'obtenir des reconstructions précises. Ils ont montré leur utilité dans la littérature pour les tâches de classification. Pour les tâches de reconstruction d'images, la méthode Orthogonal Matching Pursuit permet d'obtenir la reconstruction la plus précise et impartiale.\n",
    "\n",
    "Les objets d'apprentissage de dictionnaires offrent, via le paramètre `split_code`, la possibilité de séparer les valeurs positives et négatives dans les résultats du codage creux. Cela est utile lorsque l'apprentissage de dictionnaire est utilisé pour extraire des caractéristiques qui seront utilisées pour l'apprentissage supervisé, car cela permet à l'algorithme d'apprentissage d'attribuer des poids différents aux chargements négatifs d'un atome particulier, par rapport au chargement positif correspondant.\n",
    "\n",
    "Le code de séparation pour un seul échantillon a une longueur de `2 * n_components` et est construit en utilisant la règle suivante : d'abord, le code régulier de longueur `n_components` est calculé. Ensuite, les `n_components` premières entrées du code de séparation sont remplies avec la partie positive du vecteur de code régulier. La moitié de la deuxième partie du code de séparation est remplie avec la partie négative du vecteur de code, uniquement avec un signe positif. Par conséquent, le code de séparation est non négatif.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Codage creux avec un dictionnaire précalculé**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_5_decomposition/plot_sparse_coding.ipynb)<br/>([*Sparse coding with a precomputed dictionary*](https://scikit-learn.org/stable/auto_examples/decomposition/plot_sparse_coding.html))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a id='generic-dictionary-learning'></a> 2.5.4.2. Apprentissage de dictionnaire générique (*Generic dictionary learning*)\n",
    "\n",
    "L'apprentissage de dictionnaire ([**`DictionaryLearning`**](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.DictionaryLearning.html)) est un problème de factorisation de matrice qui consiste à trouver un dictionnaire (généralement surcomplet) qui fonctionnera bien pour encoder de manière creuse les données ajustées.\n",
    "\n",
    "Représenter les données comme des combinaisons creuses d'atomes d'un dictionnaire surcomplet est suggéré comme étant la façon dont fonctionne le cortex visuel primaire des mammifères. Par conséquent, l'apprentissage de dictionnaire appliqué aux patchs d'images a donné de bons résultats dans des tâches de traitement d'images telles que la complétion d'images, le débruitage et l'inpainting, ainsi que pour des tâches de reconnaissance supervisée.\n",
    "\n",
    "L'apprentissage de dictionnaire est un problème d'optimisation résolu en mettant à jour de manière alternative le code creux, en tant que solution à plusieurs problèmes Lasso, en considérant le dictionnaire fixe, puis en mettant à jour le dictionnaire pour qu'il s'adapte le mieux au code creux.\n",
    "\n",
    "$$\n",
    "\\begin{split}(U^*, V^*) = \\underset{U, V}{\\operatorname{arg\\,min\\,}} & \\frac{1}{2}\n",
    "             ||X-UV||_{\\text{Fro}}^2+\\alpha||U||_{1,1} \\\\\n",
    "             \\text{subject to } & ||V_k||_2 \\lt 1 \\text{ for all }\n",
    "             0 \\leq k \\lt n_{\\mathrm{atoms}}\\end{split}\n",
    "$$\n",
    "\n",
    "<img alt=\"pca_img2\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_002.png\" style=\"width: 360.0px; height: 275.4px;\" />\n",
    "<img alt=\"dict_img2\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_007.png\" style=\"width: 360.0px; height: 275.4px;\" />\n",
    "\n",
    "\n",
    "$||.||_{\\text{Fro}}$ représente la norme de Frobenius et $||.||_{1,1}$ représente la norme de la matrice élément par élément qui est la somme des valeurs absolues de toutes les entrées de la matrice. Après avoir utilisé une telle procédure pour ajuster le dictionnaire, la transformation est simplement une étape de codage creux qui partage la même implémentation avec tous les objets d'apprentissage de dictionnaire (voir la section [**codage creux avec un dictionnaire précalculé** (2.5.4.1)](#sparse-coding-with-a-precomputed-dictionary)).\n",
    "\n",
    "Il est également possible de contraindre le dictionnaire et/ou le code à être positif pour correspondre à des contraintes qui peuvent être présentes dans les données. Les visages ci-dessous présentent différentes contraintes de positivité appliquées. Le rouge indique des valeurs négatives, le bleu des valeurs positives et le blanc représente des zéros.\n",
    "\n",
    "<img alt=\"dict_img_pos1\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_010.png\" style=\"width: 360.0px; height: 275.4px;\" />\n",
    "<img alt=\"dict_img_pos2\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_011.png\" style=\"width: 360.0px; height: 275.4px;\" />\n",
    "\n",
    "<img alt=\"dict_img_pos3\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_012.png\" style=\"width: 360.0px; height: 275.4px;\" />\n",
    "<img alt=\"dict_img_pos4\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_013.png\" style=\"width: 360.0px; height: 275.4px;\" />\n",
    "\n",
    "L'image suivante montre à quoi ressemble un dictionnaire appris à partir de patchs d'images de 4x4 pixels extraits d'une partie de l'image d'un visage de raton laveur.\n",
    "\n",
    "<img alt=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_image_denoising_001.png\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_image_denoising_001.png\" style=\"width: 250.0px; height: 165.0px;\" />\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Débruitage d'image à l'aide de l'apprentissage de dictionnaire**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_5_decomposition/plot_image_denoising.ipynb)<br/>([*Image denoising using dictionary learning*](https://scikit-learn.org/stable/auto_examples/decomposition/plot_image_denoising.html))\n",
    "\n",
    "### Références\n",
    "\n",
    "[Mrl09] [“**Online Dictionary Learning for Sparse Coding**](https://www.di.ens.fr/sierra/pdfs/icml09.pdf)[”](https://drive.google.com/file/d/1Hx7MG2TRAuZ9p23Nm0EGSqDfOy3OTotL/view?usp=share_link) J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a id='mini-batch-dictionary-learning'></a> 2.5.4.3. Apprentissage de dictionnaire en mini-lot (*Mini-batch dictionary learning*)\n",
    "\n",
    "[MiniBatchDictionaryLearning]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.MiniBatchDictionaryLearning.html\n",
    "\n",
    "[MiniBatchKMeans]: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html\n",
    "\n",
    "[**`MiniBatchDictionaryLearning`**][MiniBatchDictionaryLearning] implémente une version plus rapide, mais moins précise, de l'algorithme d'apprentissage de dictionnaire qui convient mieux aux grands ensembles de données.\n",
    "\n",
    "Par défaut, [**`MiniBatchDictionaryLearning`**][MiniBatchDictionaryLearning] divise les données en mini-lots et optimise de manière en ligne en faisant des cycles sur les mini-lots pour le nombre d'itérations spécifié. Cependant, pour le moment, il n'implémente pas de condition d'arrêt.\n",
    "\n",
    "L'estimateur implémente également `partial_fit`, qui met à jour le dictionnaire en itérant une seule fois sur un mini-lot. Cela peut être utilisé pour l'apprentissage en ligne lorsque les données ne sont pas disponibles dès le départ ou lorsque les données ne rentrent pas en mémoire.\n",
    "\n",
    "<img alt=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_dict_face_patches_001.png\" class=\"align-right\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_dict_face_patches_001.png\" style=\"width: 210.0px; height: 200.0px;\" />\n",
    "\n",
    "### Regroupement pour l'apprentissage de dictionnaire\n",
    "\n",
    "Notez que lors de l'utilisation de l'apprentissage de dictionnaire pour extraire une représentation (par exemple, pour un codage creux), le regroupement peut être un bon proxy pour apprendre le dictionnaire. Par exemple, l'estimateur [**`MiniBatchKMeans`**][MiniBatchKMeans] est efficace sur le plan du calcul et implémente l'apprentissage en ligne avec une méthode `partial_fit`.\n",
    "\n",
    "### Exemple\n",
    "\n",
    "#### [**Apprentissage en ligne d'un dictionnaire de parties de visages**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_3_cluster/plot_dict_face_patches.ipynb)<br/>([*Online learning of a dictionary of parts of faces*](https://scikit-learn.org/stable/auto_examples/cluster/plot_dict_face_patches.html))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <a id='factor-analysis'></a> 2.5.5. Analyse factorielle (*Factor Analysis*)\n",
    "\n",
    "En apprentissage non supervisé, nous n'avons qu'un ensemble de données $X = \\{x_1, x_2, \\dots, x_n\n",
    "\\}$. Comment peut-on décrire mathématiquement cet ensemble de données? Un modèle de variable latente continue très simple pour $X$ est\n",
    "\n",
    "$$x_i = W h_i + \\mu + \\epsilon$$\n",
    "\n",
    "Le vecteur $h_i$ est appelé «latent» car il n'est pas observé. $\\epsilon$ est considéré comme un terme de bruit distribué selon une loi normale avec une moyenne de 0 et une covariance $\\Psi$ (c'est-à-dire $\\epsilon \\sim \\mathcal{N}(0, \\Psi)$), $\\mu$ est un vecteur de décalage arbitraire. Un tel modèle est appelé «génératif» car il décrit comment $x_i$ est généré à partir de $h_i$. Si nous utilisons tous les $x_i$ comme colonnes pour former une matrice $\\mathbf{X}$ et tous les $h_i$ comme colonnes d'une matrice $\\mathbf{H}$, nous pouvons écrire (avec une définition adéquate de $\\mathbf{M}$ et $\\mathbf{E}$) :\n",
    "\n",
    "$$\\mathbf{X} = W \\mathbf{H} + \\mathbf{M} + \\mathbf{E}$$\n",
    "\n",
    "En d'autres termes, nous avons *décomposé* la matrice $\\mathbf{X}$.\n",
    "\n",
    "Si $h_i$ est donnée, l'équation ci-dessus implique automatiquement l'interprétation probabiliste suivante :\n",
    "\n",
    "$$p(x_i|h_i) = \\mathcal{N}(Wh_i + \\mu, \\Psi)$$\n",
    "\n",
    "Pour un modèle probabiliste complet, nous avons également besoin d'une distribution a priori pour la variable latente $h$. L'hypothèse la plus simple (basée sur les bonnes propriétés de la distribution gaussienne) est $h \\sim \\mathcal{N}(0,\n",
    "\\mathbf{I})$. Cela donne une distribution gaussienne comme la distribution marginale de $x$ :\n",
    "\n",
    "$$p(x) = \\mathcal{N}(\\mu, WW^T + \\Psi)$$\n",
    "\n",
    "Maintenant, sans aucune autre hypothèse, l'idée d'avoir une variable latente $h$ serait superflue - $x$ peut être entièrement modélisé avec une moyenne et une covariance. Nous devons imposer une structure plus spécifique sur l'un de ces deux paramètres. Une simple hypothèse supplémentaire concerne la structure de la covariance de l'erreur $\\Psi$ :\n",
    "\n",
    "* $\\Psi = \\sigma^2 \\mathbf{I}$ : Cette hypothèse conduit au modèle probabiliste de l'[**`ACP`**](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html).\n",
    "* $\\Psi = \\mathrm{diag}(\\psi_1, \\psi_2, \\dots, \\psi_n)$ : Ce modèle est appelé [**`FactorAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FactorAnalysis.html), un modèle statistique classique. La matrice $\\mathbf{W}$ est parfois appelée \"matrice de chargement factoriel\".\n",
    "\n",
    "Les deux modèles estiment essentiellement une gaussienne avec une matrice de covariance de faible rang. Étant donné que les deux modèles sont probabilistes, ils peuvent être intégrés dans des modèles plus complexes, par exemple un mélange de Factor Analysers. On obtient des modèles très différents (par exemple, [**`FastICA`**](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html)) si l'on suppose des lois a priori non gaussiennes sur les variables latentes.\n",
    "\n",
    "L'analyse factorielle peut produire des composantes similaires (les colonnes de sa matrice de chargement) à celles de l'ACP. Cependant, on ne peut pas faire de déclarations générales sur ces composantes (par exemple, si elles sont orthogonales).\n",
    "\n",
    "<img alt=\"pca_img3\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_002.png\" style=\"width: 360.0px; height: 275.4px;\" />\n",
    "<img alt=\"fa_img3\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_008.png\" style=\"width: 360.0px; height: 275.4px;\" />\n",
    "\n",
    "Le principal avantage de l'Analyse Factorielle par rapport à l'ACP est qu'elle peut modéliser la variance dans toutes les directions de l'espace d'entrée de manière indépendante (bruit hétéroscédastique) :\n",
    "\n",
    "<img alt=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_009.png\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_009.png\" style=\"width: 240.0px; height: 270.0px;\" />\n",
    "\n",
    "Cela permet une meilleure sélection de modèle que l'ACP probabiliste en présence de bruit hétéroscédastique :\n",
    "\n",
    "<img alt=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_pca_vs_fa_model_selection_002.png\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_pca_vs_fa_model_selection_002.png\" style=\"width: 480.0px; height: 360.0px;\" />\n",
    "\n",
    "L'Analyse Factorielle est souvent suivie d'une rotation des facteurs (avec le paramètre `rotation`), généralement pour améliorer l'interprétabilité. Par exemple, la rotation Varimax maximise la somme des variances des chargements carrés, c'est-à-dire qu'elle tend à produire des facteurs plus creux, qui sont influencés par seulement quelques caractéristiques chacun (la \"structure simple\"). Voir par exemple, le premier exemple ci-dessous.\n",
    "\n",
    "## Exemples\n",
    "\n",
    "### [**Analyse factorielle (avec rotation) pour visualiser des motifs**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_5_decomposition/plot_varimax_fa.ipynb)<br/>([*Factor Analysis (with rotation) to visualize patterns*](https://scikit-learn.org/stable/auto_examples/decomposition/plot_varimax_fa.html))\n",
    "\n",
    "### [**Sélection du modèle avec PCA probabiliste et analyse factorielle (FA)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_5_decomposition/plot_pca_vs_fa_model_selection.ipynb)<br/>([*Model selection with Probabilistic PCA and Factor Analysis (FA)*](https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_fa_model_selection.html))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='independent-component-analysis-ica'></a> 2.5.6. Analyse en composantes indépendantes (ICA) (*Independent component analysis (ICA)*)\n",
    "\n",
    "L'analyse en composantes indépendantes (Independent component analysis ou ICA) sépare un signal multivarié en sous-composantes additives maximales indépendantes. Elle est mise en œuvre dans scikit-learn en utilisant l'algorithme Fast ICA. En général, l'ICA n'est pas utilisé pour réduire la dimensionnalité, mais pour séparer des signaux superposés. Étant donné que le modèle ICA n'inclut pas de terme de bruit, pour que le modèle soit correct, un blanchiment doit être appliqué. Cela peut être fait de manière interne en utilisant l'argument whiten ou manuellement en utilisant l'une des variantes de l'ACP.\n",
    "\n",
    "L'ICA est classiquement utilisé pour séparer des signaux mixtes (un problème connu sous le nom de séparation de sources aveugle), comme dans l'exemple ci-dessous :\n",
    "\n",
    "<img alt=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_ica_blind_source_separation_001.png\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_ica_blind_source_separation_001.png\" style=\"width: 384.0px; height: 288.0px;\" />\n",
    "\n",
    "L'ICA peut également être utilisé comme une autre décomposition non linéaire qui trouve des composantes avec un certaine *parcimonie*.\n",
    "\n",
    "<img alt=\"pca_img4\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_002.png\" style=\"width: 360.0px; height: 275.4px;\" />\n",
    "\n",
    "## Exemples\n",
    "\n",
    "### [**Séparation de sources aveugle en utilisant FastICA**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_5_decomposition/plot_ica_blind_source_separation.ipynb)<br/>([*Blind source separation using FastICA*](https://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_blind_source_separation.html))\n",
    "    \n",
    "### [**FastICA sur des nuages de points 2D**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_5_decomposition/plot_ica_vs_pca.ipynb)<br/>([*FastICA on 2D point clouds*](https://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_vs_pca.html))\n",
    "\n",
    "\n",
    "### [**Décompositions de jeux de données de visages**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_5_decomposition/plot_faces_decomposition.ipynb)<br/>([*Faces dataset decompositions*](https://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='non-negative-matrix-factorization-nmf-or-nnmf'></a> 2.5.7. Factorisation matricielle non négative (NMF ou NNMF) (*Non-negative matrix factorization (NMF or NNMF)*)\n",
    "\n",
    "##  <a id='nmf-with-the-frobenius-norm'></a> 2.5.7.1. NMF avec la norme de Frobenius (*NMF with the Frobenius norm*)\n",
    "\n",
    "[NMF]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html\n",
    "\n",
    "[PCA]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "[**`NMF`**][NMF] [1] est une approche alternative de la décomposition qui suppose que les données et les composants sont non négatifs. [**`NMF`**][NMF] peut être utilisé à la place de [**`PCA`**][PCA] ou de ses variantes dans les cas où la matrice de données ne contient pas de valeurs négatives. Il trouve une décomposition des échantillons $X$ en deux matrices $W$ et $H$ de valeurs non négatives, en optimisant la distance $d$ entre la matrice originale $X$ et le produit matriciel $WH$. La fonction de distance la plus utilisée est la norme de Frobenius au carré, qui est une extension évidente de la norme euclidienne aux matrices :\n",
    "\n",
    "$$d_{\\mathrm{Fro}}(X, Y) = \\frac{1}{2} ||X - Y||_{\\mathrm{Fro}}^2 = \\frac{1}{2} \\sum_{i,j} (X_{ij} - {Y}_{ij})^2$$\n",
    "\n",
    "Contrairement à [**`PCA`**][PCA], la représentation d'un vecteur est obtenue de manière additive, en superposant les composants sans soustraire. De tels modèles additifs sont efficaces pour représenter des images et du texte.\n",
    "\n",
    "Il a été observé dans [Hoyer, 2004] [2] que, lorsqu'elle est soigneusement contrainte, [**`NMF`**][NMF] peut produire une représentation basée sur les parties de l'ensemble de données, conduisant à des modèles interprétables. L'exemple suivant affiche 16 composantes creuses trouvées par [**`NMF`**][NMF] à partir des images de l'ensemble de données des visages Olivetti, en comparaison avec les *eigenfaces* de PCA.\n",
    "\n",
    "<img alt=\"pca_img5\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_002.png\" style=\"width: 360.0px; height: 275.4px;\" />\n",
    "\n",
    "<img alt=\"nmf_img5\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_003.png\" style=\"width: 360.0px; height: 275.4px;\" />\n",
    "\n",
    "L'attribut `init` détermine la méthode d'initialisation appliquée, qui a un impact important sur les performances de la méthode. [**`NMF`**][NMF] implémente la méthode de décomposition en valeurs singulières double non négative (Nonnegative Double Singular Value Decomposition, NNDSVD) [4], qui est basée sur deux processus de SVD, l'un approximant la matrice de données, l'autre approximant les sections positives des facteurs partiels de SVD résultants en utilisant une propriété algébrique des matrices de rang unitaire. L'algorithme de base NNDSVD convient mieux à la factorisation creuse. Ses variantes NNDSVDa (dans laquelle tous les zéros sont fixés égaux à la moyenne de tous les éléments de la donnée) et NNDSVDar (dans laquelle les zéros sont fixés à des perturbations aléatoires inférieures à la moyenne de la donnée divisée par 100) sont recommandées dans le cas dense.\n",
    "\n",
    "Notez que le solveur de mise à jour multiplicative (\"mu\") ne peut pas mettre à jour les zéros présents dans l'initialisation, il conduit donc à de moins bons résultats lorsqu'il est utilisé conjointement avec l'algorithme NNDSVD de base qui introduit beaucoup de zéros ; dans ce cas, NNDSVDa ou NNDSVDar devraient être préférés.\n",
    "\n",
    "[**`NMF`**][NMF] peut également être initialisé avec des matrices aléatoires non négatives correctement mises à l'échelle en définissant `init=\"random\"`. Un entier seed ou un objet `RandomState` peut également être passé à `random_state` pour contrôler la reproductibilité.\n",
    "\n",
    "Dans [**`NMF`**][NMF], des pénalités L1 et L2 peuvent être ajoutées à la fonction de perte pour régulariser le modèle. La pénalité L2 utilise la norme de Frobenius, tandis que la pénalité L1 utilise une norme L1 élément par élément. Comme dans `ElasticNet`, nous pouvons contrôler la combinaison de L1 et L2 avec le paramètre `l1_ratio` ($\\rho$), et l'intensité de la régularisation avec les paramètres `alpha_W` et `alpha_H` ($\\alpha_W$ et $\\alpha_H$, `alpha` et `beta` dans la version précédente de Scikit-Learn). Les pénalités sont mises à l'échelle par le nombre d'échantillons ($n_\\text{samples}$) pour `H` et le nombre de caractéristiques ($n_\\text{features}$) pour `W`, afin de maintenir leur impact équilibré les uns par rapport aux autres et à la fonction d'ajustement des données aussi indépendante que possible de la taille de l'ensemble d'entraînement. Les termes de pénalité sont les suivants:\n",
    "\n",
    "$$(\\alpha_W \\rho ||W||_1 + \\frac{\\alpha_W(1-\\rho)}{2} ||W||_{\\mathrm{Fro}} ^ 2) \\times n_\\text{features}\n",
    "+ (\\alpha_H \\rho ||H||_1 + \\frac{\\alpha_H(1-\\rho)}{2} ||H||_{\\mathrm{Fro}} ^ 2) \\times n_\\text{samples}$$\n",
    "\n",
    "et la fonction objectif régularisée est:\n",
    "\n",
    "$$d_{\\mathrm{Fro}}(X, WH)\n",
    "+ (\\alpha_W \\rho ||W||_1 + \\frac{\\alpha_W(1-\\rho)}{2} ||W||_{\\mathrm{Fro}} ^ 2) \\times n_\\text{features}\n",
    "+ (\\alpha_H \\rho ||H||_1 + \\frac{\\alpha_H(1-\\rho)}{2} ||H||_{\\mathrm{Fro}} ^ 2) \\times n_\\text{samples}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a id='nmf-with-a-beta-divergence'></a> 2.5.7.2. NMF avec une divergence beta (*NMF with a beta-divergence*)\n",
    "\n",
    "[NMF]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html\n",
    "\n",
    "Comme décrit précédemment, la fonction de distance la plus utilisée est la norme de Frobenius au carré, qui est une extension évidente de la norme euclidienne aux matrices :\n",
    "\n",
    "$$d_{\\mathrm{Fro}}(X, Y) = \\frac{1}{2} ||X - Y||_{Fro}^2 = \\frac{1}{2} \\sum_{i,j} (X_{ij} - {Y}_{ij})^2$$\n",
    "\n",
    "D'autres fonctions de distance peuvent être utilisées dans NMF, comme par exemple la divergence de Kullback-Leibler (KL) généralisée, également appelée divergence I :\n",
    "\n",
    "$$d_{KL}(X, Y) = \\sum_{i,j} (X_{ij} \\log(\\frac{X_{ij}}{Y_{ij}}) - X_{ij} + Y_{ij})$$\n",
    "\n",
    "Ou, la divergence d'Itakura-Saito (IS) :\n",
    "\n",
    "$$d_{IS}(X, Y) = \\sum_{i,j} (\\frac{X_{ij}}{Y_{ij}} - \\log(\\frac{X_{ij}}{Y_{ij}}) - 1)$$\n",
    "\n",
    "Ces trois distances sont des cas particuliers de la famille de divergences beta, avec respectivement $\\beta = 2, 1, 0$ [6]. Les divergences beta sont définies par :\n",
    "\n",
    "$$d_{\\beta}(X, Y) = \\sum_{i,j} \\frac{1}{\\beta(\\beta - 1)}(X_{ij}^\\beta + (\\beta-1)Y_{ij}^\\beta - \\beta X_{ij} Y_{ij}^{\\beta - 1})$$\n",
    "\n",
    "<img alt=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_beta_divergence_001.png\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_beta_divergence_001.png\" style=\"width: 480.0px; height: 360.0px;\" />\n",
    "\n",
    "Notez que cette définition n'est pas valable si $\\beta \\in (0; 1)$, mais elle peut être étendue de manière continue aux définitions de $d_{KL}$ et $d_{IS}$ respectivement.\n",
    "\n",
    "[**`NMF`**][NMF] implémente deux solveurs, utilisant la descente de coordonnées ('cd') [5], et la mise à jour multiplicative ('mu') [6]. Le solveur 'mu' peut optimiser chaque divergence beta, y compris bien sûr la norme de Frobenius ($\\beta=2$), la divergence de Kullback-Leibler (KL) généralisée ($\\beta=1$) et la divergence d'Itakura-Saito ($\\beta=0$). Notez que pour $\\beta \\in (1; 2)$, le solveur 'mu' est significativement plus rapide que pour d'autres valeurs de $\\beta$. Notez également qu'avec une valeur négative de $\\beta$ (ou 0, c'est-à-dire 'itakura-saito'), la matrice d'entrée ne peut pas contenir de valeurs nulles.\n",
    "\n",
    "Le solveur 'cd' ne peut optimiser que la norme de Frobenius. En raison de la non-convexité sous-jacente de NMF, les différents solveurs peuvent converger vers des minima différents, même en optimisant la même fonction de distance.\n",
    "\n",
    "NMF est mieux utilisé avec la méthode `fit_transform`, qui renvoie la matrice $W$. La matrice $H$ est stockée dans le modèle ajusté dans l'attribut `components_`; la méthode `transform` décompose une nouvelle matrice `X_new` basée sur ces composants stockés :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n",
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components=2, init='random', random_state=0)\n",
    "W = model.fit_transform(X)\n",
    "H = model.components_\n",
    "X_new = np.array([[1, 0], [1, 6.1], [1, 0], [1, 4], [3.2, 1], [0, 4]])\n",
    "W_new = model.transform(X_new)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**Décompositions de jeux de données de visages**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_5_decomposition/plot_faces_decomposition.ipynb)<br/>([*Faces dataset decompositions*](https://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html))\n",
    "\n",
    "#### [**Extraction de thème avec la factorisation de matrice non-négative et l'allocation latente de Dirichlet**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/applications/feature_selection/plot_topics_extraction_with_nmf_lda.ipynb)<br/>([*Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation*](https://scikit-learn.org/stable/applications/feature_selection/plot_topics_extraction_with_nmf_lda.html))\n",
    "\n",
    "#### [**Fonctions de perte de divergence bêta**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_5_decomposition/plot_beta_divergence.ipynb)<br/>([*Beta-divergence loss functions*](https://scikit-learn.org/stable/auto_examples/decomposition/plot_beta_divergence.html))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a id='mini-batch-non-negative-matrix-factorization'></a> 2.5.7.3. Factorisation de matrice non négative par mini-lot (Mini-Batch NNMF) (*Mini-batch Non Negative Matrix Factorization*)\n",
    "\n",
    "[MiniBatchNMF]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.MiniBatchNMF.html\n",
    "[NMF]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html\n",
    "\n",
    "[**`MiniBatchNMF`**][MiniBatchNMF] [7] implémente une version plus rapide mais moins précise de la factorisation de matrice non négative (i.e. [NMF][**`NMF`**]), mieux adaptée aux grands ensembles de données.\n",
    "\n",
    "Par défaut, [**`MiniBatchNMF`**][MiniBatchNMF] divise les données en mini-lots et optimise le modèle NMF de pour le *en ligne* en passant en boucle sur les mini-lots pour le nombre d'itérations spécifié. Le paramètre `batch_size` contrôle la taille des lots.\n",
    "\n",
    "Pour accélérer l'algorithme de mini-lot, il est également possible de mettre à l'échelle les lots précédents, en leur donnant moins d'importance que les lots plus récents. Cela se fait en introduisant un *facteur d'oubli* contrôlé par le paramètre `forget_factor`.\n",
    "\n",
    "L'estimateur implémente également `partial_fit`, qui met à jour $H$ en itérant une seule fois sur un mini-lot. Cela peut être utilisé pour l'apprentissage en ligne lorsque les données ne sont pas disponibles dès le départ ou lorsque les données ne rentrent pas en mémoire.\n",
    "\n",
    "### Références\n",
    "\n",
    "[1] [“**Learning the parts of objects by non-negative matrix factorization**](http://www.cs.columbia.edu/~blei/fogm/2020F/readings/LeeSeung1999.pdf)[”](https://drive.google.com/file/d/1ixsS5v3thm7FfzmvTqu5sY5Q7KKGFPF-/view?usp=share_link) D. Lee, S. Seung, 1999\n",
    "\n",
    "[2] [“**Non-negative Matrix Factorization with Sparseness Constraints**](https://www.jmlr.org/papers/volume5/hoyer04a/hoyer04a.pdf)[”](https://drive.google.com/file/d/1_e-MZKCEtVeMsUyynC5byY0kjp80b0kk/view?usp=share_link) P. Hoyer, 2004\n",
    "\n",
    "[4] [“**SVD based initialization: A head start for nonnegative matrix factorization**](https://www.boutsidis.org/Boutsidis_PRE_08.pdf)[”](https://drive.google.com/file/d/1xyE4McsxcbQk7MFJI17n-Yv4Q9Pvzu60/view?usp=share_link) C. Boutsidis, E. Gallopoulos, 2008\n",
    "\n",
    "[5] [“**Fast local algorithms for large scale nonnegative matrix and tensor factorizations**](https://www.researchgate.net/publication/220241471_Fast_Local_Algorithms_for_Large_Scale_Nonnegative_Matrix_and_Tensor_Factorizations)[”](https://drive.google.com/file/d/1g-lnePcoXG4KD1_tVkYCTaNHuKMb-7en/view?usp=share_link) A. Cichocki, A. Phan, 2009\n",
    "\n",
    "[6] (1,2) [“**Algorithms for nonnegative matrix factorization with the beta-divergence**](https://arxiv.org/abs/1010.1763)[”](https://drive.google.com/file/d/10Yb-ljfmdMxCV_BVaYiJiwujBBHqWkXk/view?usp=share_link) C. Fevotte, J. Idier, 2011\n",
    "\n",
    "[7] [“**Online algorithms for nonnegative matrix factorization with the Itakura-Saito divergence**](https://arxiv.org/abs/1106.4198)[”](https://drive.google.com/file/d/1lcQNCEY0Al-mAiVau80oCejKNA-iPSkG/view?usp=share_link) A. Lefevre, F. Bach, C. Fevotte, 2011"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='latent-dirichlet-allocation-lda'></a> 2.5.8. Allocation de Dirichlet latente (LDA) (*Latent Dirichlet Allocation (LDA)*)\n",
    "\n",
    "[LatentDirichletAllocation]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
    "\n",
    "L&llocation de Dirichlet latente est un modèle probabiliste génératif pour les collections de données discrètes telles que les corpus de textes. C'est également un modèle de sujet qui est utilisé pour découvrir des sujets abstraits à partir d'une collection de documents.\n",
    "\n",
    "Le modèle graphique de LDA est un modèle génératif à trois niveaux :\n",
    "\n",
    "<img alt=\"https://scikit-learn.org/stable/_images/lda_model_graph.png\" class=\"align-center\" src=\"https://scikit-learn.org/stable/_images/lda_model_graph.png\" />\n",
    "\n",
    "Remarque sur les notations présentées dans le modèle graphique ci-dessus, que l'on peut retrouver dans Hoffman et al. (2013) :\n",
    "* *Le corpus est une collection de $D$ documents.\n",
    "* Un document est une séquence de $N$ mots.\n",
    "* Il y a $K$ sujets dans le corpus.\n",
    "* Les boîtes représentent des échantillonnages répétés.\n",
    "\n",
    "Dans le modèle graphique, chaque nœud est une variable aléatoire et a un rôle dans le processus de génération. Un nœud ombré indique une variable observée et un nœud non ombré indique une variable cachée (latente). Dans ce cas, les mots dans le corpus sont les seules données que nous observons. Les variables latentes déterminent le mélange aléatoire de sujets dans le corpus et la distribution des mots dans les documents. L'objectif de LDA est d'utiliser les mots observés pour déduire la structure cachée des sujets.\n",
    "\n",
    "Lors de la modélisation de corpus de textes, le modèle suppose le processus génératif suivant pour un corpus de $D$ documents et $K$ sujets, avec $K$ correspondant à `n_components` dans l'API :\n",
    "1. Pour chaque sujet $k \\in K$, tirer $\\beta_k \\sim \\mathrm{Dirichlet}(\\eta)$. Cela fournit une distribution sur les mots, c'est-à-dire la probabilité qu'un mot apparaisse dans le sujet $k$. $\\eta$ correspondent à `topic_word_prior`.\n",
    "2. Pour chaque document $d \\in D$, tirer les proportions de sujets $\\theta_d \\sim \\mathrm{Dirichlet}(\\alpha)$ correspondent à `doc_topic_prior`.\n",
    "3. Pour chaque mot $i$ dans le document $d$ :\n",
    "    1. Tirer l'assignation de sujet $z_{di} \\sim \\mathrm{Multinomial}(\\theta_d)$\n",
    "    2. Tirer le mot observé $w_{ij} \\sim \\mathrm{Multinomial}(\\beta_{z_{di}})$\n",
    "\n",
    "Pour l'estimation des paramètres, la distribution a posteriori est :\n",
    "\n",
    "$$p(z, \\theta, \\beta |w, \\alpha, \\eta) = \\frac{p(z, \\theta, \\beta|\\alpha, \\eta)}{p(w|\\alpha, \\eta)}$$\n",
    "\n",
    "Puisque la distribution a posteriori est intractable, la méthode bayésienne variationnelle utilise une distribution plus simple $q(z,\\theta,\\beta | \\lambda, \\phi, \\gamma)$ pour l'approximer, et ces paramètres variationnels $\\lambda$, $\\phi$, $\\gamma$ sont optimisés pour maximiser la limite inférieure de l'évidence (ELBO) :\n",
    "\n",
    "$$\\log\\: P(w | \\alpha, \\eta) \\geq L(w,\\phi,\\gamma,\\lambda) \\overset{\\triangle}{=}\n",
    "  E_{q}[\\log\\:p(w,z,\\theta,\\beta|\\alpha,\\eta)] - E_{q}[\\log\\:q(z, \\theta, \\beta)]$$\n",
    "\n",
    "Maximiser ELBO revient à minimiser la divergence Kullback-Leibler(KL) entre $q(z,\\theta,\\beta)$ et la véritable $p(z, \\theta, \\beta |w, \\alpha, \\eta)$ a posteriori.\n",
    "\n",
    "[**`LatentDirichletAllocation`**][LatentDirichletAllocation] implémente l'algorithme bayésien variationnel en ligne et prend en charge les méthodes de mise à jour en ligne et en lots. Alors que la méthode en lots met à jour les variables variationnelles après chaque passage complet dans les données, la méthode en ligne met à jour les variables variationnelles à partir de mini-lots de points de données.\n",
    "\n",
    "**Note** Bien que la méthode en ligne soit garantie de converger vers un point optimal local, la qualité du point optimal et la vitesse de convergence peuvent dépendre de la taille du mini-lot et des attributs liés au réglage du taux d'apprentissage.\n",
    "\n",
    "Lorsque [**`LatentDirichletAllocation`**][LatentDirichletAllocation] est appliqué sur une matrice \"document-terme\", la matrice sera décomposée en une matrice \"sujet-terme\" et une matrice \"document-sujet\". Alors que la matrice \"sujet-terme\" est stockée en tant que `components_` dans le modèle, la matrice \"document-sujet\" peut être calculée avec la méthode `transform`.\n",
    "\n",
    "[**`LatentDirichletAllocation`**][LatentDirichletAllocation] implémente également la méthode `partial_fit`. Elle est utilisée lorsque les données peuvent être récupérées séquentiellement.\n",
    "\n",
    "## Exemples\n",
    "\n",
    "### [**Extraction de sujets avec la factorisation de matrice non-négative et l'allocation latente de Dirichlet**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/applications/feature_selection/plot_topics_extraction_with_nmf_lda.ipynb)<br/>([*Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation*](https://scikit-learn.org/stable/applications/feature_selection/plot_topics_extraction_with_nmf_lda.html))\n",
    "\n",
    "## Références\n",
    "\n",
    "* [“**Latent Dirichlet Allocation**](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)[”](https://drive.google.com/file/d/1SDTUhg9fw_6Dsd6dFQc_jNjK4ejU9Sdc/view?usp=share_link) D. Blei, A. Ng, M. Jordan, 2003\n",
    "* [“**Online Learning for Latent Dirichlet Allocation**](https://proceedings.neurips.cc/paper/2010/file/71f6278d140af599e06ad9bf1ba03cb0-Paper.pdf)[”](https://drive.google.com/file/d/15riW5yBsxZFJfbM-WMPTcfK_K9yI7p3K/view?usp=share_link) M. Hoffman, D. Blei, F. Bach, 2010\n",
    "* [“**Stochastic Variational Inference**](https://www.cs.columbia.edu/~blei/papers/HoffmanBleiWangPaisley2013.pdf)[”](https://drive.google.com/file/d/1tJOQSEILN0nWMmyz5MU3JJyh90SyzWx0/view?usp=share_link) M. Hoffman, D. Blei, C. Wang, J. Paisley, 2013\n",
    "* [“**The varimax criterion for analytic rotation in factor analysis**](https://link.springer.com/article/10.1007/BF02289233)[”](https://drive.google.com/file/d/1PJYgFkZCIKCO-TaHevyo8BthWPjv3oxf/view?usp=share_link) H. F. Kaiser, 1958\n",
    "\n",
    "## Voir aussi\n",
    "\n",
    "[**Réduction de dimensionnalité** (1.6.7.2)](https://scikit-learn.org/stable/modules/neighbors.html#nca-dim-reduction) pour la réduction de dimensionnalité avec l'analyse de composantes voisines (*Neighborhood Components Analysis*)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e03b612d84ba21ce95ed447e81b3062e1eb99b56c6d885cdab4aaa12f1b8e240"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
