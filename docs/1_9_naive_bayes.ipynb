{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='supervised-learning'></a> 1. [**Apprentissage supervis√©**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#supervised-learning)</br>([*Supervised learning*](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning))\n",
    "\n",
    "# <a id='naive-bayes'></a> 1.9. [**Bayes na√Øf**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_9_naive_bayes.ipynb)<br/>([_Naive Bayes_](https://scikit-learn.org/stable/modules/naive_bayes.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 5 pages, 0 exemples, 5 papiers\n",
    "- 1.9.1. [**Bayes na√Øf gaussien**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_9_naive_bayes.ipynb#gaussian-naive-bayes)<br/>([_Gaussian Naive Bayes_](https://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes))\n",
    "- 1.9.2. [**Bayes na√Øf multinomial**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_9_naive_bayes.ipynb#multinomial-naive-bayes)<br/>([_Multinomial Naive Bayes_](https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes))\n",
    "- 1.9.3. [**Bayes na√Øf compl√©mentaire**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_9_naive_bayes.ipynb#complement-naive-bayes)<br/>([_Complement Naive Bayes_](https://scikit-learn.org/stable/modules/naive_bayes.html#complement-naive-bayes))\n",
    "- 1.9.4. [**Bayes na√Øf Bernoulli**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_9_naive_bayes.ipynb#bernoulli-naive-bayes)<br/>([_Bernoulli Naive Bayes_](https://scikit-learn.org/stable/modules/naive_bayes.html#bernoulli-naive-bayes))\n",
    "- 1.9.5. [**Bayes na√Øf cat√©goriel**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_9_naive_bayes.ipynb#categorical-naive-bayes)<br/>([_Categorical Naive Bayes_](https://scikit-learn.org/stable/modules/naive_bayes.html#categorical-naive-bayes))\n",
    "- 1.9.6. [**Ajustement de mod√®le Bayes na√Øf hors ligne**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_9_naive_bayes.ipynb#out-of-core-naive-bayes-model-fitting)<br/>([_Out-of-core naive Bayes model fitting_](https://scikit-learn.org/stable/modules/naive_bayes.html#out-of-core-naive-bayes-model-fitting))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='naive-bayes'></a> 1.9. **Bayes na√Øf**<br/>([_Naive Bayes_](https://scikit-learn.org/stable/modules/naive_bayes.html))\n",
    "\n",
    "Les m√©thodes Bayes na√Øf sont un ensemble d'algorithmes d'apprentissage supervis√© bas√©s sur l'application du th√©or√®me de Bayes en faisant l'hypoth√®se \"na√Øve\" d'ind√©pendance conditionnelle entre chaque paire de caract√©ristiques, √©tant donn√© la valeur de la variable de classe. Le th√©or√®me de Bayes √©nonce la relation suivante, √©tant donn√© la variable de classe $y$ et le vecteur de caract√©ristiques d√©pendantes $x_1$ √† $x_n$ :\n",
    "\n",
    "$$\n",
    "P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots, x_n \\mid y)}{P(x_1, \\dots, x_n)}\n",
    "$$\n",
    "\n",
    "En utilisant l'hypoth√®se na√Øve d'ind√©pendance conditionnelle selon laquelle\n",
    "\n",
    "$$\n",
    "P(x_i | y, x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n) = P(x_i | y),\n",
    "$$\n",
    "\n",
    "pour tout $i$, cette relation est simplifi√©e en\n",
    "\n",
    "$$\n",
    "P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i \\mid y)}{P(x_1, \\dots, x_n)}\n",
    "$$\n",
    "\n",
    "√âtant donn√© que $P(x_1, \\dots, x_n)$ est constant par rapport √† l'entr√©e, nous pouvons utiliser la r√®gle de classification suivante :\n",
    "\n",
    "$$\n",
    "\\begin{align}\\begin{aligned}P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y)\\\\\n",
    "\\Downarrow\\\\\n",
    "\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i \\mid y),\\end{aligned}\\end{align}\n",
    "$$\n",
    "\n",
    "et nous pouvons utiliser l'estimation du Maximum A Posteriori (MAP) pour estimer $P(y)$ et $P(x_i \\mid y) ; le premier est alors la fr√©quence relative de la classe $y$ dans l'ensemble d'entra√Ænement.\n",
    "\n",
    "Les diff√©rents classificateurs Bayes na√Øfs diff√®rent principalement par les hypoth√®ses qu'ils font concernant la distribution de $P(x_i \\mid y)$.\n",
    "\n",
    "Malgr√© leurs hypoth√®ses apparemment simplistes, les classificateurs Bayes na√Øfs ont tr√®s bien fonctionn√© dans de nombreuses situations du monde r√©el, notamment la classification de documents et la lutte contre le spam. Ils n√©cessitent une petite quantit√© de donn√©es d'entra√Ænement pour estimer les param√®tres n√©cessaires. (Pour des raisons th√©oriques pour lesquelles Bayes na√Øf fonctionne bien, et sur quels types de donn√©es il fonctionne, consultez les r√©f√©rences ci-dessous.)\n",
    "\n",
    "Les apprenants et classificateurs Bayes na√Øfs peuvent √™tre extr√™mement rapides par rapport √† des m√©thodes plus sophistiqu√©es. La dissociation des distributions de caract√©ristiques conditionnelles de classe signifie que chaque distribution peut √™tre estim√©e de mani√®re ind√©pendante en tant que distribution unidimensionnelle. Cela contribue √† r√©soudre les probl√®mes d√©coulant de la mal√©diction de la dimensionnalit√©.\n",
    "\n",
    "D'un autre c√¥t√©, bien que le Bayes na√Øf soit connu comme un bon classificateur, il est √©galement connu pour √™tre un mauvais estimateur, donc les sorties de probabilit√© de `predict_proba` ne doivent pas √™tre prises trop au s√©rieux.\n",
    "\n",
    "## R√©f√©rences\n",
    "\n",
    "üî¨ H. Zhang (2004). [**‚ÄúThe optimality of Naive Bayes‚Äù**](https://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf). Proc. FLAIRS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='gaussian-naive-bayes'></a> 1.9.1. **Bayes na√Øf gaussien**<br/>([_Gaussian Naive Bayes_](https://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes))\n",
    "\n",
    "[**`GaussianNB`**](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB) impl√©mente l'algorithme Bayes na√Øf gaussien pour la classification. On suppose que la vraisemblance des caract√©ristiques suit une distribution gaussienne :\n",
    "\n",
    "$$\n",
    "P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right)\n",
    "$$\n",
    "\n",
    "Les param√®tres $\\sigma_y$ et $\\mu_y$ sont estim√©s en utilisant la m√©thode du maximum de vraisemblance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 75 points : 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "# Number of mislabeled points out of a total 75 points : 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='multinomial-naive-bayes'></a> 1.9.2. **Bayes na√Øf multinomial**<br/>([_Multinomial Naive Bayes_](https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes))\n",
    "\n",
    "[**`MultinomialNB`**](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) impl√©mente l'algorithme Bayes na√Øf pour les donn√©es √† distribution multinomiale. C'est l'une des deux variantes classiques de Bayes na√Øf utilis√©es en classification de texte (o√π les donn√©es sont g√©n√©ralement repr√©sent√©es sous forme de comptages de vecteurs de mots, bien que les vecteurs tf-idf soient √©galement connus pour bien fonctionner en pratique). La distribution est param√©tr√©e par des vecteurs $\\theta_y = (\\theta_{y1},\\ldots,\\theta_{yn})$ pour chaque classe $y$, o√π $n$ est le nombre de caract√©ristiques (en classification de texte, la taille du vocabulaire) et $\\theta_{yi}$ est la probabilit√© $P(x_i \\mid y)$ que la caract√©ristique $i$ apparaisse dans un √©chantillon appartenant √† la classe $y$.\n",
    "\n",
    "Les param√®tres $\\theta_y$ sont estim√©s par une version liss√©e de la m√©thode du maximum de vraisemblance, c'est-√†-dire en comptant la fr√©quence relative :\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{yi} = \\frac{N_{yi} + \\alpha}{N_y + \\alpha n}\n",
    "$$\n",
    "\n",
    "o√π $N_{yi} = \\sum_{x \\in T} x_i$ est le nombre de fois o√π la caract√©ristique $i$ appara√Æt dans un √©chantillon de la classe $y$ dans l'ensemble d'apprentissage $T$, et $N_{y} = \\sum_{i=1}^{n} N_{yi}$ est le nombre total de toutes les caract√©ristiques de la classe $y$.\n",
    "\n",
    "Les param√®tres de lissage $\\alpha \\ge 0$ tiennent compte des caract√©ristiques absentes dans les √©chantillons d'apprentissage et √©vitent les probabilit√©s nulles dans les calculs ult√©rieurs. Lorsque $\\alpha = 1$, on parle de lissage de Laplace, tandis que $\\alpha < 1$ est appel√© lissage de Lidstone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='complement-naive-bayes'></a> 1.9.3. **Bayes na√Øf compl√©mentaire**<br/>([_Complement Naive Bayes_](https://scikit-learn.org/stable/modules/naive_bayes.html#complement-naive-bayes))\n",
    "\n",
    "[**`ComplementNB`**](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.ComplementNB.html#sklearn.naive_bayes.ComplementNB) impl√©mente l'algorithme du Bayes na√Øf compl√©mentaire (CNB). Le CNB est une adaptation de l'algorithme standard du Bayes na√Øf multinomial (MNB) qui est particuli√®rement adapt√© aux ensembles de donn√©es d√©s√©quilibr√©s. Plus pr√©cis√©ment, le CNB utilise des statistiques du _compl√©ment_ de chaque classe pour calculer les poids du mod√®le. Les inventeurs du CNB montrent empiriquement que les estimations des param√®tres du CNB sont plus stables que celles du MNB. De plus, le CNB surpasse r√©guli√®rement le MNB (souvent de mani√®re consid√©rable) dans les t√¢ches de classification de texte. Le processus de calcul des poids est le suivant :\n",
    "\n",
    "$$\n",
    "\\begin{align*}\\begin{aligned}\n",
    "\\hat{\\theta}_{ci} = \\frac{\\alpha_i + \\sum_{j:y_j \\neq c} d_{ij}}\n",
    "{\\alpha + \\sum_{j:y_j \\neq c} \\sum_{k} d_{kj}}\\\\w_{ci} = \\log \\hat{\\theta}_{ci}\\\\\n",
    "w_{ci} = \\frac{w_{ci}}{\\sum_{j} |w_{cj}|}\n",
    "\\end{aligned}\\end{align*}\n",
    "$$\n",
    "\n",
    "o√π les sommes portent sur tous les documents $j$ qui ne sont pas dans la classe $c$, $d_{ij}$ est soit le d√©compte, soit la valeur tf-idf du terme $i$ dans le document $j$, $\\alpha_i$ est un hyperparam√®tre de lissage comme celui que l'on trouve dans le MNB, et $\\alpha = \\sum_{i} \\alpha_i$. La deuxi√®me normalisation traite la tendance des documents plus longs √† dominer les estimations des param√®tres dans le MNB. La r√®gle de classification est la suivante :\n",
    "\n",
    "$$\n",
    "\\hat{c} = \\arg\\min_c \\sum_{i} t_i w_{ci}\n",
    "$$\n",
    "\n",
    "c'est-√†-dire qu'un document est assign√© √† la classe qui est le _compl√©ment le moins bon_.\n",
    "\n",
    "### R√©f√©rences\n",
    "\n",
    "üî¨ Rennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003). [**‚ÄúTackling the Poor Assumptions of Naive Bayes Text Classifiers‚Äù**](https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf). In ICML (Vol. 3, pp. 616-623)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='bernoulli-naive-bayes'></a> 1.9.4. **Bayes na√Øf Bernoulli**<br/>([_Bernoulli Naive Bayes_](https://scikit-learn.org/stable/modules/naive_bayes.html#bernoulli-naive-bayes))\n",
    "\n",
    "[**`BernoulliNB`**](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB) impl√©mente les algorithmes d'entra√Ænement et de classification du Bayes na√Øf pour des donn√©es distribu√©es selon des lois de Bernoulli multivari√©es ; c'est-√†-dire, il peut y avoir de multiples caract√©ristiques, mais on suppose que chacune d'entre elles est une variable binaire (Bernoulli, bool√©enne). Par cons√©quent, cette classe n√©cessite que les √©chantillons soient repr√©sent√©s sous forme de vecteurs de caract√©ristiques √† valeurs binaires ; si on lui fournit un autre type de donn√©es, une instance de `BernoulliNB` peut binariser son entr√©e (selon le param√®tre `binarize`).\n",
    "\n",
    "La r√®gle de d√©cision pour le Bayes na√Øf Bernoulli est bas√©e sur\n",
    "\n",
    "$$\n",
    "P(x_i \\mid y) = P(x_i = 1 \\mid y) x_i + (1 - P(x_i = 1 \\mid y)) (1 - x_i)\n",
    "$$\n",
    "\n",
    "ce qui diff√®re de la r√®gle du Bayes na√Øf multinomial en ce sens qu'il p√©nalise explicitement la non-occurrence d'une caract√©ristique qui est un indicateur de la classe $y$, tandis que la variante multinomiale ignorerait simplement une caract√©ristique non-occurrence.\n",
    "\n",
    "Dans le cas de la classification de texte, des vecteurs d'occurrence de mots (plut√¥t que des vecteurs de d√©compte de mots) peuvent √™tre utilis√©s pour entra√Æner et utiliser ce classifieur. `BernoulliNB` pourrait mieux fonctionner sur certains ensembles de donn√©es, en particulier ceux avec des documents plus courts. Il est conseill√© d'√©valuer les deux mod√®les, si le temps le permet.\n",
    "\n",
    "### R√©f√©rences\n",
    "\n",
    "üìö C.D. Manning, P. Raghavan, H. Sch√ºtze, [**‚ÄúIntroduction to Information Retrieval‚Äù**](https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf). Cambridge University Press, pp. 234-265.\n",
    "\n",
    "üî¨ A. McCallum and K. Nigam (1998). [**‚ÄúA Comparison of Event Models for Naive Bayes Text Classification‚Äù**](http://courses.washington.edu/ling572/papers/mccallum1998_AAAI.pdf). Proc. AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48.\n",
    "\n",
    "üî¨ V. Metsis, I. Androutsopoulos and G. Paliouras (2006). [**‚ÄúSpam filtering with Naive Bayes ‚Äì Which Naive Bayes?‚Äù**](https://userweb.cs.txstate.edu/~v_m137/docs/papers/ceas2006_paper_corrected.pdf) 3rd Conf. on Email and Anti-Spam (CEAS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='categorical-naive-bayes'></a> 1.9.5. **Bayes na√Øf cat√©goriel**<br/>([_Categorical Naive Bayes_](https://scikit-learn.org/stable/modules/naive_bayes.html#categorical-naive-bayes))\n",
    "\n",
    "[**`CategoricalNB`**](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html#sklearn.naive_bayes.CategoricalNB) impl√©mente l'algorithme du Bayes na√Øf cat√©goriel pour des donn√©es √† distribution cat√©gorielle. Il suppose que chaque caract√©ristique, d√©crite par l'indice $i$, poss√®de sa propre distribution cat√©gorielle.\n",
    "\n",
    "Pour chaque caract√©ristique $i$ dans l'ensemble d'entra√Ænement $X$, [**`CategoricalNB`**](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html#sklearn.naive_bayes.CategoricalNB) estime une distribution cat√©gorielle pour chaque caract√©ristique $i$ de $X$ conditionn√©e √† la classe $y$. L'ensemble d'indices des √©chantillons est d√©fini comme $J = \\{1, \\ldots, m\\}$, avec $m$ √©tant le nombre d'√©chantillons.\n",
    "\n",
    "La probabilit√© de la cat√©gorie $t$ dans la caract√©ristique $i$ √©tant donn√© la classe $c$ est estim√©e comme suit :\n",
    "\n",
    "$$\n",
    "P(x_i = t \\mid y = c \\: ;\\, \\alpha) = \\frac{N_{tic} + \\alpha}{N_{c} + \\alpha n_i},\n",
    "$$\n",
    "\n",
    "o√π $N_{tic} = |\\{j \\in J \\mid x_{ij} = t, y_j = c\\}|$ est le nombre de fois o√π la cat√©gorie $t$ appara√Æt dans les √©chantillons $x_{i}$, qui appartiennent √† la classe $c$, $N_{c} = |\\{ j \\in J\\mid y_j = c\\}|$ est le nombre d'√©chantillons de la classe $c$, $\\alpha$ est un param√®tre de lissage et $n_i$ est le nombre de cat√©gories disponibles pour la caract√©ristique $i$.\n",
    "\n",
    "[**`CategoricalNB`**](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html#sklearn.naive_bayes.CategoricalNB) suppose que la matrice d'√©chantillons $X$ est encod√©e (par exemple, √† l'aide de [**`OrdinalEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder)) de telle mani√®re que toutes les cat√©gories pour chaque caract√©ristique $i$ soient repr√©sent√©es par des nombres de $0$ √† $n_i - 1$, o√π $n_i$ est le nombre de cat√©gories disponibles pour la caract√©ristique $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='out-of-core-naive-bayes-model-fitting'></a> 1.9.6. **Ajustement de mod√®le Bayes na√Øf hors ligne**<br/>([_Out-of-core naive Bayes model fitting_](https://scikit-learn.org/stable/modules/naive_bayes.html#out-of-core-naive-bayes-model-fitting))\n",
    "\n",
    "Les mod√®les de Bayes na√Øf peuvent √™tre utilis√©s pour r√©soudre des probl√®mes de classification √† grande √©chelle pour lesquels l'ensemble d'entra√Ænement complet pourrait ne pas tenir en m√©moire. Pour g√©rer ce cas, [**`MultinomialNB`**](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB), [**`BernoulliNB`**](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB) et [**`GaussianNB`**](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB) exposent une m√©thode `partial_fit` qui peut √™tre utilis√©e de mani√®re progressive, comme avec d'autres classifieurs, comme illustr√© dans [**La classification hors ligne de documents textuels**](https://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py). Tous les classifieurs Bayes na√Øfs prennent en charge la pond√©ration des √©chantillons.\n",
    "\n",
    "Contrairement √† la m√©thode `fit`, le premier appel √† `partial_fit` doit √™tre pass√© la liste de toutes les √©tiquettes de classe attendues.\n",
    "\n",
    "Pour un aper√ßu des strat√©gies disponibles dans scikit-learn, consultez √©galement la documentation sur l'[**apprentissage hors ligne** (8.1)](https://scikit-learn.org/stable/computing/scaling_strategies.html#scaling-strategies).\n",
    "\n",
    "> **Remarque :** L'appel de la m√©thode `partial_fit` des mod√®les Bayes na√Øfs introduit une certaine surcharge de calcul. Il est recommand√© d'utiliser des tailles de fragments de donn√©es aussi grandes que possible, c'est-√†-dire celles autoris√©es par la RAM disponible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
