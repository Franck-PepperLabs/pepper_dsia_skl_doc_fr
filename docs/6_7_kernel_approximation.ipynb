{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='kernel-approximation'></a> 6.7. [**Approximation de noyaux**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_7_kernel_approximation.ipynb#kernel-approximation)</br>([_Kernel Approximation_](https://scikit-learn.org/stable/modules/kernel_approximation.html#kernel-approximation))\n",
    "\n",
    "Ce sous-module contient des fonctions qui approximent les mappings de caract√©ristiques correspondant √† certains noyaux, tels qu'ils sont utilis√©s par exemple dans les machines √† vecteurs de support (voir [**Machines √† vecteurs de support** (1.4)](https://scikit-learn.org/stable/modules/svm.html#svm)). Les fonctions de caract√©ristiques suivantes effectuent des transformations non lin√©aires de l'entr√©e, qui peuvent servir de base pour la classification lin√©aire ou d'autres algorithmes.\n",
    "\n",
    "L'avantage d'utiliser des mappings de caract√©ristiques explicites approximatifs par rapport √† la [**technique des noyaux**](https://en.wikipedia.org/wiki/Kernel_trick), qui utilise les mappings de caract√©ristiques de mani√®re implicite, est que les mappings explicites peuvent √™tre mieux adapt√©s √† l'apprentissage en ligne et peuvent r√©duire consid√©rablement le co√ªt de l'apprentissage avec des ensembles de donn√©es tr√®s volumineux. Les SVM √† noyau standard ne sont pas adapt√©s aux ensembles de donn√©es volumineux, mais en utilisant une approximation du mapping de noyau, il est possible d'utiliser des SVM lin√©aires beaucoup plus efficaces. En particulier, la combinaison d'approximations de mappings de noyaux avec [**`SGDClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier) permet d'effectuer un apprentissage non lin√©aire sur des ensembles de donn√©es volumineux.\n",
    "\n",
    "Comme il n'y a pas eu beaucoup de travaux empiriques utilisant des plongements approximatifs, il est conseill√© de comparer les r√©sultats avec des m√©thodes de noyaux exactes lorsque cela est possible.\n",
    "\n",
    "**Voir aussi :** [**R√©gression polynomiale : √©tendre les mod√®les lin√©aires avec des fonctions de base** (1.1)](https://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression) pour une transformation polynomiale exacte."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "‚úî  6.7. [**Approximation du noyau**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_7_kernel_approximation.ipynb#kernel-approximation)\n",
    "([*Kernel Approximation*](https://scikit-learn.org/stable/modules/kernel_approximation.html#kernel-approximation))\n",
    "* **Volume** : 6 pages, 2 exemples, 7 papiers\n",
    "* ‚úî 6.7.1. [**M√©thode Nystroem pour l'approximation du noyau**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_7_kernel_approximation.ipynb#nystroem-method-for-kernel-approximation)\n",
    "([_Nystroem Method for Kernel Approximation_](https://scikit-learn.org/stable/modules/kernel_approximation.html#nystroem-method-for-kernel-approximation))\n",
    "* ‚úî 6.7.2. [**Noyau de fonction de base radiale**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_7_kernel_approximation.ipynb#radial-basis-function-kernel)\n",
    "([_Radial Basis Function Kernel_](https://scikit-learn.org/stable/modules/kernel_approximation.html#radial-basis-function-kernel))\n",
    "* ‚úî 6.7.3. [**Additif Chi Squared Kernel**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_7_kernel_approximation.ipynb#)\n",
    "([_Additive Chi Squared Kernel_](https://scikit-learn.org/stable/modules/kernel_approximation.html#additive-chi-squared-kernel))\n",
    "* ‚úî 6.7.4. [**Noyau au carr√© de chi asym√©trique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_7_kernel_approximation.ipynb#)\n",
    "([_Skewed Chi Squared Kernel_](https://scikit-learn.org/stable/modules/kernel_approximation.html#skewed-chi-squared-kernel))\n",
    "* ‚úî 6.7.5. [**Approximation du noyau polynomial via Tensor Sketch**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_7_kernel_approximation.ipynb#)\n",
    "([_Polynomial Kernel Approximation via Tensor Sketch_](https://scikit-learn.org/stable/modules/kernel_approximation.html#polynomial-kernel-approximation-via-tensor-sketch))\n",
    "* ‚úî 6.7.6. [**D√©tails math√©matiques**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_7_kernel_approximation.ipynb#)\n",
    "([_Mathematical Details_](https://scikit-learn.org/stable/modules/kernel_approximation.html#mathematical-details))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='nystroem-method-for-kernel-approximation'></a> 6.7.1. M√©thode Nystroem pour l'approximation du noyau\n",
    "\n",
    "La m√©thode de Nystroem, telle qu'elle est mise en ≈ìuvre dans [**`Nystroem`**](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.Nystroem.html), est une m√©thode g√©n√©rale d'approximation √† faible rang des noyaux. Elle y parvient en sous-√©chantillonnant essentiellement les donn√©es sur lesquelles le noyau est √©valu√©. Par d√©faut, [**`Nystroem`**](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.Nystroem.html) utilise le noyau `rbf`, mais il peut utiliser n'importe quelle fonction de noyau ou une matrice de noyau pr√©calcul√©e. Le nombre d'√©chantillons utilis√©s - qui est √©galement la dimensionnalit√© des caract√©ristiques calcul√©es - est donn√© par le param√®tre `n_components`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='radial-basis-function-kernel'></a> 6.7.2. Noyau de la fonction de base radiale (Radial Basis Function Kernel)\n",
    "\n",
    "Le [**`RBFSampler`**](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.RBFSampler.html#sklearn.kernel_approximation.RBFSampler) construit une approximation de la fonction de base radiale du noyau, √©galement connue sous le nom de _Random Kitchen Sinks_ [RR2007]. Cette transformation peut √™tre utilis√©e pour mod√©liser explicitement une fonction de mapping de noyau, avant d'appliquer un algorithme lin√©aire, par exemple un SVM lin√©aire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n",
    "y = [0, 0, 1, 1]\n",
    "rbf_feature = RBFSampler(gamma=1, random_state=1)\n",
    "X_features = rbf_feature.fit_transform(X)\n",
    "clf = SGDClassifier(max_iter=5)\n",
    "clf.fit(X_features, y)\n",
    "# SGDClassifier(max_iter=5)\n",
    "clf.score(X_features, y)\n",
    "# 1.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La transformation repose sur une approximation de Monte Carlo des valeurs du noyau. La fonction `fit` effectue l'√©chantillonnage de Monte Carlo, tandis que la m√©thode `transform` effectue la transformation des donn√©es. En raison de la nature al√©atoire du processus, les r√©sultats peuvent varier entre diff√©rentes appels √† la fonction `fit`.\n",
    "\n",
    "La fonction `fit` prend deux arguments : `n_components`, qui est la dimensionnalit√© cible de la transformation des caract√©ristiques, et `gamma`, le param√®tre du noyau RBF. Un `n_components` plus √©lev√© donnera une meilleure approximation du noyau et produira des r√©sultats plus similaires √† ceux produits par un SVM √† noyau. Notez que \"l'ajustement\" de la fonction de caract√©ristiques ne d√©pend pas r√©ellement des donn√©es donn√©es √† la fonction `fit`. Seule la dimensionnalit√© des donn√©es est utilis√©e. Les d√©tails sur la m√©thode peuvent √™tre trouv√©s dans [RR2007].\n",
    "\n",
    "Pour une valeur donn√©e de `n_components`, [**`RBFSampler`**](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.RBFSampler.html) est souvent moins pr√©cis que [**`Nystroem`**](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.Nystroem.html). Cependant, le calcul de [**`RBFSampler`**](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.RBFSampler.html) est moins co√ªteux, ce qui rend l'utilisation d'espaces de caract√©ristiques plus grands plus efficace.\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_kernel_approximation_002.png)\n",
    "\n",
    "Comparaison d'un noyau RBF exact (√† gauche) avec l'approximation (√† droite)\n",
    "\n",
    "### Exemples\n",
    "\n",
    "####  [**Approximation explicite de la correspondance de caract√©ristiques pour les noyaux RBF**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/misc/plot_kernel_approximation.ipynb)<br/>([*Explicit feature map approximation for RBF kernels*](https://scikit-learn.org/stable/auto_examples/misc/plot_kernel_approximation.html))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='additive-chi-squared-kernel'></a> 6.7.3. Noyau chi carr√© additif (Additive Chi Squared Kernel)\n",
    "\n",
    "Le noyau chi carr√© additif est un noyau sur les histogrammes, souvent utilis√© en vision par ordinateur.\n",
    "\n",
    "Le noyau chi carr√© additif utilis√© ici est donn√© par\n",
    "\n",
    "$$k(x, y) = \\sum_i \\frac{2x_iy_i}{x_i+y_i}$$\n",
    "\n",
    "Ce n'est pas exactement la m√™me chose que `sklearn.metrics.additive_chi2_kernel`. Les auteurs de [VZ2010] pr√©f√®rent la version ci-dessus car elle est toujours d√©finie positive. √âtant donn√© que le noyau est additif, il est possible de traiter toutes les composantes $x_i$ s√©par√©ment pour le plongement. Cela permet d'√©chantillonner la transform√©e de Fourier √† intervalles r√©guliers, au lieu d'utiliser une approximation par √©chantillonnage de Monte Carlo.\n",
    "\n",
    "La classe [**`AdditiveChi2Sampler`**](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.AdditiveChi2Sampler.html) met en ≈ìuvre cet √©chantillonnage d√©terministe par composant. Chaque composant est √©chantillonn√© $n$ fois, ce qui donne $2n+1$ dimensions par dimension d'entr√©e (le multiple de deux provient de la partie r√©elle et de la partie complexe de la transform√©e de Fourier). Dans la litt√©rature, $n$ est g√©n√©ralement choisi pour √™tre 1 ou 2, transformant l'ensemble de donn√©es en une taille de `n_samples * 5 * n_features` (dans le cas de $n = 2$).\n",
    "\n",
    "La fonctionnalit√© d'approximation de la correspondance de caract√©ristiques fournie par [**`AdditiveChi2Sampler`**](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.AdditiveChi2Sampler.html) peut √™tre combin√©e avec l'approximation de la correspondance de caract√©ristiques fournie par [**`RBFSampler`**](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.RBFSampler.html) pour obtenir une approximation de la correspondance de caract√©ristiques pour le noyau exponentiel chi carr√©. Voir [VZ2010] pour plus de d√©tails et [VVZ2010] pour la combinaison avec [**`RBFSampler`**](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.RBFSampler.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='skewed-chi-squared-kernel'></a> 6.7.4. Skewed Chi Squared Kernel (Skewed Chi Squared Kernel)\n",
    "\n",
    "Le noyau chi carr√© asym√©trique est donn√© par :\n",
    "\n",
    "$$k(x,y) = \\prod_i \\frac{2\\sqrt{x_i+c}\\sqrt{y_i+c}}{x_i + y_i + 2c}$$\n",
    "\n",
    "Il poss√®de des propri√©t√©s similaires au noyau chi carr√© exponentiel souvent utilis√© en vision par ordinateur, mais permet une approximation simple par √©chantillonnage de Monte Carlo de la correspondance de caract√©ristiques.\n",
    "\n",
    "L'utilisation de [**`SkewedChi2Sampler`**](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.SkewedChi2Sampler.html) est similaire √† celle d√©crite ci-dessus pour [**`RBFSampler`**](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.RBFSampler.html). La seule diff√©rence r√©side dans le param√®tre libre appel√© $\\eta$. Pour une motivation de cette correspondance et les d√©tails math√©matiques, voir [LS2010]."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='polynomial-kernel-approximation-via-tensor-sketch'></a> 6.7.5. Approximation du noyau polynomial via Tensor Sketch\n",
    "\n",
    "Le [**noyau polynomial** (6.8.3)](https://scikit-learn.org/stable/modules/metrics.html#polynomial-kernel) est un type populaire de fonction de noyau donn√© par :\n",
    "\n",
    "$$k(x, y) = (\\gamma x^\\top y +c_0)^d$$\n",
    "\n",
    "o√π :\n",
    "\n",
    "- $x$ et $y$ sont les vecteurs d'entr√©e,\n",
    "- $d$ est le degr√© du noyau.\n",
    "\n",
    "De mani√®re intuitive, l'espace des caract√©ristiques du noyau polynomial de degr√© $d$ comprend tous les produits possibles de degr√© $d$ entre les caract√©ristiques d'entr√©e, ce qui permet aux algorithmes d'apprentissage utilisant ce noyau de tenir compte des interactions entre les caract√©ristiques.\n",
    "\n",
    "La m√©thode TensorSketch [PP2013], mise en ≈ìuvre dans [**`PolynomialCountSketch`**](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.PolynomialCountSketch.html), est une m√©thode √©volutive, ind√©pendante des donn√©es d'entr√©e, pour l'approximation du noyau polynomial. Elle est bas√©e sur le concept de Count sketch [WIKICS] [CCF2002], une technique de r√©duction de dimensionnalit√© similaire au hachage de caract√©ristiques, qui utilise plusieurs fonctions de hachage ind√©pendantes. TensorSketch obtient un Count Sketch du produit externe de deux vecteurs (ou d'un vecteur avec lui-m√™me), qui peut √™tre utilis√© comme une approximation de l'espace des caract√©ristiques du noyau polynomial. En particulier, au lieu de calculer explicitement le produit externe, TensorSketch calcule le Count Sketch des vecteurs, puis utilise une multiplication polynomiale via la transform√©e de Fourier rapide pour calculer le Count Sketch de leur produit externe.\n",
    "\n",
    "De mani√®re pratique, la phase d'entra√Ænement de TensorSketch consiste simplement √† initialiser certaines variables al√©atoires. Elle est donc ind√©pendante des donn√©es d'entr√©e, c'est-√†-dire qu'elle d√©pend uniquement du nombre de caract√©ristiques d'entr√©e, mais pas des valeurs des donn√©es. De plus, cette m√©thode peut transformer des √©chantillons en $\\mathcal{O}(n_{\\text{samples}}(n_{\\text{features}} + n_{\\text{components}} \\log(n_{\\text{components}})))$, o√π $n_{\\text{components}}$ est la dimension de sortie souhait√©e, d√©termin√©e par `n_components`.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Apprentissage extensible avec une approximation du noyau polynomial**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/6_7_kernel_approximation/plot_scalable_poly_kernels.ipynb)<br/>([_Scalable learning with polynomial kernel approximation_](https://scikit-learn.org/stable/auto_examples/kernel_approximation/plot_scalable_poly_kernels.html))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='mathematical-details'></a> 6.7.6. D√©tails math√©matiques\n",
    "\n",
    "Les m√©thodes de noyau telles que les machines √† vecteurs de support (SVM) ou l'ACP noyaut√©e reposent sur une propri√©t√© des espaces de Hilbert √† noyau reproduisant. Pour toute fonction de noyau d√©finie positive $k$ (appel√©e noyau de Mercer), il est garanti qu'il existe une correspondance dans un espace de Hilbert $\\mathcal{H}$ telle que\n",
    "\n",
    "$$k(x,y) = \\langle \\phi(x), \\phi(y) \\rangle$$\n",
    "\n",
    "o√π $\\langle \\cdot, \\cdot \\rangle$ d√©signe le produit scalaire dans l'espace de Hilbert.\n",
    "\n",
    "Si un algorithme, tel qu'une machine √† vecteurs de support lin√©aire ou une ACP, ne d√©pend que du produit scalaire des points de donn√©es $x_i$, on peut utiliser la valeur de $k(x_i, x_j)$, ce qui correspond √† appliquer l'algorithme aux points de donn√©es correspondants $\\phi(x_i)$. L'avantage d'utiliser $k$ est que la correspondance $\\phi$ n'a jamais besoin d'√™tre calcul√©e explicitement, ce qui permet d'avoir des caract√©ristiques arbitrairement grandes (voire infinies).\n",
    "\n",
    "Un inconv√©nient des m√©thodes de noyau est qu'il peut √™tre n√©cessaire de stocker de nombreuses valeurs de noyau $k(x_i, x_j)$ pendant l'optimisation. Si un classifieur noyaut√© est appliqu√© √† de nouvelles donn√©es $y_j$, il est n√©cessaire de calculer $k(x_i, x_j)$ pour effectuer des pr√©dictions, potentiellement pour de nombreux $x_i$ diff√©rents dans l'ensemble d'entra√Ænement.\n",
    "\n",
    "Les classes de ce sous-module permettent d'approximer la correspondance $\\phi$, travaillant ainsi explicitement avec les repr√©sentations $\\phi(x_i)$, ce qui √©vite d'appliquer le noyau ou de stocker les exemples d'entra√Ænement."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R√©f√©rences\n",
    "\n",
    "üî¨ [RR2007] (1,2) [‚Äú**Random features for large-scale kernel machines**](https://papers.nips.cc/paper/2007/hash/013a006f03dbc5392effeb8f18fda755-Abstract.html)[‚Äù](https://drive.google.com/file/d/1M3PZaLgkoCFmoQMUS8Wu9LOEPJdOc0OM/view?usp=drive_link) Rahimi, A. and Recht, B. - Advances in neural information processing 2007,\n",
    "\n",
    "üî¨ [LS2010] [‚Äú**Random Fourier approximations for skewed multiplicative histogram kernels**](https://www.researchgate.net/publication/221114584_Random_Fourier_Approximations_for_Skewed_Multiplicative_Histogram_Kernels)[‚Äù](https://drive.google.com/file/d/1niKSiwVYFC2O2666aVK6gNaFlgovgQJW/view?usp=drive_link) Li, F., Ionescu, C., and Sminchisescu, C. - Pattern Recognition, DAGM 2010, Lecture Notes in Computer Science.\n",
    "\n",
    "üî¨ [VZ2010] (1,2) [‚Äú**Efficient additive kernels via explicit feature maps**](https://www.robots.ox.ac.uk/~vgg/publications/2011/Vedaldi11/vedaldi11.pdf)[‚Äù](https://drive.google.com/file/d/1HYkw9kyMhEu8X49z97-Hl3yYwAyE4wJt/view?usp=drive_link) Vedaldi, A. and Zisserman, A. - Computer Vision and Pattern Recognition 2010\n",
    "\n",
    "üî¨ [VVZ2010] [‚Äú**Generalized RBF feature maps for Efficient Detection**](https://www.robots.ox.ac.uk/~vgg/publications/2010/Sreekanth10/sreekanth10.pdf)[‚Äù](https://drive.google.com/file/d/1W6SPhxHEWmPUMT-JvmC2MP4WPhK5UqW2/view?usp=drive_link) Vempati, S. and Vedaldi, A. and Zisserman, A. and Jawahar, CV - 2010\n",
    "\n",
    "üî¨ [PP2013] [‚Äú**Fast and scalable polynomial kernels via explicit feature maps**](https://dl.acm.org/doi/10.1145/2487575.2487591)[‚Äù](https://drive.google.com/file/d/1Hv80uCR8FVdlJX4u2Cqys8uQ6MHpIVLU/view?usp=drive_link) Pham, N., & Pagh, R. - 2013\n",
    "\n",
    "üî¨ [CCF2002] [‚Äú**Finding frequent items in data streams**](https://www.cs.princeton.edu/courses/archive/spring04/cos598B/bib/CharikarCF.pdf)[‚Äù](https://drive.google.com/file/d/1hHKakg5qubQJ0jSufITH68SE6odgh04J/view?usp=drive_link) Charikar, M., Chen, K., & Farach-Colton - 2002\n",
    "\n",
    "üåê [WIKICS] [‚Äú**Wikipedia: Count sketch**‚Äù](https://en.wikipedia.org/wiki/Count_sketch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
