{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='supervised-learning'></a> 1. [**Apprentissage supervis√©**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#supervised-learning)</br>([*Supervised learning*](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning))\n",
    "\n",
    "# 1.1. [**Mod√®les lin√©aires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_1_linear_model.ipynb)<br/>([*Linear Models*](https://scikit-learn.org/stable/modules/linear_model.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 33 pages, 30 exemples, 40 papiers\n",
    "- 1.1.1. [**Moindres carr√©s ordinaires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#ordinary-least-squares)<br/>([_Ordinary Least Squares_](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares))\n",
    "    - 1.1.1.1. [**Moindres carr√©s non-n√©gatifs**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#non-negative-least-squares)<br/>([*Non-Negative Least Squares*](https://scikit-learn.org/stable/modules/linear_model.html#non-negative-least-squares))\n",
    "    - 1.1.1.2. [**Complexit√© des Moindres Carr√©s Ordinaire**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#ordinary-least-squares-complexity)<br/>([*Ordinary Least Squares Complexity*](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares-complexity))\n",
    "- 1.1.2. [**R√©gression Ridge et classification**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#ridge-regression-and-classification)<br/>([_Ridge regression and classification_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification))\n",
    "    - 1.1.2.1. [**R√©gression**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#ridge-regression)<br/>([_Regression_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression))\n",
    "    - 1.1.2.2. [**Classification**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#ridge-classification)<br/>([_Classification_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-classification))\n",
    "    - 1.1.2.3. [**Complexit√© de Ridge**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#ridge-complexity)<br/>([_Ridge Complexity_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-complexity))\n",
    "    - 1.1.2.4. [**R√©glage du param√®tre de r√©gularisation : Validation crois√©e \"Leave-One-Out\"**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#setting-the-regularization-parameter-leave-one-out-cross-validation)<br/>([_Setting the regularization parameter: leave-one-out Cross-Validation_](https://scikit-learn.org/stable/modules/linear_model.html#setting-the-regularization-parameter-leave-one-out-cross-validation))\n",
    "- 1.1.3. [**Lasso**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#lasso)<br/>([_Lasso_](https://scikit-learn.org/stable/modules/linear_model.html#lasso))\n",
    "- 1.1.4. [**Lasso multi-t√¢che**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#multi-task-lasso)<br/>([_Multi-task Lasso_](https://scikit-learn.org/stable/modules/linear_model.html#multi-task-lasso))\n",
    "- 1.1.5. [**Elastic-Net**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#elastic-net)<br/>([_Elastic-Net_](https://scikit-learn.org/stable/modules/linear_model.html#elastic-net))\n",
    "- 1.1.6. [**Elastic-Net multi-t√¢che**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#multi-task-elastic-net)<br/>([_Multi-task Elastic-Net_](https://scikit-learn.org/stable/modules/linear_model.html#multi-task-elastic-net))\n",
    "- 1.1.7. [**Least Angle Regression**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#least-angle-regression)<br/>([_Least Angle Regression_](https://scikit-learn.org/stable/modules/linear_model.html#least-angle-regression))\n",
    "- 1.1.8. [**LARS Lasso**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#lars-lasso)<br/>([_LARS Lasso_](https://scikit-learn.org/stable/modules/linear_model.html#lars-lasso))\n",
    "- 1.1.9. [**Orthogonal Matching Pursuit (OMP)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#orthogonal-matching-pursuit-omp)<br/>([_Orthogonal Matching Pursuit (OMP)_](https://scikit-learn.org/stable/modules/linear_model.html#orthogonal-matching-pursuit-omp))\n",
    "- 1.1.10. [**R√©gression bay√©sienne**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#bayesian-regression)<br/>([_Bayesian Regression_](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-regression))\n",
    "- 1.1.11. [**R√©gression logistique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#logistic-regression)<br/>([_Logistic regression_](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression))\n",
    "- 1.1.12. [**Mod√®les lin√©aires g√©n√©ralis√©s**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#generalized-linear-models)<br/>([_Generalized Linear Models_](https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-models))\n",
    "- 1.1.13. [**Descente de gradient stochastique - SGD**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#stochastic-gradient-descent-sgd)<br/>([_Stochastic Gradient Descent - SGD_](https://scikit-learn.org/stable/modules/linear_model.html#stochastic-gradient-descent-sgd))\n",
    "- 1.1.14. [**Perceptron**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#perceptron)<br/>([_Perceptron_](https://scikit-learn.org/stable/modules/linear_model.html#perceptron))\n",
    "- 1.1.15. [**Algorithmes passifs agressifs**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#passive-aggressive-algorithms)<br/>([_Passive Aggressive Algorithms_](https://scikit-learn.org/stable/modules/linear_model.html#passive-aggressive-algorithms))\n",
    "- 1.1.16. [**R√©gression robuste : valeurs aberrantes et erreurs de mod√©lisation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#robustness-regression-outliers-and-modeling-errors)<br/>([_Robustness regression: outliers and modeling errors_](https://scikit-learn.org/stable/modules/linear_model.html#robustness-regression-outliers-and-modeling-errors))\n",
    "- 1.1.17. [**R√©gression quantile**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#quantile-regression)<br/>([_Quantile Regression_](https://scikit-learn.org/stable/modules/linear_model.html#quantile-regression))\n",
    "- 1.1.18. [**R√©gression polynomiale : extension des mod√®les lin√©aires avec des fonctions de base**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#polynomial-regression-extending-linear-models-with-basis-functions)<br/>([_Polynomial regression: extending linear models with basis functions_](https://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='linear-models'></a> 1.1. **Mod√®les lin√©aires**<br/>([*Linear Models*](https://scikit-learn.org/stable/modules/linear_model.html))\n",
    "\n",
    "Ce chapitre pr√©sente un ensemble de m√©thodes con√ßues pour la r√©gression, dans lesquelles la valeur cible est suppos√©e √™tre une combinaison lin√©aire des caract√©ristiques. En notation math√©matique, si $\\hat{y}$ est la valeur pr√©dite :\n",
    "\n",
    "$$\\hat{y}(w, x) = w_0 + w_1 x_1 + \\ldots + w_p x_p = w_0 + x^\\top w$$\n",
    "\n",
    "Dans l'ensemble du module, nous d√©signons le vecteur $w = (w_1,\\ldots, w_p)$ comme `coef_` et $w_0$ comme `intercept_`.\n",
    "\n",
    "Pour effectuer une classification avec des mod√®les lin√©aires g√©n√©ralis√©s, consulter la section [**R√©gression logistique** (1.1.11)](#logistic-regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ordinary-least-squares'></a> 1.1.1. **Moindres carr√©s ordinaires**<br/>([*Ordinary Least Squares*](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares))\n",
    "\n",
    "[**`LinearRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) ajuste un mod√®le lin√©aire avec des coefficients $w = (w_1,..., w_p)$ pour minimiser la somme r√©siduelle des carr√©s entre les cibles observ√©es dans l'ensemble de donn√©es et les cibles pr√©dites par l'approximation lin√©aire. Math√©matiquement, il r√©sout un probl√®me de la forme :\n",
    "\n",
    "$$\\displaystyle\\min_{w} || X w - y||_2^2$$\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_ols_001.png\"\n",
    "    alt=\"Moindres carr√©s ordinaires\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "[**`LinearRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) prend les tableaux $X$ et $y$ dans sa m√©thode `fit` d'ajustement et stocke les coefficients $w$ du mod√®le lin√©aire dans son attribut `coef_` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coef_      : [0.5 0.5]\n",
      "intercept_ : 1.1102230246251565e-16\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n",
    "# > LinearRegression()\n",
    "print('coef_      :', reg.coef_)\n",
    "# > array([0.5, 0.5])\n",
    "print('intercept_ :', reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les estimations de coefficients pour les moindres carr√©s ordinaires (**OLS**) d√©pendent de l'ind√©pendance des caract√©ristiques. Lorsque les caract√©ristiques sont corr√©l√©es et que les colonnes de la **matrice de conception** $X$ ont une d√©pendance approximativement lin√©aire, la matrice de conception devient proche du **singularit√©** et, par cons√©quent, l'estimation des moindres carr√©s devient tr√®s sensible aux erreurs al√©atoires dans la cible observ√©e, produisant une grande **variance**. Cette situation de **multicolin√©arit√©** peut survenir, par exemple, lorsque des donn√©es sont collect√©es sans **plan exp√©rimental**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**Exemple de r√©gression lin√©aire**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_ols.ipynb)<br/>([*Linear Regression Example*](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='non-negative-least-squares'></a> 1.1.1.1. **Moindres carr√©s non-n√©gatifs**<br/>([*Non-Negative Least Squares*](https://scikit-learn.org/stable/modules/linear_model.html#non-negative-least-squares))\n",
    "\n",
    "Il est possible de contraindre tous les coefficients √† √™tre non n√©gatifs, ce qui peut √™tre utile lorsqu'ils repr√©sentent certaines quantit√©s physiques ou des valeurs naturellement non n√©gatives (par exemple, les comptages de fr√©quence ou les prix des biens). La classe [**`LinearRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) accepte un param√®tre bool√©en `positive` : lorsque ce param√®tre est d√©fini sur `True`, la m√©thode des moindres carr√©s non-n√©gatifs (**NNLS**) est appliqu√©e.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Moindres carr√©s non-n√©gatifs**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_nnls.ipynb)<br/>([*Non-negative least squares*](https://scikit-learn.org/stable/auto_examples/linear_model/plot_nnls.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='ordinary-least-squares-complexity'></a> 1.1.1.2. **Complexit√© des moindres carr√©s ordinaires**<br/>([*Ordinary Least Squares Complexity*](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares-complexity))\n",
    "\n",
    "La solution des moindres carr√©s est calcul√©e √† l'aide de la d√©composition en valeurs singuli√®res de la matrice $X$. Si $X$ est une matrice de forme `(n_samples, n_features)`, cette m√©thode a un co√ªt de $\\mathcal{O}(n_{\\text{samples}} n_{\\text{features}}^2)$, en supposant que $n_{\\text{samples}} \\geq n_{\\text{features}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ridge-regression-and-classification'></a> 1.1.2. **R√©gression Ridge et classification**<br/>([_Ridge regression and classification_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification))\n",
    "\n",
    "### <a id='ridge-regression'></a> 1.1.2.1. **R√©gression**<br/>([_Regression_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression))\n",
    "\n",
    "La r√©gression [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) r√©pond √† certains des probl√®mes des moindres carr√©s ordinaires en imposant une p√©nalit√© sur la taille des coefficients. Les coefficients Ridge minimisent une somme r√©siduelle des carr√©s p√©nalis√©e :\n",
    "\n",
    "$$\\min_{w} || X w - y||_2^2 + \\alpha ||w||_2^2$$\n",
    "\n",
    "Le param√®tre de complexit√© $\\alpha \\geq 0$ contr√¥le le niveau de r√©tr√©cissement : plus la valeur de $\\alpha$ est grande, plus le niveau de r√©tr√©cissement est √©lev√©, et ainsi les coefficients deviennent plus robustes √† la colin√©arit√©.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_ridge_path_001.png\"\n",
    "    alt=\"Coefficients Ridge comme une fonction de la r√©gularisation\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Comme pour d'autres mod√®les lin√©aires, [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) prend les tableaux $X, y$ dans sa m√©thode `fit` et stocke les coefficients $w$ du mod√®le lin√©aire dans son attribut `coef_` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34545455, 0.34545455])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.13636363636363638"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Ridge(alpha=.5)\n",
    "reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\n",
    "display(reg.coef_)\n",
    "display(reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classe [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) permet √† l'utilisateur de sp√©cifier que le solveur soit automatiquement choisi en d√©finissant `solver=\"auto\"`. Lorsque cette option est sp√©cifi√©e, [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) choisira entre les solveurs `\"lbfgs\"`, `\"cholesky\"` et `\"sparse_cg\"`. [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) commencera √† v√©rifier les conditions pr√©sent√©es dans le tableau suivant de haut en bas. Si la condition est vraie, le solveur correspondant est choisi.\n",
    "\n",
    "| Solveur | Condition |\n",
    "|---------|-----------|\n",
    "| 'lbfgs' | L'option `positive=True` est sp√©cifi√©e. |\n",
    "| 'cholesky' | Le tableau d'entr√©e X n'est pas clairsem√©. |\n",
    "| 'sparse_cg' | Aucune des conditions ci-dessus n'est remplie. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='ridge-classification'></a> 1.1.2.2. **Classification**<br/>([_Classification_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-classification))\n",
    "\n",
    "Le r√©gresseur [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) a une variante de classification : [**`RidgeClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html). Ce classifieur convertit d'abord les cibles binaires en `{-1, 1}`, puis traite le probl√®me comme une t√¢che de r√©gression, optimisant le m√™me objectif que pr√©c√©demment. La classe pr√©dite correspond au signe de la pr√©diction du r√©gresseur. Pour la classification multiclasse, le probl√®me est trait√© comme une r√©gression multi-sortie et la classe pr√©dite correspond √† la sortie avec la valeur la plus √©lev√©e.\n",
    "\n",
    "Il peut sembler discutable d'utiliser une perte des moindres carr√©s (p√©nalis√©e) pour ajuster un mod√®le de classification au lieu des pertes plus traditionnelles comme logistiques ou Ridge. Cependant, en pratique, tous ces mod√®les peuvent conduire √† des scores de validation crois√©e similaires en termes d'exactitude ou de pr√©cision/rappel. La perte des moindres carr√©s p√©nalis√©e utilis√©e par le [**`RidgeClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html) permet un choix vari√© de solveurs num√©riques avec des performances de calcul h√©t√©rog√®nes.\n",
    "\n",
    "Le [**`RidgeClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html) peut √™tre beaucoup plus rapide que la [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), par exemple, avec un grand nombre de classes, car il peut calculer la **matrice de projection** $(X^\\top X)^{-1} X^\\top$ une seule fois.\n",
    "\n",
    "Ce classifieur est parfois appel√© [**machines √† vecteurs de support √† moindres carr√©s**](https://en.wikipedia.org/wiki/Least-squares_support-vector_machine) avec un noyau lin√©aire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemples\n",
    "\n",
    "##### [**Affichage des coefficients Ridge comme fonction de la r√©gularisation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_ridge_path.ipynb)<br/>([*Plot Ridge coefficients as a function of the regularization*](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html))\n",
    "\n",
    "Montre l'effet de la colin√©arit√© sur les coefficients d'un estimateur.\n",
    "\n",
    "##### [**Classification de documents textes √† l'aide de caract√©ristiques creuses**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_document_classification_20newsgroups.ipynb)<br/>([*Classification of text documents using sparse features*](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html))\n",
    "\n",
    "\n",
    "##### [**Pi√®ges courants dans l'interpr√©tation des coefficients des mod√®les lin√©aires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/4_inspection/plot_linear_model_coefficient_interpretation.ipynb)<br/>([*Common pitfalls in the interpretation of coefficients of linear models*](https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='ridge-complexity'></a> 1.1.2.3. **Complexit√© de Ridge**<br/>([_Ridge Complexity_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-complexity))\n",
    "\n",
    "Cette m√©thode a le m√™me ordre de complexit√© que les moindres carr√©s ordinaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='setting-the-regularization-parameter-leave-one-out-cross-validation'></a> 1.1.2.4. **R√©glage du param√®tre de r√©gularisation : Validation crois√©e \"Leave-One-Out\"**<br/>([_Setting the regularization parameter: leave-one-out Cross-Validation_](https://scikit-learn.org/stable/modules/linear_model.html#setting-the-regularization-parameter-leave-one-out-cross-validation))\n",
    "\n",
    "[**`RidgeCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) impl√©mente la r√©gression Ridge avec une validation crois√©e int√©gr√©e du param√®tre `alpha`. Cet objet fonctionne de la m√™me mani√®re que [**`GridSearchCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), mais en utilisant par d√©faut la validation crois√©e \"Leave-One-Out\"¬†:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))\n",
    "reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\n",
    "# RidgeCV(alphas=array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01,\n",
    "#      1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06]))\n",
    "reg.alpha_\n",
    "# 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'attribution de la valeur de l'attribut [`cv`](https://scikit-learn.org/stable/glossary.html#term-cv) d√©clenche l'utilisation de la validation crois√©e avec [**`GridSearchCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Par exemple, `cv=10` pour une validation crois√©e √† 10 plis au lieu de la validation crois√©e \"Leave-One-Out\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R√©f√©rences\n",
    "\n",
    "‚Äú**Notes on Regularized Least Squares**‚Äù, Rifkin & Lippert ([_rapport technique_](http://cbcl.mit.edu/publications/ps/MIT-CSAIL-TR-2007-025.pdf), [_diapositives de cours_](https://www.mit.edu/~9.520/spring07/Classes/rlsslides.pdf))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='lasso'></a> 1.1.3. **Lasso**<br/>([*Lasso*](https://scikit-learn.org/stable/modules/linear_model.html#lasso))\n",
    "\n",
    "Le Lasso est un mod√®le lin√©aire qui estime des coefficients parcimonieux. Il est utile dans certains contextes en raison de sa tendance √† pr√©f√©rer les solutions avec moins de coefficients non nuls, r√©duisant ainsi le nombre de caract√©ristiques dont d√©pend la solution donn√©e. Pour cette raison, le Lasso et ses variantes sont fondamentaux dans le domaine de la d√©tection compress√©e. Sous certaines conditions, il peut r√©cup√©rer l'ensemble exact des coefficients non nuls (voir l'exemple [**D√©tection compressive : reconstruction tomographique avec L1 pr√©alable (Lasso)**](https://scikit-learn.org/stable/auto_examples/applications/plot_tomography_l1_reconstruction.html#sphx-glr-auto-examples-applications-plot-tomography-l1-reconstruction-py)).\n",
    "\n",
    "Math√©matiquement, il s'agit d'un mod√®le lin√©aire avec un terme de r√©gularisation ajout√©. La fonction objectif √† minimiser est :\n",
    "\n",
    "$\\displaystyle\\min_{w} { \\frac{1}{2n_{\\text{samples}}} ||X w - y||_2 ^ 2 + \\alpha ||w||_1}$\n",
    "\n",
    "L'estimation du lasso r√©sout donc la minimisation de la p√©nalit√© des moindres carr√©s avec ajout de $\\alpha ||w||_1$, o√π $\\alpha$ est une constante et $||w||_1$ est la $\\ell_1$-norme du vecteur coefficient.\n",
    "\n",
    "L'impl√©mentation dans la classe [**`Lasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) utilise la descente de coordonn√©es comme algorithme pour ajuster les coefficients. Voir [**R√©gression du moindre angle** (1.1.7)](https://scikit-learn.org/stable/modules/linear_model.html#least-angle-regression) pour une impl√©mentation alternative :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Lasso(alpha=0.1)\n",
    "reg.fit([[0, 0], [1, 1]], [0, 1])\n",
    "# Lasso(alpha=0.1)\n",
    "reg.predict([[1, 1]])\n",
    "# array([0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction [**`lasso_path`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.lasso_path.html#sklearn.linear_model.lasso_path) est utile pour les t√¢ches de niveau inf√©rieur, car elle calcule les coefficients le long du chemin complet des valeurs possibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemples\n",
    "\n",
    "##### [**Mod√®les bas√©s sur $\\ell_1$ pour signaux creux**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_lasso_and_elasticnet.ipynb)<br/>([_L1-based models for Sparse Signals_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_and_elasticnet.html))\n",
    "\n",
    "##### [**D√©tection compressive : reconstruction tomographique avec $\\ell_1$ pr√©alable (Lasso)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/applications/plot_tomography_l1_reconstruction.ipynb)<br/>([_Compressive sensing: tomography reconstruction with $\\ell_1$ prior (Lasso_](https://scikit-learn.org/stable/auto_examples/applications/plot_tomography_l1_reconstruction.html))\n",
    "\n",
    "##### [**Pi√®ges courants dans l'interpr√©tation des coefficients des mod√®les lin√©aires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/4_inspection/plot_linear_model_coefficient_interpretation.ipynb)<br/>([*Common pitfalls in the interpretation of coefficients of linear models*](https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note : S√©lection de caract√©ristiques avec Lasso\n",
    "\n",
    "> Comme la r√©gression Lasso produit des mod√®les parcimonieux, elle peut donc √™tre utilis√©e pour effectuer une _s√©lection de caract√©ristiques_, comme d√©taill√© dans la [**S√©lection de caract√©ristiques bas√©e sur $\\ell_1$** (1.13.4.1)](https://scikit-learn.org/stable/modules/feature_selection.html#l1-feature-selection).\n",
    "\n",
    "Les deux r√©f√©rences suivantes expliquent les it√©rations utilis√©es dans le solveur de descente de coordonn√©es de scikit-learn, ainsi que le calcul de l'√©cart de dualit√© utilis√© pour le contr√¥le de la convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R√©f√©rences\n",
    "\n",
    "üî¨ [**‚ÄúRegularization Path For Generalized linear Models by Coordinate Descent‚Äù**](https://www.jstatsoft.org/article/view/v033i01/v33i01.pdf), Friedman, Hastie & Tibshirani, J Stat Softw, 2010.\n",
    "\n",
    "üî¨ [**‚ÄúAn Interior-Point Method for Large-Scale $\\ell_1$-Regularized Least Squares‚Äù**](https://web.stanford.edu/~boyd/papers/pdf/l1_ls.pdf), S. J. Kim, K. Koh, M. Lustig, S. Boyd and D. Gorinevsky, in IEEE Journal of Selected Topics in Signal Processing, 2007."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='setting-regularization-parameter'></a> 1.1.3.1. **D√©finition du param√®tre de r√©gularisation**<br/>([_Setting regularization parameter_](https://scikit-learn.org/stable/modules/linear_model.html#setting-regularization-parameter))\n",
    "\n",
    "Le param√®tre `alpha` contr√¥le le degr√© de parcimonie des coefficients estim√©s.\n",
    "\n",
    "#### Utilisation de la validation crois√©e\n",
    "\n",
    "scikit-learn propose des objets qui d√©finissent le param√®tre `alpha` du Lasso par validation crois√©e : [**`LassoCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html) et [**`LassoLarsCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsCV.html). [**`LassoLarsCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsCV.html) est bas√© sur l'algorithme [**Least Angle Regression** (1.1.7)](https://scikit-learn.org/stable/modules/linear_model.html#least-angle-regression) expliqu√© ci-dessous.\n",
    "\n",
    "Pour les ensembles de donn√©es de grande dimension avec de nombreuses caract√©ristiques colin√©aires, [**`LassoCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html) est g√©n√©ralement pr√©f√©rable. Cependant, [**`LassoLarsCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsCV.html) a l'avantage d'explorer des valeurs du param√®tre `alpha` plus pertinentes, et si le nombre d'√©chantillons est tr√®s faible par rapport au nombre de caract√©ristiques, il est souvent plus rapide que [**`LassoCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html).\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lasso_model_selection_002.png\"\n",
    "    alt=\"Erreur quadratique moyenne pour chaque pli : descente de coordonn√©es\"\n",
    "    style=\"max-width: 40%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lasso_model_selection_003.png\"\n",
    "    alt=\"Erreur quadratique moyenne pour chaque pli: Lars\"\n",
    "    style=\"max-width: 40%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "#### S√©lection de mod√®le bas√©e sur des crit√®res d'information\n",
    "\n",
    "Comme alterative, l'estimateur [**`LassoLarsIC`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsIC.html) propose d'utiliser le crit√®re d'information d'Akaike (AIC) et le crit√®re d'information de Bayes (BIC). C'est une alternative moins co√ªteuse pour d√©terminer la valeur optimale de `alpha`, car le chemin de r√©gularisation n'est calcul√© qu'une seule fois au lieu, contre $k$ + 1 fois avec la validation crois√©e $k$-fold.\n",
    "\n",
    "En effet, ces crit√®res sont calcul√©s sur l'ensemble d'apprentissage. En bref, ils p√©nalisent les scores trop optimistes des diff√©rents mod√®les Lasso par leur flexibilit√© (voir section ¬´ D√©tails math√©matiques ¬ª ci-dessous).\n",
    "\n",
    "Cependant, ces crit√®res n√©cessitent une estimation pr√©cise des degr√©s de libert√© de la solution. Ils sont d√©riv√©s pour de grands √©chantillons (r√©sultats asymptotiques) et supposent que le mod√®le correct est inclus dans les candidats √† l'√©tude. Ils ont √©galement tendance √† dysfonctionner lorsque le probl√®me est mal conditionn√© (par exemple, plus de caract√©ristiques que d'√©chantillons).\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lasso_lars_ic_001.png\"\n",
    "    alt=\"S√©lection de mod√®le Lasso via AIC et BIC\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "##### D√©tails math√©matiques\n",
    "\n",
    "La d√©finition de l'AIC (et donc du BIC) peut diff√©rer dans la litt√©rature. Dans cette section, nous donnons plus d'informations sur le crit√®re tel que calcul√© dans scikit-learn. Le crit√®re AIC est d√©fini comme suit :\n",
    "\n",
    "$$AIC = -2 \\log(\\hat{L}) + 2 d$$\n",
    "\n",
    "o√π $\\hat{L}$ est la vraisemblance maximale du mod√®le et $d$ est le nombre de param√®tres (√©galement appel√©s _degr√©s de libert√©_ dans la section pr√©c√©dente).\n",
    "\n",
    "La d√©finition de BIC remplace la constante 2 par $\\log(N)$ :\n",
    "\n",
    "$$BIC = -2 \\log(\\hat{L}) + \\log(N) d$$\n",
    "\n",
    "o√π $N$ est le nombre d'√©chantillons.\n",
    "\n",
    "Pour un mod√®le gaussien lin√©aire, la log-vraisemblance maximale est d√©finie comme suit¬†:\n",
    "\n",
    "$$\\log(\\hat{L}) = - \\frac{n}{2} \\log(2 \\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{2\\sigma^2}$$\n",
    "\n",
    "o√π $\\sigma^2$ est une estimation de la variance du bruit, $y_i$ et $\\hat{y}_i$ sont respectivement les cibles r√©elles et pr√©dites, et $n$ est le nombre d'√©chantillons.\n",
    "\n",
    "En ins√©rant la log-vraisemblance maximale dans la formule AIC, on obtient¬†:\n",
    "\n",
    "$$AIC = n \\log(2 \\pi \\sigma^2) + \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sigma^2} + 2 d$$\n",
    "\n",
    "Le premier terme de l'expression ci-dessus est parfois ignor√© car il s'agit d'une constante lorsque $\\sigma^2$ est fourni. De plus, il est parfois affirm√© que l'AIC est √©quivalent √† la statistique $C_p$ [12]. Cependant, au sens strict, il n'est √©quivalent qu'√† une constante et √† un facteur multiplicatif pr√®s.\n",
    "\n",
    "Enfin, nous avons mentionn√© plus haut que $\\sigma^2$ est une estimation de la variance du bruit. Dans [**`LassoLarsIC`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsIC.html) lorsque le param√®tre `noise_variance` n'est pas fourni (le comportement par d√©faut), la variance du bruit est estim√©e √† l'aide de l'estimateur non biais√© [13] d√©fini comme suit¬†:\n",
    "\n",
    "$$\\sigma^2 = \\displaystyle\\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{n - p}$$\n",
    "\n",
    "o√π $p$ est le nombre de caract√©ristiques et $\\hat{y}_i$ est la cible pr√©dite √† l'aide d'une r√©gression des moindres carr√©s ordinaires. Notez que cette formule n'est valide que lorsque `n_samples > n_features`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### R√©f√©rences\n",
    "\n",
    "üî¨ [12] Zou, Hui, Trevor Hastie, and Robert Tibshirani. [**‚ÄúOn the degrees of freedom of the lasso‚Äù**](https://arxiv.org/abs/0712.0881.pdf). The Annals of Statistics 35.5 (2007): 2173-2192.\n",
    "\n",
    "üî¨ [13] Cherkassky, Vladimir, and Yunqian Ma. [**‚ÄúComparison of model selection for regression‚Äù**](http://www.ece.umn.edu/users/cherkass/comparison_paper_Mar05.pdf). Neural computation 15.7 (2003): 1691-1714."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparaison avec le param√®tre de r√©gularisation de SVM\n",
    "\n",
    "L'√©quivalence entre `alpha` et le param√®tre de r√©gularisation de SVM, `C`, est donn√©e par `alpha = 1 / C` ou `alpha = 1 / (n_samples * C)`, en fonction de l'estimateur et de la fonction objectif exacte optimis√©e par le mod√®le."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='multi-task-lasso'></a> 1.1.4. **Lasso multi-t√¢che**<br/>([*Multi-task Lasso*](https://scikit-learn.org/stable/modules/linear_model.html#multi-task-lasso))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='elastic-net'></a> 1.1.5. **Elastic-Net**<br/>([*Elastic-Net*](https://scikit-learn.org/stable/modules/linear_model.html#elastic-net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='multi-task-elastic-net'></a> 1.1.6. **Elastic-Net multi-t√¢che**<br/>([*Multi-task Elastic-Net*](https://scikit-learn.org/stable/modules/linear_model.html#multi-task-elastic-net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='least-angle-regression'></a> 1.1.7. **Least Angle Regression**<br/>([*Least Angle Regression*](https://scikit-learn.org/stable/modules/linear_model.html#least-angle-regression))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='lars-lasso'></a> 1.1.8. **LARS Lasso**<br/>([*LARS Lasso*](https://scikit-learn.org/stable/modules/linear_model.html#lars-lasso))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='orthogonal-matching-pursuit-omp'></a> 1.1.9. **Orthogonal Matching Pursuit (OMP)**<br/>([*Orthogonal Matching Pursuit (OMP)*](https://scikit-learn.org/stable/modules/linear_model.html#orthogonal-matching-pursuit-omp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='bayesian-regression'></a> 1.1.10. **R√©gression bay√©sienne**<br/>([*Bayesian Regression*](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-regression))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='logistic-regression'></a> 1.1.11. **R√©gression logistique**<br/>([*Logistic regression*](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='generalized-linear-models'></a> 1.1.12. **Mod√®les lin√©aires g√©n√©ralis√©s**<br/>([*Generalized Linear Models*](https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='stochastic-gradient-descent-sgd'></a> 1.1.13. **Descente de gradient stochastique - SGD**<br/>([*Stochastic Gradient Descent - SGD*](https://scikit-learn.org/stable/modules/linear_model.html#stochastic-gradient-descent-sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='perceptron'></a> 1.1.14. **Perceptron**<br/>([*Perceptron*](https://scikit-learn.org/stable/modules/linear_model.html#perceptron))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='passive-aggressive-algorithms'></a> 1.1.15. **Algorithmes passifs agressifs**<br/>([*Passive Aggressive Algorithms*](https://scikit-learn.org/stable/modules/linear_model.html#passive-aggressive-algorithms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='robustness-regression-outliers-and-modeling-errors'></a> 1.1.16. **R√©gression robuste : valeurs aberrantes et erreurs de mod√©lisation**<br/>([*Robustness regression: outliers and modeling errors*](https://scikit-learn.org/stable/modules/linear_model.html#robustness-regression-outliers-and-modeling-errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='quantile-regression'></a> 1.1.17. **R√©gression quantile**<br/>([*Quantile Regression*](https://scikit-learn.org/stable/modules/linear_model.html#quantile-regression))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='polynomial-regression-extending-linear-models-with-basis-functions'></a> 1.1.18. **R√©gression polynomiale : extension des mod√®les lin√©aires avec des fonctions de base**<br/>([*Polynomial regression: extending linear models with basis functions*](https://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e03b612d84ba21ce95ed447e81b3062e1eb99b56c6d885cdab4aaa12f1b8e240"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
