{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='supervised-learning'></a> 1. [**Apprentissage supervis√©**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#supervised-learning)</br>([*Supervised learning*](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning))\n",
    "\n",
    "# 1.1. [**Mod√®les lin√©aires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_1_linear_model.ipynb)<br/>([*Linear Models*](https://scikit-learn.org/stable/modules/linear_model.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 33 pages, 30 exemples, 40 papiers\n",
    "- 1.1.1. [**Moindres carr√©s ordinaires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#ordinary-least-squares)<br/>([_Ordinary Least Squares_](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares))\n",
    "    - 1.1.1.1. [**Moindres carr√©s non-n√©gatifs**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#non-negative-least-squares)<br/>([*Non-Negative Least Squares*](https://scikit-learn.org/stable/modules/linear_model.html#non-negative-least-squares))\n",
    "    - 1.1.1.2. [**Complexit√© des Moindres Carr√©s Ordinaire**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#ordinary-least-squares-complexity)<br/>([*Ordinary Least Squares Complexity*](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares-complexity))\n",
    "- 1.1.2. [**R√©gression Ridge et classification**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#ridge-regression-and-classification)<br/>([_Ridge regression and classification_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification))\n",
    "    - 1.1.2.1. [**R√©gression**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#ridge-regression)<br/>([_Regression_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression))\n",
    "    - 1.1.2.2. [**Classification**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#ridge-classification)<br/>([_Classification_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-classification))\n",
    "    - 1.1.2.3. [**Complexit√© de Ridge**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#ridge-complexity)<br/>([_Ridge Complexity_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-complexity))\n",
    "    - 1.1.2.4. [**R√©glage du param√®tre de r√©gularisation : Validation crois√©e \"Leave-One-Out\"**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#setting-the-regularization-parameter-leave-one-out-cross-validation)<br/>([_Setting the regularization parameter: leave-one-out Cross-Validation_](https://scikit-learn.org/stable/modules/linear_model.html#setting-the-regularization-parameter-leave-one-out-cross-validation))\n",
    "- 1.1.3. [**Lasso**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#lasso)<br/>([_Lasso_](https://scikit-learn.org/stable/modules/linear_model.html#lasso))\n",
    "- 1.1.4. [**Lasso multi-t√¢che**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#multi-task-lasso)<br/>([_Multi-task Lasso_](https://scikit-learn.org/stable/modules/linear_model.html#multi-task-lasso))\n",
    "- 1.1.5. [**Elastic-Net**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#elastic-net)<br/>([_Elastic-Net_](https://scikit-learn.org/stable/modules/linear_model.html#elastic-net))\n",
    "- 1.1.6. [**Elastic-Net multi-t√¢che**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#multi-task-elastic-net)<br/>([_Multi-task Elastic-Net_](https://scikit-learn.org/stable/modules/linear_model.html#multi-task-elastic-net))\n",
    "- 1.1.7. [**R√©gression de plus faible angle (LARS)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#least-angle-regression)<br/>([_Least Angle Regression_](https://scikit-learn.org/stable/modules/linear_model.html#least-angle-regression))\n",
    "- 1.1.8. [**LARS Lasso**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#lars-lasso)<br/>([_LARS Lasso_](https://scikit-learn.org/stable/modules/linear_model.html#lars-lasso))\n",
    "- 1.1.9. [**Poursuite orthogonale des correspondances (OMP)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#orthogonal-matching-pursuit-omp)<br/>([_Orthogonal Matching Pursuit (OMP)_](https://scikit-learn.org/stable/modules/linear_model.html#orthogonal-matching-pursuit-omp))\n",
    "- 1.1.10. [**R√©gression bay√©sienne**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#bayesian-regression)<br/>([_Bayesian Regression_](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-regression))\n",
    "- 1.1.11. [**R√©gression logistique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#logistic-regression)<br/>([_Logistic regression_](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression))\n",
    "- 1.1.12. [**Mod√®les lin√©aires g√©n√©ralis√©s**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#generalized-linear-models)<br/>([_Generalized Linear Models_](https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-models))\n",
    "- 1.1.13. [**Descente de gradient stochastique - SGD**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#stochastic-gradient-descent-sgd)<br/>([_Stochastic Gradient Descent - SGD_](https://scikit-learn.org/stable/modules/linear_model.html#stochastic-gradient-descent-sgd))\n",
    "- 1.1.14. [**Perceptron**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#perceptron)<br/>([_Perceptron_](https://scikit-learn.org/stable/modules/linear_model.html#perceptron))\n",
    "- 1.1.15. [**Algorithmes passifs agressifs**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#passive-aggressive-algorithms)<br/>([_Passive Aggressive Algorithms_](https://scikit-learn.org/stable/modules/linear_model.html#passive-aggressive-algorithms))\n",
    "- 1.1.16. [**R√©gression robuste : valeurs aberrantes et erreurs de mod√©lisation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#robustness-regression-outliers-and-modeling-errors)<br/>([_Robustness regression: outliers and modeling errors_](https://scikit-learn.org/stable/modules/linear_model.html#robustness-regression-outliers-and-modeling-errors))\n",
    "- 1.1.17. [**R√©gression quantile**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#quantile-regression)<br/>([_Quantile Regression_](https://scikit-learn.org/stable/modules/linear_model.html#quantile-regression))\n",
    "- 1.1.18. [**R√©gression polynomiale : extension des mod√®les lin√©aires avec des fonctions de base**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#polynomial-regression-extending-linear-models-with-basis-functions)<br/>([_Polynomial regression: extending linear models with basis functions_](https://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='linear-models'></a> 1.1. **Mod√®les lin√©aires**<br/>([*Linear Models*](https://scikit-learn.org/stable/modules/linear_model.html))\n",
    "\n",
    "Ce chapitre pr√©sente un ensemble de m√©thodes con√ßues pour la r√©gression, dans lesquelles la valeur cible est suppos√©e √™tre une combinaison lin√©aire des caract√©ristiques. En notation math√©matique, si $\\hat{y}$ est la valeur pr√©dite :\n",
    "\n",
    "$$\\hat{y}(w, x) = w_0 + w_1 x_1 + \\ldots + w_p x_p = w_0 + x^\\top w$$\n",
    "\n",
    "Dans l'ensemble du module, nous d√©signons le vecteur $w = (w_1,\\ldots, w_p)$ comme `coef_` et $w_0$ comme `intercept_`.\n",
    "\n",
    "Pour effectuer une classification avec des mod√®les lin√©aires g√©n√©ralis√©s, consulter la section [**R√©gression logistique** (1.1.11)](#logistic-regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ordinary-least-squares'></a> 1.1.1. **Moindres carr√©s ordinaires**<br/>([*Ordinary Least Squares*](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares))\n",
    "\n",
    "[**`LinearRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) ajuste un mod√®le lin√©aire avec des coefficients $w = (w_1,..., w_p)$ pour minimiser la somme r√©siduelle des carr√©s entre les cibles observ√©es dans l'ensemble de donn√©es et les cibles pr√©dites par l'approximation lin√©aire. Math√©matiquement, il r√©sout un probl√®me de la forme :\n",
    "\n",
    "$$\\displaystyle\\min_{w} || X w - y||_2^2$$\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_ols_001.png\"\n",
    "    alt=\"Moindres carr√©s ordinaires\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "[**`LinearRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) prend les tableaux $X$ et $y$ dans sa m√©thode `fit` d'ajustement et stocke les coefficients $w$ du mod√®le lin√©aire dans son attribut `coef_` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coef_      : [0.5 0.5]\n",
      "intercept_ : 1.1102230246251565e-16\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n",
    "# > LinearRegression()\n",
    "print('coef_      :', reg.coef_)\n",
    "# > array([0.5, 0.5])\n",
    "print('intercept_ :', reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les estimations de coefficients pour les moindres carr√©s ordinaires (**OLS**) d√©pendent de l'ind√©pendance des caract√©ristiques. Lorsque les caract√©ristiques sont corr√©l√©es et que les colonnes de la **matrice de conception** $X$ ont une d√©pendance approximativement lin√©aire, la matrice de conception devient proche du **singularit√©** et, par cons√©quent, l'estimation des moindres carr√©s devient tr√®s sensible aux erreurs al√©atoires dans la cible observ√©e, produisant une grande **variance**. Cette situation de **multicolin√©arit√©** peut survenir, par exemple, lorsque des donn√©es sont collect√©es sans **plan exp√©rimental**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**Exemple de r√©gression lin√©aire**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_ols.ipynb)<br/>([*Linear Regression Example*](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='non-negative-least-squares'></a> 1.1.1.1. **Moindres carr√©s non-n√©gatifs**<br/>([*Non-Negative Least Squares*](https://scikit-learn.org/stable/modules/linear_model.html#non-negative-least-squares))\n",
    "\n",
    "Il est possible de contraindre tous les coefficients √† √™tre non n√©gatifs, ce qui peut √™tre utile lorsqu'ils repr√©sentent certaines quantit√©s physiques ou des valeurs naturellement non n√©gatives (par exemple, les comptages de fr√©quence ou les prix des biens). La classe [**`LinearRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) accepte un param√®tre bool√©en `positive` : lorsque ce param√®tre est d√©fini sur `True`, la m√©thode des moindres carr√©s non-n√©gatifs (**NNLS**) est appliqu√©e.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Moindres carr√©s non-n√©gatifs**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_nnls.ipynb)<br/>([*Non-negative least squares*](https://scikit-learn.org/stable/auto_examples/linear_model/plot_nnls.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='ordinary-least-squares-complexity'></a> 1.1.1.2. **Complexit√© des moindres carr√©s ordinaires**<br/>([*Ordinary Least Squares Complexity*](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares-complexity))\n",
    "\n",
    "La solution des moindres carr√©s est calcul√©e √† l'aide de la d√©composition en valeurs singuli√®res de la matrice $X$. Si $X$ est une matrice de forme `(n_samples, n_features)`, cette m√©thode a un co√ªt de $\\mathcal{O}(n_{\\text{samples}} n_{\\text{features}}^2)$, en supposant que $n_{\\text{samples}} \\geq n_{\\text{features}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ridge-regression-and-classification'></a> 1.1.2. **R√©gression Ridge et classification**<br/>([_Ridge regression and classification_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification))\n",
    "\n",
    "### <a id='ridge-regression'></a> 1.1.2.1. **R√©gression**<br/>([_Regression_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression))\n",
    "\n",
    "La r√©gression [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) r√©pond √† certains des probl√®mes des moindres carr√©s ordinaires en imposant une p√©nalit√© sur la taille des coefficients. Les coefficients Ridge minimisent une somme r√©siduelle des carr√©s p√©nalis√©e :\n",
    "\n",
    "$$\\min_{w} || X w - y||_2^2 + \\alpha ||w||_2^2$$\n",
    "\n",
    "Le param√®tre de complexit√© $\\alpha \\geq 0$ contr√¥le le niveau de r√©tr√©cissement : plus la valeur de $\\alpha$ est grande, plus le niveau de r√©tr√©cissement est √©lev√©, et ainsi les coefficients deviennent plus robustes √† la colin√©arit√©.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_ridge_path_001.png\"\n",
    "    alt=\"Coefficients Ridge comme une fonction de la r√©gularisation\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Comme pour d'autres mod√®les lin√©aires, [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) prend les tableaux $X, y$ dans sa m√©thode `fit` et stocke les coefficients $w$ du mod√®le lin√©aire dans son attribut `coef_` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34545455, 0.34545455])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.13636363636363638"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Ridge(alpha=.5)\n",
    "reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\n",
    "display(reg.coef_)\n",
    "display(reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classe [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) permet √† l'utilisateur de sp√©cifier que le solveur soit automatiquement choisi en d√©finissant `solver=\"auto\"`. Lorsque cette option est sp√©cifi√©e, [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) choisira entre les solveurs `\"lbfgs\"`, `\"cholesky\"` et `\"sparse_cg\"`. [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) commencera √† v√©rifier les conditions pr√©sent√©es dans le tableau suivant de haut en bas. Si la condition est vraie, le solveur correspondant est choisi.\n",
    "\n",
    "| Solveur | Condition |\n",
    "|---------|-----------|\n",
    "| 'lbfgs' | L'option `positive=True` est sp√©cifi√©e. |\n",
    "| 'cholesky' | Le tableau d'entr√©e X n'est pas clairsem√©. |\n",
    "| 'sparse_cg' | Aucune des conditions ci-dessus n'est remplie. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='ridge-classification'></a> 1.1.2.2. **Classification**<br/>([_Classification_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-classification))\n",
    "\n",
    "Le r√©gresseur [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) a une variante de classification : [**`RidgeClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html). Ce classifieur convertit d'abord les cibles binaires en `{-1, 1}`, puis traite le probl√®me comme une t√¢che de r√©gression, optimisant le m√™me objectif que pr√©c√©demment. La classe pr√©dite correspond au signe de la pr√©diction du r√©gresseur. Pour la classification multiclasse, le probl√®me est trait√© comme une r√©gression multi-sortie et la classe pr√©dite correspond √† la sortie avec la valeur la plus √©lev√©e.\n",
    "\n",
    "Il peut sembler discutable d'utiliser une perte des moindres carr√©s (p√©nalis√©e) pour ajuster un mod√®le de classification au lieu des pertes plus traditionnelles comme logistiques ou Ridge. Cependant, en pratique, tous ces mod√®les peuvent conduire √† des scores de validation crois√©e similaires en termes d'exactitude ou de pr√©cision/rappel. La perte des moindres carr√©s p√©nalis√©e utilis√©e par le [**`RidgeClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html) permet un choix vari√© de solveurs num√©riques avec des performances de calcul h√©t√©rog√®nes.\n",
    "\n",
    "Le [**`RidgeClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html) peut √™tre beaucoup plus rapide que la [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), par exemple, avec un grand nombre de classes, car il peut calculer la **matrice de projection** $(X^\\top X)^{-1} X^\\top$ une seule fois.\n",
    "\n",
    "Ce classifieur est parfois appel√© [**machines √† vecteurs de support √† moindres carr√©s**](https://en.wikipedia.org/wiki/Least-squares_support-vector_machine) avec un noyau lin√©aire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemples\n",
    "\n",
    "##### [**Affichage des coefficients Ridge comme fonction de la r√©gularisation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_ridge_path.ipynb)<br/>([*Plot Ridge coefficients as a function of the regularization*](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html))\n",
    "\n",
    "Montre l'effet de la colin√©arit√© sur les coefficients d'un estimateur.\n",
    "\n",
    "##### [**Classification de documents textes √† l'aide de caract√©ristiques creuses**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_document_classification_20newsgroups.ipynb)<br/>([*Classification of text documents using sparse features*](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html))\n",
    "\n",
    "\n",
    "##### [**Pi√®ges courants dans l'interpr√©tation des coefficients des mod√®les lin√©aires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/4_inspection/plot_linear_model_coefficient_interpretation.ipynb)<br/>([*Common pitfalls in the interpretation of coefficients of linear models*](https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='ridge-complexity'></a> 1.1.2.3. **Complexit√© de Ridge**<br/>([_Ridge Complexity_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-complexity))\n",
    "\n",
    "Cette m√©thode a le m√™me ordre de complexit√© que les moindres carr√©s ordinaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='setting-the-regularization-parameter-leave-one-out-cross-validation'></a> 1.1.2.4. **R√©glage du param√®tre de r√©gularisation : Validation crois√©e \"Leave-One-Out\"**<br/>([_Setting the regularization parameter: leave-one-out Cross-Validation_](https://scikit-learn.org/stable/modules/linear_model.html#setting-the-regularization-parameter-leave-one-out-cross-validation))\n",
    "\n",
    "[**`RidgeCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) impl√©mente la r√©gression Ridge avec une validation crois√©e int√©gr√©e du param√®tre `alpha`. Cet objet fonctionne de la m√™me mani√®re que [**`GridSearchCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), mais en utilisant par d√©faut la validation crois√©e \"Leave-One-Out\"¬†:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))\n",
    "reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\n",
    "# RidgeCV(alphas=array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01,\n",
    "#      1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06]))\n",
    "reg.alpha_\n",
    "# 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'attribution de la valeur de l'attribut [`cv`](https://scikit-learn.org/stable/glossary.html#term-cv) d√©clenche l'utilisation de la validation crois√©e avec [**`GridSearchCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Par exemple, `cv=10` pour une validation crois√©e √† 10 plis au lieu de la validation crois√©e \"Leave-One-Out\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R√©f√©rences\n",
    "\n",
    "‚Äú**Notes on Regularized Least Squares**‚Äù, Rifkin & Lippert ([_rapport technique_](http://cbcl.mit.edu/publications/ps/MIT-CSAIL-TR-2007-025.pdf), [_diapositives de cours_](https://www.mit.edu/~9.520/spring07/Classes/rlsslides.pdf))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='lasso'></a> 1.1.3. **Lasso**<br/>([*Lasso*](https://scikit-learn.org/stable/modules/linear_model.html#lasso))\n",
    "\n",
    "Le Lasso est un mod√®le lin√©aire qui estime des coefficients parcimonieux. Il est utile dans certains contextes en raison de sa tendance √† pr√©f√©rer les solutions avec moins de coefficients non nuls, r√©duisant ainsi le nombre de caract√©ristiques dont d√©pend la solution donn√©e. Pour cette raison, le Lasso et ses variantes sont fondamentaux dans le domaine de la d√©tection compress√©e. Sous certaines conditions, il peut r√©cup√©rer l'ensemble exact des coefficients non nuls (voir l'exemple [**D√©tection compressive : reconstruction tomographique avec L1 pr√©alable (Lasso)**](https://scikit-learn.org/stable/auto_examples/applications/plot_tomography_l1_reconstruction.html#sphx-glr-auto-examples-applications-plot-tomography-l1-reconstruction-py)).\n",
    "\n",
    "Math√©matiquement, il s'agit d'un mod√®le lin√©aire avec un terme de r√©gularisation ajout√©. La fonction objectif √† minimiser est :\n",
    "\n",
    "$\\displaystyle\\min_{w} { \\frac{1}{2n_{\\text{samples}}} ||X w - y||_2 ^ 2 + \\alpha ||w||_1}$\n",
    "\n",
    "L'estimation du lasso r√©sout donc la minimisation de la p√©nalit√© des moindres carr√©s avec ajout de $\\alpha ||w||_1$, o√π $\\alpha$ est une constante et $||w||_1$ est la $\\ell_1$-norme du vecteur coefficient.\n",
    "\n",
    "L'impl√©mentation dans la classe [**`Lasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) utilise la descente de coordonn√©es comme algorithme pour ajuster les coefficients. Voir [**R√©gression du moindre angle** (1.1.7)](https://scikit-learn.org/stable/modules/linear_model.html#least-angle-regression) pour une impl√©mentation alternative :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Lasso(alpha=0.1)\n",
    "reg.fit([[0, 0], [1, 1]], [0, 1])\n",
    "# Lasso(alpha=0.1)\n",
    "reg.predict([[1, 1]])\n",
    "# array([0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction [**`lasso_path`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.lasso_path.html#sklearn.linear_model.lasso_path) est utile pour les t√¢ches de niveau inf√©rieur, car elle calcule les coefficients le long du chemin complet des valeurs possibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemples\n",
    "\n",
    "##### [**Mod√®les bas√©s sur $\\ell_1$ pour signaux creux**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_lasso_and_elasticnet.ipynb)<br/>([_L1-based models for Sparse Signals_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_and_elasticnet.html))\n",
    "\n",
    "##### [**D√©tection compressive : reconstruction tomographique avec $\\ell_1$ pr√©alable (Lasso)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/applications/plot_tomography_l1_reconstruction.ipynb)<br/>([_Compressive sensing: tomography reconstruction with $\\ell_1$ prior (Lasso_](https://scikit-learn.org/stable/auto_examples/applications/plot_tomography_l1_reconstruction.html))\n",
    "\n",
    "##### [**Pi√®ges courants dans l'interpr√©tation des coefficients des mod√®les lin√©aires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/4_inspection/plot_linear_model_coefficient_interpretation.ipynb)<br/>([*Common pitfalls in the interpretation of coefficients of linear models*](https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note : S√©lection de caract√©ristiques avec Lasso\n",
    "\n",
    "> Comme la r√©gression Lasso produit des mod√®les parcimonieux, elle peut donc √™tre utilis√©e pour effectuer une _s√©lection de caract√©ristiques_, comme d√©taill√© dans la [**S√©lection de caract√©ristiques bas√©e sur $\\ell_1$** (1.13.4.1)](https://scikit-learn.org/stable/modules/feature_selection.html#l1-feature-selection).\n",
    "\n",
    "Les deux r√©f√©rences suivantes expliquent les it√©rations utilis√©es dans le solveur de descente de coordonn√©es de scikit-learn, ainsi que le calcul de l'√©cart de dualit√© utilis√© pour le contr√¥le de la convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R√©f√©rences\n",
    "\n",
    "üî¨ [**‚ÄúRegularization Path For Generalized linear Models by Coordinate Descent‚Äù**](https://www.jstatsoft.org/article/view/v033i01/v33i01.pdf), Friedman, Hastie & Tibshirani, J Stat Softw, 2010.\n",
    "\n",
    "üî¨ [**‚ÄúAn Interior-Point Method for Large-Scale $\\ell_1$-Regularized Least Squares‚Äù**](https://web.stanford.edu/~boyd/papers/pdf/l1_ls.pdf), S. J. Kim, K. Koh, M. Lustig, S. Boyd and D. Gorinevsky, in IEEE Journal of Selected Topics in Signal Processing, 2007."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='setting-regularization-parameter'></a> 1.1.3.1. **D√©finition du param√®tre de r√©gularisation**<br/>([_Setting regularization parameter_](https://scikit-learn.org/stable/modules/linear_model.html#setting-regularization-parameter))\n",
    "\n",
    "Le param√®tre `alpha` contr√¥le le degr√© de parcimonie des coefficients estim√©s.\n",
    "\n",
    "#### Utilisation de la validation crois√©e\n",
    "\n",
    "scikit-learn propose des objets qui d√©finissent le param√®tre `alpha` du Lasso par validation crois√©e : [**`LassoCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html) et [**`LassoLarsCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsCV.html). [**`LassoLarsCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsCV.html) est bas√© sur l'algorithme [**Least Angle Regression** (1.1.7)](https://scikit-learn.org/stable/modules/linear_model.html#least-angle-regression) expliqu√© ci-dessous.\n",
    "\n",
    "Pour les ensembles de donn√©es de grande dimension avec de nombreuses caract√©ristiques colin√©aires, [**`LassoCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html) est g√©n√©ralement pr√©f√©rable. Cependant, [**`LassoLarsCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsCV.html) a l'avantage d'explorer des valeurs du param√®tre `alpha` plus pertinentes, et si le nombre d'√©chantillons est tr√®s faible par rapport au nombre de caract√©ristiques, il est souvent plus rapide que [**`LassoCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html).\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lasso_model_selection_002.png\"\n",
    "    alt=\"Erreur quadratique moyenne pour chaque pli : descente de coordonn√©es\"\n",
    "    style=\"max-width: 40%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lasso_model_selection_003.png\"\n",
    "    alt=\"Erreur quadratique moyenne pour chaque pli: Lars\"\n",
    "    style=\"max-width: 40%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "#### S√©lection de mod√®le bas√©e sur des crit√®res d'information\n",
    "\n",
    "Comme alterative, l'estimateur [**`LassoLarsIC`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsIC.html) propose d'utiliser le crit√®re d'information d'Akaike (AIC) et le crit√®re d'information de Bayes (BIC). C'est une alternative moins co√ªteuse pour d√©terminer la valeur optimale de `alpha`, car le chemin de r√©gularisation n'est calcul√© qu'une seule fois au lieu, contre $k$ + 1 fois avec la validation crois√©e $k$-fold.\n",
    "\n",
    "En effet, ces crit√®res sont calcul√©s sur l'ensemble d'apprentissage. En bref, ils p√©nalisent les scores trop optimistes des diff√©rents mod√®les Lasso par leur flexibilit√© (voir section ¬´ D√©tails math√©matiques ¬ª ci-dessous).\n",
    "\n",
    "Cependant, ces crit√®res n√©cessitent une estimation pr√©cise des degr√©s de libert√© de la solution. Ils sont d√©riv√©s pour de grands √©chantillons (r√©sultats asymptotiques) et supposent que le mod√®le correct est inclus dans les candidats √† l'√©tude. Ils ont √©galement tendance √† dysfonctionner lorsque le probl√®me est mal conditionn√© (par exemple, plus de caract√©ristiques que d'√©chantillons).\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lasso_lars_ic_001.png\"\n",
    "    alt=\"S√©lection de mod√®le Lasso via AIC et BIC\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "##### D√©tails math√©matiques\n",
    "\n",
    "La d√©finition de l'AIC (et donc du BIC) peut diff√©rer dans la litt√©rature. Dans cette section, nous donnons plus d'informations sur le crit√®re tel que calcul√© dans scikit-learn. Le crit√®re AIC est d√©fini comme suit :\n",
    "\n",
    "$$AIC = -2 \\log(\\hat{L}) + 2 d$$\n",
    "\n",
    "o√π $\\hat{L}$ est la vraisemblance maximale du mod√®le et $d$ est le nombre de param√®tres (√©galement appel√©s _degr√©s de libert√©_ dans la section pr√©c√©dente).\n",
    "\n",
    "La d√©finition de BIC remplace la constante 2 par $\\log(N)$ :\n",
    "\n",
    "$$BIC = -2 \\log(\\hat{L}) + \\log(N) d$$\n",
    "\n",
    "o√π $N$ est le nombre d'√©chantillons.\n",
    "\n",
    "Pour un mod√®le gaussien lin√©aire, la log-vraisemblance maximale est d√©finie comme suit¬†:\n",
    "\n",
    "$$\\log(\\hat{L}) = - \\frac{n}{2} \\log(2 \\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{2\\sigma^2}$$\n",
    "\n",
    "o√π $\\sigma^2$ est une estimation de la variance du bruit, $y_i$ et $\\hat{y}_i$ sont respectivement les cibles r√©elles et pr√©dites, et $n$ est le nombre d'√©chantillons.\n",
    "\n",
    "En ins√©rant la log-vraisemblance maximale dans la formule AIC, on obtient¬†:\n",
    "\n",
    "$$AIC = n \\log(2 \\pi \\sigma^2) + \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sigma^2} + 2 d$$\n",
    "\n",
    "Le premier terme de l'expression ci-dessus est parfois ignor√© car il s'agit d'une constante lorsque $\\sigma^2$ est fourni. De plus, il est parfois affirm√© que l'AIC est √©quivalent √† la statistique $C_p$ [12]. Cependant, au sens strict, il n'est √©quivalent qu'√† une constante et √† un facteur multiplicatif pr√®s.\n",
    "\n",
    "Enfin, nous avons mentionn√© plus haut que $\\sigma^2$ est une estimation de la variance du bruit. Dans [**`LassoLarsIC`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsIC.html) lorsque le param√®tre `noise_variance` n'est pas fourni (le comportement par d√©faut), la variance du bruit est estim√©e √† l'aide de l'estimateur non biais√© [13] d√©fini comme suit¬†:\n",
    "\n",
    "$$\\sigma^2 = \\displaystyle\\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{n - p}$$\n",
    "\n",
    "o√π $p$ est le nombre de caract√©ristiques et $\\hat{y}_i$ est la cible pr√©dite √† l'aide d'une r√©gression des moindres carr√©s ordinaires. Notez que cette formule n'est valide que lorsque `n_samples > n_features`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### R√©f√©rences\n",
    "\n",
    "üî¨ [12] Zou, Hui, Trevor Hastie, and Robert Tibshirani. [**‚ÄúOn the degrees of freedom of the lasso‚Äù**](https://arxiv.org/abs/0712.0881.pdf). The Annals of Statistics 35.5 (2007): 2173-2192.\n",
    "\n",
    "üî¨ [13] Cherkassky, Vladimir, and Yunqian Ma. [**‚ÄúComparison of model selection for regression‚Äù**](http://www.ece.umn.edu/users/cherkass/comparison_paper_Mar05.pdf). Neural computation 15.7 (2003): 1691-1714."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparaison avec le param√®tre de r√©gularisation de SVM\n",
    "\n",
    "L'√©quivalence entre `alpha` et le param√®tre de r√©gularisation de SVM, `C`, est donn√©e par `alpha = 1 / C` ou `alpha = 1 / (n_samples * C)`, en fonction de l'estimateur et de la fonction objectif exacte optimis√©e par le mod√®le."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='multi-task-lasso'></a> 1.1.4. **Lasso multi-t√¢che**<br/>([*Multi-task Lasso*](https://scikit-learn.org/stable/modules/linear_model.html#multi-task-lasso))\n",
    "\n",
    "The [**`MultiTaskLasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskLasso.html) is a linear model that estimates sparse coefficients for multiple regression problems jointly: y is a 2D array, of shape `(n_samples, n_tasks)`. The constraint is that the selected features are the same for all the regression problems, also called tasks.\n",
    "\n",
    "The following figure compares the location of the non-zero entries in the coefficient matrix $W$ obtained with a simple [**`Lasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.htm) or a [**`MultiTaskLasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskLasso.html). The [**`Lasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.htm) estimates yield scattered non-zeros while the non-zeros of the [**`MultiTaskLasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskLasso.html) are full columns.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_multi_task_lasso_support_001.png\"\n",
    "    alt=\"Coefficient non-zero location\"\n",
    "    style=\"max-width: 45%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_multi_task_lasso_support_002.png\"\n",
    "    alt=\"Multi task lasso support\"\n",
    "    style=\"max-width: 45%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "#### Examples\n",
    "\n",
    "##### Joint feature selection with multi-task Lasso\n",
    "\n",
    "##### [**Joint feature selection with multi-task Lasso**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_multi_task_lasso_support.ipynb)<br/>([_Joint feature selection with multi-task Lasso_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_multi_task_lasso_support.html))\n",
    "\n",
    "#### Mathematical details\n",
    "\n",
    "Mathematically, it consists of a linear model trained with a mixed $\\ell_1$, $\\ell_2$-norm for regularization. The objective function to minimize is:\n",
    "\n",
    "$$\\min_{W} { \\frac{1}{2n_{\\text{samples}}} ||X W - Y||_{\\text{Fro}} ^ 2 + \\alpha ||W||_{21}}$$\n",
    "\n",
    "where $\\text{Fro}$ indicates the Frobenius norm\n",
    "\n",
    "$$||A||_{\\text{Fro}} = \\sqrt{\\sum_{ij} a_{ij}^2}$$\n",
    "\n",
    "and $\\ell_1$, $\\ell_2$ reads\n",
    "\n",
    "$$||A||_{2 1} = \\sum_i \\sqrt{\\sum_j a_{ij}^2}.$$\n",
    "\n",
    "The implementation in the class [**`MultiTaskLasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskLasso.html) uses coordinate descent as the algorithm to fit the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='multi-task-lasso'></a> 1.1.4. **Lasso multi-t√¢che**<br/>([*MultiTask Lasso*](https://scikit-learn.org/stable/modules/linear_model.html#multi-task-lasso))\n",
    "\n",
    "Le [**`MultiTaskLasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskLasso.html) est un mod√®le lin√©aire qui estime des coefficients parcimonieux pour plusieurs probl√®mes de r√©gression simultan√©ment : `y` est un tableau 2D, de forme `(n_samples, n_tasks)`. La contrainte est que les caract√©ristiques s√©lectionn√©es soient les m√™mes pour tous les probl√®mes de r√©gression, √©galement appel√©s _t√¢ches_.\n",
    "\n",
    "La figure suivante compare l'emplacement des entr√©es non nulles dans la matrice de coefficients $W$ obtenue avec un simple [**`Lasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.htm) ou avec un [**`MultiTaskLasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskLasso.html). Les estimations [**`Lasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.htm) produisent des non-nuls dispers√©s tandis que les non-nuls du [**`MultiTaskLasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskLasso.html) sont des colonnes compl√®tes.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_multi_task_lasso_support_001.png\"\n",
    "    alt=\"Emplacement des coefficients non nuls\"\n",
    "    style=\"max-width: 45%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_multi_task_lasso_support_002.png\"\n",
    "    alt=\"Support du lasso multi-t√¢che\"\n",
    "    style=\"max-width: 45%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**S√©lection conjointe de caract√©ristiques avec le lasso multi-t√¢che**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_multi_task_lasso_support.ipynb)<br/>([_Joint feature selection with multi-task Lasso_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_multi_task_lasso_support.html))\n",
    "\n",
    "#### D√©tails math√©matiques\n",
    "\n",
    "Math√©matiquement, il s'agit d'un mod√®le lin√©aire entra√Æn√© avec une r√©gularisation mixte des normes $\\ell_1$ et  $\\ell_2$. La fonction objectif √† minimiser est :\n",
    "\n",
    "$$\\min_{W} { \\frac{1}{2n_{\\text{samples}}} ||X W - Y||_{\\text{Fro}} ^ 2 + \\alpha ||W||_{21}}$$\n",
    "\n",
    "o√π $\\text{Fro}$ indique la norme de Frobenius\n",
    "\n",
    "$$||A||_{\\text{Fro}} = \\sqrt{\\sum_{ij} a_{ij}^2}$$\n",
    "\n",
    "et $\\ell_1\\ell_2$ est d√©finie comme suit\n",
    "\n",
    "$$||A||_{2 1} = \\sum_i \\sqrt{\\sum_j a_{ij}^2}.$$\n",
    "\n",
    "L'impl√©mentation dans la classe [**`MultiTaskLasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskLasso.html) utilise la _descente de coordonn√©es_ comme algorithme d'ajustement des coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='elastic-net'></a> 1.1.5. **Elastic-Net**<br/>([*Elastic-Net*](https://scikit-learn.org/stable/modules/linear_model.html#elastic-net))\n",
    "\n",
    "Le [**`ElasticNet`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) est un mod√®le de r√©gression lin√©aire entra√Æn√© avec une r√©gularisation √† la fois par les normes $\\ell_1$ et $\\ell_2$ des coefficients. Cette combinaison permet d'apprendre un mod√®le parcimonieux o√π peu de poids sont non nuls, comme le [**`Lasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html), tout en conservant les propri√©t√©s de r√©gularisation de [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html). Nous contr√¥lons la combinaison convexe de $\\ell_1$ et $\\ell_2$ √† l'aide du param√®tre `l1_ratio`.\n",
    "\n",
    "L'Elastic-Net est utile lorsqu'il y a plusieurs caract√©ristiques corr√©l√©es les unes aux autres. Le Lasso est susceptible d'en choisir une au hasard, tandis que l'Elastic-Net est susceptible d'en choisir plusieurs.\n",
    "\n",
    "Un avantage pratique du compromis entre Lasso et Ridge est qu'il permet √† l'Elastic-Net d'h√©riter de la stabilit√© de Ridge en cas de rotation.\n",
    "\n",
    "La fonction objectif √† minimiser dans ce cas est\n",
    "\n",
    "$$\\min_{w} { \\frac{1}{2n_{\\text{samples}}} ||X w - y||_2 ^ 2 + \\alpha \\rho ||w||_1 +\n",
    "\\frac{\\alpha(1-\\rho)}{2} ||w||_2 ^ 2}$$\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lasso_coordinate_descent_path_001.png\"\n",
    "    alt=\"Lasso and Elastic-Net Paths\"\n",
    "    style=\"max-width: 45%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "La classe [**`ElasticNetCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html) peut √™tre utilis√©e pour d√©finir les param√®tres `alpha` ($\\alpha$) et `l1_ratio` ($\\rho$) par validation crois√©e.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Mod√®les bas√©s sur $\\ell_1$ pour signaux creux**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_lasso_and_elasticnet.ipynb)<br/>([_L1-based models for Sparse Signals_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_and_elasticnet.html))\n",
    "\n",
    "##### [**Lasso et Elastic-Net**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_lasso_coordinate_descent_path.ipynb)<br/>([_Lasso and Elastic Net_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_coordinate_descent_path.html))\n",
    "\n",
    "#### R√©f√©rences\n",
    "\n",
    "Les deux r√©f√©rences suivantes expliquent les it√©rations utilis√©es dans le solveur de descente de coordonn√©es de scikit-learn, ainsi que le calcul de l'√©cart de dualit√© utilis√© pour le contr√¥le de la convergence.\n",
    "\n",
    "üî¨ [**‚ÄúRegularization Path For Generalized linear Models by Coordinate Descent‚Äù**](https://www.jstatsoft.org/article/view/v033i01/v33i01.pdf), Friedman, Hastie & Tibshirani, J Stat Softw, 2010.\n",
    "\n",
    "üî¨ [**‚ÄúAn Interior-Point Method for Large-Scale $\\ell_1$-Regularized Least Squares‚Äù**](https://web.stanford.edu/~boyd/papers/pdf/l1_ls.pdf), S. J. Kim, K. Koh, M. Lustig, S. Boyd and D. Gorinevsky, in IEEE Journal of Selected Topics in Signal Processing, 2007."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='multi-task-elastic-net'></a> 1.1.6. **Elastic-Net multi-t√¢che**<br/>([*Multi-task Elastic-Net*](https://scikit-learn.org/stable/modules/linear_model.html#multi-task-elastic-net))\n",
    "\n",
    "Le [**`MultiTaskElasticNet`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskElasticNet.html) est un mod√®le Elastic-Net qui estime conjointement des coefficients parcimonieux pour plusieurs probl√®mes de r√©gression : `Y` est un tableau 2D de forme `(n_samples, n_tasks)`. La contrainte est que les caract√©ristiques s√©lectionn√©es soient les m√™mes pour tous les probl√®mes de r√©gression, √©galement appel√©s _t√¢ches_.\n",
    "\n",
    "Math√©matiquement, il s'agit d'un mod√®le lin√©aire entra√Æn√© avec une r√©gularisation mixte $\\ell_1\\ell_2$ et $\\ell_2$ des coefficients. La fonction objectif √† minimiser est :\n",
    "\n",
    "$$\\min_{W} { \\frac{1}{2n_{\\text{samples}}} ||X W - Y||_{\\text{Fro}}^2 + \\alpha \\rho ||W||_{2 1} +\n",
    "\\frac{\\alpha(1-\\rho)}{2} ||W||_{\\text{Fro}}^2}$$\n",
    "\n",
    "L'impl√©mentation dans la classe [**`MultiTaskElasticNet`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskElasticNet.html) utilise l'algorithme de descente de coordonn√©es pour ajuster les coefficients.\n",
    "\n",
    "La classe [**`MultiTaskElasticNetCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskElasticNetCV.html) peut √™tre utilis√©e pour d√©finir les param√®tres `alpha` ($\\alpha$) et `l1_ratio` ($\\rho$) par validation crois√©e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='least-angle-regression'></a> 1.1.7. **R√©gression de plus faible angle (LARS)**<br/>([*Least Angle Regression*](https://scikit-learn.org/stable/modules/linear_model.html#least-angle-regression))\n",
    "\n",
    "La r√©gression de plus faible angle (LARS, pour _Least-Angle Regression_) est un algorithme de r√©gression pour les donn√©es √† haute dimension, d√©velopp√© par Bradley Efron, Trevor Hastie, Iain Johnstone et Robert Tibshirani. LARS est similaire √† la r√©gression pas √† pas en avant (_Forward Stepwise Regression_). √Ä chaque √©tape, il trouve la caract√©ristique la plus corr√©l√©e √† la cible. Lorsqu'il y a plusieurs caract√©ristiques ayant une corr√©lation √©gale, au lieu de continuer le long de la m√™me caract√©ristique, il proc√®de dans une direction √©qui-angulaire entre les caract√©ristiques.\n",
    "\n",
    "Les avantages de LARS sont les suivants :\n",
    "\n",
    "- Il est num√©riquement efficace dans les contextes o√π le nombre de caract√©ristiques est nettement sup√©rieur au nombre d'√©chantillons.\n",
    "- Il est aussi rapide que la s√©lection avant et a le m√™me ordre de complexit√© que les moindres carr√©s ordinaires.\n",
    "- Il produit un chemin complet de solution lin√©aire par morceaux, ce qui est utile dans la validation crois√©e ou les tentatives similaires d'ajustement du mod√®le.\n",
    "- Si deux caract√©ristiques sont presque √©galement corr√©l√©es √† la cible, alors leurs coefficients devraient augmenter √† peu pr√®s au m√™me taux. L'algorithme se comporte donc comme l'intuition le sugg√©rerait, et il est √©galement plus stable.\n",
    "- Il peut √™tre facilement modifi√© pour produire des solutions pour d'autres estimateurs, comme le Lasso.\n",
    "\n",
    "Les inconv√©nients de la m√©thode LARS incluent :\n",
    "\n",
    "- Parce que LARS est bas√© sur un r√©ajustement it√©ratif des r√©sidus, il semble √™tre particuli√®rement sensible aux effets du bruit. Ce probl√®me est discut√© en d√©tail par Weisberg dans la section de discussion de l'article de Weisberg dans la revue Annals of Statistics (2004).\n",
    "\n",
    "Le mod√®le LARS peut √™tre utilis√© via l'estimateur [**`Lars`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lars.html), ou son impl√©mentation de bas niveau [**`lars_path`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.lars_path.html) ou [**`lars_path_gram`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.lars_path_gram.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='lars-lasso'></a> 1.1.8. **LARS Lasso**<br/>([*LARS Lasso*](https://scikit-learn.org/stable/modules/linear_model.html#lars-lasso))\n",
    "\n",
    "[**`LassoLars`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLars.html) est un mod√®le de Lasso impl√©ment√© √† l'aide de l'algorithme LARS. Contrairement √† l'impl√©mentation bas√©e sur la descente de coordonn√©es, cela produit la solution exacte, qui est lin√©aire par morceaux en fonction de la norme de ses coefficients.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lasso_lars_001.png\"\n",
    "    alt=\"Lasso Path\"\n",
    "    style=\"max-width: 45%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "L'algorithme LARS fournit le chemin complet des coefficients le long du param√®tre de r√©gularisation presque gratuitement, donc une op√©ration courante est de r√©cup√©rer le chemin avec l'une des fonctions [**`lars_path`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.lars_path.html) ou [**`lars_path_gram`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.lars_path_gram.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6, 0. ])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LassoLars(alpha=.1)\n",
    "reg.fit([[0, 0], [1, 1]], [0, 1])\n",
    "# LassoLars(alpha=0.1)\n",
    "reg.coef_\n",
    "# array([0.6..., 0.        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemples\n",
    "\n",
    "##### [**Chemin Lasso √† l'aide de LARS**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_lasso_lars.ipynb)<br/>([_Lasso path using LARS_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_lars.html))\n",
    "\n",
    "#### D√©tails math√©matiques\n",
    "\n",
    "L'algorithme est similaire √† la r√©gression pas √† pas en avant, mais au lieu d'inclure des caract√©ristiques √† chaque √©tape, les coefficients estim√©s sont augment√©s dans une direction √©qui-angulaire par rapport √† la corr√©lation de chacun avec le r√©sidu.\n",
    "\n",
    "Au lieu de fournir un r√©sultat sous forme de vecteur, la solution LARS consiste en une courbe indiquant la solution pour chaque valeur de la norme $\\ell_1$ du vecteur de param√®tres. Le chemin complet des coefficients est stock√© dans le tableau `coef_path_` de forme `(n_features, max_features + 1)`. La premi√®re colonne est toujours nulle.\n",
    "\n",
    "#### R√©f√©rences\n",
    "\n",
    "L'algorithme original est d√©taill√© dans l'article :\n",
    "\n",
    "üî¨ [**‚ÄúLeast Angle Regression‚Äù**](https://hastie.su.domains/Papers/LARS/LeastAngle_2002.pdf) by Hastie et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='orthogonal-matching-pursuit-omp'></a> 1.1.9. **Poursuite orthogonale des correspondances (OMP)**<br/>([*Orthogonal Matching Pursuit (OMP)*](https://scikit-learn.org/stable/modules/linear_model.html#orthogonal-matching-pursuit-omp))\n",
    "\n",
    "[**`OrthogonalMatchingPursuit`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.OrthogonalMatchingPursuit.html) et [**`orthogonal_mp`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.orthogonal_mp.html) mettent en ≈ìuvre l'algorithme OMP pour approcher l'ajustement d'un mod√®le lin√©aire avec des contraintes impos√©es sur le nombre de coefficients non nuls (c'est-√†-dire la pseudo-norme $\\ell_0$).\n",
    "\n",
    "√âtant une m√©thode de s√©lection de caract√©ristiques en avant, similaire √† la [**R√©gression du moindre angle (LARS)** (1.1.7)](https://scikit-learn.org/stable/modules/linear_model.html), la poursuite orthogonale des correspondances peut approcher le vecteur de solution optimal avec un nombre fix√© d'√©l√©ments non nuls :\n",
    "\n",
    "$$\\underset{w}{\\operatorname{arg\\,min\\,}}  ||y - Xw||_2^2 \\text{ sous r√©serve } ||w||_0 \\leq n_{\\text{nonzero\\_coefs}}$$\n",
    "\n",
    "Alternativement, la poursuite orthogonale des correspondances peut viser une erreur sp√©cifique plut√¥t qu'un nombre sp√©cifique de coefficients non nuls. Cela peut √™tre exprim√© comme suit :\n",
    "\n",
    "$$\\underset{w}{\\operatorname{arg\\,min\\,}} ||w||_0 \\text{ sous r√©serve } ||y-Xw||_2^2 \\leq \\text{tol}$$\n",
    "\n",
    "OMP est bas√© sur un algorithme glouton qui inclut √† chaque √©tape l'atome le plus fortement corr√©l√© avec le r√©sidu actuel. Il est similaire √† la m√©thode de poursuite des correspondances simple (MP), mais il est meilleur en ce sens qu'√† chaque it√©ration, le r√©sidu est recalcul√© en utilisant une projection orthogonale sur l'espace des √©l√©ments de dictionnaire d√©j√† choisis.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Poursuite orthogonale des correspondances**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_omp.ipynb)<br/>([_Poursuite orthogonale des correspondances_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_omp.html))\n",
    "\n",
    "Utilisation de la poursuite orthogonale des correspondances pour r√©cup√©rer un signal parcimonieux √† partir d'une mesure bruit√©e encod√©e avec un dictionnaire.\n",
    "\n",
    "#### References\n",
    "\n",
    "üî¨ [**‚ÄúEfficient Implementation of the K-SVD Algorithm using Batch Orthogonal Matching Pursuit Technion‚Äù**](https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf), R. Rubinstein, M. Zibulevsky, M. Elad.\n",
    "\n",
    "üî¨ [**‚ÄúMatching Pursuits With Time-Frequency Dictionaries‚Äù**](https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf), S. G. Mallat, Z. Zhang.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='bayesian-regression'></a> 1.1.10. **R√©gression bay√©sienne**<br/>([*Bayesian Regression*](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-regression))\n",
    "\n",
    "Bayesian regression techniques can be used to include regularization parameters in the estimation procedure: the regularization parameter is not set in a hard sense but tuned to the data at hand.\n",
    "\n",
    "This can be done by introducing [**uninformative priors**](https://en.wikipedia.org/wiki/Prior_probability#Uninformative_priors) over the hyper parameters of the model. The $\\ell_{2}$ regularization used in [**Ridge regression and classification** (1.1.2)](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression) is equivalent to finding a maximum a posteriori estimation under a Gaussian prior over the coefficients with precision $\\lambda^{-1}$. Instead of setting `lambda` manually, it is possible to treat it as a random variable to be estimated from the data.\n",
    "\n",
    "To obtain a fully probabilistic model, the output $y$ is assumed to be Gaussian distributed around $X w$:\n",
    "\n",
    "$$p(y|X,w,\\alpha) = \\mathcal{N}(y|X w,\\alpha)$$\n",
    "\n",
    "where $\\alpha$ is again treated as a random variable that is to be estimated from the data.\n",
    "\n",
    "The advantages of Bayesian Regression are:\n",
    "- It adapts to the data at hand.\n",
    "- It can be used to include regularization parameters in the estimation procedure.\n",
    "\n",
    "The disadvantages of Bayesian regression include:\n",
    "- Inference of the model can be time consuming.\n",
    "\n",
    "#### References\n",
    "\n",
    "A good introduction to Bayesian methods is given in C. Bishop: Pattern Recognition and Machine learning\n",
    "\n",
    "Original Algorithm is detailed in the book Bayesian learning for neural networks by Radford M. Neal\n",
    "\n",
    "### <a id='bayesian-ridge-regression'></a> 1.1.10.1. Bayesian Ridge Regression\n",
    "\n",
    "[**`BayesianRidge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html) estimates a probabilistic model of the regression problem as described above. The prior for the coefficient $w$ is given by a spherical Gaussian:\n",
    "\n",
    "$$p(w|\\lambda) = \\mathcal{N}(w|0,\\lambda^{-1}\\mathbf{I}_{p})$$\n",
    "\n",
    "The priors over $\\alpha$ and $\\lambda$ are chosen to be [**gamma distributions**](https://en.wikipedia.org/wiki/Gamma_distribution), the conjugate prior for the precision of the Gaussian. The resulting model is called Bayesian Ridge Regression, and is similar to the classical [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html).\n",
    "\n",
    "The parameters $w$, $\\alpha$ and $\\lambda$ are estimated jointly during the fit of the model, the regularization parameters $\\alpha$ and $\\lambda$ being estimated by maximizing the _log marginal likelihood_. The scikit-learn implementation is based on the algorithm described in Appendix A of (Tipping, 2001) where the update of the parameters $\\alpha$ and $\\lambda$ is done as suggested in (MacKay, 1992). The initial value of the maximization procedure can be set with the hyperparameters `alpha_init` and `lambda_init`.\n",
    "\n",
    "There are four more hyperparameters, $\\alpha_1$, $\\alpha_2$, $\\lambda_1$ and $\\lambda_2$ of the gamma prior distributions over $\\alpha$ and $\\lambda$. These are usually chosen to be _non-informative_. By default $\\alpha_1 = \\alpha_2 =  \\lambda_1 = \\lambda_2 = 10^{-6}$.\n",
    "\n",
    "Bayesian Ridge Regression is used for regression:\n",
    "\n",
    "---\n",
    "\n",
    "After being fitted, the model can then be used to predict new values:\n",
    "\n",
    "---\n",
    "\n",
    "The coefficients $w$ of the model can be accessed:\n",
    "\n",
    "---\n",
    "\n",
    "Due to the Bayesian framework, the weights found are slightly different to the ones found by [**Ordinary Least Squares (OLS)** (1.1.1)](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares). However, Bayesian Ridge Regression is more robust to ill-posed problems.\n",
    "\n",
    "#### Examples\n",
    "\n",
    "##### [**Curve Fitting with Bayesian Ridge Regression**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_bayesian_ridge_curvefit.ipynb)<br/>([_Curve Fitting with Bayesian Ridge Regression_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_bayesian_ridge_curvefit.html))\n",
    "\n",
    "Computes a Bayesian Ridge Regression of Sinusoids.\n",
    "\n",
    "#### References\n",
    "\n",
    "Section 3.3 in Christopher M. Bishop: Pattern Recognition and Machine Learning, 2006\n",
    "\n",
    "David J. C. MacKay, Bayesian Interpolation, 1992.\n",
    "\n",
    "Michael E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine, 2001.\n",
    "\n",
    "### <a id='automatic-relevance-determination-ard'></a> 1.1.10.2. Automatic Relevance Determination (ARD)\n",
    "\n",
    "The Automatic Relevance Determination (as being implemented in [**`ARDRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ARDRegression.html)) is a kind of linear model which is very similar to the [**Bayesian Ridge Regression** (1.1.10.1)](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-ridge-regression), but that leads to sparser coefficients[1] [2].\n",
    "\n",
    "[**`ARDRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ARDRegression.html) poses a different prior over $w$: it drops the spherical Gaussian distribution for a centered elliptic Gaussian distribution. This means each coefficient $w_i$ can itself be drawn from a Gaussian distribution, centered on zero and with a precision $\\lambda_{i}$:\n",
    "\n",
    "$$p(w|\\lambda) = \\mathcal{N}(w|0,A^{-1})$$\n",
    "\n",
    "with $A$ being a positive definite diagonal matrix and $\\text{diag}(A) = \\lambda = \\{\\lambda_{1},...,\\lambda_{p}\\}$.\n",
    "\n",
    "In contrast to the [**Bayesian Ridge Regression** (1.1.10.1)](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-ridge-regression), each coordinate of $w_i$ has its own standard deviation $\\frac{1}{\\lambda_i}$. The prior over all $\\lambda_{i}$ is chosen to be the same gamma distribution given by the hyperparameters $\\lambda_1$ and $\\lambda_2$.\n",
    "\n",
    "ARD is also known in the literature as _Sparse Bayesian Learning and Relevance Vector Machine_ [3] [4]. For a worked-out comparison between ARD and [**Bayesian Ridge Regression** (1.1.10.1)](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-ridge-regression), see the example below.\n",
    "\n",
    "#### Examples\n",
    "\n",
    "##### [**Comparaison des r√©gresseurs bay√©siens lin√©aires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_ard.ipynb)<br/>([_Comparing Linear Bayesian Regressors_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ard.html))\n",
    "\n",
    "Compares a Automatic Relevance Determination (ARD) and a Bayesian Ridge Regression.\n",
    "\n",
    "#### References\n",
    "\n",
    "[1] Christopher M. Bishop: Pattern Recognition and Machine Learning, Chapter 7.2.1\n",
    "\n",
    "[2] David Wipf and Srikantan Nagarajan: A New View of Automatic Relevance Determination\n",
    "\n",
    "[3] Michael E. Tipping: Sparse Bayesian Learning and the Relevance Vector Machine\n",
    "\n",
    "[4] Tristan Fletcher: Relevance Vector Machines Explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='bayesian-regression'></a> 1.1.10. **R√©gression bay√©sienne**<br/>([*Bayesian Regression*](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-regression))\n",
    "\n",
    "Les techniques de r√©gression bay√©sienne peuvent √™tre utilis√©es pour inclure des param√®tres de r√©gularisation dans la proc√©dure d'estimation : le param√®tre de r√©gularisation n'est pas fix√© de mani√®re stricte mais ajust√© aux donn√©es en cours.\n",
    "\n",
    "Cela peut √™tre fait en introduisant des [**_a priori_ non informatifs**](https://en.wikipedia.org/wiki/Prior_probability#Uninformative_priors) sur les hyper-param√®tres du mod√®le. La r√©gularisation $\\ell_{2}$ utilis√©e dans [**la r√©gression et la classification Ridge** (1.1.2)](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression) √©quivaut √† trouver une estimation du maximum _a posteriori_ sous une loi normale pour les coefficients $w$ avec une pr√©cision $\\lambda^{-1}$. Au lieu de d√©finir `lambda` manuellement, il est possible de le traiter comme une variable al√©atoire √† estimer √† partir des donn√©es.\n",
    "\n",
    "Pour obtenir un mod√®le enti√®rement probabiliste, la sortie $y$ est suppos√©e suivre une distribution normale autour de $X w$ :\n",
    "\n",
    "$$p(y|X,w,\\alpha) = \\mathcal{N}(y|X w,\\alpha)$$\n",
    "\n",
    "o√π $\\alpha$ est √©galement trait√© comme une variable al√©atoire √† estimer √† partir des donn√©es.\n",
    "\n",
    "Les avantages de la r√©gression bay√©sienne sont :\n",
    "- Elle s'adapte aux donn√©es en cours.\n",
    "- Elle peut √™tre utilis√©e pour inclure des param√®tres de r√©gularisation dans la proc√©dure d'estimation.\n",
    "\n",
    "Les inconv√©nients de la r√©gression bay√©sienne comprennent :\n",
    "- L'inf√©rence du mod√®le peut prendre du temps.\n",
    "\n",
    "#### R√©f√©rences\n",
    "\n",
    "üìö Bishop, [**‚ÄúPattern recognition and machine learning‚Äù**](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf), 2006.\n",
    "\n",
    "Une bonne introduction aux m√©thodes bay√©siennes.\n",
    "\n",
    "üìö [**‚ÄúBayesian learning for neural networks‚Äù**](https://link.springer.com/book/10.1007/978-1-4612-0745-0) de Radford M. Neal, 1996.\n",
    "\n",
    "Les d√©tails de l'algorithme original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='bayesian-ridge-regression'></a> 1.1.10.1. R√©gression bay√©sienne Ridge\n",
    "\n",
    "[**`BayesianRidge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html) estime un mod√®le probabiliste du probl√®me de r√©gression comme d√©crit ci-dessus. L'_a priori_ pour le coefficient $w$ est donn√© par une loi normale sph√©rique :\n",
    "\n",
    "$$p(w|\\lambda) = \\mathcal{N}(w|0,\\lambda^{-1}\\mathbf{I}_{p})$$\n",
    "\n",
    "Les _a priori_ sur $\\alpha$ et $\\lambda$ sont choisis comme √©tant des [**distributions gamma**](https://en.wikipedia.org/wiki/Gamma_distribution), l'_a priori_ conjugu√© pour la pr√©cision de la loi normale. Le mod√®le r√©sultant est appel√© _r√©gression Ridge bay√©sienne_, et elle est similaire √† la r√©gression classique [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html).\n",
    "\n",
    "Les param√®tres $w$, $\\alpha$ et $\\lambda$ sont estim√©s conjointement lors de l'ajustement du mod√®le, les param√®tres de r√©gularisation $\\alpha$ et $\\lambda$ √©tant estim√©s en maximisant la _log-vraisemblance marginale_. L'impl√©mentation de scikit-learn est bas√©e sur l'algorithme d√©crit dans l'annexe A de (Tipping, 2001) o√π la mise √† jour des param√®tres $\\alpha$ et $\\lambda$ est effectu√©e comme sugg√©r√© dans (MacKay, 1992). La valeur initiale de la proc√©dure de maximisation peut √™tre d√©finie √† l'aide des hyperparam√®tres `alpha_init` et `lambda_init`.\n",
    "\n",
    "Il y a quatre autres hyperparam√®tres, $\\alpha_1$, $\\alpha_2$, $\\lambda_1$ et $\\lambda_2$, des distributions _a priori_ gamma sur $\\alpha$ et $\\lambda$. Ceux-ci sont g√©n√©ralement choisis comme _non informatifs_. Par d√©faut, $\\alpha_1 = \\alpha_2 =  \\lambda_1 = \\lambda_2 = 10^{-6}$.\n",
    "\n",
    "La r√©gression Ridge bay√©sienne est utilis√©e pour la r√©gression :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BayesianRidge()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BayesianRidge</label><div class=\"sk-toggleable__content\"><pre>BayesianRidge()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BayesianRidge()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "X = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]]\n",
    "Y = [0., 1., 2., 3.]\n",
    "reg = linear_model.BayesianRidge()\n",
    "reg.fit(X, Y)\n",
    "# BayesianRidge("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apr√®s avoir √©t√© ajust√©, le mod√®le peut √™tre utilis√© pour pr√©dire de nouvelles valeurs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50000013])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.predict([[1, 0.]])\n",
    "# array([0.50000013])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les coefficients $w$ du mod√®le peuvent √™tre acc√©d√©s :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49999993, 0.49999993])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_\n",
    "# array([0.49999993, 0.49999993]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En raison du cadre bay√©sien, les poids trouv√©s sont l√©g√®rement diff√©rents de ceux trouv√©s par les [**moindres carr√©s ordinaires (OLS)** (1.1.1)](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares). Cependant, la r√©gression Ridge bay√©sienne est plus robuste aux probl√®mes mal pos√©s.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Ajustement de courbe avec la R√©gression Ridge Bay√©sienne**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_bayesian_ridge_curvefit.ipynb)<br/>([_Curve Fitting with Bayesian Ridge Regression_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_bayesian_ridge_curvefit.html))\n",
    "\n",
    "Calcule une R√©gression Ridge Bay√©sienne de sinuso√Ødes.\n",
    "\n",
    "#### R√©f√©rences\n",
    "\n",
    "üìö Section 3.3 dans [**‚ÄúPattern recognition and machine learning‚Äù**](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf) de Christopher M. Bishop, 2006\n",
    "\n",
    "üî¨ David J. C. MacKay, [**‚ÄúBayesian Interpolation‚Äù**](https://authors.library.caltech.edu/13792/1/MACnc92a.pdf), 1992.\n",
    "\n",
    "üî¨ Michael E. Tipping, [**‚ÄúSparse Bayesian Learning and the Relevance Vector Machine‚Äù**](https://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf), 2001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='automatic-relevance-determination-ard'></a> 1.1.10.2. D√©termination automatique de la pertinence (ARD)\n",
    "\n",
    "La d√©termination automatique de la pertinence (impl√©ment√©e dans [**`ARDRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ARDRegression.html)) est un type de mod√®le lin√©aire tr√®s similaire √† la [**r√©gression Ridge bay√©sienne** (1.1.10.1)](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-ridge-regression), mais qui conduit √† des coefficients plus parcimonieux[1] [2].\n",
    "\n",
    "[**`ARDRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ARDRegression.html) pose un _a priori_ diff√©rent sur $w$ : il abandonne la distribution gaussienne sph√©rique au profit d'une distribution gaussienne elliptique centr√©e. Cela signifie que chaque coefficient $w_i$ peut √™tre lui-m√™me tir√© d'une distribution gaussienne, centr√©e sur z√©ro et avec une pr√©cision $\\lambda_{i}$ :\n",
    "\n",
    "$$p(w|\\lambda) = \\mathcal{N}(w|0,A^{-1})$$\n",
    "\n",
    "o√π $A$ est une matrice diagonale d√©finie positive et $\\text{diag}(A) = \\lambda = \\{\\lambda_{1},...,\\lambda_{p}\\}$.\n",
    "\n",
    "Contrairement √† la [**r√©gression Ridge bay√©sienne** (1.1.10.1)](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-ridge-regression), chaque coordonn√©e de $w_i$ a sa propre d√©viation standard $\\frac{1}{\\lambda_i}$. L'_a priori_ sur toutes les $\\lambda_{i}$ est choisi comme √©tant la m√™me distribution gamma donn√©e par les hyperparam√®tres $\\lambda_1$ et $\\lambda_2$.\n",
    "\n",
    "L'ARD est √©galement connue dans la litt√©rature sous le nom de _Sparse Bayesian Learning and Relevance Vector Machine_ [3] [4]. Pour une comparaison d√©taill√©e entre ARD et la [**r√©gression Ridge bay√©sienne** (1.1.10.1)](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-ridge-regression), voir l'exemple ci-dessous.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Comparaison des r√©gresseurs bay√©siens lin√©aires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_ard.ipynb)<br/>([_Comparing Linear Bayesian Regressors_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ard.html))\n",
    "\n",
    "Compare une d√©termination automatique de la pertinence (ARD) et une r√©gression Ridge bay√©sienne.\n",
    "\n",
    "#### R√©f√©rences\n",
    "\n",
    "üìö [1] Christopher M. Bishop: [**‚ÄúPattern recognition and machine learning‚Äù**](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf), Chapitre 7.2.1\n",
    "\n",
    "üî¨ [2] David Wipf and Srikantan Nagarajan: [**‚ÄúA New View of Automatic Relevance Determination‚Äù**](https://proceedings.neurips.cc/paper_files/paper/2007/file/9c01802ddb981e6bcfbec0f0516b8e35-Paper.pdf)\n",
    "\n",
    "üî¨ [3] Michael E. Tipping, [**‚ÄúSparse Bayesian Learning and the Relevance Vector Machine‚Äù**](https://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf), 2001.\n",
    "\n",
    "üî¨ [4] Tristan Fletcher: [**‚ÄúRelevance Vector Machines Explained‚Äù**](http://www.di.fc.ul.pt/~jpn/r/PRML/chp7/Fletcher_RVM_Explained.pdf), 2010."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='logistic-regression'></a> 1.1.11. **R√©gression logistique**<br/>([*Logistic regression*](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression))\n",
    "\n",
    "The logistic regression is implemented in [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Despite its name, it is implemented as a linear model for classification rather than regression in terms of the scikit-learn/ML nomenclature. The logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a [**logistic function**](https://en.wikipedia.org/wiki/Logistic_function).\n",
    "\n",
    "This implementation can fit binary, One-vs-Rest, or multinomial logistic regression with optional $\\ell_1$, $\\ell_2$ or Elastic-Net regularization.\n",
    "\n",
    "> **Note: Regularization**\n",
    "> Regularization is applied by default, which is common in machine learning but not in statistics. Another advantage of regularization is that it improves numerical stability. No regularization amounts to setting `C` to a very high value.\n",
    "\n",
    "> **Note: Logistic Regression as a special case of the Generalized Linear Models (GLM)**\n",
    "> Logistic regression is a special case of [**Generalized Linear Models** (1.1.12)](https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-models) with a Binomial / Bernoulli conditional distribution and a Logit link. The numerical output of the logistic regression, which is the predicted probability, can be used as a classifier by applying a threshold (by default 0.5) to it. This is how it is implemented in scikit-learn, so it expects a categorical target, making the Logistic Regression a classifier.\n",
    "\n",
    "### <a id='binary-case'></a> 1.1.11.1. Binary Case\n",
    "\n",
    "For notational ease, we assume that the target $y_i$ takes values in the set $\\{0, 1\\}$ for data point $i$. Once fitted, the [**`predict_proba`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba) method of [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) predicts the probability of the positive class $P(y_i=1|X_i)$ as\n",
    "\n",
    "$$\\hat{p}(X_i) = \\operatorname{expit}(X_i w + w_0) = \\frac{1}{1 + \\exp(-X_i w - w_0)}.$$\n",
    "\n",
    "As an optimization problem, binary class logistic regression with regularization term $r(w)$ minimizes the following cost function:\n",
    "\n",
    "$$\\min_{w} C \\sum_{i=1}^n \\left(-y_i \\log(\\hat{p}(X_i)) - (1 - y_i) \\log(1 - \\hat{p}(X_i))\\right) + r(w).$$\n",
    "\n",
    "We currently provide four choices for the regularization term $r(w)$ via the `penalty` argument:\n",
    "\n",
    "|penalty|$r(w)$|\n",
    "|-|-|\n",
    "|`None`|$0$|\n",
    "|$\\ell_1$|$\\|\\|w\\|\\|_1$|\n",
    "|$\\ell_2$|$\\frac{1}{2}\\|\\|w\\|\\|_2^2 = \\frac{1}{2}w^\\top w$|\n",
    "|`ElasticNet`|$\\frac{1 - \\rho}{2}w^\\top w + \\rho \\|\\|w\\|\\|_1$|\n",
    "\n",
    "\n",
    "For ElasticNet, $\\rho$ (which corresponds to the `l1_ratio` parameter) controls the strength of $\\ell_1$ regularization vs. $\\ell_2$ regularization. Elastic-Net is equivalent to $\\ell_1$ when $\\rho=1$ and equivalent to $\\ell_2$ when $\\rho=0$.\n",
    "\n",
    "### <a id='multinomial-case'></a> 1.1.11.2. Multinomial Case\n",
    "\n",
    "The binary case can be extended to $K$ classes leading to the multinomial logistic regression, see also [**log-linear model**](https://en.wikipedia.org/wiki/Multinomial_logistic_regression#As_a_log-linear_model).\n",
    "\n",
    "> **Note:** It is possible to parameterize a $K$-class classification model using only $K-1$ weight vectors, leaving one class probability fully determined by the other class probabilities by leveraging the fact that all class probabilities must sum to one. We deliberately choose to overparameterize the model using $K$ weight vectors for ease of implementation and to preserve the symmetrical inductive bias regarding ordering of classes, see [16]. This effect becomes especially important when using regularization. The choice of overparameterization can be detrimental for unpenalized models since then the solution may not be unique, as shown in [16].\n",
    "\n",
    "Let $y_i \\in {1, \\ldots, K}$ be the label (ordinal) encoded target variable for observation $i$. Instead of a single coefficient vector, we now have a matrix of coefficients $W$ where each row vector $W_k$ corresponds to class $k$. We aim at predicting the class probabilities $P(y_i=k|X_i)$ via [**`predict_proba`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) as:\n",
    "\n",
    "$$\\hat{p}_k(X_i) = \\frac{\\exp(X_i W_k + W_{0, k})}{\\sum_{l=0}^{K-1} \\exp(X_i W_l + W_{0, l})}.$$\n",
    "\n",
    "The objective for the optimization becomes\n",
    "\n",
    "$$\\min_W -C \\sum_{i=1}^n \\sum_{k=0}^{K-1} [y_i = k] \\log(\\hat{p}_k(X_i)) + r(W).$$\n",
    "\n",
    "Where $[P]$ represents the Iverson bracket which evaluates to $0$ if $P$ is false, otherwise it evaluates to $1$. We currently provide four choices for the regularization term $r(W)$ via the `penalty` argument:\n",
    "\n",
    "|penalty|$r(W)$|\n",
    "|-|-|\n",
    "|`None`|$0$|\n",
    "|$\\ell_1$|$\\|\\|W\\|\\|_{1,1} = \\sum_{i=1}^n\\sum_{j=1}^{K}\\|W_{i,j}\\|$|\n",
    "|$\\ell_2$|$\\frac{1}{2}\\|\\|W\\|\\|_F^2 = \\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^{K} W_{i,j}^2$|\n",
    "|`ElasticNet`|$\\frac{1 - \\rho}{2}\\|\\|W\\|\\|_F^2 + \\rho \\|\\|W\\|\\|_{1,1}$|\t\n",
    "\n",
    "### <a id='solvers'></a> 1.1.11.3. Solvers\n",
    "\n",
    "The solvers implemented in the class [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) are ‚Äúlbfgs‚Äù, ‚Äúliblinear‚Äù, ‚Äúnewton-cg‚Äù, ‚Äúnewton-cholesky‚Äù, ‚Äúsag‚Äù and ‚Äúsaga‚Äù:\n",
    "\n",
    "The solver ‚Äúliblinear‚Äù uses a coordinate descent (CD) algorithm, and relies on the excellent C++ [LIBLINEAR library](https://www.csie.ntu.edu.tw/~cjlin/liblinear/), which is shipped with scikit-learn. However, the CD algorithm implemented in liblinear cannot learn a true multinomial (multiclass) model; instead, the optimization problem is decomposed in a ‚Äúone-vs-rest‚Äù fashion so separate binary classifiers are trained for all classes. This happens under the hood, so [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) instances using this solver behave as multiclass classifiers. For $\\ell_1$ regularization [**`sklearn.svm.l1_min_c`**](https://scikit-learn.org/stable/modules/generated/sklearn.svm.l1_min_c.html) allows to calculate the lower bound for C in order to get a non ‚Äúnull‚Äù (all feature weights to zero) model.\n",
    "\n",
    "The ‚Äúlbfgs‚Äù, ‚Äúnewton-cg‚Äù and ‚Äúsag‚Äù solvers only support $\\ell_2$ regularization or no regularization, and are found to converge faster for some high-dimensional data. Setting `multi_class` to ‚Äúmultinomial‚Äù with these solvers learns a true multinomial logistic regression model [5], which means that its probability estimates should be better calibrated than the default ‚Äúone-vs-rest‚Äù setting.\n",
    "\n",
    "The ‚Äúsag‚Äù solver uses Stochastic Average Gradient descent [6]. It is faster than other solvers for large datasets, when both the number of samples and the number of features are large.\n",
    "\n",
    "The ‚Äúsaga‚Äù solver [7] is a variant of ‚Äúsag‚Äù that also supports the non-smooth `penalty=\"l1\"`. This is therefore the solver of choice for sparse multinomial logistic regression. It is also the only solver that supports `penalty=\"elasticnet\"`.\n",
    "\n",
    "The ‚Äúlbfgs‚Äù is an optimization algorithm that approximates the Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShanno algorithm [8], which belongs to quasi-Newton methods. As such, it can deal with a wide range of different training data and is therefore the default solver. Its performance, however, suffers on poorly scaled datasets and on datasets with one-hot encoded categorical features with rare categories.\n",
    "\n",
    "The ‚Äúnewton-cholesky‚Äù solver is an exact Newton solver that calculates the hessian matrix and solves the resulting linear system. It is a very good choice for `n_samples` >> `n_features`, but has a few shortcomings: Only $\\ell_2$ regularization is supported. Furthermore, because the hessian matrix is explicitly computed, the memory usage has a quadratic dependency on `n_features` as well as on `n_classes`. As a consequence, only the one-vs-rest scheme is implemented for the multiclass case.\n",
    "\n",
    "For a comparison of some of these solvers, see [9].\n",
    "\n",
    "The following table summarizes the penalties supported by each solver:\n",
    "\n",
    "| Penalties                    | ‚Äòlbfgs‚Äô | ‚Äòliblinear‚Äô | ‚Äònewton-cg‚Äô | ‚Äònewton-cholesky‚Äô | ‚Äòsag‚Äô | ‚Äòsaga‚Äô |\n",
    "|------------------------------|---------|-------------|-------------|-------------------|-------|--------|\n",
    "| Multinomial + L2 penalty     | yes     | no          | yes         | no                | yes   | yes    |\n",
    "| OVR + L2 penalty             | yes     | yes         | yes         | yes               | yes   | yes    |\n",
    "| Multinomial + L1 penalty     | no      | no          | no          | no                | no    | yes    |\n",
    "| OVR + L1 penalty             | no      | yes         | no          | no                | no    | yes    |\n",
    "| Elastic-Net                  | no      | no          | no          | no                | no    | yes    |\n",
    "| No penalty (‚Äònone‚Äô)          | yes     | no          | yes         | yes               | yes   | yes    |\n",
    "| **Behaviors**                |         |             |             |                   |       |        |\n",
    "| Penalize the intercept (bad) | no      | yes         | no          | no                | no    | no     |\n",
    "| Faster for large datasets    | no      | no          | no          | no                | yes   | yes    |\n",
    "| Robust to unscaled datasets  | yes     | yes         | yes         | yes               | no    | no     |\n",
    "\n",
    "The ‚Äúlbfgs‚Äù solver is used by default for its robustness. For large datasets the ‚Äúsaga‚Äù solver is usually faster. For large dataset, you may also consider using [**`SGDClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) with `loss=\"log_loss\"`, which might be even faster but requires more tuning.\n",
    "\n",
    "#### Differences from liblinear\n",
    "\n",
    "There might be a difference in the scores obtained between [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) with `solver=liblinear` or `LinearSVC` and the external liblinear library directly, when `fit_intercept=False` and the fit `coef_` (or) the data to be predicted are zeroes. This is because for the sample(s) with `decision_function` zero, [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) and `LinearSVC` predict the negative class, while liblinear predicts the positive class. Note that a model with `fit_intercept=False` and having many samples with `decision_function` zero, is likely to be a underfit, bad model and you are advised to set `fit_intercept=True` and increase the `intercept_scaling`.\n",
    "\n",
    "> **Note: Feature selection with sparse logistic regression**\n",
    "> A logistic regression with $\\ell_1$ penalty yields sparse models, and can thus be used to perform feature selection, as detailed in example **L1-based feature selection**.\n",
    "\n",
    "> **Note: P-value estimation**\n",
    "> It is possible to obtain the p-values and confidence intervals for coefficients in cases of regression without penalization. The [statsmodels package](https://pypi.org/project/statsmodels/) natively supports this. Within sklearn, one could use bootstrapping instead as well.\n",
    "\n",
    "[**`LogisticRegressionCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html) implements Logistic Regression with built-in cross-validation support, to find the optimal `C` and `l1_ratio` parameters according to the `scoring` attribute. The ‚Äúnewton-cg‚Äù, ‚Äúsag‚Äù, ‚Äúsaga‚Äù and ‚Äúlbfgs‚Äù solvers are found to be faster for high-dimensional dense data, due to warm-starting (see [**Glossary/`warm_start`**](https://scikit-learn.org/stable/glossary.html#term-warm_start)).\n",
    "\n",
    "#### Examples\n",
    "\n",
    "##### [**L1 Penalty and Sparsity in Logistic Regression**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_logistic_l1_l2_sparsity.ipynb)<br/>([_L1 Penalty and Sparsity in Logistic Regression_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_l1_l2_sparsity.html))\n",
    "\n",
    "Comparison of the sparsity (percentage of zero coefficients) of solutions when L1, L2 and Elastic-Net penalty are used for different values of C. We can see that large values of C give more freedom to the model. Conversely, smaller values of C constrain the model more. In the L1 penalty case, this leads to sparser solutions. As expected, the Elastic-Net penalty sparsity is between that of L1 and L2.\n",
    "\n",
    "We classify 8x8 images of digits into two classes: 0-4 against 5-9. The visualization shows coefficients of the models for varying C.\n",
    "\n",
    "##### [**Regularization path of $\\ell_1$- Logistic Regression**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_logistic_path.ipynb)<br/>([_Regularization path of $\\ell_1$- Logistic Regression_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_path.html))\n",
    "\n",
    "Train $\\ell_1$-penalized logistic regression models on a binary classification problem derived from the Iris dataset.\n",
    "\n",
    "The models are ordered from strongest regularized to least regularized. The 4 coefficients of the models are collected and plotted as a ‚Äúregularization path‚Äù: on the left-hand side of the figure (strong regularizers), all the coefficients are exactly 0. When regularization gets progressively looser, coefficients can get non-zero values one after the other.\n",
    "\n",
    "Here we choose the `liblinear` solver because it can efficiently optimize for the Logistic Regression loss with a non-smooth, sparsity inducing $\\ell_1$ penalty.\n",
    "\n",
    "Also note that we set a low value for the tolerance to make sure that the model has converged before collecting the coefficients.\n",
    "\n",
    "We also use `warm_start=True` which means that the coefficients of the models are reused to initialize the next model fit to speed-up the computation of the full-path.\n",
    "\n",
    "##### [**Plot multinomial and One-vs-Rest Logistic Regression**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_logistic_multinomial.ipynb)<br/>([_Plot multinomial and One-vs-Rest Logistic Regression_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_multinomial.html))\n",
    "\n",
    "Plot decision surface of multinomial and One-vs-Rest Logistic Regression. The hyperplanes corresponding to the three One-vs-Rest (OVR) classifiers are represented by the dashed lines.\n",
    "\n",
    "##### [**Multiclass sparse logistic regression on 20newgroups**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_sparse_logistic_regression_20newsgroups.ipynb)<br/>([_Multiclass sparse logistic regression on 20newgroups_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_20newsgroups.html))\n",
    "\n",
    "Comparison of multinomial logistic $\\ell_1$ vs one-versus-rest $\\ell_1$ logistic regression to classify documents from the newgroups20 dataset. Multinomial logistic regression yields more accurate results and is faster to train on the larger scale dataset.\n",
    "\n",
    "Here we use the $\\ell_1$ sparsity that trims the weights of not informative features to zero. This is good if the goal is to extract the strongly discriminative vocabulary of each class. If the goal is to get the best predictive accuracy, it is better to use the non sparsity-inducing $\\ell_2$ penalty instead.\n",
    "\n",
    "A more traditional (and possibly better) way to predict on a sparse subset of input features would be to use univariate feature selection followed by a traditional ($\\ell_2$-penalised) logistic regression model.\n",
    "\n",
    "##### [**MNIST classification using multinomial logistic + $\\ell_1$**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_sparse_logistic_regression_mnist.ipynb)<br/>([_MNIST classification using multinomial logistic +_ $\\ell_1$](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html))\n",
    "\n",
    "Here we fit a multinomial logistic regression with $\\ell_1$ penalty on a subset of the MNIST digits classification task. We use the SAGA algorithm for this purpose: this a solver that is fast when the number of samples is significantly larger than the number of features and is able to finely optimize non-smooth objective functions which is the case with the $\\ell_1$-penalty. Test accuracy reaches > 0.8, while weight vectors remains sparse and therefore more easily _interpretable_.\n",
    "\n",
    "Note that this accuracy of this $\\ell_1$-penalized linear model is significantly below what can be reached by an $\\ell_2$-penalized linear model or a non-linear multi-layer perceptron model on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='logistic-regression'></a> 1.1.11. **R√©gression logistique**<br/>([*Logistic regression*](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression))\n",
    "\n",
    "La r√©gression logistique est impl√©ment√©e dans [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Malgr√© son nom, en termes de la nomenclature scikit-learn/ML, elle est mise en ≈ìuvre en tant que mod√®le lin√©aire pour la classification plut√¥t que pour la r√©gression. La r√©gression logistique est √©galement connue dans la litt√©rature sous le nom de r√©gression _logit_, classification d'entropie maximale (MaxEnt) ou classifieur log-lin√©aire. Dans ce mod√®le, les probabilit√©s d√©crivant les r√©sultats possibles d'un seul essai sont mod√©lis√©es √† l'aide d'une [**fonction logistique**](https://en.wikipedia.org/wiki/Logistic_function).\n",
    "\n",
    "Cette impl√©mentation peut ajuster une r√©gression logistique binaire, un-contre-tous (OVR), ou multinomiale avec une r√©gularisation optionnelle $\\ell_1$, $\\ell_2$ ou Elastic-Net.\n",
    "\n",
    "> **Remarque : R√©gularisation**\n",
    "> La r√©gularisation est appliqu√©e par d√©faut, ce qui est courant en apprentissage automatique mais pas en statistiques. Un autre avantage de la r√©gularisation est qu'elle am√©liore la stabilit√© num√©rique. L'absence de r√©gularisation revient √† fixer `C` √† une valeur tr√®s √©lev√©e.\n",
    "\n",
    "> **Remarque : R√©gression logistique comme cas sp√©cial des mod√®les lin√©aires g√©n√©ralis√©s (GLM)**\n",
    "> La r√©gression logistique est un cas sp√©cial des [**mod√®les lin√©aires g√©n√©ralis√©s (GLM)** (1.1.12)](https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-models) avec une distribution conditionnelle Binomiale / Bernoulli et une liaison Logit. La sortie num√©rique de la r√©gression logistique, qui est la probabilit√© pr√©dite, peut √™tre utilis√©e comme un classifieur en lui appliquant un seuil (par d√©faut 0.5). C'est ainsi qu'elle est impl√©ment√©e dans scikit-learn, de sorte qu'elle s'attend √† une cible cat√©gorielle, transformant ainsi la r√©gression logistique en classifieur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='binary-case'></a> 1.1.11.1. Cas binaire\n",
    "\n",
    "Par commodit√© de notation, nous supposons que la cible $y_i$ prend ses valeurs dans l'ensemble $\\{0, 1\\}$ pour le point de donn√©es $i$. Une fois ajust√©e, la m√©thode [**`predict_proba`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba) de [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) pr√©dit la probabilit√© de la classe positive $P(y_i=1|X_i)$ comme suit :\n",
    "\n",
    "$$\\hat{p}(X_i) = \\operatorname{expit}(X_i w + w_0) = \\frac{1}{1 + \\exp(-X_i w - w_0)}.$$\n",
    "\n",
    "En tant que probl√®me d'optimisation, la r√©gression logistique binaire avec terme de r√©gularisation $r(w)$ minimise la fonction de co√ªt suivante :\n",
    "\n",
    "$$\\min_{w} C \\sum_{i=1}^n \\left(-y_i \\log(\\hat{p}(X_i)) - (1 - y_i) \\log(1 - \\hat{p}(X_i))\\right) + r(w).$$\n",
    "\n",
    "Nous fournissons actuellement quatre choix pour le terme de r√©gularisation $r(w)$ via l'argument `penalty` :\n",
    "\n",
    "|penalty|$r(w)$|\n",
    "|-|-|\n",
    "|`None`|$0$|\n",
    "|$\\ell_1$|$\\|\\|w\\|\\|_1$|\n",
    "|$\\ell_2$|$\\frac{1}{2}\\|\\|w\\|\\|_2^2 = \\frac{1}{2}w^\\top w$|\n",
    "|`ElasticNet`|$\\frac{1 - \\rho}{2}w^\\top w + \\rho \\|\\|w\\|\\|_1$|\n",
    "\n",
    "Pour ElasticNet, $\\rho$ (qui correspond au param√®tre `l1_ratio`) contr√¥le la force raltive de la r√©gularisation $\\ell_1$ par rapport √† celle de la r√©gularisation $\\ell_2$. Elastic-Net est √©quivalent √† $\\ell_1$ lorsque $\\rho=1$ et √† $\\ell_2$ lorsque $\\rho=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='multinomial-case'></a> 1.1.11.2. Cas multinomial\n",
    "\n",
    "Le cas binaire peut √™tre √©tendu √† $K$ classes, conduisant √† la r√©gression logistique multinomiale, voir aussi [**mod√®le lin√©aire logarithmique**](https://en.wikipedia.org/wiki/Multinomial_logistic_regression#As_a_log-linear_model).\n",
    "\n",
    "> **Remarque :** Il est possible de param√©trer un mod√®le de classification √† $K$ classes en utilisant uniquement $K-1$ vecteurs de poids, laissant une probabilit√© de classe enti√®rement d√©termin√©e par les probabilit√©s des autres classes en tirant parti du fait que la somme de toutes les probabilit√©s de classe vaut $1$. Nous choisissons d√©lib√©r√©ment de sur-param√©trer le mod√®le en utilisant $K$ vecteurs de poids pour faciliter la mise en ≈ìuvre et pr√©server le biais inductif sym√©trique concernant l'ordre des classes. Cet effet devient particuli√®rement important lors de l'utilisation de la r√©gularisation. Le choix du sur-param√©trage peut √™tre pr√©judiciable pour les mod√®les non p√©nalis√©s, car dans ce cas, comme le montre [16], la solution peut ne pas √™tre unique.\n",
    "\n",
    "Soit $y_i \\in {1, \\ldots, K}$ l'√©tiquette (ordinale) encod√©e comme variable cible pour l'observation $i$. Au lieu d'un unique vecteur de coefficient, nous avons √† pr√©sent une matrice de coefficients $W$ o√π chaque vecteur de ligne $W_k$ correspond √† la classe $k$. Nous cherchons √† pr√©dire les probabilit√©s de classe $P(y_i=k|X_i)$ via [**`predict_proba`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) comme suit :\n",
    "\n",
    "$$\\hat{p}_k(X_i) = \\frac{\\exp(X_i W_k + W_{0, k})}{\\sum_{l=0}^{K-1} \\exp(X_i W_l + W_{0, l})}.$$\n",
    "\n",
    "L'objectif de l'optimisation devient\n",
    "\n",
    "$$\\min_W -C \\sum_{i=1}^n \\sum_{k=0}^{K-1} [y_i = k] \\log(\\hat{p}_k(X_i)) + r(W).$$\n",
    "\n",
    "O√π $[P]$ repr√©sente la notation d'Iverson qui √©value √† $0$ si $P$ est faux, sinon √† $1$. Nous fournissons actuellement quatre choix pour le terme de r√©gularisation $r(W)$ via l'argument `penalty` :\n",
    "\n",
    "|penalty|$r(W)$|\n",
    "|-|-|\n",
    "|`None`|$0$|\n",
    "|$\\ell_1$|$\\|\\|W\\|\\|_{1,1} = \\sum_{i=1}^n\\sum_{j=1}^{K}\\|W_{i,j}\\|$|\n",
    "|$\\ell_2$|$\\frac{1}{2}\\|\\|W\\|\\|_F^2 = \\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^{K} W_{i,j}^2$|\n",
    "|`ElasticNet`|$\\frac{1 - \\rho}{2}\\|\\|W\\|\\|_F^2 + \\rho \\|\\|W\\|\\|_{1,1}$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='solvers'></a> 1.1.11.3. Solveurs\n",
    "\n",
    "Les solveurs impl√©ment√©s dans la classe [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) sont \"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\" et \"saga\" :\n",
    "\n",
    "Le solveur \"liblinear\" utilise un algorithme de descente de coordonn√©es (CD) et s'appuie sur l'excellente biblioth√®que C++ [LIBLINEAR](https://www.csie.ntu.edu.tw/~cjlin/liblinear/), qui est incluse avec scikit-learn. Cependant, l'algorithme de CD impl√©ment√© dans liblinear ne peut pas apprendre un mod√®le multinomial (multiclasse) v√©ritable ; √† la place, le probl√®me d'optimisation est d√©compos√© en \"un-contre-tous\" (OCR), de sorte que des classifieurs binaires distincts sont entra√Æn√©s pour toutes les classes. Cela se produit en interne, donc les instances [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) utilisant ce solveur se comportent comme des classifieurs multiclasse. Pour la r√©gularisation $\\ell_1$, [**`sklearn.svm.l1_min_c`**](https://scikit-learn.org/stable/modules/generated/sklearn.svm.l1_min_c.html) permet de calculer la borne inf√©rieure pour `C` afin d'obtenir un mod√®le non \"nul\" (tous les poids des caract√©ristiques √† z√©ro).\n",
    "\n",
    "Les solveurs \"lbfgs\", \"newton-cg\" et \"sag\" prennent uniquement en charge la r√©gularisation $\\ell_2$ ou l'absence de r√©gularisation, et sont plus rapides √† converger pour certaines donn√©es de grande dimension. En d√©finissant `multi_class` sur \"multinomial\" avec ces solveurs, un v√©ritable mod√®le de r√©gression logistique multinomiale est appris [5], ce qui signifie que ses estimations de probabilit√© devraient √™tre mieux calibr√©es que le param√®tre par d√©faut \"un-contre-tous\".\n",
    "\n",
    "Le solveur \"sag\" utilise la descente du gradient moyen stochastique (SAG - _Stochastic Average Gradient_) [6]. Il est plus rapide que d'autres solveurs pour les grands ensembles de donn√©es, lorsque le nombre d'√©chantillons et le nombre de caract√©ristiques sont grands.\n",
    "\n",
    "Le solveur \"saga\" [7] est une variante de \"sag\" qui prend √©galement en charge la p√©nalit√© non lisse `penalty=\"l1\"`. C'est donc le solveur de choix pour la r√©gression logistique multinomiale clairsem√©e. C'est √©galement le seul solveur qui prend en charge `penalty=\"elasticnet\"`.\n",
    "\n",
    "Le solveur \"lbfgs\" est un algorithme d'optimisation qui approxime l'algorithme de Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShanno [8], qui appartient aux m√©thodes quasi-Newton. En tant que tel, il peut traiter une large gamme de donn√©es d'entra√Ænement diff√©rentes et est donc le solveur par d√©faut. Cependant, ses performances sont m√©diocres sur les ensembles de donn√©es mal mis √† l'√©chelle et sur les ensembles de donn√©es avec des caract√©ristiques cat√©gorielles encod√©es en one-hot avec des cat√©gories rares.\n",
    "\n",
    "Le solveur \"newton-cholesky\" est un solveur de Newton exact qui calcule la matrice hessienne et r√©sout le syst√®me lin√©aire r√©sultant. C'est un tr√®s bon choix pour `n_samples` >> `n_features`, mais il pr√©sente quelques lacunes : Seule la r√©gularisation $\\ell_2$ est prise en charge. De plus, comme la matrice hessienne est calcul√©e explicitement, l'utilisation de la m√©moire d√©pend quadratiquement de `n_features` ainsi que de `n_classes`. Par cons√©quent, seul le sch√©ma un-contre-tous (OVR) est impl√©ment√© pour le cas multiclasse.\n",
    "\n",
    "Pour une comparaison de certains de ces solveurs, voir [9].\n",
    "\n",
    "Le tableau suivant r√©sume les p√©nalit√©s prises en charge par chaque solveur :\n",
    "\n",
    "| P√©nalit√©s                    | ‚Äòlbfgs‚Äô | ‚Äòliblinear‚Äô | ‚Äònewton-cg‚Äô | ‚Äònewton-cholesky‚Äô | ‚Äòsag‚Äô | ‚Äòsaga‚Äô |\n",
    "|------------------------------|---------|-------------|-------------|-------------------|-------|--------|\n",
    "| Multinomiale + p√©nalit√© L2     | oui     | non          | oui         | non                | oui   | oui    |\n",
    "| OVR + p√©nalit√© L2             | oui     | oui         | oui         | oui               | oui   | oui    |\n",
    "| Multinomiale + p√©nalit√© L1     | non      | non          | non          | non                | non    | oui    |\n",
    "| OVR + p√©nalit√© L1             | non      | oui         | non          | non                | non    | oui    |\n",
    "| Elastic-Net                  | non      | non          | non          | non                | non    | oui    |\n",
    "| Pas de p√©nalit√© (‚Äònone‚Äô)          | oui     | non          | oui         | oui               | oui   | oui    |\n",
    "| **Comportements**                |         |             |             |                   |       |        |\n",
    "| P√©naliser l'intercept (mauvais) | non      | oui         | non          | non                | non    | non     |\n",
    "| Plus rapide pour les grands ensembles de donn√©es    | non      | non          | non          | non                | oui   | oui    |\n",
    "| Robuste aux ensembles de donn√©es non mis √† l'√©chelle  | oui     | oui         | oui         | oui               | non    | non     |\n",
    "\n",
    "Le solveur \"lbfgs\" est utilis√© par d√©faut pour sa robustesse. Pour les grands ensembles de donn√©es, le solveur \"saga\" est g√©n√©ralement plus rapide. Pour les grands ensembles de donn√©es, vous pouvez √©galement envisager d'utiliser [**`SGDClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) avec `loss=\"log_loss\"`, ce qui pourrait √™tre encore plus rapide mais n√©cessite un r√©glage plus fin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diff√©rences par rapport √† liblinear\n",
    "\n",
    "Il peut y avoir une diff√©rence dans les scores obtenus entre [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) avec `solver=liblinear` ou `LinearSVC` et la biblioth√®que externe liblinear directement, lorsque `fit_intercept=False` et le `coef_` d'ajustement (ou) les donn√©es √† pr√©dire sont nulles. Cela est d√ª au fait que pour l'√©chantillon (ou les √©chantillons) avec une fonction de d√©cision nulle, [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) et `LinearSVC` pr√©disent la classe n√©gative, tandis que liblinear pr√©voit la classe positive. Notez qu'un mod√®le avec `fit_intercept=False` et ayant de nombreux √©chantillons avec une fonction de d√©cision nulle est susceptible d'√™tre un mod√®le mal ajust√© et vous √™tes invit√© √† d√©finir `fit_intercept=True` et √† augmenter `intercept_scaling`.\n",
    "\n",
    "> **Note : S√©lection de caract√©ristiques avec la r√©gression logistique parcimonieuse**\n",
    "> Une r√©gression logistique avec une p√©nalit√© $\\ell_1$ produit des mod√®les parcimonieux et peut donc √™tre utilis√©e pour effectuer une s√©lection de caract√©ristiques, comme d√©taill√© dans [**S√©lection de caract√©ristiques bas√©e sur L1** (1.13.4.1)](https://scikit-learn.org/stable/modules/feature_selection.html#l1-feature-selection).\n",
    "\n",
    "> **Note : Estimation de la valeur p**\n",
    "> Il est possible d'obtenir les valeurs p et les intervalles de confiance pour les coefficients dans les cas de r√©gression sans p√©nalit√©. La biblioth√®que [statsmodels](https://pypi.org/project/statsmodels/) prend en charge cela de mani√®re native. Dans scikit-learn, on pourrait √©galement utiliser la m√©thode de bootstrap.\n",
    "\n",
    "[**`LogisticRegressionCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html) impl√©mente la r√©gression logistique avec une prise en charge int√©gr√©e de la validation crois√©e, pour trouver les param√®tres optimaux `C` et `l1_ratio` en fonction de l'attribut `scoring`. Les solveurs \"newton-cg\", \"sag\", \"saga\" et \"lbfgs\" sont plus rapides pour les donn√©es denses de grande dimension, en raison du d√©marrage √† chaud (voir [**Glossaire/`warm_start`**](https://scikit-learn.org/stable/glossary.html#term-warm_start))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemples\n",
    "\n",
    "##### [**P√©nalit√© $\\ell_1$ et parcimonie en r√©gression logistique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_logistic_l1_l2_sparsity.ipynb)<br/>([$\\ell_1$ _Penalty and Sparsity in Logistic Regression_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_l1_l2_sparsity.html))\n",
    "\n",
    "Comparaison de la parcimonie (pourcentage de coefficients nuls) des solutions lorsque les p√©nalit√©s $\\ell_1$, $\\ell_2$ et Elastic-Net sont utilis√©es pour diff√©rentes valeurs de `C`.\n",
    "\n",
    "##### [**Chemin de r√©gularisation de la r√©gression logistique $\\ell_1$**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_logistic_path.ipynb)<br/>([_Regularization path of $\\ell_1$- Logistic Regression_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_path.html))\n",
    "\n",
    "Mod√®les de r√©gression logistique p√©nalis√©e $\\ell_1$ pour un probl√®me de classification binaire d√©riv√© de l'ensemble de donn√©es Iris.\n",
    "\n",
    "##### [**Tracer la r√©gression logistique multinomiale et un contre tous (OVR)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_logistic_multinomial.ipynb)<br/>([_Plot multinomial and One-vs-Rest Logistic Regression_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_multinomial.html))\n",
    "\n",
    "Trac√© de la surface de d√©cision de la r√©gression logistique multinomiale et un contre tous. Les hyperplans correspondant aux trois classificateurs un contre tous (OvR) sont repr√©sent√©s par les lignes en pointill√©.\n",
    "\n",
    "##### [**R√©gression logistique multiclasse parcimonieuse sur 20newgroups**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_sparse_logistic_regression_20newsgroups.ipynb)<br/>([_Multiclass sparse logistic regression on 20newgroups_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_20newsgroups.html))\n",
    "\n",
    "Comparaison de la r√©gression logistique multinomiale $\\ell_1$ et de la r√©gression logistique $\\ell_1$ un contre tous (OVR) pour classer les documents de l'ensemble de donn√©es newsgroups20.\n",
    "\n",
    "##### [**Classification MNIST en utilisant la r√©gression logistique multinomiale avec $\\ell_1$**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_sparse_logistic_regression_mnist.ipynb)<br/>([_MNIST classification using multinomial logistic +_ $\\ell_1$](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html))\n",
    "\n",
    "Ajustement d'une r√©gression logistique multinomiale avec une p√©nalit√© $\\ell_1$ sur un sous-ensemble de la t√¢che de classification des chiffres MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "üìö [5] Christopher M. Bishop: [**‚ÄúPattern recognition and machine learning‚Äù**](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf), Chapter 4.3.4.\n",
    "\n",
    "üìö [6] Mark Schmidt, Nicolas Le Roux, and Francis Bach: [**‚ÄúMinimizing Finite Sums with the Stochastic Average Gradient‚Äù**](https://inria.hal.science/hal-00860051/document).\n",
    "\n",
    "üî¨ [7] Aaron Defazio, Francis Bach, Simon Lacoste-Julien: [**‚ÄúSAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives‚Äù**](https://arxiv.org/pdf/1407.0202.pdf).\n",
    "\n",
    "üåê [8] Wikipedia entry for [**Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShanno algorithm**](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm)\n",
    "\n",
    "üî¨ [9] Thomas P. Minka [**‚ÄúA comparison of numerical optimizers for logistic regression‚Äù**](https://tminka.github.io/papers/logreg/minka-logreg.pdf).\n",
    "\n",
    "üî¨ [16] (1,2) Simon, Noah, J. Friedman and T. Hastie. [**‚ÄúA Blockwise Descent Algorithm for Group-penalized Multiresponse and Multinomial Regression‚Äù**](https://arxiv.org/pdf/1311.6529.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='generalized-linear-models'></a> 1.1.12. **Mod√®les lin√©aires g√©n√©ralis√©s (GLM)**<br/>([*Generalized Linear Models*](https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='stochastic-gradient-descent-sgd'></a> 1.1.13. **Descente de gradient stochastique (SGD)**<br/>([*Stochastic Gradient Descent - SGD*](https://scikit-learn.org/stable/modules/linear_model.html#stochastic-gradient-descent-sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='perceptron'></a> 1.1.14. **Perceptron**<br/>([*Perceptron*](https://scikit-learn.org/stable/modules/linear_model.html#perceptron))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='passive-aggressive-algorithms'></a> 1.1.15. **Algorithmes passifs agressifs (PAA)**<br/>([*Passive Aggressive Algorithms*](https://scikit-learn.org/stable/modules/linear_model.html#passive-aggressive-algorithms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='robustness-regression-outliers-and-modeling-errors'></a> 1.1.16. **R√©gression robuste : valeurs aberrantes et erreurs de mod√©lisation**<br/>([*Robustness regression: outliers and modeling errors*](https://scikit-learn.org/stable/modules/linear_model.html#robustness-regression-outliers-and-modeling-errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='quantile-regression'></a> 1.1.17. **R√©gression quantile**<br/>([*Quantile Regression*](https://scikit-learn.org/stable/modules/linear_model.html#quantile-regression))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='polynomial-regression-extending-linear-models-with-basis-functions'></a> 1.1.18. **R√©gression polynomiale : extension des mod√®les lin√©aires avec des fonctions de base**<br/>([*Polynomial regression: extending linear models with basis functions*](https://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e03b612d84ba21ce95ed447e81b3062e1eb99b56c6d885cdab4aaa12f1b8e240"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
