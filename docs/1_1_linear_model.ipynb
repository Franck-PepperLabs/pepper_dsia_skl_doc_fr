{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='supervised-learning'></a> 1. [**Apprentissage supervisé**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#supervised-learning)</br>([*Supervised learning*](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning))\n",
    "\n",
    "# 1.1. [**Modèles linéaires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_1_linear_model.ipynb)<br/>([*Linear Models*](https://scikit-learn.org/stable/modules/linear_model.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 33 pages, 30 exemples, 40 papiers\n",
    "- 1.1.1. [**Moindres carrés ordinaires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#ordinary-least-squares)<br/>([_Ordinary Least Squares_](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares))\n",
    "    - 1.1.1.1. [**Moindres carrés non-négatifs**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#non-negative-least-squares)<br/>([*Non-Negative Least Squares*](https://scikit-learn.org/stable/modules/linear_model.html#non-negative-least-squares))\n",
    "    - 1.1.1.2. [**Complexité des Moindres Carrés Ordinaire**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#ordinary-least-squares-complexity)<br/>([*Ordinary Least Squares Complexity*](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares-complexity))\n",
    "- 1.1.2. [**Régression Ridge et classification**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#ridge-regression-and-classification)<br/>([_Ridge regression and classification_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification))\n",
    "    - 1.1.2.1. [**Régression**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#ridge-regression)<br/>([_Regression_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression))\n",
    "    - 1.1.2.2. [**Classification**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#ridge-classification)<br/>([_Classification_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-classification))\n",
    "    - 1.1.2.3. [**Complexité de Ridge**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#ridge-complexity)<br/>([_Ridge Complexity_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-complexity))\n",
    "    - 1.1.2.4. [**Réglage du paramètre de régularisation : Validation croisée \"Leave-One-Out\"**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#setting-the-regularization-parameter-leave-one-out-cross-validation)<br/>([_Setting the regularization parameter: leave-one-out Cross-Validation_](https://scikit-learn.org/stable/modules/linear_model.html#setting-the-regularization-parameter-leave-one-out-cross-validation))\n",
    "- 1.1.3. [**Lasso**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#lasso)<br/>([_Lasso_](https://scikit-learn.org/stable/modules/linear_model.html#lasso))\n",
    "- 1.1.4. [**Lasso multi-tâche**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#multi-task-lasso)<br/>([_Multi-task Lasso_](https://scikit-learn.org/stable/modules/linear_model.html#multi-task-lasso))\n",
    "- 1.1.5. [**Elastic-Net**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#elastic-net)<br/>([_Elastic-Net_](https://scikit-learn.org/stable/modules/linear_model.html#elastic-net))\n",
    "- 1.1.6. [**Elastic-Net multi-tâche**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#multi-task-elastic-net)<br/>([_Multi-task Elastic-Net_](https://scikit-learn.org/stable/modules/linear_model.html#multi-task-elastic-net))\n",
    "- 1.1.7. [**Least Angle Regression**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#least-angle-regression)<br/>([_Least Angle Regression_](https://scikit-learn.org/stable/modules/linear_model.html#least-angle-regression))\n",
    "- 1.1.8. [**LARS Lasso**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#lars-lasso)<br/>([_LARS Lasso_](https://scikit-learn.org/stable/modules/linear_model.html#lars-lasso))\n",
    "- 1.1.9. [**Orthogonal Matching Pursuit (OMP)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#orthogonal-matching-pursuit-omp)<br/>([_Orthogonal Matching Pursuit (OMP)_](https://scikit-learn.org/stable/modules/linear_model.html#orthogonal-matching-pursuit-omp))\n",
    "- 1.1.10. [**Régression bayésienne**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#bayesian-regression)<br/>([_Bayesian Regression_](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-regression))\n",
    "- 1.1.11. [**Régression logistique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#logistic-regression)<br/>([_Logistic regression_](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression))\n",
    "- 1.1.12. [**Modèles linéaires généralisés**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#generalized-linear-models)<br/>([_Generalized Linear Models_](https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-models))\n",
    "- 1.1.13. [**Descente de gradient stochastique - SGD**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#stochastic-gradient-descent-sgd)<br/>([_Stochastic Gradient Descent - SGD_](https://scikit-learn.org/stable/modules/linear_model.html#stochastic-gradient-descent-sgd))\n",
    "- 1.1.14. [**Perceptron**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#perceptron)<br/>([_Perceptron_](https://scikit-learn.org/stable/modules/linear_model.html#perceptron))\n",
    "- 1.1.15. [**Algorithmes passifs agressifs**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#passive-aggressive-algorithms)<br/>([_Passive Aggressive Algorithms_](https://scikit-learn.org/stable/modules/linear_model.html#passive-aggressive-algorithms))\n",
    "- 1.1.16. [**Régression robuste : valeurs aberrantes et erreurs de modélisation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#robustness-regression-outliers-and-modeling-errors)<br/>([_Robustness regression: outliers and modeling errors_](https://scikit-learn.org/stable/modules/linear_model.html#robustness-regression-outliers-and-modeling-errors))\n",
    "- 1.1.17. [**Régression quantile**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#quantile-regression)<br/>([_Quantile Regression_](https://scikit-learn.org/stable/modules/linear_model.html#quantile-regression))\n",
    "- 1.1.18. [**Régression polynomiale : extension des modèles linéaires avec des fonctions de base**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#polynomial-regression-extending-linear-models-with-basis-functions)<br/>([_Polynomial regression: extending linear models with basis functions_](https://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='linear-models'></a> 1.1. **Modèles linéaires**<br/>([*Linear Models*](https://scikit-learn.org/stable/modules/linear_model.html))\n",
    "\n",
    "Ce chapitre présente un ensemble de méthodes conçues pour la régression, dans lesquelles la valeur cible est supposée être une combinaison linéaire des caractéristiques. En notation mathématique, si $\\hat{y}$ est la valeur prédite :\n",
    "\n",
    "$$\\hat{y}(w, x) = w_0 + w_1 x_1 + \\ldots + w_p x_p = w_0 + x^\\top w$$\n",
    "\n",
    "Dans l'ensemble du module, nous désignons le vecteur $w = (w_1,\\ldots, w_p)$ comme `coef_` et $w_0$ comme `intercept_`.\n",
    "\n",
    "Pour effectuer une classification avec des modèles linéaires généralisés, consulter la section [**Régression logistique** (1.1.11)](#logistic-regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ordinary-least-squares'></a> 1.1.1. **Moindres carrés ordinaires**<br/>([*Ordinary Least Squares*](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares))\n",
    "\n",
    "[**`LinearRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) ajuste un modèle linéaire avec des coefficients $w = (w_1,..., w_p)$ pour minimiser la somme résiduelle des carrés entre les cibles observées dans l'ensemble de données et les cibles prédites par l'approximation linéaire. Mathématiquement, il résout un problème de la forme :\n",
    "\n",
    "$$\\displaystyle\\min_{w} || X w - y||_2^2$$\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_ols_001.png\"\n",
    "    alt=\"Moindres carrés ordinaires\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "[**`LinearRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) prend les tableaux $X$ et $y$ dans sa méthode `fit` d'ajustement et stocke les coefficients $w$ du modèle linéaire dans son attribut `coef_` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coef_      : [0.5 0.5]\n",
      "intercept_ : 1.1102230246251565e-16\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n",
    "# > LinearRegression()\n",
    "print('coef_      :', reg.coef_)\n",
    "# > array([0.5, 0.5])\n",
    "print('intercept_ :', reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les estimations de coefficients pour les moindres carrés ordinaires (**OLS**) dépendent de l'indépendance des caractéristiques. Lorsque les caractéristiques sont corrélées et que les colonnes de la **matrice de conception** $X$ ont une dépendance approximativement linéaire, la matrice de conception devient proche du **singularité** et, par conséquent, l'estimation des moindres carrés devient très sensible aux erreurs aléatoires dans la cible observée, produisant une grande **variance**. Cette situation de **multicolinéarité** peut survenir, par exemple, lorsque des données sont collectées sans **plan expérimental**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**Exemple de régression linéaire**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_ols.ipynb)<br/>([*Linear Regression Example*](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='non-negative-least-squares'></a> 1.1.1.1. **Moindres carrés non-négatifs**<br/>([*Non-Negative Least Squares*](https://scikit-learn.org/stable/modules/linear_model.html#non-negative-least-squares))\n",
    "\n",
    "Il est possible de contraindre tous les coefficients à être non négatifs, ce qui peut être utile lorsqu'ils représentent certaines quantités physiques ou des valeurs naturellement non négatives (par exemple, les comptages de fréquence ou les prix des biens). La classe [**`LinearRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) accepte un paramètre booléen `positive` : lorsque ce paramètre est défini sur `True`, la méthode des moindres carrés non-négatifs (**NNLS**) est appliquée.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Moindres carrés non-négatifs**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_nnls.ipynb)<br/>([*Non-negative least squares*](https://scikit-learn.org/stable/auto_examples/linear_model/plot_nnls.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='ordinary-least-squares-complexity'></a> 1.1.1.2. **Complexité des moindres carrés ordinaires**<br/>([*Ordinary Least Squares Complexity*](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares-complexity))\n",
    "\n",
    "La solution des moindres carrés est calculée à l'aide de la décomposition en valeurs singulières de la matrice $X$. Si $X$ est une matrice de forme `(n_samples, n_features)`, cette méthode a un coût de $\\mathcal{O}(n_{\\text{samples}} n_{\\text{features}}^2)$, en supposant que $n_{\\text{samples}} \\geq n_{\\text{features}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ridge-regression-and-classification'></a> 1.1.2. **Régression Ridge et classification**<br/>([_Ridge regression and classification_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification))\n",
    "\n",
    "### <a id='ridge-regression'></a> 1.1.2.1. **Régression**<br/>([_Regression_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression))\n",
    "\n",
    "La régression [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) répond à certains des problèmes des moindres carrés ordinaires en imposant une pénalité sur la taille des coefficients. Les coefficients Ridge minimisent une somme résiduelle des carrés pénalisée :\n",
    "\n",
    "$$\\min_{w} || X w - y||_2^2 + \\alpha ||w||_2^2$$\n",
    "\n",
    "Le paramètre de complexité $\\alpha \\geq 0$ contrôle le niveau de rétrécissement : plus la valeur de $\\alpha$ est grande, plus le niveau de rétrécissement est élevé, et ainsi les coefficients deviennent plus robustes à la colinéarité.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_ridge_path_001.png\"\n",
    "    alt=\"Coefficients Ridge comme une fonction de la régularisation\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Comme pour d'autres modèles linéaires, [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) prend les tableaux $X, y$ dans sa méthode `fit` et stocke les coefficients $w$ du modèle linéaire dans son attribut `coef_` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34545455, 0.34545455])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.13636363636363638"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Ridge(alpha=.5)\n",
    "reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\n",
    "display(reg.coef_)\n",
    "display(reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classe [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) permet à l'utilisateur de spécifier que le solveur soit automatiquement choisi en définissant `solver=\"auto\"`. Lorsque cette option est spécifiée, [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) choisira entre les solveurs `\"lbfgs\"`, `\"cholesky\"` et `\"sparse_cg\"`. [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) commencera à vérifier les conditions présentées dans le tableau suivant de haut en bas. Si la condition est vraie, le solveur correspondant est choisi.\n",
    "\n",
    "| Solveur | Condition |\n",
    "|---------|-----------|\n",
    "| 'lbfgs' | L'option `positive=True` est spécifiée. |\n",
    "| 'cholesky' | Le tableau d'entrée X n'est pas clairsemé. |\n",
    "| 'sparse_cg' | Aucune des conditions ci-dessus n'est remplie. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='ridge-classification'></a> 1.1.2.2. **Classification**<br/>([_Classification_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-classification))\n",
    "\n",
    "Le régresseur [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) a une variante de classification : [**`RidgeClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html). Ce classifieur convertit d'abord les cibles binaires en `{-1, 1}`, puis traite le problème comme une tâche de régression, optimisant le même objectif que précédemment. La classe prédite correspond au signe de la prédiction du régresseur. Pour la classification multiclasse, le problème est traité comme une régression multi-sortie et la classe prédite correspond à la sortie avec la valeur la plus élevée.\n",
    "\n",
    "Il peut sembler discutable d'utiliser une perte des moindres carrés (pénalisée) pour ajuster un modèle de classification au lieu des pertes plus traditionnelles comme logistiques ou Ridge. Cependant, en pratique, tous ces modèles peuvent conduire à des scores de validation croisée similaires en termes d'exactitude ou de précision/rappel. La perte des moindres carrés pénalisée utilisée par le [**`RidgeClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html) permet un choix varié de solveurs numériques avec des performances de calcul hétérogènes.\n",
    "\n",
    "Le [**`RidgeClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html) peut être beaucoup plus rapide que la [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), par exemple, avec un grand nombre de classes, car il peut calculer la **matrice de projection** $(X^\\top X)^{-1} X^\\top$ une seule fois.\n",
    "\n",
    "Ce classifieur est parfois appelé [**machines à vecteurs de support à moindres carrés**](https://en.wikipedia.org/wiki/Least-squares_support-vector_machine) avec un noyau linéaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemples\n",
    "\n",
    "##### [**Affichage des coefficients Ridge comme fonction de la régularisation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_ridge_path.ipynb)<br/>([*Plot Ridge coefficients as a function of the regularization*](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html))\n",
    "\n",
    "Montre l'effet de la colinéarité sur les coefficients d'un estimateur.\n",
    "\n",
    "##### [**Classification de documents textes à l'aide de caractéristiques creuses**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_document_classification_20newsgroups.ipynb)<br/>([*Classification of text documents using sparse features*](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html))\n",
    "\n",
    "\n",
    "##### [**Pièges courants dans l'interprétation des coefficients des modèles linéaires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/4_inspection/plot_linear_model_coefficient_interpretation.ipynb)<br/>([*Common pitfalls in the interpretation of coefficients of linear models*](https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='ridge-complexity'></a> 1.1.2.3. **Complexité de Ridge**<br/>([_Ridge Complexity_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-complexity))\n",
    "\n",
    "Cette méthode a le même ordre de complexité que les moindres carrés ordinaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='setting-the-regularization-parameter-leave-one-out-cross-validation'></a> 1.1.2.4. **Réglage du paramètre de régularisation : Validation croisée \"Leave-One-Out\"**<br/>([_Setting the regularization parameter: leave-one-out Cross-Validation_](https://scikit-learn.org/stable/modules/linear_model.html#setting-the-regularization-parameter-leave-one-out-cross-validation))\n",
    "\n",
    "[**`RidgeCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) implémente la régression Ridge avec une validation croisée intégrée du paramètre `alpha`. Cet objet fonctionne de la même manière que [**`GridSearchCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), mais en utilisant par défaut la validation croisée \"Leave-One-Out\" :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))\n",
    "reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\n",
    "# RidgeCV(alphas=array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01,\n",
    "#      1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06]))\n",
    "reg.alpha_\n",
    "# 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'attribution de la valeur de l'attribut [`cv`](https://scikit-learn.org/stable/glossary.html#term-cv) déclenche l'utilisation de la validation croisée avec [**`GridSearchCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Par exemple, `cv=10` pour une validation croisée à 10 plis au lieu de la validation croisée \"Leave-One-Out\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Références\n",
    "\n",
    "“**Notes on Regularized Least Squares**”, Rifkin & Lippert ([_rapport technique_](http://cbcl.mit.edu/publications/ps/MIT-CSAIL-TR-2007-025.pdf), [_diapositives de cours_](https://www.mit.edu/~9.520/spring07/Classes/rlsslides.pdf))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='lasso'></a> 1.1.3. **Lasso**<br/>([*Lasso*](https://scikit-learn.org/stable/modules/linear_model.html#lasso))\n",
    "\n",
    "Le Lasso est un modèle linéaire qui estime des coefficients parcimonieux. Il est utile dans certains contextes en raison de sa tendance à préférer les solutions avec moins de coefficients non nuls, réduisant ainsi le nombre de caractéristiques dont dépend la solution donnée. Pour cette raison, le Lasso et ses variantes sont fondamentaux dans le domaine de la détection compressée. Sous certaines conditions, il peut récupérer l'ensemble exact des coefficients non nuls (voir l'exemple [**Détection compressive : reconstruction tomographique avec L1 préalable (Lasso)**](https://scikit-learn.org/stable/auto_examples/applications/plot_tomography_l1_reconstruction.html#sphx-glr-auto-examples-applications-plot-tomography-l1-reconstruction-py)).\n",
    "\n",
    "Mathématiquement, il s'agit d'un modèle linéaire avec un terme de régularisation ajouté. La fonction objectif à minimiser est :\n",
    "\n",
    "$\\displaystyle\\min_{w} { \\frac{1}{2n_{\\text{samples}}} ||X w - y||_2 ^ 2 + \\alpha ||w||_1}$\n",
    "\n",
    "L'estimation du lasso résout donc la minimisation de la pénalité des moindres carrés avec ajout de $\\alpha ||w||_1$, où $\\alpha$ est une constante et $||w||_1$ est la $\\ell_1$-norme du vecteur coefficient.\n",
    "\n",
    "L'implémentation dans la classe [**`Lasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) utilise la descente de coordonnées comme algorithme pour ajuster les coefficients. Voir [**Régression du moindre angle** (1.1.7)](https://scikit-learn.org/stable/modules/linear_model.html#least-angle-regression) pour une implémentation alternative :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Lasso(alpha=0.1)\n",
    "reg.fit([[0, 0], [1, 1]], [0, 1])\n",
    "# Lasso(alpha=0.1)\n",
    "reg.predict([[1, 1]])\n",
    "# array([0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction [**`lasso_path`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.lasso_path.html#sklearn.linear_model.lasso_path) est utile pour les tâches de niveau inférieur, car elle calcule les coefficients le long du chemin complet des valeurs possibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemples\n",
    "\n",
    "##### [**Modèles basés sur $\\ell_1$ pour signaux creux**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_lasso_and_elasticnet.ipynb)<br/>([_L1-based models for Sparse Signals_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_and_elasticnet.html))\n",
    "\n",
    "##### [**Détection compressive : reconstruction tomographique avec $\\ell_1$ préalable (Lasso)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/applications/plot_tomography_l1_reconstruction.ipynb)<br/>([_Compressive sensing: tomography reconstruction with $\\ell_1$ prior (Lasso_](https://scikit-learn.org/stable/auto_examples/applications/plot_tomography_l1_reconstruction.html))\n",
    "\n",
    "##### [**Pièges courants dans l'interprétation des coefficients des modèles linéaires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/4_inspection/plot_linear_model_coefficient_interpretation.ipynb)<br/>([*Common pitfalls in the interpretation of coefficients of linear models*](https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note : Sélection de caractéristiques avec Lasso\n",
    "\n",
    "> Comme la régression Lasso produit des modèles parcimonieux, elle peut donc être utilisée pour effectuer une _sélection de caractéristiques_, comme détaillé dans la [**Sélection de caractéristiques basée sur $\\ell_1$** (1.13.4.1)](https://scikit-learn.org/stable/modules/feature_selection.html#l1-feature-selection).\n",
    "\n",
    "Les deux références suivantes expliquent les itérations utilisées dans le solveur de descente de coordonnées de scikit-learn, ainsi que le calcul de l'écart de dualité utilisé pour le contrôle de la convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Références\n",
    "\n",
    "🔬 [**“Regularization Path For Generalized linear Models by Coordinate Descent”**](https://www.jstatsoft.org/article/view/v033i01/v33i01.pdf), Friedman, Hastie & Tibshirani, J Stat Softw, 2010.\n",
    "\n",
    "🔬 [**“An Interior-Point Method for Large-Scale $\\ell_1$-Regularized Least Squares”**](https://web.stanford.edu/~boyd/papers/pdf/l1_ls.pdf), S. J. Kim, K. Koh, M. Lustig, S. Boyd and D. Gorinevsky, in IEEE Journal of Selected Topics in Signal Processing, 2007."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='setting-regularization-parameter'></a> 1.1.3.1. **Définition du paramètre de régularisation**<br/>([_Setting regularization parameter_](https://scikit-learn.org/stable/modules/linear_model.html#setting-regularization-parameter))\n",
    "\n",
    "Le paramètre `alpha` contrôle le degré de parcimonie des coefficients estimés.\n",
    "\n",
    "#### Utilisation de la validation croisée\n",
    "\n",
    "scikit-learn propose des objets qui définissent le paramètre `alpha` du Lasso par validation croisée : [**`LassoCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html) et [**`LassoLarsCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsCV.html). [**`LassoLarsCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsCV.html) est basé sur l'algorithme [**Least Angle Regression** (1.1.7)](https://scikit-learn.org/stable/modules/linear_model.html#least-angle-regression) expliqué ci-dessous.\n",
    "\n",
    "Pour les ensembles de données de grande dimension avec de nombreuses caractéristiques colinéaires, [**`LassoCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html) est généralement préférable. Cependant, [**`LassoLarsCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsCV.html) a l'avantage d'explorer des valeurs du paramètre `alpha` plus pertinentes, et si le nombre d'échantillons est très faible par rapport au nombre de caractéristiques, il est souvent plus rapide que [**`LassoCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html).\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lasso_model_selection_002.png\"\n",
    "    alt=\"Erreur quadratique moyenne pour chaque pli : descente de coordonnées\"\n",
    "    style=\"max-width: 40%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lasso_model_selection_003.png\"\n",
    "    alt=\"Erreur quadratique moyenne pour chaque pli: Lars\"\n",
    "    style=\"max-width: 40%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "#### Sélection de modèle basée sur des critères d'information\n",
    "\n",
    "Comme alterative, l'estimateur [**`LassoLarsIC`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsIC.html) propose d'utiliser le critère d'information d'Akaike (AIC) et le critère d'information de Bayes (BIC). C'est une alternative moins coûteuse pour déterminer la valeur optimale de `alpha`, car le chemin de régularisation n'est calculé qu'une seule fois au lieu, contre $k$ + 1 fois avec la validation croisée $k$-fold.\n",
    "\n",
    "En effet, ces critères sont calculés sur l'ensemble d'apprentissage. En bref, ils pénalisent les scores trop optimistes des différents modèles Lasso par leur flexibilité (voir section « Détails mathématiques » ci-dessous).\n",
    "\n",
    "Cependant, ces critères nécessitent une estimation précise des degrés de liberté de la solution. Ils sont dérivés pour de grands échantillons (résultats asymptotiques) et supposent que le modèle correct est inclus dans les candidats à l'étude. Ils ont également tendance à dysfonctionner lorsque le problème est mal conditionné (par exemple, plus de caractéristiques que d'échantillons).\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lasso_lars_ic_001.png\"\n",
    "    alt=\"Sélection de modèle Lasso via AIC et BIC\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "##### Détails mathématiques\n",
    "\n",
    "La définition de l'AIC (et donc du BIC) peut différer dans la littérature. Dans cette section, nous donnons plus d'informations sur le critère tel que calculé dans scikit-learn. Le critère AIC est défini comme suit :\n",
    "\n",
    "$$AIC = -2 \\log(\\hat{L}) + 2 d$$\n",
    "\n",
    "où $\\hat{L}$ est la vraisemblance maximale du modèle et $d$ est le nombre de paramètres (également appelés _degrés de liberté_ dans la section précédente).\n",
    "\n",
    "La définition de BIC remplace la constante 2 par $\\log(N)$ :\n",
    "\n",
    "$$BIC = -2 \\log(\\hat{L}) + \\log(N) d$$\n",
    "\n",
    "où $N$ est le nombre d'échantillons.\n",
    "\n",
    "Pour un modèle gaussien linéaire, la log-vraisemblance maximale est définie comme suit :\n",
    "\n",
    "$$\\log(\\hat{L}) = - \\frac{n}{2} \\log(2 \\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{2\\sigma^2}$$\n",
    "\n",
    "où $\\sigma^2$ est une estimation de la variance du bruit, $y_i$ et $\\hat{y}_i$ sont respectivement les cibles réelles et prédites, et $n$ est le nombre d'échantillons.\n",
    "\n",
    "En insérant la log-vraisemblance maximale dans la formule AIC, on obtient :\n",
    "\n",
    "$$AIC = n \\log(2 \\pi \\sigma^2) + \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sigma^2} + 2 d$$\n",
    "\n",
    "Le premier terme de l'expression ci-dessus est parfois ignoré car il s'agit d'une constante lorsque $\\sigma^2$ est fourni. De plus, il est parfois affirmé que l'AIC est équivalent à la statistique $C_p$ [12]. Cependant, au sens strict, il n'est équivalent qu'à une constante et à un facteur multiplicatif près.\n",
    "\n",
    "Enfin, nous avons mentionné plus haut que $\\sigma^2$ est une estimation de la variance du bruit. Dans [**`LassoLarsIC`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsIC.html) lorsque le paramètre `noise_variance` n'est pas fourni (le comportement par défaut), la variance du bruit est estimée à l'aide de l'estimateur non biaisé [13] défini comme suit :\n",
    "\n",
    "$$\\sigma^2 = \\displaystyle\\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{n - p}$$\n",
    "\n",
    "où $p$ est le nombre de caractéristiques et $\\hat{y}_i$ est la cible prédite à l'aide d'une régression des moindres carrés ordinaires. Notez que cette formule n'est valide que lorsque `n_samples > n_features`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Références\n",
    "\n",
    "🔬 [12] Zou, Hui, Trevor Hastie, and Robert Tibshirani. [**“On the degrees of freedom of the lasso”**](https://arxiv.org/abs/0712.0881.pdf). The Annals of Statistics 35.5 (2007): 2173-2192.\n",
    "\n",
    "🔬 [13] Cherkassky, Vladimir, and Yunqian Ma. [**“Comparison of model selection for regression”**](http://www.ece.umn.edu/users/cherkass/comparison_paper_Mar05.pdf). Neural computation 15.7 (2003): 1691-1714."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparaison avec le paramètre de régularisation de SVM\n",
    "\n",
    "L'équivalence entre `alpha` et le paramètre de régularisation de SVM, `C`, est donnée par `alpha = 1 / C` ou `alpha = 1 / (n_samples * C)`, en fonction de l'estimateur et de la fonction objectif exacte optimisée par le modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='multi-task-lasso'></a> 1.1.4. **Lasso multi-tâche**<br/>([*Multi-task Lasso*](https://scikit-learn.org/stable/modules/linear_model.html#multi-task-lasso))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='elastic-net'></a> 1.1.5. **Elastic-Net**<br/>([*Elastic-Net*](https://scikit-learn.org/stable/modules/linear_model.html#elastic-net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='multi-task-elastic-net'></a> 1.1.6. **Elastic-Net multi-tâche**<br/>([*Multi-task Elastic-Net*](https://scikit-learn.org/stable/modules/linear_model.html#multi-task-elastic-net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='least-angle-regression'></a> 1.1.7. **Least Angle Regression**<br/>([*Least Angle Regression*](https://scikit-learn.org/stable/modules/linear_model.html#least-angle-regression))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='lars-lasso'></a> 1.1.8. **LARS Lasso**<br/>([*LARS Lasso*](https://scikit-learn.org/stable/modules/linear_model.html#lars-lasso))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='orthogonal-matching-pursuit-omp'></a> 1.1.9. **Orthogonal Matching Pursuit (OMP)**<br/>([*Orthogonal Matching Pursuit (OMP)*](https://scikit-learn.org/stable/modules/linear_model.html#orthogonal-matching-pursuit-omp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='bayesian-regression'></a> 1.1.10. **Régression bayésienne**<br/>([*Bayesian Regression*](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-regression))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='logistic-regression'></a> 1.1.11. **Régression logistique**<br/>([*Logistic regression*](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='generalized-linear-models'></a> 1.1.12. **Modèles linéaires généralisés**<br/>([*Generalized Linear Models*](https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='stochastic-gradient-descent-sgd'></a> 1.1.13. **Descente de gradient stochastique - SGD**<br/>([*Stochastic Gradient Descent - SGD*](https://scikit-learn.org/stable/modules/linear_model.html#stochastic-gradient-descent-sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='perceptron'></a> 1.1.14. **Perceptron**<br/>([*Perceptron*](https://scikit-learn.org/stable/modules/linear_model.html#perceptron))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='passive-aggressive-algorithms'></a> 1.1.15. **Algorithmes passifs agressifs**<br/>([*Passive Aggressive Algorithms*](https://scikit-learn.org/stable/modules/linear_model.html#passive-aggressive-algorithms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='robustness-regression-outliers-and-modeling-errors'></a> 1.1.16. **Régression robuste : valeurs aberrantes et erreurs de modélisation**<br/>([*Robustness regression: outliers and modeling errors*](https://scikit-learn.org/stable/modules/linear_model.html#robustness-regression-outliers-and-modeling-errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='quantile-regression'></a> 1.1.17. **Régression quantile**<br/>([*Quantile Regression*](https://scikit-learn.org/stable/modules/linear_model.html#quantile-regression))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='polynomial-regression-extending-linear-models-with-basis-functions'></a> 1.1.18. **Régression polynomiale : extension des modèles linéaires avec des fonctions de base**<br/>([*Polynomial regression: extending linear models with basis functions*](https://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e03b612d84ba21ce95ed447e81b3062e1eb99b56c6d885cdab4aaa12f1b8e240"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
