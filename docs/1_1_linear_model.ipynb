{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='linear-models'></a> 1.1. [**Modèles linéaires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_1_linear_model.ipynb)<br/>([*Linear Models*](https://scikit-learn.org/stable/modules/linear_model.html))\n",
    "\n",
    "Ce chapitre présente un ensemble de méthodes destinées à la régression dans lesquelles la valeur cible est supposée être une combinaison linéaire des caractéristiques. En notation mathématique, si $\\hat{y}$ est la valeur prédite :\n",
    "\n",
    "$$\\hat{y}(w, x) = w_0 + w_1 x_1 + ... + w_p x_p = w_0 + x^\\top w$$\n",
    "\n",
    "Dans tout le module, nous désignons le vecteur $w = (w_1,..., w_p)$ par `coef_` et $w_0$ par `intercept_`.\n",
    "\n",
    "Pour effectuer une classification avec des modèles linéaires généralisés, voir [**Régression logistique** (1.1.11)](#logistic-regression).\n",
    "\n",
    "✔ 1.1.1. [**Moindres carrés ordinaires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_1_linear_model.ipynb#ordinary-least-squares)<br/>([*Ordinary Least Squares*](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='ordinary-least-squares'></a> 1.1.1. Moindres carrés ordinaires<br/>([*Ordinary Least Squares*](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares))\n",
    "\n",
    "[`LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) ajuste un modèle linéaire avec des coefficients $w = (w_1,..., w_p)$ pour minimiser la somme résiduelle des carrés entre les cibles observées dans l'ensemble de données et les cibles prédites par l'approximation linéaire. Mathématiquement, il résout un problème de la forme :\n",
    "\n",
    "$$\\displaystyle\\min_{w} || X w - y||_2^2$$\n",
    "\n",
    "<p style=\"text-align: center\"><img alt=\"sphx_glr_plot_ols_001.png\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_ols_001.png\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_ols_001.png\" style=\"width: 320.0px; height: 240.0px\" /></a></p>\n",
    "\n",
    "[`LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) prend les tableaux $X$ et $y$ dans sa méthode `fit` d'ajustement et stocke les coefficients $w$ du modèle linéaire dans son membre `coef_` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coef_      : [0.5 0.5]\n",
      "intercept_ : 2.220446049250313e-16\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2]) # > LinearRegression()\n",
    "print('coef_      :', reg.coef_) # > array([0.5, 0.5])\n",
    "print('intercept_ :', reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les estimations de coefficient pour les moindres carrés ordinaires (**OLS**) reposent sur l'indépendance des caractéristiques. Lorsque les caractéristiques sont corrélées et que les colonnes de la **matrice de conception** ont une dépendance approximativement linéaire, la matrice de conception devient proche du **singulier** et, par conséquent, l'estimation des moindres carrés devient très sensible aux erreurs aléatoires dans la cible observée, produisant une grande **variance**. Cette situation de **multicolinéarité** peut survenir, par exemple, lorsque des données sont collectées sans **plan expérimental**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**Exemple de régression linéaire**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_ols.ipynb)<br/>([*Linear Regression Example*](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='non-negative-least-squares'></a> 1.1.1.1. Moindres Carrés Non-Négatif<br/>([*Non-Negative Least Squares*](https://scikit-learn.org/stable/modules/linear_model.html#non-negative-least-squares))\n",
    "\n",
    "Il est possible de contraindre tous les coefficients à être non négatifs, ce qui peut être utile lorsqu'ils représentent certaines quantités physiques ou naturellement non négatives (par exemple, les comptages de fréquence ou les prix des biens). [`LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) accepte un paramètre booléen `positive` : lorsqu'il est défini `True` la méthode des moindres carrés non-négative (**NNLS**) est alors appliquée.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Moindres Carrés Non-Négatif**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_nnls.ipynb)<br/>([*Non-negative least squares*](https://scikit-learn.org/stable/auto_examples/linear_model/plot_nnls.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ordinary-least-squares-complexity'></a> 1.1.1.2. Complexité des Moindres Carrés Ordinaire<br/>([*Ordinary Least Squares Complexity*](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares-complexity))\n",
    "\n",
    "La solution des moindres carrés est calculée à l'aide de la décomposition en valeurs singulières de X. Si X est une matrice de forme `(n_samples, n_features)`, cette méthode a un coût de $O(n p^2)$, en admettant que $n \\geq p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='ridge-regression-and-classification'></a> 1.1.2. Régression et classification de crête<br/>([*Ridge regression and classification*](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification))\n",
    "\n",
    "## <a id='regression'></a> 1.1.2.1. Régression\n",
    "\n",
    "La régression de crête [`Ridge`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) résout certains des problèmes des moindres carrés ordinaires en imposant une pénalité à la taille des coefficients. Les coefficients d'arête minimisent une somme résiduelle des carrés pénalisée :\n",
    "\n",
    "$$\\min_{w} || X w - y||_2^2 + \\alpha ||w||_2^2$$\n",
    "\n",
    "Le paramètre de complexité $\\alpha \\geq 0$ contrôle le niveau de rétrécissement : plus la valeur de $\\alpha$ est importante, plus le niveau de rétrécissement est important, et donc les coefficients deviennent plus robustes à la colinéarité.\n",
    "\n",
    "<p style=\"text-align: center\"><img alt=\"sphx_glr_plot_ridge_path_001.png\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_ridge_path_001.png\" style=\"width: 320.0px; height: 240.0px;\" /></a>\n",
    "\n",
    "Comme avec les autres modèles linéaires, `Ridge` prendra dans sa méthode `fit` les tableaux $X, y$ et stockera les coefficients $w$ du modèle linéaire dans son membre `coef_` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34545455, 0.34545455])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.1363636363636364"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Ridge(alpha=.5)\n",
    "reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\n",
    "display(reg.coef_)\n",
    "display(reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='classification'></a> 1.1.2.2. Classification\n",
    "\n",
    "Le régresseur [`Ridge`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) a une variante classifieur : [`RidgeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html#sklearn.linear_model.RidgeClassifier). Ce classifieur convertit d'abord les cibles binaires en `{-1, 1}`, puis traite le problème comme une tâche de régression, optimisant le même objectif que ci-dessus. La classe prédite correspond au signe de la prédiction du régresseur. Pour la classification multiclasse, le problème est traité comme une régression multi-sortie et la classe prédite correspond à la sortie avec la valeur la plus élevée.\n",
    "\n",
    "Il peut sembler discutable d'utiliser une perte des moindres carrés (pénalisée) pour ajuster un modèle de classification au lieu de pertes logistiques ou de crête plus traditionnelles. Cependant, en pratique, tous ces modèles peuvent conduire à des scores de validation croisée similaires en termes d'exactitude ou de précision/rappel, tandis que la perte des moindres carrés pénalisée utilisée par le [`RidgeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html#sklearn.linear_model.RidgeClassifier) permet un choix très différent des solveurs numériques avec des profils de performances de calcul distincts.\n",
    "\n",
    "Le [`RidgeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html#sklearn.linear_model.RidgeClassifier) peut être beaucoup plus rapide que par exemple la [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) avec un grand nombre de classes car il peut calculer la **matrice de projection** $(X^\\top X)^{-1} X^\\top$ juste une fois.\n",
    "\n",
    "Ce classifieur est parfois appelé [wkp:**machines à vecteurs de support à moindres carrés**](https://en.wikipedia.org/wiki/Least-squares_support-vector_machine) avec noyau linéaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ridge-complexity'></a> 1.1.2.3. Complexité du \"Ridge\"\n",
    "\n",
    "Cette méthode a le même ordre de complexité que les moindres carrés ordinaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='setting-the-regularization-parameter-leave-one-out-cross-validation'></a> 1.1.2.4. Définition du paramètre de régularisation : leave-one-out Cross-Validation\n",
    "\n",
    "[RidgeCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV) implémente la régression de crête avec validation croisée intégrée du paramètre alpha. L'objet fonctionne de la même manière que GridSearchCV sauf qu'il est par défaut à Leave-One-Out Cross-Validation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))\n",
    "reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\n",
    "# RidgeCV(alphas=array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01,\n",
    "#      1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06]))\n",
    "reg.alpha_\n",
    "# 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La spécification de la valeur de l'attribut [`cv`](https://scikit-learn.org/stable/glossary.html#term-cv) déclenchera l'utilisation de la validation croisée avec [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV), par exemple `cv=10` pour une validation croisée 10 fois, plutôt que la validation croisée Leave-One-Out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemples\n",
    "\n",
    "### [**Affichage des coefficients de crête comme fonction de la régularisation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_ridge_path.ipynb)<br/>([*Plot Ridge coefficients as a function of the regularization*](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html))\n",
    "\n",
    "Montre l'effet de la colinéarité sur les coefficients d'un estimateur.\n",
    "\n",
    "### [**Classification de documents textes à l'aide de caractéristiques parcimonieuses**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_document_classification_20newsgroups.ipynb)<br/>([*Classification of text documents using sparse features*](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html))\n",
    "\n",
    "\n",
    "### [**Pièges courants dans l'interprétation des coefficients des modèles linéaires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/4_inspection/plot_linear_model_coefficient_interpretation.ipynb)<br/>([*Common pitfalls in the interpretation of coefficients of linear models*](https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Références\n",
    "\n",
    "“**Notes on Regularized Least Squares**”, Rifkin & Lippert [(](https://drive.google.com/file/d/14EuBuRok9fMNfXPAExnb8-r2xBbkYU5K/view?usp=share_link)[*rapport technique*](http://cbcl.mit.edu/publications/ps/MIT-CSAIL-TR-2007-025.pdf), [*diapositives de cours*](https://www.mit.edu/~9.520/spring07/Classes/rlsslides.pdf)[)](https://drive.google.com/file/d/1EEnlzb0jI5yxebwN7atU9QnbVQwtYh0_/view?usp=share_link)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e03b612d84ba21ce95ed447e81b3062e1eb99b56c6d885cdab4aaa12f1b8e240"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
