{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='supervised-learning'></a> 1. [**Apprentissage supervisé**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#supervised-learning)</br>([*Supervised learning*](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning))\n",
    "\n",
    "# 1.1. [**Modèles linéaires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_1_linear_model.ipynb)<br/>([*Linear Models*](https://scikit-learn.org/stable/modules/linear_model.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 33 pages, 30 exemples, 40 papiers\n",
    "- 1.1.1. [**Moindres carrés ordinaires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#ordinary-least-squares)<br/>([_Ordinary Least Squares_](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares))\n",
    "    - 1.1.1.1. [**Moindres carrés non-négatifs**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#non-negative-least-squares)<br/>([*Non-Negative Least Squares*](https://scikit-learn.org/stable/modules/linear_model.html#non-negative-least-squares))\n",
    "    - 1.1.1.2. [**Complexité des Moindres Carrés Ordinaire**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#ordinary-least-squares-complexity)<br/>([*Ordinary Least Squares Complexity*](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares-complexity))\n",
    "- 1.1.2. [**Régression Ridge et classification**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#ridge-regression-and-classification)<br/>([_Ridge regression and classification_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification))\n",
    "    - 1.1.2.1. [**Régression**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#ridge-regression)<br/>([_Regression_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression))\n",
    "    - 1.1.2.2. [**Classification**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#ridge-classification)<br/>([_Classification_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-classification))\n",
    "    - 1.1.2.3. [**Complexité de Ridge**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#ridge-complexity)<br/>([_Ridge Complexity_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-complexity))\n",
    "    - 1.1.2.4. [**Réglage du paramètre de régularisation : Validation croisée \"Leave-One-Out\"**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#setting-the-regularization-parameter-leave-one-out-cross-validation)<br/>([_Setting the regularization parameter: leave-one-out Cross-Validation_](https://scikit-learn.org/stable/modules/linear_model.html#setting-the-regularization-parameter-leave-one-out-cross-validation))\n",
    "- 1.1.3. [**Lasso**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#lasso)<br/>([_Lasso_](https://scikit-learn.org/stable/modules/linear_model.html#lasso))\n",
    "- 1.1.4. [**Lasso multi-tâche**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#multi-task-lasso)<br/>([_Multi-task Lasso_](https://scikit-learn.org/stable/modules/linear_model.html#multi-task-lasso))\n",
    "- 1.1.5. [**Elastic-Net**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#elastic-net)<br/>([_Elastic-Net_](https://scikit-learn.org/stable/modules/linear_model.html#elastic-net))\n",
    "- 1.1.6. [**Elastic-Net multi-tâche**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#multi-task-elastic-net)<br/>([_Multi-task Elastic-Net_](https://scikit-learn.org/stable/modules/linear_model.html#multi-task-elastic-net))\n",
    "- 1.1.7. [**Régression de plus faible angle (LARS)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#least-angle-regression)<br/>([_Least Angle Regression_](https://scikit-learn.org/stable/modules/linear_model.html#least-angle-regression))\n",
    "- 1.1.8. [**LARS Lasso**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#lars-lasso)<br/>([_LARS Lasso_](https://scikit-learn.org/stable/modules/linear_model.html#lars-lasso))\n",
    "- 1.1.9. [**Poursuite orthogonale des correspondances (OMP)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#orthogonal-matching-pursuit-omp)<br/>([_Orthogonal Matching Pursuit (OMP)_](https://scikit-learn.org/stable/modules/linear_model.html#orthogonal-matching-pursuit-omp))\n",
    "- 1.1.10. [**Régression bayésienne**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#bayesian-regression)<br/>([_Bayesian Regression_](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-regression))\n",
    "- 1.1.11. [**Régression logistique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#logistic-regression)<br/>([_Logistic regression_](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression))\n",
    "- 1.1.12. [**Modèles linéaires généralisés**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#generalized-linear-models)<br/>([_Generalized Linear Models_](https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-models))\n",
    "- 1.1.13. [**Descente de gradient stochastique - SGD**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#stochastic-gradient-descent-sgd)<br/>([_Stochastic Gradient Descent - SGD_](https://scikit-learn.org/stable/modules/linear_model.html#stochastic-gradient-descent-sgd))\n",
    "- 1.1.14. [**Perceptron**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#perceptron)<br/>([_Perceptron_](https://scikit-learn.org/stable/modules/linear_model.html#perceptron))\n",
    "- 1.1.15. [**Algorithmes passifs agressifs**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#passive-aggressive-algorithms)<br/>([_Passive Aggressive Algorithms_](https://scikit-learn.org/stable/modules/linear_model.html#passive-aggressive-algorithms))\n",
    "- 1.1.16. [**Régression robuste : valeurs aberrantes et erreurs de modélisation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#robustness-regression-outliers-and-modeling-errors)<br/>([_Robustness regression: outliers and modeling errors_](https://scikit-learn.org/stable/modules/linear_model.html#robustness-regression-outliers-and-modeling-errors))\n",
    "- 1.1.17. [**Régression quantile**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#quantile-regression)<br/>([_Quantile Regression_](https://scikit-learn.org/stable/modules/linear_model.html#quantile-regression))\n",
    "- 1.1.18. [**Régression polynomiale : extension des modèles linéaires avec des fonctions de base**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#polynomial-regression-extending-linear-models-with-basis-functions)<br/>([_Polynomial regression: extending linear models with basis functions_](https://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='linear-models'></a> 1.1. **Modèles linéaires**<br/>([*Linear Models*](https://scikit-learn.org/stable/modules/linear_model.html))\n",
    "\n",
    "Ce chapitre présente un ensemble de méthodes conçues pour la régression, dans lesquelles la valeur cible est supposée être une combinaison linéaire des caractéristiques. En notation mathématique, si $\\hat{y}$ est la valeur prédite :\n",
    "\n",
    "$$\\hat{y}(w, x) = w_0 + w_1 x_1 + \\ldots + w_p x_p = w_0 + x^\\top w$$\n",
    "\n",
    "Dans l'ensemble du module, nous désignons le vecteur $w = (w_1,\\ldots, w_p)$ comme `coef_` et $w_0$ comme `intercept_`.\n",
    "\n",
    "Pour effectuer une classification avec des modèles linéaires généralisés, consulter la section [**Régression logistique** (1.1.11)](#logistic-regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ordinary-least-squares'></a> 1.1.1. **Moindres carrés ordinaires**<br/>([*Ordinary Least Squares*](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares))\n",
    "\n",
    "[**`LinearRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) ajuste un modèle linéaire avec des coefficients $w = (w_1,..., w_p)$ pour minimiser la somme résiduelle des carrés entre les cibles observées dans l'ensemble de données et les cibles prédites par l'approximation linéaire. Mathématiquement, il résout un problème de la forme :\n",
    "\n",
    "$$\\displaystyle\\min_{w} || X w - y||_2^2$$\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_ols_001.png\"\n",
    "    alt=\"Moindres carrés ordinaires\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "[**`LinearRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) prend les tableaux $X$ et $y$ dans sa méthode `fit` d'ajustement et stocke les coefficients $w$ du modèle linéaire dans son attribut `coef_` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coef_      : [0.5 0.5]\n",
      "intercept_ : 1.1102230246251565e-16\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n",
    "# > LinearRegression()\n",
    "print('coef_      :', reg.coef_)\n",
    "# > array([0.5, 0.5])\n",
    "print('intercept_ :', reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les estimations de coefficients pour les moindres carrés ordinaires (**OLS**) dépendent de l'indépendance des caractéristiques. Lorsque les caractéristiques sont corrélées et que les colonnes de la **matrice de conception** $X$ ont une dépendance approximativement linéaire, la matrice de conception devient proche du **singularité** et, par conséquent, l'estimation des moindres carrés devient très sensible aux erreurs aléatoires dans la cible observée, produisant une grande **variance**. Cette situation de **multicolinéarité** peut survenir, par exemple, lorsque des données sont collectées sans **plan expérimental**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**Exemple de régression linéaire**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_ols.ipynb)<br/>([*Linear Regression Example*](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='non-negative-least-squares'></a> 1.1.1.1. **Moindres carrés non-négatifs**<br/>([*Non-Negative Least Squares*](https://scikit-learn.org/stable/modules/linear_model.html#non-negative-least-squares))\n",
    "\n",
    "Il est possible de contraindre tous les coefficients à être non négatifs, ce qui peut être utile lorsqu'ils représentent certaines quantités physiques ou des valeurs naturellement non négatives (par exemple, les comptages de fréquence ou les prix des biens). La classe [**`LinearRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) accepte un paramètre booléen `positive` : lorsque ce paramètre est défini sur `True`, la méthode des moindres carrés non-négatifs (**NNLS**) est appliquée.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Moindres carrés non-négatifs**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_nnls.ipynb)<br/>([*Non-negative least squares*](https://scikit-learn.org/stable/auto_examples/linear_model/plot_nnls.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='ordinary-least-squares-complexity'></a> 1.1.1.2. **Complexité des moindres carrés ordinaires**<br/>([*Ordinary Least Squares Complexity*](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares-complexity))\n",
    "\n",
    "La solution des moindres carrés est calculée à l'aide de la décomposition en valeurs singulières de la matrice $X$. Si $X$ est une matrice de forme `(n_samples, n_features)`, cette méthode a un coût de $\\mathcal{O}(n_{\\text{samples}} n_{\\text{features}}^2)$, en supposant que $n_{\\text{samples}} \\geq n_{\\text{features}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ridge-regression-and-classification'></a> 1.1.2. **Régression Ridge et classification**<br/>([_Ridge regression and classification_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification))\n",
    "\n",
    "### <a id='ridge-regression'></a> 1.1.2.1. **Régression**<br/>([_Regression_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression))\n",
    "\n",
    "La régression [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) répond à certains des problèmes des moindres carrés ordinaires en imposant une pénalité sur la taille des coefficients. Les coefficients Ridge minimisent une somme résiduelle des carrés pénalisée :\n",
    "\n",
    "$$\\min_{w} || X w - y||_2^2 + \\alpha ||w||_2^2$$\n",
    "\n",
    "Le paramètre de complexité $\\alpha \\geq 0$ contrôle le niveau de rétrécissement : plus la valeur de $\\alpha$ est grande, plus le niveau de rétrécissement est élevé, et ainsi les coefficients deviennent plus robustes à la colinéarité.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_ridge_path_001.png\"\n",
    "    alt=\"Coefficients Ridge comme une fonction de la régularisation\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Comme pour d'autres modèles linéaires, [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) prend les tableaux $X, y$ dans sa méthode `fit` et stocke les coefficients $w$ du modèle linéaire dans son attribut `coef_` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34545455, 0.34545455])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.13636363636363638"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Ridge(alpha=.5)\n",
    "reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\n",
    "display(reg.coef_)\n",
    "display(reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classe [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) permet à l'utilisateur de spécifier que le solveur soit automatiquement choisi en définissant `solver=\"auto\"`. Lorsque cette option est spécifiée, [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) choisira entre les solveurs `\"lbfgs\"`, `\"cholesky\"` et `\"sparse_cg\"`. [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) commencera à vérifier les conditions présentées dans le tableau suivant de haut en bas. Si la condition est vraie, le solveur correspondant est choisi.\n",
    "\n",
    "| Solveur | Condition |\n",
    "|---------|-----------|\n",
    "| 'lbfgs' | L'option `positive=True` est spécifiée. |\n",
    "| 'cholesky' | Le tableau d'entrée X n'est pas clairsemé. |\n",
    "| 'sparse_cg' | Aucune des conditions ci-dessus n'est remplie. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='ridge-classification'></a> 1.1.2.2. **Classification**<br/>([_Classification_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-classification))\n",
    "\n",
    "Le régresseur [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) a une variante de classification : [**`RidgeClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html). Ce classifieur convertit d'abord les cibles binaires en `{-1, 1}`, puis traite le problème comme une tâche de régression, optimisant le même objectif que précédemment. La classe prédite correspond au signe de la prédiction du régresseur. Pour la classification multiclasse, le problème est traité comme une régression multi-sortie et la classe prédite correspond à la sortie avec la valeur la plus élevée.\n",
    "\n",
    "Il peut sembler discutable d'utiliser une perte des moindres carrés (pénalisée) pour ajuster un modèle de classification au lieu des pertes plus traditionnelles comme logistiques ou Ridge. Cependant, en pratique, tous ces modèles peuvent conduire à des scores de validation croisée similaires en termes d'exactitude ou de précision/rappel. La perte des moindres carrés pénalisée utilisée par le [**`RidgeClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html) permet un choix varié de solveurs numériques avec des performances de calcul hétérogènes.\n",
    "\n",
    "Le [**`RidgeClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html) peut être beaucoup plus rapide que la [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), par exemple, avec un grand nombre de classes, car il peut calculer la **matrice de projection** $(X^\\top X)^{-1} X^\\top$ une seule fois.\n",
    "\n",
    "Ce classifieur est parfois appelé [**machines à vecteurs de support à moindres carrés**](https://en.wikipedia.org/wiki/Least-squares_support-vector_machine) avec un noyau linéaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemples\n",
    "\n",
    "##### [**Affichage des coefficients Ridge comme fonction de la régularisation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_ridge_path.ipynb)<br/>([*Plot Ridge coefficients as a function of the regularization*](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html))\n",
    "\n",
    "Montre l'effet de la colinéarité sur les coefficients d'un estimateur.\n",
    "\n",
    "##### [**Classification de documents textes à l'aide de caractéristiques creuses**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_document_classification_20newsgroups.ipynb)<br/>([*Classification of text documents using sparse features*](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html))\n",
    "\n",
    "\n",
    "##### [**Pièges courants dans l'interprétation des coefficients des modèles linéaires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/4_inspection/plot_linear_model_coefficient_interpretation.ipynb)<br/>([*Common pitfalls in the interpretation of coefficients of linear models*](https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='ridge-complexity'></a> 1.1.2.3. **Complexité de Ridge**<br/>([_Ridge Complexity_](https://scikit-learn.org/stable/modules/linear_model.html#ridge-complexity))\n",
    "\n",
    "Cette méthode a le même ordre de complexité que les moindres carrés ordinaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='setting-the-regularization-parameter-leave-one-out-cross-validation'></a> 1.1.2.4. **Réglage du paramètre de régularisation : Validation croisée \"Leave-One-Out\"**<br/>([_Setting the regularization parameter: leave-one-out Cross-Validation_](https://scikit-learn.org/stable/modules/linear_model.html#setting-the-regularization-parameter-leave-one-out-cross-validation))\n",
    "\n",
    "[**`RidgeCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) implémente la régression Ridge avec une validation croisée intégrée du paramètre `alpha`. Cet objet fonctionne de la même manière que [**`GridSearchCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), mais en utilisant par défaut la validation croisée \"Leave-One-Out\" :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))\n",
    "reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\n",
    "# RidgeCV(alphas=array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01,\n",
    "#      1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06]))\n",
    "reg.alpha_\n",
    "# 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'attribution de la valeur de l'attribut [`cv`](https://scikit-learn.org/stable/glossary.html#term-cv) déclenche l'utilisation de la validation croisée avec [**`GridSearchCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Par exemple, `cv=10` pour une validation croisée à 10 plis au lieu de la validation croisée \"Leave-One-Out\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Références\n",
    "\n",
    "“**Notes on Regularized Least Squares**”, Rifkin & Lippert ([_rapport technique_](http://cbcl.mit.edu/publications/ps/MIT-CSAIL-TR-2007-025.pdf), [_diapositives de cours_](https://www.mit.edu/~9.520/spring07/Classes/rlsslides.pdf))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='lasso'></a> 1.1.3. **Lasso**<br/>([*Lasso*](https://scikit-learn.org/stable/modules/linear_model.html#lasso))\n",
    "\n",
    "Le Lasso est un modèle linéaire qui estime des coefficients parcimonieux. Il est utile dans certains contextes en raison de sa tendance à préférer les solutions avec moins de coefficients non nuls, réduisant ainsi le nombre de caractéristiques dont dépend la solution donnée. Pour cette raison, le Lasso et ses variantes sont fondamentaux dans le domaine de la détection compressée. Sous certaines conditions, il peut récupérer l'ensemble exact des coefficients non nuls (voir l'exemple [**Détection compressive : reconstruction tomographique avec L1 préalable (Lasso)**](https://scikit-learn.org/stable/auto_examples/applications/plot_tomography_l1_reconstruction.html#sphx-glr-auto-examples-applications-plot-tomography-l1-reconstruction-py)).\n",
    "\n",
    "Mathématiquement, il s'agit d'un modèle linéaire avec un terme de régularisation ajouté. La fonction objectif à minimiser est :\n",
    "\n",
    "$\\displaystyle\\min_{w} { \\frac{1}{2n_{\\text{samples}}} ||X w - y||_2 ^ 2 + \\alpha ||w||_1}$\n",
    "\n",
    "L'estimation du lasso résout donc la minimisation de la pénalité des moindres carrés avec ajout de $\\alpha ||w||_1$, où $\\alpha$ est une constante et $||w||_1$ est la $\\ell_1$-norme du vecteur coefficient.\n",
    "\n",
    "L'implémentation dans la classe [**`Lasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) utilise la descente de coordonnées comme algorithme pour ajuster les coefficients. Voir [**Régression du moindre angle** (1.1.7)](https://scikit-learn.org/stable/modules/linear_model.html#least-angle-regression) pour une implémentation alternative :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Lasso(alpha=0.1)\n",
    "reg.fit([[0, 0], [1, 1]], [0, 1])\n",
    "# Lasso(alpha=0.1)\n",
    "reg.predict([[1, 1]])\n",
    "# array([0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction [**`lasso_path`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.lasso_path.html#sklearn.linear_model.lasso_path) est utile pour les tâches de niveau inférieur, car elle calcule les coefficients le long du chemin complet des valeurs possibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemples\n",
    "\n",
    "##### [**Modèles basés sur $\\ell_1$ pour signaux creux**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_lasso_and_elasticnet.ipynb)<br/>([_L1-based models for Sparse Signals_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_and_elasticnet.html))\n",
    "\n",
    "##### [**Détection compressive : reconstruction tomographique avec $\\ell_1$ préalable (Lasso)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/applications/plot_tomography_l1_reconstruction.ipynb)<br/>([_Compressive sensing: tomography reconstruction with $\\ell_1$ prior (Lasso_](https://scikit-learn.org/stable/auto_examples/applications/plot_tomography_l1_reconstruction.html))\n",
    "\n",
    "##### [**Pièges courants dans l'interprétation des coefficients des modèles linéaires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/4_inspection/plot_linear_model_coefficient_interpretation.ipynb)<br/>([*Common pitfalls in the interpretation of coefficients of linear models*](https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note : Sélection de caractéristiques avec Lasso\n",
    "\n",
    "> Comme la régression Lasso produit des modèles parcimonieux, elle peut donc être utilisée pour effectuer une _sélection de caractéristiques_, comme détaillé dans la [**Sélection de caractéristiques basée sur $\\ell_1$** (1.13.4.1)](https://scikit-learn.org/stable/modules/feature_selection.html#l1-feature-selection).\n",
    "\n",
    "Les deux références suivantes expliquent les itérations utilisées dans le solveur de descente de coordonnées de scikit-learn, ainsi que le calcul de l'écart de dualité utilisé pour le contrôle de la convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Références\n",
    "\n",
    "🔬 [**“Regularization Path For Generalized linear Models by Coordinate Descent”**](https://www.jstatsoft.org/article/view/v033i01/v33i01.pdf), Friedman, Hastie & Tibshirani, J Stat Softw, 2010.\n",
    "\n",
    "🔬 [**“An Interior-Point Method for Large-Scale $\\ell_1$-Regularized Least Squares”**](https://web.stanford.edu/~boyd/papers/pdf/l1_ls.pdf), S. J. Kim, K. Koh, M. Lustig, S. Boyd and D. Gorinevsky, in IEEE Journal of Selected Topics in Signal Processing, 2007."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='setting-regularization-parameter'></a> 1.1.3.1. **Définition du paramètre de régularisation**<br/>([_Setting regularization parameter_](https://scikit-learn.org/stable/modules/linear_model.html#setting-regularization-parameter))\n",
    "\n",
    "Le paramètre `alpha` contrôle le degré de parcimonie des coefficients estimés.\n",
    "\n",
    "#### Utilisation de la validation croisée\n",
    "\n",
    "scikit-learn propose des objets qui définissent le paramètre `alpha` du Lasso par validation croisée : [**`LassoCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html) et [**`LassoLarsCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsCV.html). [**`LassoLarsCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsCV.html) est basé sur l'algorithme [**Least Angle Regression** (1.1.7)](https://scikit-learn.org/stable/modules/linear_model.html#least-angle-regression) expliqué ci-dessous.\n",
    "\n",
    "Pour les ensembles de données de grande dimension avec de nombreuses caractéristiques colinéaires, [**`LassoCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html) est généralement préférable. Cependant, [**`LassoLarsCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsCV.html) a l'avantage d'explorer des valeurs du paramètre `alpha` plus pertinentes, et si le nombre d'échantillons est très faible par rapport au nombre de caractéristiques, il est souvent plus rapide que [**`LassoCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html).\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lasso_model_selection_002.png\"\n",
    "    alt=\"Erreur quadratique moyenne pour chaque pli : descente de coordonnées\"\n",
    "    style=\"max-width: 40%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lasso_model_selection_003.png\"\n",
    "    alt=\"Erreur quadratique moyenne pour chaque pli: Lars\"\n",
    "    style=\"max-width: 40%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "#### Sélection de modèle basée sur des critères d'information\n",
    "\n",
    "Comme alterative, l'estimateur [**`LassoLarsIC`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsIC.html) propose d'utiliser le critère d'information d'Akaike (AIC) et le critère d'information de Bayes (BIC). C'est une alternative moins coûteuse pour déterminer la valeur optimale de `alpha`, car le chemin de régularisation n'est calculé qu'une seule fois au lieu, contre $k$ + 1 fois avec la validation croisée $k$-fold.\n",
    "\n",
    "En effet, ces critères sont calculés sur l'ensemble d'apprentissage. En bref, ils pénalisent les scores trop optimistes des différents modèles Lasso par leur flexibilité (voir section « Détails mathématiques » ci-dessous).\n",
    "\n",
    "Cependant, ces critères nécessitent une estimation précise des degrés de liberté de la solution. Ils sont dérivés pour de grands échantillons (résultats asymptotiques) et supposent que le modèle correct est inclus dans les candidats à l'étude. Ils ont également tendance à dysfonctionner lorsque le problème est mal conditionné (par exemple, plus de caractéristiques que d'échantillons).\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lasso_lars_ic_001.png\"\n",
    "    alt=\"Sélection de modèle Lasso via AIC et BIC\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "##### Détails mathématiques\n",
    "\n",
    "La définition de l'AIC (et donc du BIC) peut différer dans la littérature. Dans cette section, nous donnons plus d'informations sur le critère tel que calculé dans scikit-learn. Le critère AIC est défini comme suit :\n",
    "\n",
    "$$AIC = -2 \\log(\\hat{L}) + 2 d$$\n",
    "\n",
    "où $\\hat{L}$ est la vraisemblance maximale du modèle et $d$ est le nombre de paramètres (également appelés _degrés de liberté_ dans la section précédente).\n",
    "\n",
    "La définition de BIC remplace la constante 2 par $\\log(N)$ :\n",
    "\n",
    "$$BIC = -2 \\log(\\hat{L}) + \\log(N) d$$\n",
    "\n",
    "où $N$ est le nombre d'échantillons.\n",
    "\n",
    "Pour un modèle gaussien linéaire, la log-vraisemblance maximale est définie comme suit :\n",
    "\n",
    "$$\\log(\\hat{L}) = - \\frac{n}{2} \\log(2 \\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{2\\sigma^2}$$\n",
    "\n",
    "où $\\sigma^2$ est une estimation de la variance du bruit, $y_i$ et $\\hat{y}_i$ sont respectivement les cibles réelles et prédites, et $n$ est le nombre d'échantillons.\n",
    "\n",
    "En insérant la log-vraisemblance maximale dans la formule AIC, on obtient :\n",
    "\n",
    "$$AIC = n \\log(2 \\pi \\sigma^2) + \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sigma^2} + 2 d$$\n",
    "\n",
    "Le premier terme de l'expression ci-dessus est parfois ignoré car il s'agit d'une constante lorsque $\\sigma^2$ est fourni. De plus, il est parfois affirmé que l'AIC est équivalent à la statistique $C_p$ [12]. Cependant, au sens strict, il n'est équivalent qu'à une constante et à un facteur multiplicatif près.\n",
    "\n",
    "Enfin, nous avons mentionné plus haut que $\\sigma^2$ est une estimation de la variance du bruit. Dans [**`LassoLarsIC`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsIC.html) lorsque le paramètre `noise_variance` n'est pas fourni (le comportement par défaut), la variance du bruit est estimée à l'aide de l'estimateur non biaisé [13] défini comme suit :\n",
    "\n",
    "$$\\sigma^2 = \\displaystyle\\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{n - p}$$\n",
    "\n",
    "où $p$ est le nombre de caractéristiques et $\\hat{y}_i$ est la cible prédite à l'aide d'une régression des moindres carrés ordinaires. Notez que cette formule n'est valide que lorsque `n_samples > n_features`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Références\n",
    "\n",
    "🔬 [12] Zou, Hui, Trevor Hastie, and Robert Tibshirani. [**“On the degrees of freedom of the lasso”**](https://arxiv.org/abs/0712.0881.pdf). The Annals of Statistics 35.5 (2007): 2173-2192.\n",
    "\n",
    "🔬 [13] Cherkassky, Vladimir, and Yunqian Ma. [**“Comparison of model selection for regression”**](http://www.ece.umn.edu/users/cherkass/comparison_paper_Mar05.pdf). Neural computation 15.7 (2003): 1691-1714."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparaison avec le paramètre de régularisation de SVM\n",
    "\n",
    "L'équivalence entre `alpha` et le paramètre de régularisation de SVM, `C`, est donnée par `alpha = 1 / C` ou `alpha = 1 / (n_samples * C)`, en fonction de l'estimateur et de la fonction objectif exacte optimisée par le modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='multi-task-lasso'></a> 1.1.4. **Lasso multi-tâche**<br/>([*Multi-task Lasso*](https://scikit-learn.org/stable/modules/linear_model.html#multi-task-lasso))\n",
    "\n",
    "The [**`MultiTaskLasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskLasso.html) is a linear model that estimates sparse coefficients for multiple regression problems jointly: y is a 2D array, of shape `(n_samples, n_tasks)`. The constraint is that the selected features are the same for all the regression problems, also called tasks.\n",
    "\n",
    "The following figure compares the location of the non-zero entries in the coefficient matrix $W$ obtained with a simple [**`Lasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.htm) or a [**`MultiTaskLasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskLasso.html). The [**`Lasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.htm) estimates yield scattered non-zeros while the non-zeros of the [**`MultiTaskLasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskLasso.html) are full columns.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_multi_task_lasso_support_001.png\"\n",
    "    alt=\"Coefficient non-zero location\"\n",
    "    style=\"max-width: 45%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_multi_task_lasso_support_002.png\"\n",
    "    alt=\"Multi task lasso support\"\n",
    "    style=\"max-width: 45%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "#### Examples\n",
    "\n",
    "##### Joint feature selection with multi-task Lasso\n",
    "\n",
    "##### [**Joint feature selection with multi-task Lasso**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_multi_task_lasso_support.ipynb)<br/>([_Joint feature selection with multi-task Lasso_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_multi_task_lasso_support.html))\n",
    "\n",
    "#### Mathematical details\n",
    "\n",
    "Mathematically, it consists of a linear model trained with a mixed $\\ell_1$, $\\ell_2$-norm for regularization. The objective function to minimize is:\n",
    "\n",
    "$$\\min_{W} { \\frac{1}{2n_{\\text{samples}}} ||X W - Y||_{\\text{Fro}} ^ 2 + \\alpha ||W||_{21}}$$\n",
    "\n",
    "where $\\text{Fro}$ indicates the Frobenius norm\n",
    "\n",
    "$$||A||_{\\text{Fro}} = \\sqrt{\\sum_{ij} a_{ij}^2}$$\n",
    "\n",
    "and $\\ell_1$, $\\ell_2$ reads\n",
    "\n",
    "$$||A||_{2 1} = \\sum_i \\sqrt{\\sum_j a_{ij}^2}.$$\n",
    "\n",
    "The implementation in the class [**`MultiTaskLasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskLasso.html) uses coordinate descent as the algorithm to fit the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='multi-task-lasso'></a> 1.1.4. **Lasso multi-tâche**<br/>([*MultiTask Lasso*](https://scikit-learn.org/stable/modules/linear_model.html#multi-task-lasso))\n",
    "\n",
    "Le [**`MultiTaskLasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskLasso.html) est un modèle linéaire qui estime des coefficients parcimonieux pour plusieurs problèmes de régression simultanément : `y` est un tableau 2D, de forme `(n_samples, n_tasks)`. La contrainte est que les caractéristiques sélectionnées soient les mêmes pour tous les problèmes de régression, également appelés _tâches_.\n",
    "\n",
    "La figure suivante compare l'emplacement des entrées non nulles dans la matrice de coefficients $W$ obtenue avec un simple [**`Lasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.htm) ou avec un [**`MultiTaskLasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskLasso.html). Les estimations [**`Lasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.htm) produisent des non-nuls dispersés tandis que les non-nuls du [**`MultiTaskLasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskLasso.html) sont des colonnes complètes.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_multi_task_lasso_support_001.png\"\n",
    "    alt=\"Emplacement des coefficients non nuls\"\n",
    "    style=\"max-width: 45%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_multi_task_lasso_support_002.png\"\n",
    "    alt=\"Support du lasso multi-tâche\"\n",
    "    style=\"max-width: 45%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Sélection conjointe de caractéristiques avec le lasso multi-tâche**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_multi_task_lasso_support.ipynb)<br/>([_Joint feature selection with multi-task Lasso_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_multi_task_lasso_support.html))\n",
    "\n",
    "#### Détails mathématiques\n",
    "\n",
    "Mathématiquement, il s'agit d'un modèle linéaire entraîné avec une régularisation mixte des normes $\\ell_1$ et  $\\ell_2$. La fonction objectif à minimiser est :\n",
    "\n",
    "$$\\min_{W} { \\frac{1}{2n_{\\text{samples}}} ||X W - Y||_{\\text{Fro}} ^ 2 + \\alpha ||W||_{21}}$$\n",
    "\n",
    "où $\\text{Fro}$ indique la norme de Frobenius\n",
    "\n",
    "$$||A||_{\\text{Fro}} = \\sqrt{\\sum_{ij} a_{ij}^2}$$\n",
    "\n",
    "et $\\ell_1\\ell_2$ est définie comme suit\n",
    "\n",
    "$$||A||_{2 1} = \\sum_i \\sqrt{\\sum_j a_{ij}^2}.$$\n",
    "\n",
    "L'implémentation dans la classe [**`MultiTaskLasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskLasso.html) utilise la _descente de coordonnées_ comme algorithme d'ajustement des coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='elastic-net'></a> 1.1.5. **Elastic-Net**<br/>([*Elastic-Net*](https://scikit-learn.org/stable/modules/linear_model.html#elastic-net))\n",
    "\n",
    "Le [**`ElasticNet`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) est un modèle de régression linéaire entraîné avec une régularisation à la fois par les normes $\\ell_1$ et $\\ell_2$ des coefficients. Cette combinaison permet d'apprendre un modèle parcimonieux où peu de poids sont non nuls, comme le [**`Lasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html), tout en conservant les propriétés de régularisation de [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html). Nous contrôlons la combinaison convexe de $\\ell_1$ et $\\ell_2$ à l'aide du paramètre `l1_ratio`.\n",
    "\n",
    "L'Elastic-Net est utile lorsqu'il y a plusieurs caractéristiques corrélées les unes aux autres. Le Lasso est susceptible d'en choisir une au hasard, tandis que l'Elastic-Net est susceptible d'en choisir plusieurs.\n",
    "\n",
    "Un avantage pratique du compromis entre Lasso et Ridge est qu'il permet à l'Elastic-Net d'hériter de la stabilité de Ridge en cas de rotation.\n",
    "\n",
    "La fonction objectif à minimiser dans ce cas est\n",
    "\n",
    "$$\\min_{w} { \\frac{1}{2n_{\\text{samples}}} ||X w - y||_2 ^ 2 + \\alpha \\rho ||w||_1 +\n",
    "\\frac{\\alpha(1-\\rho)}{2} ||w||_2 ^ 2}$$\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lasso_coordinate_descent_path_001.png\"\n",
    "    alt=\"Lasso and Elastic-Net Paths\"\n",
    "    style=\"max-width: 45%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "La classe [**`ElasticNetCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html) peut être utilisée pour définir les paramètres `alpha` ($\\alpha$) et `l1_ratio` ($\\rho$) par validation croisée.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Modèles basés sur $\\ell_1$ pour signaux creux**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_lasso_and_elasticnet.ipynb)<br/>([_L1-based models for Sparse Signals_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_and_elasticnet.html))\n",
    "\n",
    "##### [**Lasso et Elastic-Net**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_lasso_coordinate_descent_path.ipynb)<br/>([_Lasso and Elastic Net_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_coordinate_descent_path.html))\n",
    "\n",
    "#### Références\n",
    "\n",
    "Les deux références suivantes expliquent les itérations utilisées dans le solveur de descente de coordonnées de scikit-learn, ainsi que le calcul de l'écart de dualité utilisé pour le contrôle de la convergence.\n",
    "\n",
    "🔬 [**“Regularization Path For Generalized linear Models by Coordinate Descent”**](https://www.jstatsoft.org/article/view/v033i01/v33i01.pdf), Friedman, Hastie & Tibshirani, J Stat Softw, 2010.\n",
    "\n",
    "🔬 [**“An Interior-Point Method for Large-Scale $\\ell_1$-Regularized Least Squares”**](https://web.stanford.edu/~boyd/papers/pdf/l1_ls.pdf), S. J. Kim, K. Koh, M. Lustig, S. Boyd and D. Gorinevsky, in IEEE Journal of Selected Topics in Signal Processing, 2007."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='multi-task-elastic-net'></a> 1.1.6. **Elastic-Net multi-tâche**<br/>([*Multi-task Elastic-Net*](https://scikit-learn.org/stable/modules/linear_model.html#multi-task-elastic-net))\n",
    "\n",
    "Le [**`MultiTaskElasticNet`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskElasticNet.html) est un modèle Elastic-Net qui estime conjointement des coefficients parcimonieux pour plusieurs problèmes de régression : `Y` est un tableau 2D de forme `(n_samples, n_tasks)`. La contrainte est que les caractéristiques sélectionnées soient les mêmes pour tous les problèmes de régression, également appelés _tâches_.\n",
    "\n",
    "Mathématiquement, il s'agit d'un modèle linéaire entraîné avec une régularisation mixte $\\ell_1\\ell_2$ et $\\ell_2$ des coefficients. La fonction objectif à minimiser est :\n",
    "\n",
    "$$\\min_{W} { \\frac{1}{2n_{\\text{samples}}} ||X W - Y||_{\\text{Fro}}^2 + \\alpha \\rho ||W||_{2 1} +\n",
    "\\frac{\\alpha(1-\\rho)}{2} ||W||_{\\text{Fro}}^2}$$\n",
    "\n",
    "L'implémentation dans la classe [**`MultiTaskElasticNet`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskElasticNet.html) utilise l'algorithme de descente de coordonnées pour ajuster les coefficients.\n",
    "\n",
    "La classe [**`MultiTaskElasticNetCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskElasticNetCV.html) peut être utilisée pour définir les paramètres `alpha` ($\\alpha$) et `l1_ratio` ($\\rho$) par validation croisée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='least-angle-regression'></a> 1.1.7. **Régression de plus faible angle (LARS)**<br/>([*Least Angle Regression*](https://scikit-learn.org/stable/modules/linear_model.html#least-angle-regression))\n",
    "\n",
    "La régression de plus faible angle (LARS, pour _Least-Angle Regression_) est un algorithme de régression pour les données à haute dimension, développé par Bradley Efron, Trevor Hastie, Iain Johnstone et Robert Tibshirani. LARS est similaire à la régression pas à pas en avant (_Forward Stepwise Regression_). À chaque étape, il trouve la caractéristique la plus corrélée à la cible. Lorsqu'il y a plusieurs caractéristiques ayant une corrélation égale, au lieu de continuer le long de la même caractéristique, il procède dans une direction équi-angulaire entre les caractéristiques.\n",
    "\n",
    "Les avantages de LARS sont les suivants :\n",
    "\n",
    "- Il est numériquement efficace dans les contextes où le nombre de caractéristiques est nettement supérieur au nombre d'échantillons.\n",
    "- Il est aussi rapide que la sélection avant et a le même ordre de complexité que les moindres carrés ordinaires.\n",
    "- Il produit un chemin complet de solution linéaire par morceaux, ce qui est utile dans la validation croisée ou les tentatives similaires d'ajustement du modèle.\n",
    "- Si deux caractéristiques sont presque également corrélées à la cible, alors leurs coefficients devraient augmenter à peu près au même taux. L'algorithme se comporte donc comme l'intuition le suggérerait, et il est également plus stable.\n",
    "- Il peut être facilement modifié pour produire des solutions pour d'autres estimateurs, comme le Lasso.\n",
    "\n",
    "Les inconvénients de la méthode LARS incluent :\n",
    "\n",
    "- Parce que LARS est basé sur un réajustement itératif des résidus, il semble être particulièrement sensible aux effets du bruit. Ce problème est discuté en détail par Weisberg dans la section de discussion de l'article de Weisberg dans la revue Annals of Statistics (2004).\n",
    "\n",
    "Le modèle LARS peut être utilisé via l'estimateur [**`Lars`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lars.html), ou son implémentation de bas niveau [**`lars_path`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.lars_path.html) ou [**`lars_path_gram`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.lars_path_gram.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='lars-lasso'></a> 1.1.8. **LARS Lasso**<br/>([*LARS Lasso*](https://scikit-learn.org/stable/modules/linear_model.html#lars-lasso))\n",
    "\n",
    "[**`LassoLars`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLars.html) est un modèle de Lasso implémenté à l'aide de l'algorithme LARS. Contrairement à l'implémentation basée sur la descente de coordonnées, cela produit la solution exacte, qui est linéaire par morceaux en fonction de la norme de ses coefficients.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lasso_lars_001.png\"\n",
    "    alt=\"Lasso Path\"\n",
    "    style=\"max-width: 45%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "L'algorithme LARS fournit le chemin complet des coefficients le long du paramètre de régularisation presque gratuitement, donc une opération courante est de récupérer le chemin avec l'une des fonctions [**`lars_path`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.lars_path.html) ou [**`lars_path_gram`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.lars_path_gram.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6, 0. ])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LassoLars(alpha=.1)\n",
    "reg.fit([[0, 0], [1, 1]], [0, 1])\n",
    "# LassoLars(alpha=0.1)\n",
    "reg.coef_\n",
    "# array([0.6..., 0.        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemples\n",
    "\n",
    "##### [**Chemin Lasso à l'aide de LARS**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_lasso_lars.ipynb)<br/>([_Lasso path using LARS_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_lars.html))\n",
    "\n",
    "#### Détails mathématiques\n",
    "\n",
    "L'algorithme est similaire à la régression pas à pas en avant, mais au lieu d'inclure des caractéristiques à chaque étape, les coefficients estimés sont augmentés dans une direction équi-angulaire par rapport à la corrélation de chacun avec le résidu.\n",
    "\n",
    "Au lieu de fournir un résultat sous forme de vecteur, la solution LARS consiste en une courbe indiquant la solution pour chaque valeur de la norme $\\ell_1$ du vecteur de paramètres. Le chemin complet des coefficients est stocké dans le tableau `coef_path_` de forme `(n_features, max_features + 1)`. La première colonne est toujours nulle.\n",
    "\n",
    "#### Références\n",
    "\n",
    "L'algorithme original est détaillé dans l'article :\n",
    "\n",
    "🔬 [**“Least Angle Regression”**](https://hastie.su.domains/Papers/LARS/LeastAngle_2002.pdf) by Hastie et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='orthogonal-matching-pursuit-omp'></a> 1.1.9. **Poursuite orthogonale des correspondances (OMP)**<br/>([*Orthogonal Matching Pursuit (OMP)*](https://scikit-learn.org/stable/modules/linear_model.html#orthogonal-matching-pursuit-omp))\n",
    "\n",
    "[**`OrthogonalMatchingPursuit`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.OrthogonalMatchingPursuit.html) et [**`orthogonal_mp`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.orthogonal_mp.html) mettent en œuvre l'algorithme OMP pour approcher l'ajustement d'un modèle linéaire avec des contraintes imposées sur le nombre de coefficients non nuls (c'est-à-dire la pseudo-norme $\\ell_0$).\n",
    "\n",
    "Étant une méthode de sélection de caractéristiques en avant, similaire à la [**Régression du moindre angle (LARS)** (1.1.7)](https://scikit-learn.org/stable/modules/linear_model.html), la poursuite orthogonale des correspondances peut approcher le vecteur de solution optimal avec un nombre fixé d'éléments non nuls :\n",
    "\n",
    "$$\\underset{w}{\\operatorname{arg\\,min\\,}}  ||y - Xw||_2^2 \\text{ sous réserve } ||w||_0 \\leq n_{\\text{nonzero\\_coefs}}$$\n",
    "\n",
    "Alternativement, la poursuite orthogonale des correspondances peut viser une erreur spécifique plutôt qu'un nombre spécifique de coefficients non nuls. Cela peut être exprimé comme suit :\n",
    "\n",
    "$$\\underset{w}{\\operatorname{arg\\,min\\,}} ||w||_0 \\text{ sous réserve } ||y-Xw||_2^2 \\leq \\text{tol}$$\n",
    "\n",
    "OMP est basé sur un algorithme glouton qui inclut à chaque étape l'atome le plus fortement corrélé avec le résidu actuel. Il est similaire à la méthode de poursuite des correspondances simple (MP), mais il est meilleur en ce sens qu'à chaque itération, le résidu est recalculé en utilisant une projection orthogonale sur l'espace des éléments de dictionnaire déjà choisis.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Poursuite orthogonale des correspondances**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_omp.ipynb)<br/>([_Poursuite orthogonale des correspondances_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_omp.html))\n",
    "\n",
    "Utilisation de la poursuite orthogonale des correspondances pour récupérer un signal parcimonieux à partir d'une mesure bruitée encodée avec un dictionnaire.\n",
    "\n",
    "#### References\n",
    "\n",
    "🔬 [**“Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal Matching Pursuit Technion”**](https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf), R. Rubinstein, M. Zibulevsky, M. Elad.\n",
    "\n",
    "🔬 [**“Matching Pursuits With Time-Frequency Dictionaries”**](https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf), S. G. Mallat, Z. Zhang.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='bayesian-regression'></a> 1.1.10. **Régression bayésienne**<br/>([*Bayesian Regression*](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-regression))\n",
    "\n",
    "Bayesian regression techniques can be used to include regularization parameters in the estimation procedure: the regularization parameter is not set in a hard sense but tuned to the data at hand.\n",
    "\n",
    "This can be done by introducing [**uninformative priors**](https://en.wikipedia.org/wiki/Prior_probability#Uninformative_priors) over the hyper parameters of the model. The $\\ell_{2}$ regularization used in [**Ridge regression and classification** (1.1.2)](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression) is equivalent to finding a maximum a posteriori estimation under a Gaussian prior over the coefficients with precision $\\lambda^{-1}$. Instead of setting `lambda` manually, it is possible to treat it as a random variable to be estimated from the data.\n",
    "\n",
    "To obtain a fully probabilistic model, the output $y$ is assumed to be Gaussian distributed around $X w$:\n",
    "\n",
    "$$p(y|X,w,\\alpha) = \\mathcal{N}(y|X w,\\alpha)$$\n",
    "\n",
    "where $\\alpha$ is again treated as a random variable that is to be estimated from the data.\n",
    "\n",
    "The advantages of Bayesian Regression are:\n",
    "- It adapts to the data at hand.\n",
    "- It can be used to include regularization parameters in the estimation procedure.\n",
    "\n",
    "The disadvantages of Bayesian regression include:\n",
    "- Inference of the model can be time consuming.\n",
    "\n",
    "#### References\n",
    "\n",
    "A good introduction to Bayesian methods is given in C. Bishop: Pattern Recognition and Machine learning\n",
    "\n",
    "Original Algorithm is detailed in the book Bayesian learning for neural networks by Radford M. Neal\n",
    "\n",
    "### <a id='bayesian-ridge-regression'></a> 1.1.10.1. Bayesian Ridge Regression\n",
    "\n",
    "[**`BayesianRidge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html) estimates a probabilistic model of the regression problem as described above. The prior for the coefficient $w$ is given by a spherical Gaussian:\n",
    "\n",
    "$$p(w|\\lambda) = \\mathcal{N}(w|0,\\lambda^{-1}\\mathbf{I}_{p})$$\n",
    "\n",
    "The priors over $\\alpha$ and $\\lambda$ are chosen to be [**gamma distributions**](https://en.wikipedia.org/wiki/Gamma_distribution), the conjugate prior for the precision of the Gaussian. The resulting model is called Bayesian Ridge Regression, and is similar to the classical [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html).\n",
    "\n",
    "The parameters $w$, $\\alpha$ and $\\lambda$ are estimated jointly during the fit of the model, the regularization parameters $\\alpha$ and $\\lambda$ being estimated by maximizing the _log marginal likelihood_. The scikit-learn implementation is based on the algorithm described in Appendix A of (Tipping, 2001) where the update of the parameters $\\alpha$ and $\\lambda$ is done as suggested in (MacKay, 1992). The initial value of the maximization procedure can be set with the hyperparameters `alpha_init` and `lambda_init`.\n",
    "\n",
    "There are four more hyperparameters, $\\alpha_1$, $\\alpha_2$, $\\lambda_1$ and $\\lambda_2$ of the gamma prior distributions over $\\alpha$ and $\\lambda$. These are usually chosen to be _non-informative_. By default $\\alpha_1 = \\alpha_2 =  \\lambda_1 = \\lambda_2 = 10^{-6}$.\n",
    "\n",
    "Bayesian Ridge Regression is used for regression:\n",
    "\n",
    "---\n",
    "\n",
    "After being fitted, the model can then be used to predict new values:\n",
    "\n",
    "---\n",
    "\n",
    "The coefficients $w$ of the model can be accessed:\n",
    "\n",
    "---\n",
    "\n",
    "Due to the Bayesian framework, the weights found are slightly different to the ones found by [**Ordinary Least Squares (OLS)** (1.1.1)](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares). However, Bayesian Ridge Regression is more robust to ill-posed problems.\n",
    "\n",
    "#### Examples\n",
    "\n",
    "##### [**Curve Fitting with Bayesian Ridge Regression**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_bayesian_ridge_curvefit.ipynb)<br/>([_Curve Fitting with Bayesian Ridge Regression_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_bayesian_ridge_curvefit.html))\n",
    "\n",
    "Computes a Bayesian Ridge Regression of Sinusoids.\n",
    "\n",
    "#### References\n",
    "\n",
    "Section 3.3 in Christopher M. Bishop: Pattern Recognition and Machine Learning, 2006\n",
    "\n",
    "David J. C. MacKay, Bayesian Interpolation, 1992.\n",
    "\n",
    "Michael E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine, 2001.\n",
    "\n",
    "### <a id='automatic-relevance-determination-ard'></a> 1.1.10.2. Automatic Relevance Determination (ARD)\n",
    "\n",
    "The Automatic Relevance Determination (as being implemented in [**`ARDRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ARDRegression.html)) is a kind of linear model which is very similar to the [**Bayesian Ridge Regression** (1.1.10.1)](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-ridge-regression), but that leads to sparser coefficients[1] [2].\n",
    "\n",
    "[**`ARDRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ARDRegression.html) poses a different prior over $w$: it drops the spherical Gaussian distribution for a centered elliptic Gaussian distribution. This means each coefficient $w_i$ can itself be drawn from a Gaussian distribution, centered on zero and with a precision $\\lambda_{i}$:\n",
    "\n",
    "$$p(w|\\lambda) = \\mathcal{N}(w|0,A^{-1})$$\n",
    "\n",
    "with $A$ being a positive definite diagonal matrix and $\\text{diag}(A) = \\lambda = \\{\\lambda_{1},...,\\lambda_{p}\\}$.\n",
    "\n",
    "In contrast to the [**Bayesian Ridge Regression** (1.1.10.1)](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-ridge-regression), each coordinate of $w_i$ has its own standard deviation $\\frac{1}{\\lambda_i}$. The prior over all $\\lambda_{i}$ is chosen to be the same gamma distribution given by the hyperparameters $\\lambda_1$ and $\\lambda_2$.\n",
    "\n",
    "ARD is also known in the literature as _Sparse Bayesian Learning and Relevance Vector Machine_ [3] [4]. For a worked-out comparison between ARD and [**Bayesian Ridge Regression** (1.1.10.1)](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-ridge-regression), see the example below.\n",
    "\n",
    "#### Examples\n",
    "\n",
    "##### [**Comparaison des régresseurs bayésiens linéaires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_ard.ipynb)<br/>([_Comparing Linear Bayesian Regressors_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ard.html))\n",
    "\n",
    "Compares a Automatic Relevance Determination (ARD) and a Bayesian Ridge Regression.\n",
    "\n",
    "#### References\n",
    "\n",
    "[1] Christopher M. Bishop: Pattern Recognition and Machine Learning, Chapter 7.2.1\n",
    "\n",
    "[2] David Wipf and Srikantan Nagarajan: A New View of Automatic Relevance Determination\n",
    "\n",
    "[3] Michael E. Tipping: Sparse Bayesian Learning and the Relevance Vector Machine\n",
    "\n",
    "[4] Tristan Fletcher: Relevance Vector Machines Explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='bayesian-regression'></a> 1.1.10. **Régression bayésienne**<br/>([*Bayesian Regression*](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-regression))\n",
    "\n",
    "Les techniques de régression bayésienne peuvent être utilisées pour inclure des paramètres de régularisation dans la procédure d'estimation : le paramètre de régularisation n'est pas fixé de manière stricte mais ajusté aux données en cours.\n",
    "\n",
    "Cela peut être fait en introduisant des [**_a priori_ non informatifs**](https://en.wikipedia.org/wiki/Prior_probability#Uninformative_priors) sur les hyper-paramètres du modèle. La régularisation $\\ell_{2}$ utilisée dans [**la régression et la classification Ridge** (1.1.2)](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression) équivaut à trouver une estimation du maximum _a posteriori_ sous une loi normale pour les coefficients $w$ avec une précision $\\lambda^{-1}$. Au lieu de définir `lambda` manuellement, il est possible de le traiter comme une variable aléatoire à estimer à partir des données.\n",
    "\n",
    "Pour obtenir un modèle entièrement probabiliste, la sortie $y$ est supposée suivre une distribution normale autour de $X w$ :\n",
    "\n",
    "$$p(y|X,w,\\alpha) = \\mathcal{N}(y|X w,\\alpha)$$\n",
    "\n",
    "où $\\alpha$ est également traité comme une variable aléatoire à estimer à partir des données.\n",
    "\n",
    "Les avantages de la régression bayésienne sont :\n",
    "- Elle s'adapte aux données en cours.\n",
    "- Elle peut être utilisée pour inclure des paramètres de régularisation dans la procédure d'estimation.\n",
    "\n",
    "Les inconvénients de la régression bayésienne comprennent :\n",
    "- L'inférence du modèle peut prendre du temps.\n",
    "\n",
    "#### Références\n",
    "\n",
    "📚 Bishop, [**“Pattern recognition and machine learning”**](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf), 2006.\n",
    "\n",
    "Une bonne introduction aux méthodes bayésiennes.\n",
    "\n",
    "📚 [**“Bayesian learning for neural networks”**](https://link.springer.com/book/10.1007/978-1-4612-0745-0) de Radford M. Neal, 1996.\n",
    "\n",
    "Les détails de l'algorithme original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='bayesian-ridge-regression'></a> 1.1.10.1. Régression bayésienne Ridge\n",
    "\n",
    "[**`BayesianRidge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html) estime un modèle probabiliste du problème de régression comme décrit ci-dessus. L'_a priori_ pour le coefficient $w$ est donné par une loi normale sphérique :\n",
    "\n",
    "$$p(w|\\lambda) = \\mathcal{N}(w|0,\\lambda^{-1}\\mathbf{I}_{p})$$\n",
    "\n",
    "Les _a priori_ sur $\\alpha$ et $\\lambda$ sont choisis comme étant des [**distributions gamma**](https://en.wikipedia.org/wiki/Gamma_distribution), l'_a priori_ conjugué pour la précision de la loi normale. Le modèle résultant est appelé _régression Ridge bayésienne_, et elle est similaire à la régression classique [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html).\n",
    "\n",
    "Les paramètres $w$, $\\alpha$ et $\\lambda$ sont estimés conjointement lors de l'ajustement du modèle, les paramètres de régularisation $\\alpha$ et $\\lambda$ étant estimés en maximisant la _log-vraisemblance marginale_. L'implémentation de scikit-learn est basée sur l'algorithme décrit dans l'annexe A de (Tipping, 2001) où la mise à jour des paramètres $\\alpha$ et $\\lambda$ est effectuée comme suggéré dans (MacKay, 1992). La valeur initiale de la procédure de maximisation peut être définie à l'aide des hyperparamètres `alpha_init` et `lambda_init`.\n",
    "\n",
    "Il y a quatre autres hyperparamètres, $\\alpha_1$, $\\alpha_2$, $\\lambda_1$ et $\\lambda_2$, des distributions _a priori_ gamma sur $\\alpha$ et $\\lambda$. Ceux-ci sont généralement choisis comme _non informatifs_. Par défaut, $\\alpha_1 = \\alpha_2 =  \\lambda_1 = \\lambda_2 = 10^{-6}$.\n",
    "\n",
    "La régression Ridge bayésienne est utilisée pour la régression :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BayesianRidge()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BayesianRidge</label><div class=\"sk-toggleable__content\"><pre>BayesianRidge()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BayesianRidge()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "X = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]]\n",
    "Y = [0., 1., 2., 3.]\n",
    "reg = linear_model.BayesianRidge()\n",
    "reg.fit(X, Y)\n",
    "# BayesianRidge("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir été ajusté, le modèle peut être utilisé pour prédire de nouvelles valeurs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50000013])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.predict([[1, 0.]])\n",
    "# array([0.50000013])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les coefficients $w$ du modèle peuvent être accédés :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49999993, 0.49999993])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_\n",
    "# array([0.49999993, 0.49999993]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En raison du cadre bayésien, les poids trouvés sont légèrement différents de ceux trouvés par les [**moindres carrés ordinaires (OLS)** (1.1.1)](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares). Cependant, la régression Ridge bayésienne est plus robuste aux problèmes mal posés.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Ajustement de courbe avec la Régression Ridge Bayésienne**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_bayesian_ridge_curvefit.ipynb)<br/>([_Curve Fitting with Bayesian Ridge Regression_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_bayesian_ridge_curvefit.html))\n",
    "\n",
    "Calcule une Régression Ridge Bayésienne de sinusoïdes.\n",
    "\n",
    "#### Références\n",
    "\n",
    "📚 Section 3.3 dans [**“Pattern recognition and machine learning”**](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf) de Christopher M. Bishop, 2006\n",
    "\n",
    "🔬 David J. C. MacKay, [**“Bayesian Interpolation”**](https://authors.library.caltech.edu/13792/1/MACnc92a.pdf), 1992.\n",
    "\n",
    "🔬 Michael E. Tipping, [**“Sparse Bayesian Learning and the Relevance Vector Machine”**](https://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf), 2001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='automatic-relevance-determination-ard'></a> 1.1.10.2. Détermination automatique de la pertinence (ARD)\n",
    "\n",
    "La détermination automatique de la pertinence (implémentée dans [**`ARDRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ARDRegression.html)) est un type de modèle linéaire très similaire à la [**régression Ridge bayésienne** (1.1.10.1)](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-ridge-regression), mais qui conduit à des coefficients plus parcimonieux[1] [2].\n",
    "\n",
    "[**`ARDRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ARDRegression.html) pose un _a priori_ différent sur $w$ : il abandonne la distribution gaussienne sphérique au profit d'une distribution gaussienne elliptique centrée. Cela signifie que chaque coefficient $w_i$ peut être lui-même tiré d'une distribution gaussienne, centrée sur zéro et avec une précision $\\lambda_{i}$ :\n",
    "\n",
    "$$p(w|\\lambda) = \\mathcal{N}(w|0,A^{-1})$$\n",
    "\n",
    "où $A$ est une matrice diagonale définie positive et $\\text{diag}(A) = \\lambda = \\{\\lambda_{1},...,\\lambda_{p}\\}$.\n",
    "\n",
    "Contrairement à la [**régression Ridge bayésienne** (1.1.10.1)](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-ridge-regression), chaque coordonnée de $w_i$ a sa propre déviation standard $\\frac{1}{\\lambda_i}$. L'_a priori_ sur toutes les $\\lambda_{i}$ est choisi comme étant la même distribution gamma donnée par les hyperparamètres $\\lambda_1$ et $\\lambda_2$.\n",
    "\n",
    "L'ARD est également connue dans la littérature sous le nom de _Sparse Bayesian Learning and Relevance Vector Machine_ [3] [4]. Pour une comparaison détaillée entre ARD et la [**régression Ridge bayésienne** (1.1.10.1)](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-ridge-regression), voir l'exemple ci-dessous.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Comparaison des régresseurs bayésiens linéaires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_ard.ipynb)<br/>([_Comparing Linear Bayesian Regressors_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ard.html))\n",
    "\n",
    "Compare une détermination automatique de la pertinence (ARD) et une régression Ridge bayésienne.\n",
    "\n",
    "#### Références\n",
    "\n",
    "📚 [1] Christopher M. Bishop: [**“Pattern recognition and machine learning”**](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf), Chapitre 7.2.1\n",
    "\n",
    "🔬 [2] David Wipf and Srikantan Nagarajan: [**“A New View of Automatic Relevance Determination”**](https://proceedings.neurips.cc/paper_files/paper/2007/file/9c01802ddb981e6bcfbec0f0516b8e35-Paper.pdf)\n",
    "\n",
    "🔬 [3] Michael E. Tipping, [**“Sparse Bayesian Learning and the Relevance Vector Machine”**](https://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf), 2001.\n",
    "\n",
    "🔬 [4] Tristan Fletcher: [**“Relevance Vector Machines Explained”**](http://www.di.fc.ul.pt/~jpn/r/PRML/chp7/Fletcher_RVM_Explained.pdf), 2010."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='logistic-regression'></a> 1.1.11. **Régression logistique**<br/>([*Logistic regression*](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression))\n",
    "\n",
    "The logistic regression is implemented in [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Despite its name, it is implemented as a linear model for classification rather than regression in terms of the scikit-learn/ML nomenclature. The logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a [**logistic function**](https://en.wikipedia.org/wiki/Logistic_function).\n",
    "\n",
    "This implementation can fit binary, One-vs-Rest, or multinomial logistic regression with optional $\\ell_1$, $\\ell_2$ or Elastic-Net regularization.\n",
    "\n",
    "> **Note: Regularization**\n",
    "> Regularization is applied by default, which is common in machine learning but not in statistics. Another advantage of regularization is that it improves numerical stability. No regularization amounts to setting `C` to a very high value.\n",
    "\n",
    "> **Note: Logistic Regression as a special case of the Generalized Linear Models (GLM)**\n",
    "> Logistic regression is a special case of [**Generalized Linear Models** (1.1.12)](https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-models) with a Binomial / Bernoulli conditional distribution and a Logit link. The numerical output of the logistic regression, which is the predicted probability, can be used as a classifier by applying a threshold (by default 0.5) to it. This is how it is implemented in scikit-learn, so it expects a categorical target, making the Logistic Regression a classifier.\n",
    "\n",
    "### <a id='binary-case'></a> 1.1.11.1. Binary Case\n",
    "\n",
    "For notational ease, we assume that the target $y_i$ takes values in the set $\\{0, 1\\}$ for data point $i$. Once fitted, the [**`predict_proba`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba) method of [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) predicts the probability of the positive class $P(y_i=1|X_i)$ as\n",
    "\n",
    "$$\\hat{p}(X_i) = \\operatorname{expit}(X_i w + w_0) = \\frac{1}{1 + \\exp(-X_i w - w_0)}.$$\n",
    "\n",
    "As an optimization problem, binary class logistic regression with regularization term $r(w)$ minimizes the following cost function:\n",
    "\n",
    "$$\\min_{w} C \\sum_{i=1}^n \\left(-y_i \\log(\\hat{p}(X_i)) - (1 - y_i) \\log(1 - \\hat{p}(X_i))\\right) + r(w).$$\n",
    "\n",
    "We currently provide four choices for the regularization term $r(w)$ via the `penalty` argument:\n",
    "\n",
    "|penalty|$r(w)$|\n",
    "|-|-|\n",
    "|`None`|$0$|\n",
    "|$\\ell_1$|$\\|\\|w\\|\\|_1$|\n",
    "|$\\ell_2$|$\\frac{1}{2}\\|\\|w\\|\\|_2^2 = \\frac{1}{2}w^\\top w$|\n",
    "|`ElasticNet`|$\\frac{1 - \\rho}{2}w^\\top w + \\rho \\|\\|w\\|\\|_1$|\n",
    "\n",
    "\n",
    "For ElasticNet, $\\rho$ (which corresponds to the `l1_ratio` parameter) controls the strength of $\\ell_1$ regularization vs. $\\ell_2$ regularization. Elastic-Net is equivalent to $\\ell_1$ when $\\rho=1$ and equivalent to $\\ell_2$ when $\\rho=0$.\n",
    "\n",
    "### <a id='multinomial-case'></a> 1.1.11.2. Multinomial Case\n",
    "\n",
    "The binary case can be extended to $K$ classes leading to the multinomial logistic regression, see also [**log-linear model**](https://en.wikipedia.org/wiki/Multinomial_logistic_regression#As_a_log-linear_model).\n",
    "\n",
    "> **Note:** It is possible to parameterize a $K$-class classification model using only $K-1$ weight vectors, leaving one class probability fully determined by the other class probabilities by leveraging the fact that all class probabilities must sum to one. We deliberately choose to overparameterize the model using $K$ weight vectors for ease of implementation and to preserve the symmetrical inductive bias regarding ordering of classes, see [16]. This effect becomes especially important when using regularization. The choice of overparameterization can be detrimental for unpenalized models since then the solution may not be unique, as shown in [16].\n",
    "\n",
    "Let $y_i \\in {1, \\ldots, K}$ be the label (ordinal) encoded target variable for observation $i$. Instead of a single coefficient vector, we now have a matrix of coefficients $W$ where each row vector $W_k$ corresponds to class $k$. We aim at predicting the class probabilities $P(y_i=k|X_i)$ via [**`predict_proba`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) as:\n",
    "\n",
    "$$\\hat{p}_k(X_i) = \\frac{\\exp(X_i W_k + W_{0, k})}{\\sum_{l=0}^{K-1} \\exp(X_i W_l + W_{0, l})}.$$\n",
    "\n",
    "The objective for the optimization becomes\n",
    "\n",
    "$$\\min_W -C \\sum_{i=1}^n \\sum_{k=0}^{K-1} [y_i = k] \\log(\\hat{p}_k(X_i)) + r(W).$$\n",
    "\n",
    "Where $[P]$ represents the Iverson bracket which evaluates to $0$ if $P$ is false, otherwise it evaluates to $1$. We currently provide four choices for the regularization term $r(W)$ via the `penalty` argument:\n",
    "\n",
    "|penalty|$r(W)$|\n",
    "|-|-|\n",
    "|`None`|$0$|\n",
    "|$\\ell_1$|$\\|\\|W\\|\\|_{1,1} = \\sum_{i=1}^n\\sum_{j=1}^{K}\\|W_{i,j}\\|$|\n",
    "|$\\ell_2$|$\\frac{1}{2}\\|\\|W\\|\\|_F^2 = \\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^{K} W_{i,j}^2$|\n",
    "|`ElasticNet`|$\\frac{1 - \\rho}{2}\\|\\|W\\|\\|_F^2 + \\rho \\|\\|W\\|\\|_{1,1}$|\t\n",
    "\n",
    "### <a id='solvers'></a> 1.1.11.3. Solvers\n",
    "\n",
    "The solvers implemented in the class [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) are “lbfgs”, “liblinear”, “newton-cg”, “newton-cholesky”, “sag” and “saga”:\n",
    "\n",
    "The solver “liblinear” uses a coordinate descent (CD) algorithm, and relies on the excellent C++ [LIBLINEAR library](https://www.csie.ntu.edu.tw/~cjlin/liblinear/), which is shipped with scikit-learn. However, the CD algorithm implemented in liblinear cannot learn a true multinomial (multiclass) model; instead, the optimization problem is decomposed in a “one-vs-rest” fashion so separate binary classifiers are trained for all classes. This happens under the hood, so [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) instances using this solver behave as multiclass classifiers. For $\\ell_1$ regularization [**`sklearn.svm.l1_min_c`**](https://scikit-learn.org/stable/modules/generated/sklearn.svm.l1_min_c.html) allows to calculate the lower bound for C in order to get a non “null” (all feature weights to zero) model.\n",
    "\n",
    "The “lbfgs”, “newton-cg” and “sag” solvers only support $\\ell_2$ regularization or no regularization, and are found to converge faster for some high-dimensional data. Setting `multi_class` to “multinomial” with these solvers learns a true multinomial logistic regression model [5], which means that its probability estimates should be better calibrated than the default “one-vs-rest” setting.\n",
    "\n",
    "The “sag” solver uses Stochastic Average Gradient descent [6]. It is faster than other solvers for large datasets, when both the number of samples and the number of features are large.\n",
    "\n",
    "The “saga” solver [7] is a variant of “sag” that also supports the non-smooth `penalty=\"l1\"`. This is therefore the solver of choice for sparse multinomial logistic regression. It is also the only solver that supports `penalty=\"elasticnet\"`.\n",
    "\n",
    "The “lbfgs” is an optimization algorithm that approximates the Broyden–Fletcher–Goldfarb–Shanno algorithm [8], which belongs to quasi-Newton methods. As such, it can deal with a wide range of different training data and is therefore the default solver. Its performance, however, suffers on poorly scaled datasets and on datasets with one-hot encoded categorical features with rare categories.\n",
    "\n",
    "The “newton-cholesky” solver is an exact Newton solver that calculates the hessian matrix and solves the resulting linear system. It is a very good choice for `n_samples` >> `n_features`, but has a few shortcomings: Only $\\ell_2$ regularization is supported. Furthermore, because the hessian matrix is explicitly computed, the memory usage has a quadratic dependency on `n_features` as well as on `n_classes`. As a consequence, only the one-vs-rest scheme is implemented for the multiclass case.\n",
    "\n",
    "For a comparison of some of these solvers, see [9].\n",
    "\n",
    "The following table summarizes the penalties supported by each solver:\n",
    "\n",
    "| Penalties                    | ‘lbfgs’ | ‘liblinear’ | ‘newton-cg’ | ‘newton-cholesky’ | ‘sag’ | ‘saga’ |\n",
    "|------------------------------|---------|-------------|-------------|-------------------|-------|--------|\n",
    "| Multinomial + L2 penalty     | yes     | no          | yes         | no                | yes   | yes    |\n",
    "| OVR + L2 penalty             | yes     | yes         | yes         | yes               | yes   | yes    |\n",
    "| Multinomial + L1 penalty     | no      | no          | no          | no                | no    | yes    |\n",
    "| OVR + L1 penalty             | no      | yes         | no          | no                | no    | yes    |\n",
    "| Elastic-Net                  | no      | no          | no          | no                | no    | yes    |\n",
    "| No penalty (‘none’)          | yes     | no          | yes         | yes               | yes   | yes    |\n",
    "| **Behaviors**                |         |             |             |                   |       |        |\n",
    "| Penalize the intercept (bad) | no      | yes         | no          | no                | no    | no     |\n",
    "| Faster for large datasets    | no      | no          | no          | no                | yes   | yes    |\n",
    "| Robust to unscaled datasets  | yes     | yes         | yes         | yes               | no    | no     |\n",
    "\n",
    "The “lbfgs” solver is used by default for its robustness. For large datasets the “saga” solver is usually faster. For large dataset, you may also consider using [**`SGDClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) with `loss=\"log_loss\"`, which might be even faster but requires more tuning.\n",
    "\n",
    "#### Differences from liblinear\n",
    "\n",
    "There might be a difference in the scores obtained between [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) with `solver=liblinear` or `LinearSVC` and the external liblinear library directly, when `fit_intercept=False` and the fit `coef_` (or) the data to be predicted are zeroes. This is because for the sample(s) with `decision_function` zero, [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) and `LinearSVC` predict the negative class, while liblinear predicts the positive class. Note that a model with `fit_intercept=False` and having many samples with `decision_function` zero, is likely to be a underfit, bad model and you are advised to set `fit_intercept=True` and increase the `intercept_scaling`.\n",
    "\n",
    "> **Note: Feature selection with sparse logistic regression**\n",
    "> A logistic regression with $\\ell_1$ penalty yields sparse models, and can thus be used to perform feature selection, as detailed in example **L1-based feature selection**.\n",
    "\n",
    "> **Note: P-value estimation**\n",
    "> It is possible to obtain the p-values and confidence intervals for coefficients in cases of regression without penalization. The [statsmodels package](https://pypi.org/project/statsmodels/) natively supports this. Within sklearn, one could use bootstrapping instead as well.\n",
    "\n",
    "[**`LogisticRegressionCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html) implements Logistic Regression with built-in cross-validation support, to find the optimal `C` and `l1_ratio` parameters according to the `scoring` attribute. The “newton-cg”, “sag”, “saga” and “lbfgs” solvers are found to be faster for high-dimensional dense data, due to warm-starting (see [**Glossary/`warm_start`**](https://scikit-learn.org/stable/glossary.html#term-warm_start)).\n",
    "\n",
    "#### Examples\n",
    "\n",
    "##### [**L1 Penalty and Sparsity in Logistic Regression**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_logistic_l1_l2_sparsity.ipynb)<br/>([_L1 Penalty and Sparsity in Logistic Regression_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_l1_l2_sparsity.html))\n",
    "\n",
    "Comparison of the sparsity (percentage of zero coefficients) of solutions when L1, L2 and Elastic-Net penalty are used for different values of C. We can see that large values of C give more freedom to the model. Conversely, smaller values of C constrain the model more. In the L1 penalty case, this leads to sparser solutions. As expected, the Elastic-Net penalty sparsity is between that of L1 and L2.\n",
    "\n",
    "We classify 8x8 images of digits into two classes: 0-4 against 5-9. The visualization shows coefficients of the models for varying C.\n",
    "\n",
    "##### [**Regularization path of $\\ell_1$- Logistic Regression**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_logistic_path.ipynb)<br/>([_Regularization path of $\\ell_1$- Logistic Regression_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_path.html))\n",
    "\n",
    "Train $\\ell_1$-penalized logistic regression models on a binary classification problem derived from the Iris dataset.\n",
    "\n",
    "The models are ordered from strongest regularized to least regularized. The 4 coefficients of the models are collected and plotted as a “regularization path”: on the left-hand side of the figure (strong regularizers), all the coefficients are exactly 0. When regularization gets progressively looser, coefficients can get non-zero values one after the other.\n",
    "\n",
    "Here we choose the `liblinear` solver because it can efficiently optimize for the Logistic Regression loss with a non-smooth, sparsity inducing $\\ell_1$ penalty.\n",
    "\n",
    "Also note that we set a low value for the tolerance to make sure that the model has converged before collecting the coefficients.\n",
    "\n",
    "We also use `warm_start=True` which means that the coefficients of the models are reused to initialize the next model fit to speed-up the computation of the full-path.\n",
    "\n",
    "##### [**Plot multinomial and One-vs-Rest Logistic Regression**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_logistic_multinomial.ipynb)<br/>([_Plot multinomial and One-vs-Rest Logistic Regression_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_multinomial.html))\n",
    "\n",
    "Plot decision surface of multinomial and One-vs-Rest Logistic Regression. The hyperplanes corresponding to the three One-vs-Rest (OVR) classifiers are represented by the dashed lines.\n",
    "\n",
    "##### [**Multiclass sparse logistic regression on 20newgroups**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_sparse_logistic_regression_20newsgroups.ipynb)<br/>([_Multiclass sparse logistic regression on 20newgroups_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_20newsgroups.html))\n",
    "\n",
    "Comparison of multinomial logistic $\\ell_1$ vs one-versus-rest $\\ell_1$ logistic regression to classify documents from the newgroups20 dataset. Multinomial logistic regression yields more accurate results and is faster to train on the larger scale dataset.\n",
    "\n",
    "Here we use the $\\ell_1$ sparsity that trims the weights of not informative features to zero. This is good if the goal is to extract the strongly discriminative vocabulary of each class. If the goal is to get the best predictive accuracy, it is better to use the non sparsity-inducing $\\ell_2$ penalty instead.\n",
    "\n",
    "A more traditional (and possibly better) way to predict on a sparse subset of input features would be to use univariate feature selection followed by a traditional ($\\ell_2$-penalised) logistic regression model.\n",
    "\n",
    "##### [**MNIST classification using multinomial logistic + $\\ell_1$**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_sparse_logistic_regression_mnist.ipynb)<br/>([_MNIST classification using multinomial logistic +_ $\\ell_1$](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html))\n",
    "\n",
    "Here we fit a multinomial logistic regression with $\\ell_1$ penalty on a subset of the MNIST digits classification task. We use the SAGA algorithm for this purpose: this a solver that is fast when the number of samples is significantly larger than the number of features and is able to finely optimize non-smooth objective functions which is the case with the $\\ell_1$-penalty. Test accuracy reaches > 0.8, while weight vectors remains sparse and therefore more easily _interpretable_.\n",
    "\n",
    "Note that this accuracy of this $\\ell_1$-penalized linear model is significantly below what can be reached by an $\\ell_2$-penalized linear model or a non-linear multi-layer perceptron model on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='logistic-regression'></a> 1.1.11. **Régression logistique**<br/>([*Logistic regression*](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression))\n",
    "\n",
    "La régression logistique est implémentée dans [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Malgré son nom, en termes de la nomenclature scikit-learn/ML, elle est mise en œuvre en tant que modèle linéaire pour la classification plutôt que pour la régression. La régression logistique est également connue dans la littérature sous le nom de régression _logit_, classification d'entropie maximale (MaxEnt) ou classifieur log-linéaire. Dans ce modèle, les probabilités décrivant les résultats possibles d'un seul essai sont modélisées à l'aide d'une [**fonction logistique**](https://en.wikipedia.org/wiki/Logistic_function).\n",
    "\n",
    "Cette implémentation peut ajuster une régression logistique binaire, un-contre-tous (OVR), ou multinomiale avec une régularisation optionnelle $\\ell_1$, $\\ell_2$ ou Elastic-Net.\n",
    "\n",
    "> **Remarque : Régularisation**\n",
    "> La régularisation est appliquée par défaut, ce qui est courant en apprentissage automatique mais pas en statistiques. Un autre avantage de la régularisation est qu'elle améliore la stabilité numérique. L'absence de régularisation revient à fixer `C` à une valeur très élevée.\n",
    "\n",
    "> **Remarque : Régression logistique comme cas spécial des modèles linéaires généralisés (GLM)**\n",
    "> La régression logistique est un cas spécial des [**modèles linéaires généralisés (GLM)** (1.1.12)](https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-models) avec une distribution conditionnelle Binomiale / Bernoulli et une liaison Logit. La sortie numérique de la régression logistique, qui est la probabilité prédite, peut être utilisée comme un classifieur en lui appliquant un seuil (par défaut 0.5). C'est ainsi qu'elle est implémentée dans scikit-learn, de sorte qu'elle s'attend à une cible catégorielle, transformant ainsi la régression logistique en classifieur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='binary-case'></a> 1.1.11.1. Cas binaire\n",
    "\n",
    "Par commodité de notation, nous supposons que la cible $y_i$ prend ses valeurs dans l'ensemble $\\{0, 1\\}$ pour le point de données $i$. Une fois ajustée, la méthode [**`predict_proba`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba) de [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) prédit la probabilité de la classe positive $P(y_i=1|X_i)$ comme suit :\n",
    "\n",
    "$$\\hat{p}(X_i) = \\operatorname{expit}(X_i w + w_0) = \\frac{1}{1 + \\exp(-X_i w - w_0)}.$$\n",
    "\n",
    "En tant que problème d'optimisation, la régression logistique binaire avec terme de régularisation $r(w)$ minimise la fonction de coût suivante :\n",
    "\n",
    "$$\\min_{w} C \\sum_{i=1}^n \\left(-y_i \\log(\\hat{p}(X_i)) - (1 - y_i) \\log(1 - \\hat{p}(X_i))\\right) + r(w).$$\n",
    "\n",
    "Nous fournissons actuellement quatre choix pour le terme de régularisation $r(w)$ via l'argument `penalty` :\n",
    "\n",
    "|penalty|$r(w)$|\n",
    "|-|-|\n",
    "|`None`|$0$|\n",
    "|$\\ell_1$|$\\|\\|w\\|\\|_1$|\n",
    "|$\\ell_2$|$\\frac{1}{2}\\|\\|w\\|\\|_2^2 = \\frac{1}{2}w^\\top w$|\n",
    "|`ElasticNet`|$\\frac{1 - \\rho}{2}w^\\top w + \\rho \\|\\|w\\|\\|_1$|\n",
    "\n",
    "Pour ElasticNet, $\\rho$ (qui correspond au paramètre `l1_ratio`) contrôle la force raltive de la régularisation $\\ell_1$ par rapport à celle de la régularisation $\\ell_2$. Elastic-Net est équivalent à $\\ell_1$ lorsque $\\rho=1$ et à $\\ell_2$ lorsque $\\rho=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='multinomial-case'></a> 1.1.11.2. Cas multinomial\n",
    "\n",
    "Le cas binaire peut être étendu à $K$ classes, conduisant à la régression logistique multinomiale, voir aussi [**modèle linéaire logarithmique**](https://en.wikipedia.org/wiki/Multinomial_logistic_regression#As_a_log-linear_model).\n",
    "\n",
    "> **Remarque :** Il est possible de paramétrer un modèle de classification à $K$ classes en utilisant uniquement $K-1$ vecteurs de poids, laissant une probabilité de classe entièrement déterminée par les probabilités des autres classes en tirant parti du fait que la somme de toutes les probabilités de classe vaut $1$. Nous choisissons délibérément de sur-paramétrer le modèle en utilisant $K$ vecteurs de poids pour faciliter la mise en œuvre et préserver le biais inductif symétrique concernant l'ordre des classes. Cet effet devient particulièrement important lors de l'utilisation de la régularisation. Le choix du sur-paramétrage peut être préjudiciable pour les modèles non pénalisés, car dans ce cas, comme le montre [16], la solution peut ne pas être unique.\n",
    "\n",
    "Soit $y_i \\in {1, \\ldots, K}$ l'étiquette (ordinale) encodée comme variable cible pour l'observation $i$. Au lieu d'un unique vecteur de coefficient, nous avons à présent une matrice de coefficients $W$ où chaque vecteur de ligne $W_k$ correspond à la classe $k$. Nous cherchons à prédire les probabilités de classe $P(y_i=k|X_i)$ via [**`predict_proba`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) comme suit :\n",
    "\n",
    "$$\\hat{p}_k(X_i) = \\frac{\\exp(X_i W_k + W_{0, k})}{\\sum_{l=0}^{K-1} \\exp(X_i W_l + W_{0, l})}.$$\n",
    "\n",
    "L'objectif de l'optimisation devient\n",
    "\n",
    "$$\\min_W -C \\sum_{i=1}^n \\sum_{k=0}^{K-1} [y_i = k] \\log(\\hat{p}_k(X_i)) + r(W).$$\n",
    "\n",
    "Où $[P]$ représente la notation d'Iverson qui évalue à $0$ si $P$ est faux, sinon à $1$. Nous fournissons actuellement quatre choix pour le terme de régularisation $r(W)$ via l'argument `penalty` :\n",
    "\n",
    "|penalty|$r(W)$|\n",
    "|-|-|\n",
    "|`None`|$0$|\n",
    "|$\\ell_1$|$\\|\\|W\\|\\|_{1,1} = \\sum_{i=1}^n\\sum_{j=1}^{K}\\|W_{i,j}\\|$|\n",
    "|$\\ell_2$|$\\frac{1}{2}\\|\\|W\\|\\|_F^2 = \\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^{K} W_{i,j}^2$|\n",
    "|`ElasticNet`|$\\frac{1 - \\rho}{2}\\|\\|W\\|\\|_F^2 + \\rho \\|\\|W\\|\\|_{1,1}$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='solvers'></a> 1.1.11.3. Solveurs\n",
    "\n",
    "Les solveurs implémentés dans la classe [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) sont \"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\" et \"saga\" :\n",
    "\n",
    "Le solveur \"liblinear\" utilise un algorithme de descente de coordonnées (CD) et s'appuie sur l'excellente bibliothèque C++ [LIBLINEAR](https://www.csie.ntu.edu.tw/~cjlin/liblinear/), qui est incluse avec scikit-learn. Cependant, l'algorithme de CD implémenté dans liblinear ne peut pas apprendre un modèle multinomial (multiclasse) véritable ; à la place, le problème d'optimisation est décomposé en \"un-contre-tous\" (OCR), de sorte que des classifieurs binaires distincts sont entraînés pour toutes les classes. Cela se produit en interne, donc les instances [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) utilisant ce solveur se comportent comme des classifieurs multiclasse. Pour la régularisation $\\ell_1$, [**`sklearn.svm.l1_min_c`**](https://scikit-learn.org/stable/modules/generated/sklearn.svm.l1_min_c.html) permet de calculer la borne inférieure pour `C` afin d'obtenir un modèle non \"nul\" (tous les poids des caractéristiques à zéro).\n",
    "\n",
    "Les solveurs \"lbfgs\", \"newton-cg\" et \"sag\" prennent uniquement en charge la régularisation $\\ell_2$ ou l'absence de régularisation, et sont plus rapides à converger pour certaines données de grande dimension. En définissant `multi_class` sur \"multinomial\" avec ces solveurs, un véritable modèle de régression logistique multinomiale est appris [5], ce qui signifie que ses estimations de probabilité devraient être mieux calibrées que le paramètre par défaut \"un-contre-tous\".\n",
    "\n",
    "Le solveur \"sag\" utilise la descente du gradient moyen stochastique (SAG - _Stochastic Average Gradient_) [6]. Il est plus rapide que d'autres solveurs pour les grands ensembles de données, lorsque le nombre d'échantillons et le nombre de caractéristiques sont grands.\n",
    "\n",
    "Le solveur \"saga\" [7] est une variante de \"sag\" qui prend également en charge la pénalité non lisse `penalty=\"l1\"`. C'est donc le solveur de choix pour la régression logistique multinomiale clairsemée. C'est également le seul solveur qui prend en charge `penalty=\"elasticnet\"`.\n",
    "\n",
    "Le solveur \"lbfgs\" est un algorithme d'optimisation qui approxime l'algorithme de Broyden–Fletcher–Goldfarb–Shanno [8], qui appartient aux méthodes quasi-Newton. En tant que tel, il peut traiter une large gamme de données d'entraînement différentes et est donc le solveur par défaut. Cependant, ses performances sont médiocres sur les ensembles de données mal mis à l'échelle et sur les ensembles de données avec des caractéristiques catégorielles encodées en one-hot avec des catégories rares.\n",
    "\n",
    "Le solveur \"newton-cholesky\" est un solveur de Newton exact qui calcule la matrice hessienne et résout le système linéaire résultant. C'est un très bon choix pour `n_samples` >> `n_features`, mais il présente quelques lacunes : Seule la régularisation $\\ell_2$ est prise en charge. De plus, comme la matrice hessienne est calculée explicitement, l'utilisation de la mémoire dépend quadratiquement de `n_features` ainsi que de `n_classes`. Par conséquent, seul le schéma un-contre-tous (OVR) est implémenté pour le cas multiclasse.\n",
    "\n",
    "Pour une comparaison de certains de ces solveurs, voir [9].\n",
    "\n",
    "Le tableau suivant résume les pénalités prises en charge par chaque solveur :\n",
    "\n",
    "| Pénalités                    | ‘lbfgs’ | ‘liblinear’ | ‘newton-cg’ | ‘newton-cholesky’ | ‘sag’ | ‘saga’ |\n",
    "|------------------------------|---------|-------------|-------------|-------------------|-------|--------|\n",
    "| Multinomiale + pénalité L2     | oui     | non          | oui         | non                | oui   | oui    |\n",
    "| OVR + pénalité L2             | oui     | oui         | oui         | oui               | oui   | oui    |\n",
    "| Multinomiale + pénalité L1     | non      | non          | non          | non                | non    | oui    |\n",
    "| OVR + pénalité L1             | non      | oui         | non          | non                | non    | oui    |\n",
    "| Elastic-Net                  | non      | non          | non          | non                | non    | oui    |\n",
    "| Pas de pénalité (‘none’)          | oui     | non          | oui         | oui               | oui   | oui    |\n",
    "| **Comportements**                |         |             |             |                   |       |        |\n",
    "| Pénaliser l'intercept (mauvais) | non      | oui         | non          | non                | non    | non     |\n",
    "| Plus rapide pour les grands ensembles de données    | non      | non          | non          | non                | oui   | oui    |\n",
    "| Robuste aux ensembles de données non mis à l'échelle  | oui     | oui         | oui         | oui               | non    | non     |\n",
    "\n",
    "Le solveur \"lbfgs\" est utilisé par défaut pour sa robustesse. Pour les grands ensembles de données, le solveur \"saga\" est généralement plus rapide. Pour les grands ensembles de données, vous pouvez également envisager d'utiliser [**`SGDClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) avec `loss=\"log_loss\"`, ce qui pourrait être encore plus rapide mais nécessite un réglage plus fin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Différences par rapport à liblinear\n",
    "\n",
    "Il peut y avoir une différence dans les scores obtenus entre [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) avec `solver=liblinear` ou `LinearSVC` et la bibliothèque externe liblinear directement, lorsque `fit_intercept=False` et le `coef_` d'ajustement (ou) les données à prédire sont nulles. Cela est dû au fait que pour l'échantillon (ou les échantillons) avec une fonction de décision nulle, [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) et `LinearSVC` prédisent la classe négative, tandis que liblinear prévoit la classe positive. Notez qu'un modèle avec `fit_intercept=False` et ayant de nombreux échantillons avec une fonction de décision nulle est susceptible d'être un modèle mal ajusté et vous êtes invité à définir `fit_intercept=True` et à augmenter `intercept_scaling`.\n",
    "\n",
    "> **Note : Sélection de caractéristiques avec la régression logistique parcimonieuse**\n",
    "> Une régression logistique avec une pénalité $\\ell_1$ produit des modèles parcimonieux et peut donc être utilisée pour effectuer une sélection de caractéristiques, comme détaillé dans [**Sélection de caractéristiques basée sur L1** (1.13.4.1)](https://scikit-learn.org/stable/modules/feature_selection.html#l1-feature-selection).\n",
    "\n",
    "> **Note : Estimation de la valeur p**\n",
    "> Il est possible d'obtenir les valeurs p et les intervalles de confiance pour les coefficients dans les cas de régression sans pénalité. La bibliothèque [statsmodels](https://pypi.org/project/statsmodels/) prend en charge cela de manière native. Dans scikit-learn, on pourrait également utiliser la méthode de bootstrap.\n",
    "\n",
    "[**`LogisticRegressionCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html) implémente la régression logistique avec une prise en charge intégrée de la validation croisée, pour trouver les paramètres optimaux `C` et `l1_ratio` en fonction de l'attribut `scoring`. Les solveurs \"newton-cg\", \"sag\", \"saga\" et \"lbfgs\" sont plus rapides pour les données denses de grande dimension, en raison du démarrage à chaud (voir [**Glossaire/`warm_start`**](https://scikit-learn.org/stable/glossary.html#term-warm_start))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemples\n",
    "\n",
    "##### [**Pénalité $\\ell_1$ et parcimonie en régression logistique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_logistic_l1_l2_sparsity.ipynb)<br/>([$\\ell_1$ _Penalty and Sparsity in Logistic Regression_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_l1_l2_sparsity.html))\n",
    "\n",
    "Comparaison de la parcimonie (pourcentage de coefficients nuls) des solutions lorsque les pénalités $\\ell_1$, $\\ell_2$ et Elastic-Net sont utilisées pour différentes valeurs de `C`.\n",
    "\n",
    "##### [**Chemin de régularisation de la régression logistique $\\ell_1$**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_logistic_path.ipynb)<br/>([_Regularization path of $\\ell_1$- Logistic Regression_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_path.html))\n",
    "\n",
    "Modèles de régression logistique pénalisée $\\ell_1$ pour un problème de classification binaire dérivé de l'ensemble de données Iris.\n",
    "\n",
    "##### [**Tracer la régression logistique multinomiale et un contre tous (OVR)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_logistic_multinomial.ipynb)<br/>([_Plot multinomial and One-vs-Rest Logistic Regression_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_multinomial.html))\n",
    "\n",
    "Tracé de la surface de décision de la régression logistique multinomiale et un contre tous. Les hyperplans correspondant aux trois classificateurs un contre tous (OvR) sont représentés par les lignes en pointillé.\n",
    "\n",
    "##### [**Régression logistique multiclasse parcimonieuse sur 20newgroups**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_sparse_logistic_regression_20newsgroups.ipynb)<br/>([_Multiclass sparse logistic regression on 20newgroups_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_20newsgroups.html))\n",
    "\n",
    "Comparaison de la régression logistique multinomiale $\\ell_1$ et de la régression logistique $\\ell_1$ un contre tous (OVR) pour classer les documents de l'ensemble de données newsgroups20.\n",
    "\n",
    "##### [**Classification MNIST en utilisant la régression logistique multinomiale avec $\\ell_1$**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_sparse_logistic_regression_mnist.ipynb)<br/>([_MNIST classification using multinomial logistic +_ $\\ell_1$](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html))\n",
    "\n",
    "Ajustement d'une régression logistique multinomiale avec une pénalité $\\ell_1$ sur un sous-ensemble de la tâche de classification des chiffres MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "📚 [5] Christopher M. Bishop: [**“Pattern recognition and machine learning”**](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf), Chapter 4.3.4.\n",
    "\n",
    "📚 [6] Mark Schmidt, Nicolas Le Roux, and Francis Bach: [**“Minimizing Finite Sums with the Stochastic Average Gradient”**](https://inria.hal.science/hal-00860051/document).\n",
    "\n",
    "🔬 [7] Aaron Defazio, Francis Bach, Simon Lacoste-Julien: [**“SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives”**](https://arxiv.org/pdf/1407.0202.pdf).\n",
    "\n",
    "🌐 [8] Wikipedia entry for [**Broyden–Fletcher–Goldfarb–Shanno algorithm**](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm)\n",
    "\n",
    "🔬 [9] Thomas P. Minka [**“A comparison of numerical optimizers for logistic regression”**](https://tminka.github.io/papers/logreg/minka-logreg.pdf).\n",
    "\n",
    "🔬 [16] (1,2) Simon, Noah, J. Friedman and T. Hastie. [**“A Blockwise Descent Algorithm for Group-penalized Multiresponse and Multinomial Regression”**](https://arxiv.org/pdf/1311.6529.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='generalized-linear-models'></a> 1.1.12. **Modèles linéaires généralisés (GLM)**<br/>([*Generalized Linear Models*](https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='stochastic-gradient-descent-sgd'></a> 1.1.13. **Descente de gradient stochastique (SGD)**<br/>([*Stochastic Gradient Descent - SGD*](https://scikit-learn.org/stable/modules/linear_model.html#stochastic-gradient-descent-sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='perceptron'></a> 1.1.14. **Perceptron**<br/>([*Perceptron*](https://scikit-learn.org/stable/modules/linear_model.html#perceptron))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='passive-aggressive-algorithms'></a> 1.1.15. **Algorithmes passifs agressifs (PAA)**<br/>([*Passive Aggressive Algorithms*](https://scikit-learn.org/stable/modules/linear_model.html#passive-aggressive-algorithms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='robustness-regression-outliers-and-modeling-errors'></a> 1.1.16. **Régression robuste : valeurs aberrantes et erreurs de modélisation**<br/>([*Robustness regression: outliers and modeling errors*](https://scikit-learn.org/stable/modules/linear_model.html#robustness-regression-outliers-and-modeling-errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='quantile-regression'></a> 1.1.17. **Régression quantile**<br/>([*Quantile Regression*](https://scikit-learn.org/stable/modules/linear_model.html#quantile-regression))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='polynomial-regression-extending-linear-models-with-basis-functions'></a> 1.1.18. **Régression polynomiale : extension des modèles linéaires avec des fonctions de base**<br/>([*Polynomial regression: extending linear models with basis functions*](https://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e03b612d84ba21ce95ed447e81b3062e1eb99b56c6d885cdab4aaa12f1b8e240"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
