{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='supervised-learning'></a> 1. [**Apprentissage supervisé**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#supervised-learning)</br>([*Supervised learning*](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning))\n",
    "\n",
    "# <a id='gaussian-processes'></a> 1.7. [**Processus gaussiens (GP)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_7_gaussian_process.ipynb)<br/>([_Gaussian Processes_](https://scikit-learn.org/stable/modules/gaussian_process.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 19 pages, 4 exemples, 2 papiers\n",
    "- 1.7.1. [**Régression par processus gaussien (GPR)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_7_gaussian_process.ipynb#gaussian-process-regression-gpr)<br/>([_Gaussian Process Regression (GPR)_](https://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process-regression-gpr))\n",
    "- 1.7.2. [**Classification par processus gaussien (GPC)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_7_gaussian_process.ipynb#gaussian-process-classification-gpc)<br/>([_Gaussian Process Classification (GPC)_](https://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process-classification-gpc))\n",
    "- 1.7.3. [**Exemples de GPC**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_7_gaussian_process.ipynb#gpc-examples)<br/>([_GPC examples_](https://scikit-learn.org/stable/modules/gaussian_process.html#gpc-examples))\n",
    "    - 1.7.3.1. [**Prédictions probabilistes avec GPC**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_7_gaussian_process.ipynb#probabilistic-predictions-with-gpc)<br/>([_Probabilistic predictions with GPC_](https://scikit-learn.org/stable/modules/gaussian_process.html#probabilistic-predictions-with-gpc))\n",
    "    - 1.7.3.2. [**Illustration de GPC sur l'ensemble de données XOR**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_7_gaussian_process.ipynb#illustration-of-gpc-on-the-xor-dataset)<br/>([_Illustration of GPC on the XOR dataset_](https://scikit-learn.org/stable/modules/gaussian_process.html#illustration-of-gpc-on-the-xor-dataset))\n",
    "    - 1.7.3.3. [**Classification par processus gaussien (GPC) sur l'ensemble de données iris**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_7_gaussian_process.ipynb#gaussian-process-classification-gpc-on-iris-dataset)<br/>([_Gaussian process classification (GPC) on iris dataset_](https://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process-classification-gpc-on-iris-dataset))\n",
    "- 1.7.4. [**Noyaux pour les processus gaussiens**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_7_gaussian_process.ipynb#kernels-for-gaussian-processes)<br/>([_Kernels for Gaussian Processes_](https://scikit-learn.org/stable/modules/gaussian_process.html#kernels-for-gaussian-processes))\n",
    "    - 1.7.4.1. [**API des noyaux pour les processus gaussiens**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_7_gaussian_process.ipynb#gaussian-process-kernel-api)<br/>([_Gaussian Process Kernel API_](https://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process-kernel-api))\n",
    "    - 1.7.4.2. [**Noyaux de base**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_7_gaussian_process.ipynb#basic-kernels)<br/>([_Basic kernels_](https://scikit-learn.org/stable/modules/gaussian_process.html#basic-kernels))\n",
    "    - 1.7.4.3. [**Opérateurs de noyaux**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_7_gaussian_process.ipynb#kernel-operators)<br/>([_Kernel operators_](https://scikit-learn.org/stable/modules/gaussian_process.html#kernel-operators))\n",
    "    - 1.7.4.4. [**Noyau de fonction de base radiale (RBF)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_7_gaussian_process.ipynb#radial-basis-function-rbf-kernel)<br/>([_Radial basis function (RBF) kernel_](https://scikit-learn.org/stable/modules/gaussian_process.html#radial-basis-function-rbf-kernel))\n",
    "    - 1.7.4.5. [**Noyau Matérn**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_7_gaussian_process.ipynb#matern-kernel)<br/>([_Matérn kernel_](https://scikit-learn.org/stable/modules/gaussian_process.html#matern-kernel))\n",
    "    - 1.7.4.6. [**Noyau rationnel quadratique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_7_gaussian_process.ipynb#rational-quadratic-kernel)<br/>([_Rational quadratic kernel_](https://scikit-learn.org/stable/modules/gaussian_process.html#rational-quadratic-kernel))\n",
    "    - 1.7.4.7. [**Noyau exponentiel sinusoidal**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_7_gaussian_process.ipynb#exp-sine-squared-kernel)<br/>([_Exp-Sine-Squared kernel_](https://scikit-learn.org/stable/modules/gaussian_process.html#exp-sine-squared-kernel))\n",
    "    - 1.7.4.8. [**Noyau de produit scalaire**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_7_gaussian_process.ipynb#dot-product-kernel)<br/>([_Dot-Product kernel_](https://scikit-learn.org/stable/modules/gaussian_process.html#dot-product-kernel))\n",
    "    - 1.7.4.9. [**Références**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_7_gaussian_process.ipynb#references)<br/>([_References_](https://scikit-learn.org/stable/modules/gaussian_process.html#references))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='gaussian-processes'></a> 1.7. **Processus gaussiens (GP)**<br/>([_Gaussian Processes_](https://scikit-learn.org/stable/modules/gaussian_process.html))\n",
    "\n",
    "**Les processus gaussiens (GP)** sont une méthode d'apprentissage supervisée non paramétrique utilisée pour résoudre des problèmes de _régression_ et de _classification probabiliste_.\n",
    "\n",
    "Les avantages des processus gaussiens sont les suivants :\n",
    "- Les prédictions interpolent les observations (du moins pour les noyaux réguliers).\n",
    "- Les prédictions sont probabilistes (gaussiennes), ce qui permet de calculer des intervalles de confiance empiriques et de décider en fonction de ceux-ci s'il est nécessaire de refaire l'ajustement (ajustement en ligne, ajustement adaptatif) de la prédiction dans certaines régions d'intérêt.\n",
    "- Polyvalence : différents [**noyaux** (1.7.4)](#gp-kernels) peuvent être spécifiés. Des noyaux communs sont fournis, mais il est également possible de spécifier des noyaux personnalisés.\n",
    "\n",
    "Les inconvénients des processus gaussiens incluent :\n",
    "- Notre implémentation n'est pas parcimonieuse, c'est-à-dire qu'elle utilise l'ensemble des informations des échantillons/caractéristiques pour effectuer la prédiction.\n",
    "- Ils perdent en efficacité dans des espaces de grande dimension, notamment lorsque le nombre de caractéristiques dépasse quelques dizaines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='gaussian-process-regression-gpr'></a> 1.7.1. [**Régression par processus gaussien (GPR)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_7_gaussian_process.ipynb#gaussian-process-regression-gpr)<br/>([_Gaussian Process Regression (GPR)_](https://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process-regression-gpr))\n",
    "\n",
    "Le [**`GaussianProcessRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor) met en œuvre les processus gaussiens (GP) à des fins de régression. Pour cela, le GP nécessite que la probabilité a priori du GP soit spécifiée. Le GP combinera cette probabilité a priori avec la fonction de vraisemblance basée sur les échantillons d'entraînement. Elle permet d'adopter une approche probabiliste de la prédiction en fournissant la moyenne et l'écart type en sortie lors de la prédiction.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_noisy_targets_002.png\"\n",
    "    alt=\"Régression par processus gaussien sur un jeu de données sans bruit\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "La moyenne a priori est supposée constante et égale à zéro (pour `normalize_y=False`) ou à la moyenne des données d'entraînement (pour `normalize_y=True`). La covariance a priori est spécifiée en passant un objet [**noyau** (1.7.4)](#gp-kernels). Les hyperparamètres du noyau sont optimisés lors de l'ajustement du [**`GaussianProcessRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor) en maximisant la log-vraisemblance marginale (LML) basée sur l'optimiseur passé en paramètre. Étant donné que la LML peut avoir plusieurs optima locaux, l'optimiseur peut être démarré de manière répétée en spécifiant `n_restarts_optimizer`. La première exécution est toujours effectuée en démarrant à partir des valeurs initiales des hyperparamètres du noyau ; les exécutions ultérieures sont effectuées à partir de valeurs d'hyperparamètres qui ont été choisies au hasard dans la plage des valeurs autorisées. Si les hyperparamètres initiaux doivent être conservés tels quels, vous pouvez passer `None` en tant qu'optimiseur.\n",
    "\n",
    "Le niveau de bruit dans les cibles peut être spécifié en le passant via le paramètre `alpha`, soit globalement en tant que scalaire, soit pour chaque point de données. Notez qu'un niveau de bruit modéré peut également être utile pour gérer les instabilités numériques lors de l'ajustement, car il est effectivement implémenté sous la forme d'une régularisation de Tikhonov, c'est-à-dire en l'ajoutant à la diagonale de la matrice du noyau. Une alternative à la spécification explicite du niveau de bruit consiste à inclure un composant [**`WhiteKernel`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.WhiteKernel.html#sklearn.gaussian_process.kernels.WhiteKernel) dans le noyau, qui peut estimer le niveau de bruit global à partir des données (voir l'exemple ci-dessous). La figure ci-dessous montre l'effet des cibles bruitées gérées en définissant le paramètre `alpha`.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_noisy_targets_003.png\"\n",
    "    alt=\"Régression par processus gaussien sur un jeu de données bruité\"\n",
    "    style=\"max-width: 50%; height; auto;\"/>\n",
    "</div>\n",
    "\n",
    "L'implémentation est basée sur l'algorithme 2.1 de [RW2006]. En plus de l'API des estimateurs standard de scikit-learn, [**`GaussianProcessRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor) :\n",
    "\n",
    "- permet des prédictions sans ajustement préalable (basées sur l'a priori du GP)\n",
    "- fournit une méthode supplémentaire `sample_y(X)`, qui évalue les échantillons tirés du GPR (a priori ou a posteriori) aux entrées données\n",
    "- expose une méthode `log_marginal_likelihood(theta)`, qui peut être utilisée de manière externe pour d'autres méthodes de sélection des hyperparamètres, par exemple, via une chaîne de Markov Monte Carlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**Régression des processus gaussiens : exemple introductif de base**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_7_gaussian_process/plot_gpr_noisy_targets.ipynb)<br/>([_Gaussian Processes regression: basic introductory example_](https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html))\n",
    "\n",
    "#### [**Capacité de la régression des processus gaussiens (GPR) à estimer le niveau de bruit des données**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_7_gaussian_process/plot_gpr_noisy.ipynb)<br/>([_Ability of Gaussian process regression (GPR) to estimate data noise-level_](https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy.html))\n",
    "\n",
    "#### [**Comparaison de la régression à noyau et de la régression des processus gaussiens**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_7_gaussian_process/plot_compare_gpr_krr.ipynb)<br/>([_Comparison of kernel ridge and Gaussian process regression_](https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_compare_gpr_krr.html))\n",
    "\n",
    "#### [**Prévision du niveau de CO2 sur l'ensemble de données de Mona Loa à l'aide de la régression des processus gaussiens (GPR)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_7_gaussian_process/plot_gpr_co2.ipynb)<br/>([_Forecasting of CO2 level on Mona Loa dataset using Gaussian process regression (GPR)_](https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_co2.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='gaussian-process-classification-gpc'></a> 1.7.2. **Classification par processus gaussien (GPC)**<br/>([_Gaussian Process Classification (GPC)_](https://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process-classification-gpc))\n",
    "\n",
    "Le [**`GaussianProcessClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier) implémente des processus gaussiens (GP) à des fins de classification, plus précisément à des fins de classification probabiliste, où les prédictions de test prennent la forme de probabilités de classe. [**`GaussianProcessClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier) place un a priori GP sur une fonction latente $f$, qui est ensuite écrasée à travers une fonction de liaison pour obtenir la classification probabiliste. La fonction latente $f$ est une fonction dite de nuisance, dont les valeurs ne sont ni observées ni pertinentes en elles-mêmes. Son but est de permettre une formulation pratique du modèle, et $f$ est supprimée (intégrée) lors de la prédiction. [**`GaussianProcessClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier) implémente la fonction de liaison logistique, pour laquelle l'intégrale ne peut pas être calculée analytiquement, mais peut être facilement approximée dans le cas binaire.\n",
    "\n",
    "Contrairement au cadre de la régression, l'a posteriori de la fonction latente $f$ n'est pas gaussien même pour un un a priori GP, car une vraisemblance gaussienne est inappropriée pour les étiquettes de classe discrètes. Au lieu de cela, une vraisemblance non gaussienne correspondant à la fonction de liaison logistique (logit) est utilisée. [**`GaussianProcessClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier) approche l'a posteriori non gaussien par un gaussien basé sur l'approximation de Laplace. De plus amples détails se trouvent dans le chapitre 3 de [RW2006].\n",
    "\n",
    "On suppose que la moyenne de l'a priori GP est nulle. La covariance de l'a priori est spécifiée en passant un objet [**noyau** (1.7.4)](#gp-kernels). Les hyperparamètres du noyau sont optimisés lors de l'ajustement de [**`GaussianProcessRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor) en maximisant la log-vraisemblance marginale (LML) basée sur l'optimiseur passé. Comme la LML peut avoir plusieurs optima locaux, l'optimiseur peut être lancé plusieurs fois en spécifiant `n_restarts_optimizer`. La première exécution est toujours effectuée à partir des valeurs initiales des hyperparamètres du noyau ; les exécutions suivantes sont effectuées à partir des valeurs d'hyperparamètres qui ont été choisies au hasard dans la plage des valeurs autorisées. Si les hyperparamètres initiaux doivent être conservés, `None` peut être passé en tant qu'optimiseur.\n",
    "\n",
    "[**`GaussianProcessClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier) prend en charge la classification multi-classe en effectuant soit un entraînement et une prédiction en \"un-contre-tous\" (one-versus-rest) ou en \"un-contre-un\" (one-versus-one). En \"un-contre-tous\", un classifieur de processus gaussien binaire est ajusté pour chaque classe, formé pour séparer cette classe du reste. En \"un-contre-un\", un classificateur de processus gaussien binaire est ajusté pour chaque paire de classes, entraîné pour séparer ces deux classes. Les prédictions de ces prédicteurs binaires sont combinées en prédictions multi-classes. Consultez la section sur la [**classification multi-classe** (1.12)](https://scikit-learn.org/stable/modules/multiclass.html#multiclass) pour plus de détails.\n",
    "\n",
    "Dans le cas de la classification par processus gaussien, \"un-contre-un\" peut être moins coûteux en calcul, car il doit résoudre de nombreux problèmes impliquant uniquement un sous-ensemble de l'ensemble d'entraînement global, au lieu de moins de problèmes mais sur l'intégralité du jeu de données. Étant donné que la classification par processus gaussien évolue de manière cubique avec la taille de l'ensemble de données, cela peut être considérablement plus rapide. Cependant, notez que \"un-contre-un\" ne prend pas en charge la prédiction des estimations de probabilité, mais uniquement des prédictions simples. De plus, notez que [**`GaussianProcessClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier) n'implémente pas (encore) une véritable approximation de Laplace multi-classe en interne, mais comme discuté précédemment, il est basé sur la résolution de plusieurs tâches de classification binaire en interne, qui sont combinées en \"un-contre-tous\" ou \"un-contre-un\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='gpc-examples'></a> 1.7.3. **Exemples de GPC**<br/>([_GPC examples_](https://scikit-learn.org/stable/modules/gaussian_process.html#gpc-examples))\n",
    "\n",
    "### <a id='probabilistic-predictions-with-gpc'></a> 1.7.3.1. **Prédictions probabilistes avec GPC**<br/>([_Probabilistic predictions with GPC_](https://scikit-learn.org/stable/modules/gaussian_process.html#probabilistic-predictions-with-gpc))\n",
    "\n",
    "Cet exemple illustre la probabilité prédite par GPC pour un noyau RBF avec différents choix d'hyperparamètres. La première figure montre la probabilité prédite par GPC avec des hyperparamètres choisis de manière arbitraire et avec les hyperparamètres correspondant à la log-vraisemblance marginale maximale (LML).\n",
    "\n",
    "Alors que les hyperparamètres choisis en optimisant la LML ont une LML considérablement plus élevée, ils ont des performances légèrement inférieures selon la perte logarithmique sur les données de test. La figure montre que cela est dû au fait qu'ils présentent un changement abrupt des probabilités de classe aux frontières des classes (ce qui est bon), mais ont des probabilités prédites proches de 0,5 loin des frontières des classes (ce qui est moins bon). Cet effet indésirable est causé par l'approximation de Laplace utilisée en interne par GPC.\n",
    "\n",
    "La deuxième figure montre la log-vraisemblance marginale pour différents choix des hyperparamètres du noyau, en mettant en évidence les deux choix d'hyperparamètres utilisés dans la première figure par des points noirs.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_gpc_001.png\"\n",
    "    alt=\"Prédictions probabilistes avec la classification par processus gaussien (GPC), probabilité\"\n",
    "    style=\"max-width: 40%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_gpc_002.png\"\n",
    "    alt=\"Prédictions probabilistes avec la classification par processus gaussien (GPC), log-vraisemblance marginale\"\n",
    "    style=\"max-width: 40%; height: auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='illustration-of-gpc-on-the-xor-dataset'></a> 1.7.3.2. **Illustration de GPC sur l'ensemble de données XOR**<br/>([_Illustration of GPC on the XOR dataset_](https://scikit-learn.org/stable/modules/gaussian_process.html#illustration-of-gpc-on-the-xor-dataset))\n",
    "\n",
    "Cet exemple illustre l'utilisation de la classification par processus gaussien (GPC) sur des données XOR. Il compare un noyau stationnaire et isotrope ([**`RBF`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html#sklearn.gaussian_process.kernels.RBF)) avec un noyau non stationnaire ([**`DotProduct`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.DotProduct.html#sklearn.gaussian_process.kernels.DotProduct)). Sur cet ensemble de données particulier, le noyau [**`DotProduct`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.DotProduct.html#sklearn.gaussian_process.kernels.DotProduct) obtient des résultats considérablement meilleurs car les frontières de classe sont linéaires et coïncident avec les axes de coordonnées. Cependant, en pratique, les noyaux stationnaires tels que [**`RBF`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html#sklearn.gaussian_process.kernels.RBF) obtiennent souvent de meilleurs résultats.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_gpc_xor_001.png\"\n",
    "    alt=\"Illustration de la classification par processus gaussien (GPC) sur l'ensemble de données XOR\"\n",
    "    style=\"max-width: 75%; height: auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='gaussian-process-classification-gpc-on-iris-dataset'></a> 1.7.3.3. **Classification par processus gaussien (GPC) sur l'ensemble de données iris**<br/>([_Gaussian process classification (GPC) on iris dataset_](https://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process-classification-gpc-on-iris-dataset))\n",
    "\n",
    "Cet exemple illustre la probabilité prédite par la classification par processus gaussien (GPC) pour un noyau RBF isotrope et anisotrope sur une version bidimensionnelle de l'ensemble de données iris. Cela illustre l'applicabilité de GPC à la classification non binaire. Le noyau RBF anisotrope obtient une légèrement plus grande log-vraisemblance marginale en attribuant des longueurs d'échelle différentes aux deux dimensions des caractéristiques.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_gpc_iris_001.png\"\n",
    "    alt=\"Classification par processus gaussien (GPC) sur l'ensemble de données iris\"\n",
    "    style=\"max-width: 75%; height: auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**Prédictions probabilistes avec la classification par processus gaussien (GPC)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_7_gaussian_process/plot_gpc.ipynb)<br/>([_Probabilistic predictions with Gaussian process classification (GPC)_](https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpc.html))\n",
    "\n",
    "#### [**Illustration de la classification par processus gaussien (GPC) sur l'ensemble de données XOR**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_7_gaussian_process/plot_gpc_xor.ipynb)<br/>([_Illustration of Gaussian process classification (GPC) on the XOR dataset_](https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpc_xor.html))\n",
    "\n",
    "#### [**Classification par processus gaussien (GPC) sur l'ensemble de données iris**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_7_gaussian_process/plot_gpc_iris.ipynb)<br/>([_Gaussian process classification (GPC) on iris dataset_](https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpc_iris.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='kernels-for-gaussian-processes'></a> 1.7.4. **Noyaux pour les processus gaussiens**<br/>([_Kernels for Gaussian Processes_](https://scikit-learn.org/stable/modules/gaussian_process.html#kernels-for-gaussian-processes))\n",
    "\n",
    "Les noyaux (appelés également \"fonctions de covariance\" dans le contexte des processus gaussiens) sont un ingrédient essentiel des processus gaussiens, car ils déterminent la forme de l'a priori et de l'a posteriori du processus gaussien. Ils encodent les hypothèses sur la fonction à apprendre en définissant la \"similitude\" entre deux points de données, combinée à l'hypothèse selon laquelle des points de données similaires devraient avoir des valeurs cibles similaires. Deux catégories de noyaux peuvent être distinguées : les noyaux stationnaires dépendent uniquement de la distance entre deux points de données et non de leurs valeurs absolues $k(x_i, x_j)= k(d(x_i, x_j))$ et sont donc invariants aux translations dans l'espace d'entrée, tandis que les noyaux non stationnaires dépendent également des valeurs spécifiques des points de données. Les noyaux stationnaires peuvent en outre être subdivisés en noyaux isotropes et anisotropes, les noyaux isotropes étant également invariants aux rotations dans l'espace d'entrée. Pour plus de détails, nous vous renvoyons au chapitre 4 de [RW2006]. Pour des conseils sur la meilleure combinaison de différents noyaux, nous vous renvoyons à [Duv2014]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='gaussian-process-kernel-api'></a> 1.7.4.1. **API des noyaux pour les processus gaussiens**<br/>([_Gaussian Process Kernel API_](https://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process-kernel-api))\n",
    "\n",
    "L'utilisation principale d'un [**`Kernel`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Kernel.html#sklearn.gaussian_process.kernels.Kernel) est de calculer la covariance du processus gaussien entre les points de données. Pour ce faire, la méthode `__call__` du noyau peut être utilisée. Cette méthode peut être utilisée pour calculer la \"auto-covariance\" de toutes les paires de points de données dans un tableau 2D `X`, ou la \"covariance croisée\" de toutes les combinaisons de points de données d'un tableau 2D `X` avec des points de données dans un tableau 2D `Y`. L'identité suivante est vraie pour tous les noyaux `k` (à l'exception du [**`WhiteKernel`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.WhiteKernel.html#sklearn.gaussian_process.kernels.WhiteKernel)): `k(X) == K(X, Y=X)`\n",
    "\n",
    "Si seule la diagonale de l'auto-covariance est utilisée, la méthode `diag()` d'un noyau peut être appelée, ce qui est plus efficace du point de vue du calcul que l'appel équivalent à `__call__`: `np.diag(k(X, X)) == k.diag(X)`\n",
    "\n",
    "Les noyaux sont paramétrés par un vecteur $\\theta$ dd'hyperparamètres. Ces hyperparamètres peuvent, par exemple, contrôler les longueurs d'échelle ou la périodicité d'un noyau (voir ci-dessous). Tous les noyaux supportent le calcul des gradients analytiques de l'auto-covariance du noyau par rapport à $\\log(\\theta)$ en définissant `eval_gradient=True` dans la méthode `__call__`. Ainsi, un tableau de dimensions `(len(X), len(X), len(theta))` est renvoyé, où l'entrée `[i, j, l]` contient $\\displaystyle \\frac{\\partial k_\\theta(x_i, x_j)}{\\partial \\log(\\theta_l)}$. Ce gradient est utilisé par le processus gaussien (à la fois le régresseur et le classifieur) pour calculer le gradient de la log-vraisemblance marginale, qui est à son tour utilisé pour déterminer la valeur de $\\theta$ qui maximise la log-vraisemblance marginale, via une ascension du gradient. Pour chaque paramètre hyper, il est nécessaire de spécifier la valeur initiale et les bornes lors de la création d'une instance du noyau. La valeur actuelle de $\\theta$ peut être obtenue et définie via la propriété `theta` de l'objet noyau. De plus, les bornes des paramètres hyper peuvent être consultées via la propriété `bounds` du noyau. Notez que les deux propriétés (`theta` et `bounds`) renvoient les valeurs transformées par le logarithme des valeurs internes utilisées, car celles-ci sont généralement plus adaptées à l'optimisation basée sur les gradients. La spécification de chaque hyperparamètre est stockée sous forme d'une instance de [**`Hyperparameter`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Hyperparameter.html#sklearn.gaussian_process.kernels.Hyperparameter) dans le noyau respectif. Notez qu'un noyau utilisant un hyperparamètre portant le nom `x` doit avoir les attributs `self.x` et `self.x_bounds`.\n",
    "\n",
    "La classe de base abstraite pour tous les noyaux est [**`Kernel`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Kernel.html#sklearn.gaussian_process.kernels.Kernel). Le noyau implémente une interface similaire à celle de [**`BaseEstimator`**](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html#sklearn.base.BaseEstimator), fournissant les méthodes `get_params()`, `set_params()` et `clone()`. Cela permet de définir les valeurs des noyaux également via des méta-estimateurs tels que [**`Pipeline`**](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) ou [**`GridSearchCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV). Notez qu'en raison de la structure imbriquée des noyaux (en appliquant des opérateurs de noyau, voir ci-dessous), les noms des paramètres des noyaux peuvent devenir relativement compliqués. En général, pour un opérateur de noyau binaire, les paramètres de l'opérande de gauche sont préfixés par `k1__` et les paramètres de l'opérande de droite par `k2__`. Une méthode supplémentaire pratique est `clone_with_theta(theta)`, qui renvoie une version clonée du noyau mais avec les paramètres hyper réglés sur `theta`. Un exemple illustratif :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter(name='k1__k1__constant_value', value_type='numeric', bounds=array([[ 0., 10.]]), n_elements=1, fixed=False)\n",
      "Hyperparameter(name='k1__k2__length_scale', value_type='numeric', bounds=array([[ 0., 10.]]), n_elements=1, fixed=False)\n",
      "Hyperparameter(name='k2__length_scale', value_type='numeric', bounds=array([[ 0., 10.]]), n_elements=1, fixed=False)\n",
      "k1 : 1**2 * RBF(length_scale=0.5)\n",
      "k1__k1 : 1**2\n",
      "k1__k1__constant_value : 1.0\n",
      "k1__k1__constant_value_bounds : (0.0, 10.0)\n",
      "k1__k2 : RBF(length_scale=0.5)\n",
      "k1__k2__length_scale : 0.5\n",
      "k1__k2__length_scale_bounds : (0.0, 10.0)\n",
      "k2 : RBF(length_scale=2)\n",
      "k2__length_scale : 2.0\n",
      "k2__length_scale_bounds : (0.0, 10.0)\n",
      "[ 0.         -0.69314718  0.69314718]\n",
      "[[      -inf 2.30258509]\n",
      " [      -inf 2.30258509]\n",
      " [      -inf 2.30258509]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:334: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(np.vstack(bounds))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.gaussian_process.kernels import ConstantKernel, RBF\n",
    "\n",
    "kernel = (\n",
    "    ConstantKernel(\n",
    "        constant_value=1.0,\n",
    "        constant_value_bounds=(0.0, 10.0)\n",
    "    )\n",
    "    * RBF(length_scale=0.5, length_scale_bounds=(0.0, 10.0))\n",
    "    + RBF(length_scale=2.0, length_scale_bounds=(0.0, 10.0))\n",
    ")\n",
    "\n",
    "for hyperparameter in kernel.hyperparameters:\n",
    "    print(hyperparameter)\n",
    "# Hyperparameter(name='k1__k1__constant_value', value_type='numeric', bounds=array([[ 0., 10.]]), n_elements=1, fixed=False)\n",
    "# Hyperparameter(name='k1__k2__length_scale', value_type='numeric', bounds=array([[ 0., 10.]]), n_elements=1, fixed=False)\n",
    "# Hyperparameter(name='k2__length_scale', value_type='numeric', bounds=array([[ 0., 10.]]), n_elements=1, fixed=False)\n",
    "\n",
    "params = kernel.get_params()\n",
    "for key in sorted(params):\n",
    "    print(f\"{key} : {params[key]}\")\n",
    "# k1 : 1**2 * RBF(length_scale=0.5)\n",
    "# k1__k1 : 1**2\n",
    "# k1__k1__constant_value : 1.0\n",
    "# k1__k1__constant_value_bounds : (0.0, 10.0)\n",
    "# k1__k2 : RBF(length_scale=0.5)\n",
    "# k1__k2__length_scale : 0.5\n",
    "# k1__k2__length_scale_bounds : (0.0, 10.0)\n",
    "# k2 : RBF(length_scale=2)\n",
    "# k2__length_scale : 2.0\n",
    "# k2__length_scale_bounds : (0.0, 10.0)\n",
    "print(kernel.theta)  # Note: log-transformed\n",
    "# [ 0.         -0.69314718  0.69314718]\n",
    "print(kernel.bounds)  # Note: log-transformed\n",
    "# [[      -inf 2.30258509]\n",
    "#  [      -inf 2.30258509]\n",
    "#  [      -inf 2.30258509]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tous les noyaux de processus gaussiens sont interopérables avec [**`sklearn.metrics.pairwise`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics.pairwise) et vice versa : les instances des sous-classes de [**`Kernel`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Kernel.html#sklearn.gaussian_process.kernels.Kernel) peuvent être transmises en tant que \"métrique\" à `pairwise_kernels` de [**`sklearn.metrics.pairwise`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics.pairwise). De plus, les fonctions de noyau de pairwise peuvent être utilisées comme noyaux de processus gaussiens en utilisant la classe d'enrobage [**`PairwiseKernel`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.PairwiseKernel.html#sklearn.gaussian_process.kernels.PairwiseKernel). La seule mise en garde est que le gradient des hyperparamètres n'est pas analytique, mais numérique, et que tous ces noyaux ne supportent que les distances isotropes. Le paramètre `gamma` est considéré comme un hyperparamètre et peut être optimisé. Les autres paramètres du noyau sont définis directement à l'initialisation et restent fixes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='basic-kernels'></a> 1.7.4.2. **Noyaux de base**<br/>([_Basic kernels_](https://scikit-learn.org/stable/modules/gaussian_process.html#basic-kernels))\n",
    "\n",
    "Le noyau [**`ConstantKernel`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.ConstantKernel.html#sklearn.gaussian_process.kernels.ConstantKernel) peut être utilisé comme partie d'un noyau [**`Product`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Product.html#sklearn.gaussian_process.kernels.Product) où il ajuste l'amplitude de l'autre facteur (nouveau noyau), ou comme partie d'un noyau [**`Sum`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Sum.html#sklearn.gaussian_process.kernels.Sum), où il modifie la moyenne du processus gaussien. Il dépend d'un paramètre $a$ = `constant_value`. Il est défini comme suit :\n",
    "\n",
    "$$\n",
    "k(x_i, x_j) = a \\;\\forall\\; x_1, x_2\n",
    "$$\n",
    "\n",
    "Le principal cas d'utilisation du noyau [**`WhiteKernel`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.WhiteKernel.html#sklearn.gaussian_process.kernels.WhiteKernel) est de faire partie d'un noyau de somme, où il explique la composante de bruit du signal. L'ajustement de son paramètre $b$ = `noise_level` correspond à l'estimation du niveau de bruit. Il est défini comme suit :\n",
    "\n",
    "$$\n",
    "k(x_i, x_j) = b \\text{ si } x_i == x_j \\text{ sinon } 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='kernel-operators'></a> 1.7.4.3. **Opérateurs de noyaux**<br/>([_Kernel operators_](https://scikit-learn.org/stable/modules/gaussian_process.html#kernel-operators))\n",
    "\n",
    "Les opérateurs de noyaux prennent un ou deux noyaux de base et les combinent pour créer un nouveau noyau. Le noyau [**`Sum`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Sum.html#sklearn.gaussian_process.kernels.Sum) prend deux noyaux $k_1$ et $k_2$ et les combine comme suit : $k_{sum}(X, Y) = k_1(X, Y) + k_2(X, Y)$. Le noyau [**`Product`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Product.html#sklearn.gaussian_process.kernels.Product) prend deux noyaux $k_1$ et $k_2$ et les combine comme suit : $k_{product}(X, Y) = k_1(X, Y) \\times k_2(X, Y)$. Le noyau [**`Exponentiation`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Exponentiation.html#sklearn.gaussian_process.kernels.Exponentiation) prend un noyau de base et un paramètre scalaire et les combine comme suit : $k_{exp}(X, Y) = k(X, Y)^p$. Notez que les méthodes magiques `__add__`, `__mul__` et `__pow__` sont redéfinies sur les objets noyaux, de sorte que vous pouvez par exemple utiliser `RBF() + RBF()` comme raccourci pour `Sum(RBF(), RBF())`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='radial-basis-function-rbf-kernel'></a> 1.7.4.4. **Noyau de fonction de base radiale (RBF)**<br/>([_Radial basis function (RBF) kernel_](https://scikit-learn.org/stable/modules/gaussian_process.html#radial-basis-function-rbf-kernel))\n",
    "\n",
    "Le noyau [**`RBF`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html#sklearn.gaussian_process.kernels.RBF) est un noyau stationnaire. Il est également connu sous le nom de noyau \"exponentiel carré\". Il est paramétré par un paramètre d'échelle $l > 0$, qui peut être soit un scalaire (variante isotrope du noyau) soit un vecteur avec le même nombre de dimensions que les entrées $x$ (variante anisotrope du noyau). Le noyau est donné par :\n",
    "\n",
    "$$\n",
    "k(x_i, x_j) = \\text{exp}\\left(- \\frac{d(x_i, x_j)^2}{2l^2} \\right)\n",
    "$$\n",
    "\n",
    "où $d(\\cdot, \\cdot)$ est la distance euclidienne. Ce noyau est infiniment différentiable, ce qui implique que les processus gaussiens avec ce noyau en tant que fonction de covariance ont des dérivées de toutes les ordres à carrés moyens, et sont donc très lisses. L'a priori et l'a posteriori d'un GP résultant d'un noyau RBF sont illustrés dans la figure suivante :\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_prior_posterior_001.png\"\n",
    "    alt=\"Noyau de fonction de base radiale\"\n",
    "    style=\"max-width: 75%; height: auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='matern-kernel'></a> 1.7.4.5. **Noyau Matérn**<br/>([_Matérn kernel_](https://scikit-learn.org/stable/modules/gaussian_process.html#matern-kernel))\n",
    "\n",
    "Le noyau [**`Matern`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Matern.html#sklearn.gaussian_process.kernels.Matern) est un noyau stationnaire et une généralisation du noyau [**`RBF`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html#sklearn.gaussian_process.kernels.RBF). Il possède un paramètre supplémentaire qui contrôle la régularité de la fonction résultante. Il est paramétré par un paramètre d'échelle $l > 0$, qui peut être soit un scalaire (variante isotrope du noyau) soit un vecteur avec le même nombre de dimensions que les entrées $x$ (variante anisotrope du noyau). Le noyau est donné par :\n",
    "\n",
    "$$\n",
    "k(x_i, x_j) = \\frac{1}{\\Gamma(\\nu)2^{\\nu-1}}\\Bigg(\\frac{\\sqrt{2\\nu}}{l} d(x_i , x_j )\\Bigg)^\\nu K_\\nu\\Bigg(\\frac{\\sqrt{2\\nu}}{l} d(x_i , x_j )\\Bigg),\n",
    "$$\n",
    "\n",
    "où $d(\\cdot,\\cdot)$ est la distance euclidienne, $K_\\nu(\\cdot)$ est une fonction de Bessel modifiée et $\\Gamma(\\cdot)$ est la fonction gamma. Lorsque $\\nu\\rightarrow\\infty$, le noyau Matérn converge vers le noyau RBF. Lorsque $\\nu = 1/2$, le noyau Matérn devient identique au noyau exponentiel absolu, c'est-à-dire,\n",
    "\n",
    "$$\n",
    "k(x_i, x_j) = \\exp \\Bigg(- \\frac{1}{l} d(x_i , x_j ) \\Bigg) \\quad \\quad \\nu= \\tfrac{1}{2}\n",
    "$$\n",
    "\n",
    "En particulier, pour $\\nu = 3/2$ :\n",
    "\n",
    "$$\n",
    "k(x_i, x_j) =  \\Bigg(1 + \\frac{\\sqrt{3}}{l} d(x_i , x_j )\\Bigg) \\exp \\Bigg(-\\frac{\\sqrt{3}}{l} d(x_i , x_j ) \\Bigg) \\quad \\quad \\nu= \\tfrac{3}{2}\n",
    "$$\n",
    "\n",
    "et pour $\\nu = 5/2\" :\n",
    "\n",
    "$$\n",
    "k(x_i, x_j) = \\Bigg(1 + \\frac{\\sqrt{5}}{l} d(x_i , x_j ) +\\frac{5}{3l} d(x_i , x_j )^2 \\Bigg) \\exp \\Bigg(-\\frac{\\sqrt{5}}{l} d(x_i , x_j ) \\Bigg) \\quad \\quad \\nu= \\tfrac{5}{2}\n",
    "$$\n",
    "\n",
    "sont des choix populaires pour apprendre des fonctions qui ne sont pas infiniment dérivables (comme le suppose le noyau RBF), mais au moins une fois ($\\nu = 3/2$) ou deux fois ($\\nu = 5/2$) dérivables.\n",
    "\n",
    "La flexibilité de contrôler la régularité de la fonction apprise via $\\nu$ permet de s'adapter aux propriétés de la vraie relation fonctionnelle sous-jacente. L'a priori et l'a posteriori d'un GP résultant d'un noyau Matérn sont illustrés dans la figure suivante :\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_prior_posterior_005.png\"\n",
    "    alt=\"Noyau Matérn\"\n",
    "    style=\"max-width: 75%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Consultez [RW2006], p. 84, pour plus de détails sur les différentes variantes du noyau Matérn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='rational-quadratic-kernel'></a> 1.7.4.6. **Noyau rationnel quadratique**<br/>([_Rational quadratic kernel_](https://scikit-learn.org/stable/modules/gaussian_process.html#rational-quadratic-kernel))\n",
    "\n",
    "Le noyau [**`RationalQuadratic`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RationalQuadratic.html#sklearn.gaussian_process.kernels.RationalQuadratic) peut être vu comme un mélange d'échelle (une somme infinie) de noyaux [**`RBF`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html#sklearn.gaussian_process.kernels.RBF) avec différentes longueurs caractéristiques. Il est paramétré par un paramètre d'échelle $l > 0$ et un paramètre de mélange d'échelle $\\alpha > 0$. Seule la variante isotrope où $l$ est un scalaire est actuellement prise en charge. Le noyau est donné par :\n",
    "\n",
    "$$\n",
    "k(x_i, x_j) = \\left(1 + \\frac{d(x_i, x_j)^2}{2\\alpha l^2}\\right)^{-\\alpha}\n",
    "$$\n",
    "\n",
    "L'a priori et l'a posteriori d'un GP résultant d'un noyau [**`RationalQuadratic`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RationalQuadratic.html#sklearn.gaussian_process.kernels.RationalQuadratic) sont illustrés dans la figure suivante :\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_prior_posterior_002.png\"\n",
    "    alt=\"Noyau rationnel quadratique\"\n",
    "    style=\"max-width: 75%; height: auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='exp-sine-squared-kernel'></a> 1.7.4.7. **Noyau exponentiel sinusoidal**<br/>([_Exp-Sine-Squared kernel_](https://scikit-learn.org/stable/modules/gaussian_process.html#exp-sine-squared-kernel))\n",
    "\n",
    "Le noyau [**`ExpSineSquared`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.ExpSineSquared.html#sklearn.gaussian_process.kernels.ExpSineSquared) permet de modéliser des fonctions périodiques. Il est paramétré par un paramètre d'échelle $l > 0$ et un paramètre de périodicité $p > 0$. Seule la variante isotrope où $l$ est un scalaire est actuellement prise en charge. Le noyau est donné par :\n",
    "\n",
    "$$\n",
    "k(x_i, x_j) = \\text{exp}\\left(- \\frac{ 2\\sin^2(\\pi d(x_i, x_j) / p) }{ l^ 2} \\right)\n",
    "$$\n",
    "\n",
    "L'a priori et l'a posteriori d'un GP résultant d'un noyau [**`ExpSineSquared`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.ExpSineSquared.html#sklearn.gaussian_process.kernels.ExpSineSquared) sont illustrés dans la figure suivante :\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_prior_posterior_003.png\"\n",
    "    alt=\"Noyau exponentiel sinusoidal\"\n",
    "    style=\"max-width: 75%; height: auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='dot-product-kernel'></a> 1.7.4.8. **Noyau de produit scalaire**<br/>([_Dot-Product kernel_](https://scikit-learn.org/stable/modules/gaussian_process.html#dot-product-kernel))\n",
    "\n",
    "Le noyau [**`DotProduct`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.DotProduct.html#sklearn.gaussian_process.kernels.DotProduct) n'est pas stationnaire et peut être obtenu à partir d'une régression linéaire en plaçant des a priori $\\mathcal{N}(0, 1)$ sur les coefficients de $x_d (d = 1, \\ldots, D)$ et un a priori $\\mathcal{N}(0, \\sigma_0^2)$ sur le biais. Le noyau [**`DotProduct`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.DotProduct.html#sklearn.gaussian_process.kernels.DotProduct) est invariant à une rotation des coordonnées autour de l'origine, mais pas aux translations. Il est paramétré par un paramètre $\\sigma_0^2$. Pour $\\sigma_0^2 = 0$, le noyau est appelé le noyau linéaire homogène, sinon il est inhomogène. Le noyau est donné par :\n",
    "\n",
    "$$\n",
    "k(x_i, x_j) = \\sigma_0 ^ 2 + x_i \\cdot x_j\n",
    "$$\n",
    "\n",
    "Le noyau [**`DotProduct`**](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.DotProduct.html#sklearn.gaussian_process.kernels.DotProduct) est couramment combiné avec l'exponentiation. Un exemple avec un exposant de 2 est illustré dans la figure suivante :\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_prior_posterior_004.png\"\n",
    "    alt=\"Noyau de produit scalaire\"\n",
    "    style=\"max-width: 75%; height: auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='references'></a> 1.7.4.9. **Références**<br/>([_References_](https://scikit-learn.org/stable/modules/gaussian_process.html#references))\n",
    "\n",
    "📚 [RW2006] (1,2,3,4) Carl E. Rasmussen and Christopher K.I. Williams, [**“Gaussian Processes for Machine Learning”**](https://gaussianprocess.org/gpml/chapters/RW.pdf), MIT Press 2006\n",
    "\n",
    "🌐 [Duv2014] David Duvenaud, [**“The Kernel Cookbook: Advice on Covariance functions”**](https://www.cs.toronto.edu/~duvenaud/cookbook/), 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**Illustration de processus gaussiens a priori et a posteriori pour différents noyaux**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_7_gaussian_process/plot_gpc_iris.ipynb)<br/>([_Illustration of prior and posterior Gaussian process for different kernels_](https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpc_iris.html))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
