{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='unsupervised-learning'></a> 2. [**Apprentissage non supervis√©**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_unsupervised_learning.ipynb#model-selection-and-evaluation)</br>([*Unsupervised learning*](https://scikit-learn.org/stable/unsupervised_learning.html#unsupervised-learning))\n",
    "\n",
    "# 2.3. [**Regroupement**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#clustering)<br/>([_Clustering_](https://scikit-learn.org/stable/modules/clustering.html#clustering))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 54 pages, 20 exemples, 40 papiers\n",
    "- 2.3.1. [**Aper√ßu des m√©thodes de regroupement**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#overview-of-clustering-methods)<br/>([_Overview of clustering methods_](https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods))\n",
    "- 2.3.2. [**K-moyennes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#k-means)<br/>([_K-means_](https://scikit-learn.org/stable/modules/clustering.html#k-means))\n",
    "    - 2.3.2.1. [**Parall√©lisme de bas niveau**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#low-level-parallelism)<br/>([_Low-level parallelism_](https://scikit-learn.org/stable/modules/clustering.html#low-level-parallelism))\n",
    "    - 2.3.2.2. [**K-moyennes par mini-lots**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#mini-batch-k-means)<br/>([_Mini Batch K-Means_](https://scikit-learn.org/stable/modules/clustering.html#mini-batch-k-means))\n",
    "- 2.3.3. [**Propagation d'affinit√©**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#affinity-propagation)<br/>([_Affinity Propagation_](https://scikit-learn.org/stable/modules/clustering.html#affinity-propagation))\n",
    "- 2.3.4. [**D√©placement moyen**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#mean-shift)<br/>([_Mean Shift_](https://scikit-learn.org/stable/modules/clustering.html#mean-shift))\n",
    "- 2.3.5. [**Regroupement spectral**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#spectral-clustering)<br/>([_Spectral clustering_](https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering))\n",
    "    - 2.3.5.1. [**Strat√©gies d'attribution d'√©tiquettes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#different-label-assignment-strategies)<br/>([_Different label assignment strategies_](https://scikit-learn.org/stable/modules/clustering.html#different-label-assignment-strategies))\n",
    "    - 2.3.5.2. [**Graphes de regroupement spectral**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#spectral-clustering-graphs)<br/>([_Spectral Clustering Graphs_](https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering-graphs))\n",
    "- 2.3.6. [**Regroupement hi√©rarchique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#hierarchical-clustering)<br/>([_Hierarchical clustering_](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering))\n",
    "    - 2.3.6.1. [**Types de liaison : Ward, compl√®te, moyenne et liaison simple**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#different-linkage-type-ward-complete-average-and-single-linkage)<br/>([_Different linkage type: Ward, complete, average, and single linkage_](https://scikit-learn.org/stable/modules/clustering.html#different-linkage-type-ward-complete-average-and-single-linkage))\n",
    "    - 2.3.6.2. [**Visualisation de la hi√©rarchie des groupes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#visualization-of-cluster-hierarchy)<br/>([_Visualization of cluster hierarchy_](https://scikit-learn.org/stable/modules/clustering.html#visualization-of-cluster-hierarchy))\n",
    "    - 2.3.6.3. [**Ajout de contraintes de connectivit√©**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#adding-connectivity-constraints)<br/>([_Adding connectivity constraints_](https://scikit-learn.org/stable/modules/clustering.html#adding-connectivity-constraints))\n",
    "    - 2.3.6.4. [**Variation de la m√©trique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#varying-the-metric)<br/>([_Varying the metric_](https://scikit-learn.org/stable/modules/clustering.html#varying-the-metric))\n",
    "    - 2.3.6.5. [**K-moyennes √† bissection**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#bisecting-k-means)<br/>([_Bisecting K-Means_](https://scikit-learn.org/stable/modules/clustering.html#bisecting-k-means))\n",
    "- 2.3.7. [**DBSCAN**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#dbscan)<br/>([_DBSCAN_](https://scikit-learn.org/stable/modules/clustering.html#dbscan))\n",
    "- 2.3.8. [**HDBSCAN**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#hdbscan)<br/>([_HDBSCAN_](https://scikit-learn.org/stable/modules/clustering.html#hdbscan))\n",
    "    - 2.3.8.1. [**Graphe d'accessibilit√© mutuelle**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#mutual-reachability-graph)<br/>([_Mutual Reachability Graph_](https://scikit-learn.org/stable/modules/clustering.html#mutual-reachability-graph))\n",
    "    - 2.3.8.2. [**Regroupement hi√©rarchique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#hierarchical-clustering)<br/>([_Hierarchical Clustering_](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering))\n",
    "- 2.3.9. [**OPTICS**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#optics)<br/>([_OPTICS_](https://scikit-learn.org/stable/modules/clustering.html#optics))\n",
    "- 2.3.10. [**BIRCH**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#birch)<br/>([_BIRCH_](https://scikit-learn.org/stable/modules/clustering.html#birch))\n",
    "- 2.3.11. [**√âvaluation des performances du regroupement**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#clustering-performance-evaluation)<br/>([_Clustering performance evaluation_](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation))\n",
    "    - 2.3.11.1. [**Indice de Rand**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#rand-index)<br/>([_Rand index_](https://scikit-learn.org/stable/modules/clustering.html#rand-index))\n",
    "    - 2.3.11.2. [**Scores bas√©s sur l'information mutuelle**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#mutual-information-based-scores)<br/>([_Mutual Information based scores_](https://scikit-learn.org/stable/modules/clustering.html#mutual-information-based-scores))\n",
    "    - 2.3.11.3. [**Homog√©n√©it√©, compl√©tude et V-mesure**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#homogeneity-completeness-and-v-measure)<br/>([_Homogeneity, completeness and V-measure_](https://scikit-learn.org/stable/modules/clustering.html#homogeneity-completeness-and-v-measure))\n",
    "    - 2.3.11.4. [**Scores Fowlkes-Mallows**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#fowlkes-mallows-scores)<br/>([_Fowlkes-Mallows scores_](https://scikit-learn.org/stable/modules/clustering.html#fowlkes-mallows-scores))\n",
    "    - 2.3.11.5. [**Coefficient de Silhouette**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#silhouette-coefficient)<br/>([_Silhouette Coefficient_](https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient))\n",
    "    - 2.3.11.6. [**Indice de Calinski-Harabasz**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#calinski-harabasz-index)<br/>([_Calinski-Harabasz Index_](https://scikit-learn.org/stable/modules/clustering.html#calinski-harabasz-index))\n",
    "    - 2.3.11.7. [**Indice de Davies-Bouldin**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#davies-bouldin-index)<br/>([_Davies-Bouldin Index_](https://scikit-learn.org/stable/modules/clustering.html#davies-bouldin-index))\n",
    "    - 2.3.11.8. [**Matrice de contingence**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#contingency-matrix)<br/>([_Contingency Matrix_](https://scikit-learn.org/stable/modules/clustering.html#contingency-matrix))\n",
    "    - 2.3.11.9. [**Matrice de confusion par paire**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb#pair-confusion-matrix)<br/>([_Pair Confusion Matrix_](https://scikit-learn.org/stable/modules/clustering.html#pair-confusion-matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## <a id='birch'></a> 2.3.10. **BIRCH**<br/>([*BIRCH*](https://scikit-learn.org/stable/modules/clustering.html#birch))\n",
    "\n",
    "## <a id='clustering-performance-evaluation'></a> 2.3.11. **√âvaluation des performances du regroupement**<br/>([*Clustering performance evaluation*](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation))\n",
    "### <a id='rand-index'></a> 2.3.11.1. **Indice de Rand**<br/>([*Rand index*](https://scikit-learn.org/stable/modules/clustering.html#rand-index))\n",
    "### <a id='mutual-information-based-scores'></a> 2.3.11.2. **Scores bas√©s sur l'information mutuelle**<br/>([*Mutual Information based scores*](https://scikit-learn.org/stable/modules/clustering.html#mutual-information-based-scores))\n",
    "### <a id='homogeneity-completeness-and-v-measure'></a> 2.3.11.3. **Homog√©n√©it√©, compl√©tude et V-mesure**<br/>([*Homogeneity, completeness and V-measure*](https://scikit-learn.org/stable/modules/clustering.html#homogeneity-completeness-and-v-measure))\n",
    "### <a id='fowlkes-mallows-scores'></a> 2.3.11.4. **Scores Fowlkes-Mallows**<br/>([*Fowlkes-Mallows scores*](https://scikit-learn.org/stable/modules/clustering.html#fowlkes-mallows-scores))\n",
    "### <a id='silhouette-coefficient'></a> 2.3.11.5. **Coefficient de Silhouette**<br/>([*Silhouette Coefficient*](https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient))\n",
    "### <a id='calinski-harabasz-index'></a> 2.3.11.6. **Indice de Calinski-Harabasz**<br/>([*Calinski-Harabasz Index*](https://scikit-learn.org/stable/modules/clustering.html#calinski-harabasz-index))\n",
    "### <a id='davies-bouldin-index'></a> 2.3.11.7. **Indice de Davies-Bouldin**<br/>([*Davies-Bouldin Index*](https://scikit-learn.org/stable/modules/clustering.html#davies-bouldin-index))\n",
    "### <a id='contingency-matrix'></a> 2.3.11.8. **Matrice de contingence**<br/>([*Contingency Matrix*](https://scikit-learn.org/stable/modules/clustering.html#contingency-matrix))\n",
    "### <a id='pair-confusion-matrix'></a> 2.3.11.9. **Matrice de confusion par paire**<br/>([*Pair Confusion Matrix*](https://scikit-learn.org/stable/modules/clustering.html#pair-confusion-matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='clustering'></a> 2.3. **Regroupement**<br/>([_Clustering_](https://scikit-learn.org/stable/modules/clustering.html#clustering))\n",
    "\n",
    "Le [**regroupement de donn√©es**](https://en.wikipedia.org/wiki/Cluster_analysis) (*clustering*) non √©tiquet√©es peut √™tre effectu√© avec le module [**`sklearn.cluster`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster).\n",
    "\n",
    "Chaque algorithme de regroupement se d√©cline en deux variantes¬†: une classe, qui impl√©mente la m√©thode `fit` pour apprendre les groupes sur les donn√©es d'entra√Ænement, et une fonction qui, compte tenu des donn√©es d'entra√Ænement, renvoie un tableau d'√©tiquettes enti√®res correspondant aux diff√©rents groupes. Pour la classe, les √©tiquettes sur les donn√©es d'regroupement se trouvent dans l'attribut `labels_`.\n",
    "\n",
    "**Donn√©es d'entr√©e**\n",
    "\n",
    "Il est important de noter que les algorithmes impl√©ment√©s dans ce module peuvent prendre diff√©rents types de matrices en entr√©e. Toutes les m√©thodes acceptent des matrices de donn√©es standard de forme `(n_samples, n_features)`. Celles-ci peuvent √™tre obtenues √† partir des classes du module [**`sklearn.feature_extraction`**](https://scikit-learn.org/stable/modules/clustering.html). Pour [**`AffinityPropagation`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html), [**`SpectralClustering`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html) et [**`DBSCAN`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html), il est √©galement possible d'entrer des matrices de similarit√© de forme `(n_samples, n_samples)`. Celles-ci peuvent √™tre obtenues √† partir des fonctions du module [**`sklearn.metrics.pairwise`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics.pairwise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='overview-of-clustering-methods'></a> 2.3.1. **Aper√ßu des m√©thodes de regroupement**<br/>([_Overview of clustering methods_](https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods))\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png\"\n",
    "    alt=\"Comparaison des algorithmes de regroupement dans scikit-learn\"\n",
    "    style=\"max-width: 100%; height: auto;\"/>\n",
    "    <p>Comparaison des algorithmes de regroupement dans scikit-learn</p>\n",
    "</div>\n",
    "\n",
    "|Nom de la m√©thode|Param√®tres|√âvolutivit√©|Usage|G√©om√©trie (m√©trique utilis√©e)|\n",
    "|-|-|-|-|-|\n",
    "|[**K-moyennes** (2.3.2)](https://scikit-learn.org/stable/modules/clustering.html#k-means)|nombre de groupes|`n_samples` tr√®s grand, `n_clusters` moyen avec [**mini-lots** (2.3.2.2)](https://scikit-learn.org/stable/modules/clustering.html#mini-batch-kmeans)|Usage g√©n√©ral, taille de groupe uniforme, g√©om√©trie plate, pas trop de groupes, inductif|Distances entre les points|\n",
    "|[**Propagation d'affinit√©** (2.3.3)](https://scikit-learn.org/stable/modules/clustering.html#affinity-propagation)|amortissement, pr√©f√©rence d'√©chantillon|Non scalable avec `n_samples`|De nombreux groupes, taille de groupe in√©gale, g√©om√©trie non plate, inductif|Distance du graphe (par exemple, graphe des voisins les plus proches)|\n",
    "|[**D√©placement moyen** (2.3.4)](https://scikit-learn.org/stable/modules/clustering.html#mean-shift)|bande passante|Non scalable avec `n_samples`|De nombreux groupes, taille de groupe in√©gale, g√©om√©trie non plate, inductif|Distances entre les points|\n",
    "|[**Regroupement spectral** (2.3.5)](https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering)|nombre de groupes|`n_samples` moyen, petit `n_clusters`|Peu de groupes, taille de groupe uniforme, g√©om√©trie non plate, transductif|Distance du graphe (par exemple, graphe des voisins les plus proches)|\n",
    "|[**Regroupement hi√©rarchique** (2.3.6)](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering)|nombre de groupes ou seuil de distance|Grands `n_samples` et `n_clusters`|De nombreux groupes, √©ventuellement des contraintes de connectivit√©, transductif|Distances entre les points|\n",
    "|[**Regroupement agglom√©ratif** (2.3.6)](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering)|nombre de groupes ou seuil de distance, type de liaison, distance|Grands `n_samples` et `n_clusters`|De nombreux groupes, √©ventuellement des contraintes de connectivit√©, distances non euclidiennes, transductif|Toute distance par paire|\n",
    "|[**DBSCAN** (2.3.7)](https://scikit-learn.org/stable/modules/clustering.html#dbscan)|taille de voisinage|`n_samples` tr√®s grand, `n_clusters` moyen|G√©om√©trie non plate, tailles de groupe in√©gales, √©limination des valeurs aberrantes, transductif|Distances entre les points les plus proches|\n",
    "|[**OPTICS** (2.3.8)](https://scikit-learn.org/stable/modules/clustering.html#optics)|appartenance minimale au groupe|Tr√®s grand `n_samples`, grand `n_clusters`|G√©om√©trie non plate, tailles de groupe in√©gales, densit√© de groupe variable, √©limination des valeurs aberrantes, transductif|Distances entre les points|\n",
    "|[**Mod√®les de m√©lange gaussien** (2.1)](https://scikit-learn.org/stable/modules/mixture.html#mixture)|nombreux|Non scalable|G√©om√©trie plate, bon pour l'estimation de densit√©, inductif|Distances de Mahalanobis aux centres|\n",
    "|[**BIRCH** (2.3.9)](https://scikit-learn.org/stable/modules/clustering.html#birch)|facteur de ramification, seuil, cluster global facultatif|Grands `n_clusters` et `n_samples`|Grand ensemble de donn√©es, √©limination des valeurs aberrantes, r√©duction de donn√©es, inductif|Distance euclidienne entre les points|\n",
    "|[**K-moyennes √† bissection** (2.3.6.5)](https://scikit-learn.org/stable/modules/clustering.html#bisect-k-means)|nombre de groupes|`n_samples` tr√®s grand, `n_clusters` moyen|Usage g√©n√©ral, taille de groupe uniforme, g√©om√©trie plate, pas de groupes vides, inductif, hi√©rarchique|Distances entre les points|\n",
    "\n",
    "Le regroupement avec une g√©om√©trie non plane est utile lorsque les groupes ont une forme sp√©cifique, c'est-√†-dire une vari√©t√© non plane, et que la distance euclidienne standard n'est pas la bonne m√©trique. Ce cas se pr√©sente dans les deux premi√®res lignes de la figure ci-dessus.\n",
    "\n",
    "Les mod√®les de m√©lange gaussien, utiles pour le regroupement, sont d√©crits dans un [**autre chapitre de la documentation d√©di√© aux mod√®les de m√©lange** (2.1)](https://scikit-learn.org/stable/modules/mixture.html#mixture). K-moyennes peut √™tre consid√©r√© comme un cas particulier de mod√®le de m√©lange gaussien avec une covariance √©gale par composant.\n",
    "\n",
    "Les m√©thodes de regroupement [**transductives**](https://scikit-learn.org/stable/glossary.html#term-transductive) (par opposition aux m√©thodes de regroupement [**inductives**](https://scikit-learn.org/stable/glossary.html#term-inductive)) ne sont pas con√ßues pour √™tre appliqu√©es √† de nouvelles donn√©es invisibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='k-means'></a> 2.3.2. **K-moyennes**<br/>([*K-means*](https://scikit-learn.org/stable/modules/clustering.html#k-means))\n",
    "\n",
    "L'algorithme [**`KMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) regroupe les donn√©es en essayant de s√©parer les √©chantillons en $n$ groupes de variance √©gale, minimisant un crit√®re connu sous le nom d'inertie ou de somme des carr√©s intra-cluster (voir ci-dessous). Cet algorithme exige que le nombre de groupes soit sp√©cifi√©. Il fonctionne bien pour un grand nombre d'√©chantillons et a √©t√© utilis√© dans une grande vari√©t√© de domaines d'application dans de nombreux champs diff√©rents.\n",
    "\n",
    "L'algorithme des k-moyennes divise un ensemble d'√©chantillons en $K$ groupes disjoints, chacun d√©crit par la moyenne $\\mu_j$ des √©chantillons du groupe. Les moyennes sont commun√©ment appel√©es les centro√Ødes (barycentres) du groupe; noter qu'ils ne sont pas, en g√©n√©ral, des points de $X$, bien qu'ils peuplent le m√™me espace.\n",
    "\n",
    "L'algorithme des k-moyennes vise √† choisir les centro√Ødes qui minimisent l'inertie, ou le crit√®re de somme des carr√©s intra-cluster :\n",
    "\n",
    "$$\\sum_{i=0}^{n}\\min_{\\mu_j \\in C}(||x_i - \\mu_j||^2)$$\n",
    "\n",
    "L'inertie peut √™tre vue comme une mesure de la coh√©rence interne des groupes. Cependant, elle pr√©sente certaines limitations :\n",
    "\n",
    "* L'inertie suppose que les groupes sont convexes et isotropes, ce qui n'est pas toujours le cas. Elle r√©agit mal aux groupes allong√©s ou aux vari√©t√©s aux formes irr√©guli√®res.\n",
    "* L'inertie n'est pas une m√©trique normalis√©e : nous savons seulement que des valeurs plus basses sont meilleures et que z√©ro est optimal. Cependant, dans les espaces de tr√®s grande dimension, les distances euclidiennes ont tendance √† devenir disproportionn√©es (c'est un cas particulier de ce qu'on appelle la \"mal√©diction de la dimension\"). L'utilisation d'un algorithme de r√©duction de dimension comme l'[**analyse en composantes principales (ACP)** (2.5.1)](https://scikit-learn.org/stable/modules/decomposition.html#pca) avant le regroupement k-moyennes peut att√©nuer ce probl√®me et acc√©l√©rer les calculs.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_kmeans_assumptions_002.png\"\n",
    "    alt=\"Groupes K-moyennes inappropri√©s\"\n",
    "    style=\"max-width: 75%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Les k-moyennes sont souvent appel√©es algorithme de Lloyd. En termes simples, l'algorithme comporte trois √©tapes. La premi√®re √©tape consiste √† choisir les centro√Ødes initiaux, la m√©thode la plus basique √©tant de choisir $k$ √©chantillons du jeu de donn√©es $X$. Apr√®s l'initialisation, les k-moyennes consistent en une boucle entre les deux autres √©tapes. La premi√®re √©tape attribue chaque √©chantillon au centro√Øde le plus proche. La deuxi√®me √©tape cr√©e de nouveaux centro√Ødes en prenant la valeur moyenne de tous les √©chantillons attribu√©s √† chaque centro√Øde pr√©c√©dent. La diff√©rence entre les anciens et les nouveaux centro√Ødes est calcul√©e et l'algorithme r√©p√®te ces deux derni√®res √©tapes jusqu'√† ce que cette valeur soit inf√©rieure √† un seuil. En d'autres termes, il se r√©p√®te jusqu'√† ce que les centro√Ødes ne bougent pas significativement.\n",
    "\n",
    "Les k-moyennes sont √©quivalentes √† l'algorithme d'esp√©rance-maximisation avec une petite matrice de covariance diagonale √©gale pour tous les groupes.\n",
    "\n",
    "L'algorithme peut √©galement √™tre compris √† travers le concept de [**diagrammes de Vorono√Ø** (wkpd)](https://en.wikipedia.org/wiki/Voronoi_diagram). Tout d'abord, le diagramme de Vorono√Ø des points est calcul√© en utilisant les centro√Ødes actuels. Chaque segment du diagramme de Vorono√Ø devient un groupe distinct. Ensuite, les centro√Ødes sont mis √† jour en prenant la valeur moyenne de chaque segment. L'algorithme r√©p√®te cela jusqu'√† ce qu'un crit√®re d'arr√™t soit satisfait. Habituellement, l'algorithme s'arr√™te lorsque la diminution relative de la fonction objectif entre les it√©rations est inf√©rieure √† la valeur de tol√©rance donn√©e. Ce n'est pas le cas dans cette impl√©mentation : l'it√©ration s'arr√™te lorsque les centro√Ødes bougent moins que la tol√©rance.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_kmeans_digits_001.png\"\n",
    "    alt=\"K-moyennes des chiffres manuscrits\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Avec suffisamment de temps, les k-moyennes convergeront toujours, cependant cela peut se faire vers un minimum local. Cela d√©pend fortement de l'initialisation des centro√Ødes. En cons√©quence, le calcul est souvent r√©p√©t√© plusieurs fois, avec des initialisations diff√©rentes des centro√Ødes. Une m√©thode pour rem√©dier √† ce probl√®me est le sch√©ma d'initialisation k-moyennes++, qui a √©t√© mis en ≈ìuvre dans scikit-learn (utilisez le param√®tre `init='k-means++'`). Cela initialise les centro√Ødes pour qu'ils soient (g√©n√©ralement) √©loign√©s les uns des autres, conduisant probablement √† de meilleurs r√©sultats qu'une initialisation al√©atoire, comme montr√© dans la r√©f√©rence.\n",
    "\n",
    "L'initialisation k-moyennes++ peut √©galement √™tre appel√©e ind√©pendamment pour s√©lectionner des amorces pour d'autres algorithmes de regroupement, voir [**`sklearn.cluster.kmeans_plusplus`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.kmeans_plusplus.html) pour plus de d√©tails et des exemples d'utilisation.\n",
    "\n",
    "L'algorithme supporte les poids d'√©chantillon, qui peuvent √™tre sp√©cifi√©s via le param√®tre `sample_weight`. Cela permet d'attribuer plus de poids √† certains √©chantillons lors du calcul des centro√Ødes de groupe et des valeurs d'inertie. Par exemple, attribuer un poids de 2 √† un √©chantillon √©quivaut √† ajouter une copie de cet √©chantillon √† l'ensemble de donn√©es $X$.\n",
    "\n",
    "Les k-moyennes peuvent √™tre utilis√©es pour la quantification vectorielle. Cela est accompli en utilisant la m√©thode `transform` d'un mod√®le entra√Æn√© de [**`KMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='low-level-parallelism'></a> 2.3.2.1. **Parall√©lisme de bas niveau**<br/>([*Low-level parallelism*](https://scikit-learn.org/stable/modules/clustering.html#low-level-parallelism))\n",
    "\n",
    "[**`KMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) b√©n√©ficie du parall√©lisme bas√© sur OpenMP via Cython. De petits morceaux de donn√©es (256 √©chantillons) sont trait√©s en parall√®le, ce qui permet en outre de r√©duire la consommation de m√©moire. Pour plus de d√©tails sur la mani√®re de contr√¥ler le nombre de threads, veuillez vous r√©f√©rer √† nos notes sur le [**parall√©lisme** (8.3.1)](https://scikit-learn.org/stable/computing/parallelism.html#parallelism).\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**D√©monstration des hypoth√®ses de k-moyennes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_3_cluster/plot_kmeans_assumptions.ipynb)<br/>([_Demonstration of k-means assumptions_](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html))\n",
    "\n",
    "D√©monstration de quand les k-moyennes fonctionnent de mani√®re intuitive et quand ce n'est pas le cas.\n",
    "\n",
    "##### [**Une d√©mo du clustering K-moyennes sur les donn√©es des chiffres manuscrits**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_3_cluster/plot_kmeans_digits.ipynb)<br/>([_A demo of K-Means clustering on the handwritten digits data_](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html))\n",
    "\n",
    "Clustering des chiffres manuscrits.\n",
    "\n",
    "#### R√©f√©rences\n",
    "\n",
    "üî¨ Arthur, David and Sergei Vassilvitskii [**‚Äú`k-means++`: The Advantages of Careful Seeding‚Äú**](http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf), Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, Society for Industrial and Applied Mathematics (2007)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='mini-batch-k-means'></a> 2.3.2.2. **K-moyennes par mini-lots**<br/>([*Mini Batch K-Means*](https://scikit-learn.org/stable/modules/clustering.html#mini-batch-k-means))\n",
    "\n",
    "Le [**`MiniBatchKMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html) est une variante de l'algorithme [**`KMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) qui utilise des mini-lots pour r√©duire le temps de calcul, tout en essayant d'optimiser la m√™me fonction objectif. Les mini-lots sont des sous-ensembles des donn√©es d'entr√©e, √©chantillonn√©s de mani√®re al√©atoire √† chaque it√©ration d'apprentissage. Ces mini-lots r√©duisent consid√©rablement la quantit√© de calculs n√©cessaires pour converger vers une solution locale. Contrairement √† d'autres algorithmes qui r√©duisent le temps de convergence des k-moyennes, les mini-batch k-means produisent des r√©sultats qui ne sont g√©n√©ralement que l√©g√®rement moins bons que l'algorithme standard.\n",
    "\n",
    "L'algorithme it√®re entre deux √©tapes principales, similaires aux k-moyennes classiques. Dans la premi√®re √©tape, des √©chantillons sont tir√©s au hasard de l'ensemble de donn√©es pour former un mini-lot. Ces √©chantillons sont ensuite attribu√©s au centro√Øde le plus proche. Dans la deuxi√®me √©tape, les centro√Ødes sont mis √† jour. Contrairement aux k-moyennes, cela se fait sur une base par √©chantillon. Pour chaque √©chantillon du mini-lot, le centro√Øde attribu√© est mis √† jour en calculant la moyenne continue de l'√©chantillon et de tous les √©chantillons pr√©c√©dents attribu√©s √† ce centro√Øde. Cela a pour effet de r√©duire le taux de changement d'un centro√Øde au fil du temps. Ces √©tapes sont ex√©cut√©es jusqu'√† la convergence ou jusqu'√† ce qu'un nombre pr√©d√©termin√© d'it√©rations soit atteint.\n",
    "\n",
    "Le [**`MiniBatchKMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html) converge plus rapidement que [**`KMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), mais la qualit√© des r√©sultats est r√©duite. En pratique, cette diff√©rence de qualit√© peut √™tre assez faible, comme le montrent l'exemple et la r√©f√©rence cit√©e.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_mini_batch_kmeans_001.png\"\n",
    "    alt=\"Comparaison des algorithmes de regroupement K-Means et MiniBatchKMeans\"\n",
    "    style=\"max-width: 75%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Comparaison des algorithmes de regroupement `KMeans` et `MiniBatchKMeans`**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_3_cluster/plot_mini_batch_kmeans.ipynb)<br/>([*Comparison of the `KMeans` and `MiniBatchKMeans` clustering algorithms*](https://scikit-learn.org/stable/auto_examples/cluster/plot_mini_batch_kmeans.html))\n",
    "\n",
    "Comparaison des algorithmes [**`KMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) et [**`MiniBatchKMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html).\n",
    "\n",
    "##### [**Regroupement de documents texte √† l'aide de k-means**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/text/plot_document_clustering.ipynb)<br/>([*Clustering text documents using k-means*](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html))\n",
    "\n",
    "Regroupement de documents texte √† l'aide de MiniBatchKMeans pour donn√©es √©parses.\n",
    "\n",
    "##### [**Apprentissage en ligne d'un dictionnaire de parties de visages**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_3_cluster/plot_dict_face_patches.ipynb)<br/>([*Online learning of a dictionary of parts of faces*](https://scikit-learn.org/stable/auto_examples/cluster/plot_dict_face_patches.html))\n",
    "\n",
    "#### R√©f√©rences\n",
    "\n",
    "üî¨ D. Sculley [**‚ÄúWeb Scale K-Means Clustering‚Äù**](https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf), Proceedings of the 19th international conference on World wide web (2010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='affinity-propagation'></a> 2.3.3. **Propagation d'affinit√©**<br/>([*Affinity Propagation*](https://scikit-learn.org/stable/modules/clustering.html#affinity-propagation))\n",
    "\n",
    "La **Propagation d'affinit√©** r√©alis√©e par la classe [**`AffinityPropagation`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html) cr√©e des grappes en envoyant des messages entre des paires d'√©chantillons jusqu'√† convergence. Un ensemble de donn√©es est ensuite d√©crit √† l'aide d'un petit nombre d'exemplaires, qui sont identifi√©s comme les plus repr√©sentatifs des autres √©chantillons. Les messages envoy√©s entre les paires repr√©sentent l'ad√©quation pour qu'un √©chantillon soit l'exemplaire de l'autre, et ces valeurs sont mises √† jour en r√©ponse aux valeurs des autres paires. Cette mise √† jour se fait de mani√®re it√©rative jusqu'√† la convergence, moment o√π les exemplaires finaux sont s√©lectionn√©s et, par cons√©quent, le regroupement final est obtenu.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_affinity_propagation_001.png\"\n",
    "    alt=\"D√©mo de l'algorithme de clustering par propagation d'affinit√©\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "La Propagation d'affinit√© peut √™tre int√©ressante car elle choisit le nombre de grappes en fonction des donn√©es fournies. √Ä cette fin, les deux param√®tres importants sont la _pr√©f√©rence_, qui contr√¥le le nombre d'exemplaires utilis√©s, et le _facteur d'amortissement_ qui att√©nue les messages de responsabilit√© et de disponibilit√© pour √©viter les oscillations num√©riques lors de la mise √† jour de ces messages.\n",
    "\n",
    "Le principal inconv√©nient de la Propagation d'affinit√© est sa complexit√©. L'algorithme a une complexit√© temporelle de l'ordre de $\\mathcal{O}(N^2 T)$, o√π $N$ est le nombre d'√©chantillons et $T$ est le nombre d'it√©rations jusqu'√† la convergence. De plus, la complexit√© m√©moire est de l'ordre de $\\mathcal{O}(N^2)$ si une matrice de similarit√© dense est utilis√©e, mais elle est r√©duite si une matrice de similarit√© creuse est utilis√©e. Cela rend la Propagation d'affinit√© plus appropri√©e pour les ensembles de donn√©es de petite √† moyenne taille.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**D√©mo de l'algorithme de clustering par propagation d'affinit√©**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_3_cluster/plot_affinity_propagation.ipynb)<br/>([*Demo of affinity propagation clustering algorithm*](https://scikit-learn.org/stable/auto_examples/cluster/plot_affinity_propagation.html))\n",
    "\n",
    "Propagation d'affinit√© sur un jeu de donn√©es 2D synth√©tique avec 3 classes.\n",
    "\n",
    "#### [**Visualisation de la structure du march√© boursier**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/applications/plot_stock_market.ipynb)<br/>([*Visualizing the stock market structure*](https://scikit-learn.org/stable/auto_examples/applications/plot_stock_market.html))\n",
    "\n",
    "Propagation d'affinit√© sur des s√©ries chronologiques financi√®res pour identifier des groupes d'entreprises.\n",
    "\n",
    "### R√©f√©rences\n",
    "\n",
    "üî¨ Brendan J. Frey, Delbert Dueck, [**‚ÄúClustering by Passing Messages Between Data Points‚Äù**](https://www.science.org/cms/asset/231f2875-11f5-44cd-98dc-b3e583f9fddd/pap.pdf), Science Feb. 2007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description de l'algorithme\n",
    "\n",
    "Les messages envoy√©s entre les points appartiennent √† l'une des deux cat√©gories. Le premier est la responsabilit√© $r(i, k)$, qui repr√©sente les preuves accumul√©es que l'√©chantillon $k$ devrait √™tre le repr√©sentant de l'√©chantillon $i$. Le second est la disponibilit√© $a(i, k)$ qui repr√©sente les preuves accumul√©es que l'√©chantillon $i$ devrait choisir l'√©chantillon $k$ comme son repr√©sentant, et consid√®re les valeurs de tous les autres √©chantillons dont $k$ devrait √™tre le repr√©sentant. De cette mani√®re, les repr√©sentants sont choisis par des √©chantillons s'ils sont (1) suffisamment similaires √† de nombreux √©chantillons et (2) choisis par de nombreux √©chantillons pour √™tre repr√©sentatifs d'eux-m√™mes.\n",
    "\n",
    "Plus formellement, la responsabilit√© d'un √©chantillon $k$ pour √™tre le repr√©sentant de l'√©chantillon $i$ est donn√© par¬†:\n",
    "\n",
    "$$r(i, k) \\leftarrow s(i, k) - max [ a(i, k') + s(i, k') \\forall k' \\neq k ]$$\n",
    "\n",
    "O√π $s(i, k)$ est la similarit√© entre les √©chantillons $i$ et $k$. La disponibilit√© de l'√©chantillon $k$ pour √™tre le repr√©sentant de l'√©chantillon $i$ est donn√© par:\n",
    "\n",
    "$$a(i, k) \\leftarrow min [0, r(k, k) + \\sum_{i'~s.t.~i' \\notin \\{i, k\\}}{r(i', k)}]$$\n",
    "\n",
    "Pour commencer, toutes les valeurs de $r$ et $a$ sont mises √† z√©ro, et le calcul de chacun it√®re jusqu'√† convergence. Comme discut√© ci-dessus, afin d'√©viter les oscillations num√©riques lors de la mise √† jour des messages, le facteur d'amortissement $\\lambda$ est introduit au processus d'it√©ration¬†:\n",
    "\n",
    "$$r_{t+1}(i, k) = \\lambda\\cdot r_{t}(i, k) + (1-\\lambda)\\cdot r_{t+1}(i, k)$$\n",
    "$$a_{t+1}(i, k) = \\lambda\\cdot a_{t}(i, k) + (1-\\lambda)\\cdot a_{t+1}(i, k)$$\n",
    "\n",
    "o√π $t$ indique les temps d'it√©ration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='mean-shift'></a> 2.3.4. **D√©placement moyen**<br/>([*Mean Shift*](https://scikit-learn.org/stable/modules/clustering.html#mean-shift))\n",
    "\n",
    "Le regroupement [**`MeanShift`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html) vise √† d√©couvrir des _blobs_ dans une densit√© lisse d'√©chantillons. Il s'agit d'un algorithme bas√© sur le centro√Øde, qui fonctionne en mettant √† jour les candidats pour les centro√Ødes afin qu'ils soient la moyenne des points dans une r√©gion donn√©e. Ces candidats sont ensuite filtr√©s dans une √©tape de post-traitement pour √©liminer les quasi-doublons pour former l'ensemble final de centro√Ødes.\n",
    "\n",
    "La position des candidats pour les centro√Ødes est ajust√©e de mani√®re it√©rative √† l'aide d'une technique appel√©e _hill climbing_ (escalade de colline), qui recherche les maxima locaux de la densit√© de probabilit√© estim√©e. √âtant donn√© un candidat pour le centro√Øde $x_i$ √† l'it√©ration $t$, le candidat est mis √† jour selon l'√©quation suivante :\n",
    "\n",
    "$$x_i^{t+1} = m(x_i^t)$$\n",
    "\n",
    "O√π $N(x_i)$ repr√©sente le voisinage des √©chantillons √† une certaines distance de $x_i$ et $m$ est le vecteur de *d√©placement moyen* qui est calcul√© pour chaque centro√Øde et pointe vers une r√©gion d'augmentation maximale de la densit√© de points. Cela est calcul√© √† l'aide de l'√©quation suivante, mettant efficacement √† jour un centro√Øde pour qu'il corresponde √† la moyenne des √©chantillons dans son voisinage¬†:\n",
    "\n",
    "$$m(x_i) = \\frac{\\sum_{x_j \\in N(x_i)}K(x_j - x_i)x_j}{\\sum_{x_j \\in N(x_i)}K(x_j - x_i)}$$\n",
    "\n",
    "L'algorithme d√©termine automatiquement le nombre de grappes au lieu de d√©pendre d'un param√®tre de largeur de bande `bandwidth`, qui contr√¥le la taille de la r√©gion √† explorer. Ce param√®tre peut √™tre fix√© manuellement, mais il peut √™tre √©galement estim√© √† l'aide de la fonction `estimate_bandwidth` fournie, qui est appel√©e si la largeur de bande n'est pas d√©finie.\n",
    "\n",
    "L'algorithme n'est pas tr√®s √©volutif, car il n√©cessite plusieurs recherches des plus proches voisins pendant son ex√©cution. Toutefois, l'algorithme converge n√©cessairement, et il cesse d'it√©rer lorsque le changement dans les centro√Ødes devient faible.\n",
    "\n",
    "L'√©tiquetage d'un nouvel √©chantillon est effectu√© en trouvant le centro√Øde le plus proche pour un √©chantillon donn√©.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_mean_shift_001.png\"\n",
    "    alt=\"Estimated number of clusters: 3\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Une d√©monstration de l'algorithme de regroupement par d√©placement moyen**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_3_cluster/plot_mean_shift.ipynb)<br/>([*A demo of the mean-shift clustering algorithm*](https://scikit-learn.org/stable/auto_examples/cluster/plot_mean_shift.html))\n",
    "\n",
    "Regroupement par d√©placement moyen sur un jeu de donn√©es 2D synth√©tique avec 3¬†classes.\n",
    "\n",
    "### R√©f√©rences\n",
    "\n",
    "üî¨ Dorin Comaniciu, Peter Meer, [**‚ÄúMean Shift: A Robust Approach Toward Feature Space Analysis‚Äù**](https://ieeexplore.ieee.org/document/1000236). IEEE Transactions on Pattern Analysis and Machine Intelligence. 2002. pp. 603-619."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='spectral-clustering'></a> 2.3.5. **Regroupement spectral**<br/>([*Spectral clustering*](https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering))\n",
    "\n",
    "[**`SpectralClustering`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html) effectue un plongement de faible dimension de la matrice d'affinit√© entre les √©chantillons, suivie d'un regroupement, par exemple, par KMeans, des composantes des vecteurs propres dans l'espace de faible dimension. Il est particuli√®rement efficace en termes de calcul si la matrice d'affinit√© est creuse et que le solveur `amg` est utilis√© pour le probl√®me des valeurs propres (notez que le solveur `amg` n√©cessite que le module [**`pyamg`**](https://github.com/pyamg/pyamg) soit install√©.)\n",
    "\n",
    "La version actuelle de [**`SpectralClustering`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html) n√©cessite que le nombre de grappes soit sp√©cifi√© √† l'avance. Cela fonctionne bien pour un petit nombre de grappes, mais n'est pas conseill√© pour de nombreux grappes.\n",
    "\n",
    "Pour deux grappes, [**`SpectralClustering`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html) r√©sout une relaxation convexe du probl√®me des [coupes normalis√©es](https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf) sur le graphe de similarit√© : il s'agit de diviser le graphe en deux parties de mani√®re √† ce que le poids des ar√™tes coup√©es soit faible par rapport aux poids des ar√™tes √† l'int√©rieur de chaque grappe. Ce crit√®re est particuli√®rement int√©ressant en traitement d'images, o√π les sommets du graphe repr√©sentent les pixels et les poids des ar√™tes du graphe de similarit√© sont calcul√©s en fonction du gradient de l'image.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_segmentation_toy_001.png\"\n",
    "    alt=\"Spectral clustering for image segmentation\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_segmentation_toy_002.png\"\n",
    "    alt=\"Spectral clustering for image segmentation\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "### Avertissement : Transformer la distance en similitudes bien √©lev√©es\n",
    "\n",
    "> Notez que si les valeurs de votre matrice de similarit√© ne sont pas correctement r√©parties, par exemple avec des valeurs n√©gatives ou avec une matrice de distances plut√¥t qu'une matrice de similarit√©s, le probl√®me spectral sera singulier et insoluble. Dans ce cas, il est recommand√© d'appliquer une transformation aux entr√©es de la matrice. Par exemple, dans le cas d'une matrice de distances sign√©es, il est courant d'appliquer un noyau de chaleur :\n",
    ">\n",
    "> ```similarit√© = np.exp(-beta * distance / distance.std())```\n",
    ">\n",
    "> Consultez les exemples pour une telle application.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Regroupement spectral pour la segmentation d'images**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_3_cluster/plot_segmentation_toy.ipynb)<br/>([*Spectral clustering for image segmentation*](https://scikit-learn.org/stable/auto_examples/cluster/plot_segmentation_toy.html))\n",
    "\n",
    "Segmentation d'objets √† partir d'un arri√®re-plan bruyant en utilisant le regroupement spectral.\n",
    "\n",
    "#### [**Segmenter en r√©gions l'image des pi√®ces de monnaie grecques**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_3_cluster/plot_coin_segmentation.ipynb)<br/>([*Segmenting the picture of greek coins in regions*](https://scikit-learn.org/stable/auto_examples/cluster/plot_coin_segmentation.html))\n",
    "\n",
    "Regroupement spectral pour diviser l'image des pi√®ces de monnaie grecques en r√©gions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='different-label-assignment-strategies'></a> 2.3.5.1. **Strat√©gies d'attribution d'√©tiquettes**<br/>([*Different label assignment strategies*](https://scikit-learn.org/stable/modules/clustering.html#different-label-assignment-strategies))\n",
    "\n",
    "Diff√©rentes strat√©gies d'attribution d'√©tiquettes peuvent √™tre utilis√©es, correspondant au param√®tre `assign_labels` de [**`SpectralClustering`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html). La strat√©gie `\"kmeans\"` peut correspondre √† des d√©tails plus fins, mais peut √™tre instable. En particulier, √† moins que vous ne contr√¥liez le `random_state`, il peut ne pas √™tre reproductible d'une ex√©cution √† l'autre, car il d√©pend de l'initialisation al√©atoire. La strat√©gie alternative `\"discretize\"` est reproductible √† 100%, mais tend √† cr√©er des parcelles de forme assez r√©guli√®re et g√©om√©trique. L'option `\"cluster_qr\"` r√©cemment ajout√©e est une alternative d√©terministe qui tend √† cr√©er le meilleur partitionnement visuel sur l'exemple d'application ci-dessous.\n",
    "\n",
    "<table class=\"docutils align-default\">\n",
    "<thead>\n",
    "<tr class=\"row-odd\"><th class=\"head\"><p><code class=\"docutils literal notranslate\"><span class=\"pre\">assign_labels=&quot;kmeans&quot;</span></code></p></th>\n",
    "<th class=\"head\"><p><code class=\"docutils literal notranslate\"><span class=\"pre\">assign_labels=&quot;discretize&quot;</span></code></p></th>\n",
    "<th class=\"head\"><p><code class=\"docutils literal notranslate\"><span class=\"pre\">assign_labels=&quot;cluster_qr&quot;</span></code></p></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr class=\"row-even\"><td><p><a class=\"reference external\" href=\"https://scikit-learn.org/stable/auto_examples/cluster/plot_coin_segmentation.html\"><img alt=\"coin_kmeans\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_coin_segmentation_001.png\" style=\"width: 175.0px; height: 175.0px;\" /></a></p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://scikit-learn.org/stable/auto_examples/cluster/plot_coin_segmentation.html\"><img alt=\"coin_discretize\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_coin_segmentation_002.png\" style=\"width: 175.0px; height: 175.0px;\" /></a></p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://scikit-learn.org/stable/auto_examples/cluster/plot_coin_segmentation.html\"><img alt=\"coin_cluster_qr\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_coin_segmentation_003.png\" style=\"width: 175.0px; height: 175.0px;\" /></a></p></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "##### R√©f√©rences\n",
    "\n",
    "üî¨ Stella X. Yu, Jianbo Shi, [**‚ÄúMulticlass spectral clustering‚Äù**](http://lear.inrialpes.fr/people/triggs/events/iccv03/cdrom/iccv03/0313_yu.pdf), 2003\n",
    "\n",
    "üî¨ Anil Damle, Victor Minden, Lexing Ying, [**‚ÄúSimple, direct, and efficient multi-way spectral clustering‚Äù**](https://doi.org/10.1093/imaiai/iay008), 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='spectral-clustering-graphs'></a> 2.3.5.2. **Graphes de regroupement spectral**<br/>([*Spectral Clustering Graphs*](https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering-graphs))\n",
    "\n",
    "Le regroupement spectral peut √©galement √™tre utilis√© pour partitionner des graphes via leurs plongements spectraux. Dans ce cas, la matrice d'affinit√© est la matrice d'adjacence du graphe, et SpectralClustering est initialis√© avec `affinity='precomputed'` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "sc = SpectralClustering(3, affinity='precomputed', n_init=100,\n",
    "                        assign_labels='discretize')\n",
    "sc.fit_predict(adjacency_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### R√©f√©rences\n",
    "\n",
    "üî¨ Ulrike von Luxburg,  [**‚ÄúA Tutorial on Spectral Clustering‚Äù**](https://arxiv.org/pdf/0711.0189.pdf), 2007\n",
    "\n",
    "üî¨ Jianbo Shi, Jitendra Malik, [**‚ÄúNormalized cuts and image segmentation‚Äù**](https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf), 2000\n",
    "\n",
    "üî¨ Marina Meila, Jianbo Shi, [**‚ÄúA Random Walks View of Spectral Segmentation‚Äù**](https://www.ri.cmu.edu/pub_files/pub3/maila_marina_2001_2/maila_marina_2001_2.pdf), 2001\n",
    "\n",
    "üî¨ Andrew Y. Ng, Michael I. Jordan, Yair Weiss, [**‚ÄúOn Spectral Clustering: Analysis and an algorithm‚Äù**](https://proceedings.neurips.cc/paper/2001/file/801272ee79cfde7fa5960571fee36b9b-Paper.pdf), 2001\n",
    "\n",
    "üî¨ David Zhuzhunashvili, Andrew Knyazev, [**‚ÄúPreconditioned Spectral Clustering for Stochastic Block Partition Streaming Graph Challenge‚Äù**](https://arxiv.org/pdf/1708.07481.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='hierarchical-clustering'></a> 2.3.6. **Regroupement hi√©rarchique**<br/>([*Hierarchical clustering*](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering))\n",
    "\n",
    "Le regroupement hi√©rarchique est une famille g√©n√©rale d'algorithmes de regroupement qui construisent des grappes imbriqu√©es en les fusionnant ou en les divisant successivement. Cette hi√©rarchie de grappes est repr√©sent√©e sous forme d'arbre (ou dendrogramme). La racine de l'arbre est la grappe unique qui rassemble tous les √©chantillons, les feuilles √©tant les grappes √† un seul √©chantillon. Consulter la [**page Wikip√©dia**](https://en.wikipedia.org/wiki/Hierarchical_clustering) pour plus de d√©tails.\n",
    "\n",
    "L'objet [**`AgglomerativeClustering`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) effectue un regroupement hi√©rarchique en utilisant une approche ascendante¬†: chaque observation commence dans sa propre grappe, et les grappes sont successivement fusionn√©es. Les crit√®res de liaison d√©terminent la m√©trique utilis√©e pour la strat√©gie de fusion¬†:\n",
    "\n",
    "* **Ward** minimise la somme des diff√©rences au carr√© dans toutes les grappes. Il s'agit d'une approche de minimisation de la variance et, √† cet √©gard, elle est similaire √† la fonction objectif k-moyennes, mais abord√©e avec une approche hi√©rarchique agglom√©rative.\n",
    "* **La liaison maximale** ou **compl√®te** minimise la distance maximale entre les observations de paires de grappes.\n",
    "* **Le couplage moyen** minimise la moyenne des distances entre toutes les observations de paires de grappes.\n",
    "* **Le couplage unique** minimise la distance entre les observations les plus proches des paires de grappes.\n",
    "\n",
    "L'[**`AgglomerativeClustering`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) peut √©galement √©voluer vers un grand nombre d'√©chantillons lorsqu'il est utilis√© conjointement avec une matrice de connectivit√©, mais il est co√ªteux en calcul lorsque aucune contrainte de connectivit√© n'est ajout√©e entre les √©chantillons : il consid√®re √† chaque √©tape toutes les fusions possibles.\n",
    "\n",
    "[**`FeatureAgglomeration`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)\n",
    "\n",
    "Le [**`FeatureAgglomeration`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) utilise le regroupement agglom√©ratif pour regrouper les caract√©ristiques qui semblent tr√®s similaires, r√©duisant ainsi le nombre de caract√©ristiques. C'est un outil de r√©duction de dimensionnalit√©, voir [**R√©duction de dimensionnalit√© non supervis√©e** (6.5)](https://scikit-learn.org/stable/modules/unsupervised_reduction.html#data-reduction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='different-linkage-type-ward-complete-average-and-single-linkage'></a> 2.3.6.1. **Types de liaison : Ward, compl√®te, moyenne et liaison simple**<br/>([_Different linkage type: Ward, complete, average, and single linkage_](https://scikit-learn.org/stable/modules/clustering.html#different-linkage-type-ward-complete-average-and-single-linkage))\n",
    "\n",
    "L'[**`AgglomerativeClustering`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) prend en charge les strat√©gies de liaison Ward, unique, moyenne et compl√®te.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_linkage_comparison_001.png\"\n",
    "    alt=\"Single Linkage, Average Linkage, Complete Linkage, Ward Linkage\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Le regroupement agglom√©ratif pr√©sente un comportement de type \"le riche devient plus riche\" qui conduit √† des tailles de grappes in√©gales. √Ä cet √©gard, la liaison simple est la strat√©gie la moins performante, tandis que Ward permet d'obtenir des tailles de grappes plus r√©guli√®res. Cependant, avec Ward, l'affinit√© (ou la distance utilis√©e dans le regroupement) ne peut pas √™tre modifi√©e. Ainsi, pour les mesures non euclidiennes, la liaison moyenne est une bonne alternative. La liaison simple, bien qu'elle ne soit pas robuste face aux donn√©es bruit√©es, peut √™tre calcul√©e tr√®s efficacement, ce qui la rend utile pour effectuer un regroupement hi√©rarchique sur de grands ensembles de donn√©es. La liaison simple peut √©galement bien fonctionner sur des donn√©es non globulaires.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Divers regroupement agglom√©ratifs sur un plongement 2D de chiffres**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_3_cluster/plot_digits_linkage.ipynb)<br/>([*Various Agglomerative Clustering on a 2D embedding of digits*](https://scikit-learn.org/stable/auto_examples/cluster/plot_digits_linkage.html))\n",
    "\n",
    "Exploration des diff√©rentes strat√©gies de liaison dans un jeu de donn√©es r√©el."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='visualization-of-cluster-hierarchy'></a> 2.3.6.2. **Visualisation de la hi√©rarchie des groupes**<br/>([_Visualization of cluster hierarchy_](https://scikit-learn.org/stable/modules/clustering.html#visualization-of-cluster-hierarchy))\n",
    "\n",
    "Il est possible de visualiser l'arbre repr√©sentant la fusion hi√©rarchique des grappes sous forme de dendrogramme. L'inspection visuelle peut souvent √™tre utile pour comprendre la structure des donn√©es, mais plus encore dans le cas d'√©chantillons de petite taille.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_dendrogram_001.png\"\n",
    "    alt=\"Hierarchical Clustering Dendrogram\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='adding-connectivity-constraints'></a> 2.3.6.3. **Ajout de contraintes de connectivit√©**<br/>([*Adding connectivity constraints*](https://scikit-learn.org/stable/modules/clustering.html#adding-connectivity-constraints))\n",
    "\n",
    "Un aspect int√©ressant de l'[**`AgglomerativeClustering`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) est la possibilit√© d'ajouter des contraintes de connectivit√© √† cet algorithme (seuls les groupes adjacents peuvent √™tre fusionn√©s), via une matrice de connectivit√© qui d√©finit pour chaque √©chantillon les √©chantillons voisins suivant une structure donn√©e des donn√©es. Par exemple, dans l'exemple du \"swiss roll\" ci-dessous, les contraintes de connectivit√© interdisent la fusion des points qui ne sont pas adjacents sur le \"swiss roll\", √©vitant ainsi la formation de groupes qui s'√©tendent sur des plis superpos√©s du \"roll\".\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_ward_structured_vs_unstructured_001.png\"\n",
    "    alt=\"Without connectivity constraints\"\n",
    "    style=\"max-width: 45%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_ward_structured_vs_unstructured_002.png\"\n",
    "    alt=\"With connectivity constraints\"\n",
    "    style=\"max-width: 45%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Ces contraintes sont utiles pour imposer une certaine structure locale, mais elles rendent aussi l'algorithme plus rapide, surtout lorsque le nombre d'√©chantillons est √©lev√©.\n",
    "\n",
    "Les contraintes de connectivit√© sont impos√©es via une matrice de connectivit√© : une matrice creuse de Scipy qui ne contient d'√©l√©ments qu'√† l'intersection d'une ligne et d'une colonne ayant des indices de l'ensemble de donn√©es qui doivent √™tre connect√©s. Cette matrice peut √™tre construite √† partir d'informations a priori : par exemple, vous pouvez souhaiter regrouper des pages Web en ne fusionnant que les pages avec un lien pointant de l'une √† l'autre. Il peut √©galement √™tre appris √† partir des donn√©es, par exemple en utilisant [**`sklearn.neighbors.kneighbors_graph`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.kneighbors_graph.html) pour restreindre la fusion aux voisins les plus proches, comme dans [**cet exemple**](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_clustering.html), ou en utilisant [**`sklearn.feature_extraction.image.grid_to_graph`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.image.grid_to_graph.html) pour permettre uniquement la fusion des pixels voisins sur une image, comme dans l'[**exemple de la pi√®ce**](https://scikit-learn.org/stable/auto_examples/cluster/plot_coin_ward_segmentation.html).\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "#### [**D√©mo de regroupement hi√©rarchique Ward structur√© sur une image de pi√®ces de monnaie**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_3_cluster/plot_coin_ward_segmentation.ipynb)<br/>([*A demo of structured Ward hierarchical clustering on an image of coins*](https://scikit-learn.org/stable/auto_examples/cluster/plot_coin_ward_segmentation.html))\n",
    "\n",
    "Utilisation de l'algorithme Ward pour diviser l'image de pi√®ces de monnaie en r√©gions.\n",
    "\n",
    "#### [**Regroupement hi√©rarchique : ward structur√© vs non structur√©**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_3_cluster/plot_coin_ward_segmentation.ipynb)<br/>([*Hierarchical clustering: structured vs unstructured ward*](https://scikit-learn.org/stable/auto_examples/cluster/plot_coin_ward_segmentation.html))\n",
    "\n",
    "Exemple de l'algorithme de Ward sur un \"swiss roll\", comparaison entre les approches structur√©es et non structur√©es.\n",
    "\n",
    "#### [**Agglom√©ration de caract√©ristiques vs s√©lection univari√©e**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_3_cluster/plot_feature_agglomeration_vs_univariate_selection.ipynb)<br/>([*Feature agglomeration vs. univariate selection*](https://scikit-learn.org/stable/auto_examples/cluster/plot_feature_agglomeration_vs_univariate_selection.html))\n",
    "\n",
    "Exemple de r√©duction de dimensionnalit√© avec agglom√©ration de caract√©ristiques bas√©e sur le regroupement hi√©rarchique de Ward.\n",
    "\n",
    "#### [**Agglom√©ration avec et sans structure**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_3_cluster/plot_agglomerative_clustering.ipynb)<br/>([*Agglomerative clustering with and without structure*](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_clustering.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avertissement : **Contraintes de connectivit√© avec liaison simple, moyenne et compl√®te**\n",
    "\n",
    "> Les contraintes de connectivit√© et les liaisons simples, compl√®tes ou moyennes peuvent renforcer l'aspect \"le riche devient plus riche\" du regroupement agglom√©ratif, en particulier si elles sont construites avec [**`sklearn.neighbors.kneighbors_graph`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.kneighbors_graph.html). Avec un petit nombre de grappes, elles ont tendance √† produire quelques grappes tr√®s peupl√©es et d'autres presque vides sur le plan macroscopique. (voir la discussion dans [**Agglom√©ration avec et sans structure**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_3_cluster/plot_agglomerative_clustering.ipynb)). La liaison unique est l'option de liaison la plus vuln√©rable √† cet √©gard.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_clustering_001.png\"\n",
    "    alt=\"n_cluster=30, connectivity=False, linkage=average\"\n",
    "    style=\"max-width: 45%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_clustering_002.png\"\n",
    "    alt=\"n_cluster=3, connectivity=False, linkage=average\"\n",
    "    style=\"max-width: 45%; height: auto;\"/>\n",
    "</div>\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_clustering_003.png\"\n",
    "    alt=\"n_cluster=30, connectivity=True, linkage=average\"\n",
    "    style=\"max-width: 45%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_clustering_004.png\"\n",
    "    alt=\"n_cluster=3, connectivity=True, linkage=average\"\n",
    "    style=\"max-width: 45%; height: auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='varying-the-metric'></a> 2.3.6.4. **Variation de la m√©trique**<br/>([*Varying the metric*](https://scikit-learn.org/stable/modules/clustering.html#varying-the-metric))\n",
    "\n",
    "Une liaison simple, moyenne et compl√®te peut √™tre utilis√©e avec une vari√©t√© de distances (ou affinit√©s), en particulier la distance euclidienne ($\\ell_2$), la distance de Manhattan (ou Cityblock, ou $\\ell_1$), la distance cosinus ou toute matrice d'affinit√© pr√©calcul√©e.\n",
    "* La distance $\\ell_1$ est souvent bien adapt√©e pour des caract√©ristiques creuses ou du bruit creux: c'est-√†-dire lorsque bon nombre des caract√©ristiques sont nulles, comme dans l'exploration de texte utilisant des occurrences de mots rares.\n",
    "* La distance cosinus est int√©ressante car elle est invariante par rapport aux mises √† l'√©chelle globales du signal.\n",
    "\n",
    "Les directives pour choisir une m√©trique sconsistent √† utiliser celle qui maximise la distance entre les √©chantillons de diff√©rentes classes et minimise cette distance au sein de chaque classe.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_clustering_metrics_005.png\"\n",
    "    alt=\"AgglomerativeClustering(metric=cosine)\"\n",
    "    style=\"max-width: 30%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_clustering_metrics_006.png\"\n",
    "    alt=\"AgglomerativeClustering(metric=euclidean)\"\n",
    "    style=\"max-width: 30%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_clustering_metrics_007.png\"\n",
    "    alt=\"AgglomerativeClustering(metric=cityblock)\"\n",
    "    style=\"max-width: 30%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Regroupement agglom√©ratif avec diff√©rentes m√©triques**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_3_cluster/plot_agglomerative_clustering_metrics.ipynb)<br/>([*Agglomerative clustering with different metrics*](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_clustering_metrics.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='bisecting-k-means'></a> 2.3.6.5. **K-moyennes √† bissection**<br/>([*Bisecting K-Means*](https://scikit-learn.org/stable/modules/clustering.html#bisecting-k-means))\n",
    "\n",
    "Le [**`BisectingKMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.BisectingKMeans.html) est une variante it√©rative de l'algorithme [**`KMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), qui utilise le regroupement hi√©rarchique de division. Au lieu de cr√©er tous les centro√Ødes en une seule fois, les centro√Ødes sont s√©lectionn√©s progressivement en fonction d'un regroupement pr√©c√©dent¬†: une grappe est divis√©e en deux nouvelles grappes √† plusieurs reprises jusqu'√† ce que le nombre cible de grappes soit atteint.\n",
    "\n",
    "Le [**`BisectingKMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.BisectingKMeans.html) est plus efficace que l'algorithme [**`KMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) lorsque le nombre de grappes est important, car il ne travaille que sur un sous-ensemble de donn√©es √† chaque division, tandis que [**`KMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) travaille toujours sur l'ensemble du jeu de donn√©es.\n",
    "\n",
    "Bien que le [**`BisectingKMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.BisectingKMeans.html) ne puisse pas b√©n√©ficier des avantages de l'initialisation `\"k-means++\"` par conception, il produira n√©anmoins des r√©sultats comparables √† `KMeans(init=\"k-means++\")` en termes d'inertie, tout en √©tant plus √©conome en co√ªt de calcul, et il produira probablement de meilleurs r√©sultats que `KMeans` avec une initialisation al√©atoire.\n",
    "\n",
    "Cette variante est plus efficace pour le regroupement agglom√©ratif si le nombre de grappes est marginal par rapport au nombre de points de donn√©es.\n",
    "\n",
    "Cette variante ne produit pas non plus de grappes vides.\n",
    "\n",
    "**Il existe deux strat√©gies pour s√©lectionner la grappe √† diviser :**\n",
    "* `bisecting_strategy=\"largest_cluster\"` s√©lectionne la grappe ayant le plus de points\n",
    "* `bisecting_strategy=\"biggest_inertia\"` s√©lectionne la grappe avec la plus grande inertie (c'est-√†-dire la grappe avec la plus grande somme des carr√©s d'erreurs)\n",
    "\n",
    "La s√©lection en fonction du plus grand nombre de points de donn√©es produit g√©n√©ralement des r√©sultats aussi pr√©cis que la s√©lection bas√©e sur l'inertie, et elle est plus rapide (surtout pour un grand nombre de points de donn√©es, o√π le calcul de l'erreur peut √™tre co√ªteux).\n",
    "\n",
    "La s√©lection bas√©e sur le plus grand nombre de points de donn√©es est √©galement susceptible de produire des clusters de tailles similaires, tandis que `KMeans` est connu pour g√©n√©rer des clusters de tailles diff√©rentes.\n",
    "\n",
    "La diff√©rence entre Bisecting K-Means et Regular K-Means peut √™tre vue sur l'exemple [**Comparaison des performances entre le Bisecting K-Means et le Regular K-Means **](https://scikit-learn.org/stable/auto_examples/cluster/plot_bisect_kmeans.html). Alors que l'algorithme K-Means r√©gulier a tendance √† cr√©er des grappes non li√©es, les grappes de Bisecting K-Means sont bien ordonn√©es et cr√©ent une hi√©rarchie assez visible.\n",
    "\n",
    "#### R√©f√©rences\n",
    "\n",
    "üî¨ Michael Steinbach, George Karypis and Vipin Kumar, [**‚ÄúA Comparison of Document Clustering Techniques‚Äù**](http://www.philippe-fournier-viger.com/spmf/bisectingkmeans.pdf), Department of Computer Science and Egineering, University of Minnesota (June 2000)\n",
    "\n",
    "üî¨ K.Abirami and Dr.P.Mayilvahanan, [**‚ÄúPerformance Analysis of K-Means and Bisecting K-Means Algorithms in Weblog Data‚Äù**](https://ijeter.everscience.org/Manuscripts/Volume-4/Issue-8/Vol-4-issue-8-M-23.pdf), International Journal of Emerging Technologies in Engineering Research (IJETER) Volume 4, Issue 8, (August 2016)\n",
    "\n",
    "üî¨ Jian Di, Xinyue Gou, [**‚ÄúBisecting K-means Algorithm Based on K-valued Self-determining and Clustering Center Optimization‚Äù**](http://www.jcomputers.us/vol13/jcp1306-01.pdf), School of Control and Computer Engineering, North China Electric Power University, Baoding, Hebei, China (August 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='dbscan'></a> 2.3.7. **DBSCAN**<br/>([*DBSCAN*](https://scikit-learn.org/stable/modules/clustering.html#dbscan))\n",
    "\n",
    "L'algorithme [**`DBSCAN`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) consid√®re les clusters comme des zones de haute densit√© s√©par√©es par des zones de faible densit√©. En raison de cette vue plut√¥t g√©n√©rique, les clusters trouv√©s par DBSCAN peuvent avoir n'importe quelle forme, contrairement √† k-means qui suppose que les clusters sont de forme convexe. L'√©l√©ment central du DBSCAN est le concept d'**√©chantillons centraux** (*core samples*), qui sont des √©chantillons qui se trouvent dans des zones √† haute densit√©. Une grappe est donc un ensemble d'√©chantillons centraux, chacun proche l'un de l'autre (mesur√© par une mesure de distance) et un ensemble d'√©chantillons non centraux qui sont proches d'un √©chantillon central (mais ne sont pas eux-m√™mes des √©chantillons centraux). Il y a deux param√®tres dans l'algorithme, `min_samples` et `eps`, qui d√©finissent formellement ce que nous voulons dire quand nous disons *dense*. Des `min_samples` plus √©lev√©s ou des `eps` plus faibles indiquent une densit√© plus √©lev√©e n√©cessaire pour former un cluster.\n",
    "\n",
    "Plus formellement, nous d√©finissons un √©chantillon central comme √©tant un √©chantillon dans l'ensemble de donn√©es tel qu'il existe `min_samples` d'autres √©chantillons √† une distance de `eps`, qui sont d√©finis comme voisins de l'√©chantillon central. Cela nous indique que l'√©chantillon central se trouve dans une zone dense de l'espace vectoriel. Un cluster est un ensemble d'√©chantillons centraux qui peuvent √™tre construits en prenant de mani√®re r√©cursive un √©chantillon central, en trouvant tous ses voisins qui sont des √©chantillons centraux, en trouvant tous leurs voisins qui sont des √©chantillons centraux, etc. Un cluster a √©galement un ensemble d'√©chantillons non centraux, qui sont des √©chantillons voisins d'un √©chantillon central dans le cluster mais qui ne sont pas eux-m√™mes des √©chantillons centraux. Intuitivement, ces √©chantillons sont en marge d'un cluster.\n",
    "\n",
    "Tout √©chantillon central fait partie d'une grappe, par d√©finition. Tout √©chantillon qui n'est pas un √©chantillon central et qui se trouve √† une distance d'au moins `eps` de tout √©chantillon central est consid√©r√© comme une valeur aberrante par l'algorithme.\n",
    "\n",
    "Alors que le param√®tre `min_samples` contr√¥le principalement la tol√©rance de l'algorithme au bruit (sur des ensembles de donn√©es bruyants et volumineux, il peut √™tre souhaitable d'augmenter ce param√®tre), le param√®tre `eps` est *crucial pour choisir de mani√®re appropri√©e*  l'ensemble de donn√©es et la fonction de distance et ne peut g√©n√©ralement pas √™tre laiss√© √† sa valeur par d√©faut. Il contr√¥le le voisinage local des points. Lorsque cette valeur est trop petite, la plupart des donn√©es ne seront pas du tout regroup√©es (et √©tiquet√©es √† `-1` pour ¬´¬†bruit¬†¬ª). Lorsqu'elle est trop grande, cel√† provoque la fusion des clusters proches en un seul cluster, et finalement le retour de l'ensemble de donn√©es complet en un seul cluster. Certaines heuristiques pour choisir ce param√®tre ont √©t√© discut√©es dans la litt√©rature, par exemple bas√©es sur un genou dans le trac√© des distances du plus proche voisin (comme discut√© dans les r√©f√©rences ci-dessous).\n",
    "\n",
    "Dans la figure ci-dessous, la couleur indique l'appartenance au cluster, avec de grands cercles indiquant les √©chantillons centraux trouv√©s par l'algorithme. Les cercles plus petits sont des √©chantillons non centraux qui font toujours partie d'un cluster. De plus, les valeurs aberrantes sont indiqu√©es par des points noirs ci-dessous.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_dbscan_002.png\"\n",
    "    alt=\"Estimated number of clusters: 3\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "### Exemple\n",
    "\n",
    "#### [**D√©mo de l'algorithme de regroupement DBSCAN**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_3_cluster/plot_dbscan.ipynb)<br/>([*Demo of DBSCAN clustering algorithm*](https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impl√©mentation\n",
    "\n",
    "L'algorithme DBSCAN est d√©terministe, g√©n√©rant toujours les m√™mes grappes pour les m√™mes donn√©es dans le m√™me ordre. Ces r√©sultats peuvent n√©anmoins diff√©rer si les donn√©es sont fournies dans un ordre diff√©rent. D'abord, m√™me si les √©chantillons centraux seront toujours affect√©s aux m√™mes grappes, les √©tiquettes de ces grappes d√©pendent de l'ordre dans lequel ces √©chantillons sont rencontr√©s dans les donn√©es. Ensuite, et plus important encore, les grappes auxquelles les √©chantillons non centraux sont affect√©s peuvent diff√©rer selon l'ordre des donn√©es. Cela se produit lorsqu'un √©chantillon non central a une distance inf√©rieure √† `eps` √† deux √©chantillons centraux dans des grappes diff√©rentes. En raison de l'in√©galit√© triangulaire, ces deux √©chantillons centraux doivent √™tre plus √©loign√©s que `eps` l'un de l'autre, sinon ils seraient dans la m√™me grappe. L'√©chantillon non central est affect√© √† la premi√®re grappe g√©n√©r√©e lors d'un passage √† travers les donn√©es, et les r√©sultats d√©pendent donc de l'ordre des donn√©es.\n",
    "\n",
    "L'impl√©mentation actuelle utilise des arbres √† billes et des kd-arbres pour d√©terminer le voisinage des points, ce qui √©vite de calculer la matrice de distance compl√®te (comme cela √©tait fait dans les versions scikit-learn ant√©rieures √† 0.14). La possibilit√© d'utiliser des m√©triques personnalis√©es est conserv√©e ; pour plus de d√©tails, voir `NearestNeighbors`.\n",
    "\n",
    "### Consommation m√©moire pour les √©chantillons de grande taille\n",
    "\n",
    "Par d√©faut, cette impl√©mentation n'est pas particuli√®rement efficace en termes de m√©moire, car elle construit une matrice de similarit√© compl√®te en termes de paires lorsque les arbres kd ou les arbres de billes ne peuvent pas √™tre utilis√©s (par exemple, avec des matrices creuses). Cette matrice consommera $n^2$ nombres flottants. Il existe cependant quelques m√©canismes pour contourner ce probl√®me :\n",
    "* Utiliser le clustering [**OPTICS** (2.3.8)](https://scikit-learn.org/stable/modules/clustering.html#optics) conjointement avec la m√©thode `extract_dbscan`. Le regroupement OPTICS calcule √©galement la matrice de similarit√© compl√®te en termes de paires, mais ne conserve qu'une seule ligne en m√©moire √† la fois (complexit√© m√©moire de l'ordre de $n$).\n",
    "* Un graphe de voisinage √† rayon creux (o√π les entr√©es manquantes sont suppos√©es √™tre en dehors `eps`) peut √™tre pr√©calcul√© d'une mani√®re efficace en termes de m√©moire et DBSCAN peut √™tre ex√©cut√© dessus avec l'argument `metric='precomputed'`. Voir [**`sklearn.neighbors.NearestNeighbors.radius_neighbors_graph`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html).\n",
    "* L'ensemble de donn√©es peut √™tre compress√©, soit en supprimant les doublons exacts s'ils existent dans vos donn√©es, soit en utilisant l'algorithme BIRCH. Dans ce cas, vous disposez d'un nombre relativement restreint de repr√©sentants pour un grand nombre de points. Vous pouvez ensuite fournir un poids d'√©chantillon (`sample_weight`) lors de l'ajustement de DBSCAN.\n",
    "\n",
    "### R√©f√©rences\n",
    "\n",
    "üî¨ Ester, M., H. P. Kriegel, J. Sander, and X. Xu, [**‚ÄúA Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise‚Äù**](https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf), In Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, OR, AAAI Press, pp. 226‚Äì231. 1996\n",
    "\n",
    "üî¨ Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X., [**‚ÄúDBSCAN revisited, revisited: why and how you should (still) use DBSCAN‚Äù**](https://www.ccs.neu.edu/home/vip/teach/DMcourse/2_cluster_EM_mixt/notes_slides/revisitofrevisitDBSCAN.pdf), In ACM Transactions on Database Systems (TODS), 42(3), 19, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='hdbscan'></a> 2.3.8. **HDBSCAN**<br/>([*HDBSCAN*](https://scikit-learn.org/stable/modules/clustering.html#hdbscan))\n",
    "\n",
    "L'algorithme [**`HDBSCAN`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.HDBSCAN.html) peut √™tre consid√©r√© comme une extension de [**`DBSCAN`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) et [**`OPTICS`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.OPTICS.html). Plus pr√©cis√©ment, [**`DBSCAN`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) suppose que le crit√®re de regroupement (c'est-√†-dire l'exigence de densit√©) est _globalement homog√®ne_. En d'autres termes, [**`DBSCAN`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) peut avoir du mal √† capturer avec succ√®s des grappes de densit√©s diff√©rentes. [**`HDBSCAN`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.HDBSCAN.html) att√©nue cette hypoth√®se et explore toutes les √©chelles de densit√© possibles en construisant une repr√©sentation alternative du probl√®me de regroupement.\n",
    "\n",
    "**Note:** Cette impl√©mentation est adapt√©e de l'impl√©mentation originale de HDBSCAN, [scikit-learn-contrib/hdbscan](https://github.com/scikit-learn-contrib/hdbscan), bas√©e sur [LJ2017]. \n",
    "\n",
    "### <a id='mutual-reachability-graph'></a> 2.3.8.1. **Graphe d'accessibilit√© mutuelle**<br/>([*Mutual Reachability Graph*](https://scikit-learn.org/stable/modules/clustering.html#mutual-reachability-graph))\n",
    "\n",
    "HDBSCAN d√©finit d'abord $d_c(x_p)$, la _distance centrale_ d'un √©chantillon $x_p$, comme la distance jusqu'√† son `min_samples`-√®me voisin le plus proche (en se comptant lui-m√™me). Par exemple, si `min_samples=5` et que $x_*$ est le 5√®me voisin le plus proche de $x_p$, alors la distance centrale est :\n",
    "\n",
    "$$d_c(x_p) = d(x_p, x_*)$$\n",
    "\n",
    "Ensuite, il d√©finit $d_m(x_p, x_q)$, la _distance d'accessibilit√© mutuelle_ de deux points $x_p, x_q$, comme :\n",
    "\n",
    "$$d_m(x_p, x_q) = \\max\\{d_c(x_p), d_c(x_q), d(x_p, x_q)\\}$$\n",
    "\n",
    "Ces deux notions nous permettent de construire le _graphe d'accessibilit√© mutuelle_ $G_{ms}$ d√©fini pour un choix donn√© de `min_samples` en associant chaque √©chantillon $x_p$ √† un sommet du graphe, et donc les ar√™tes entre les points $x_p, x_q$ sont leur distance d'accessibilit√© mutuelle $d_m(x_p, x_q)$. Nous pouvons construire des sous-ensembles de ce graphe, not√©s $G_{ms,\\varepsilon}$, en supprimant toutes les ar√™tes dont la valeur est sup√©rieure √† $\\varepsilon$. √Ä ce stade, tous les points dont la distance centrale est inf√©rieure √† $\\varepsilon$ sont marqu√©s comme du bruit. Les points restants sont ensuite regroup√©s en trouvant les composantes connexes de ce graphe r√©duit.\n",
    "\n",
    "> **Note:** Prendre les composantes connexes d'un graphe r√©duit $G_{ms,\\varepsilon}$ √©quivaut √† ex√©cuter DBSCAN* avec `min_samples` et $\\varepsilon$. DBSCAN* est une version l√©g√®rement modifi√©e de DBSCAN mentionn√©e dans [CM2013].\n",
    "\n",
    "\n",
    "### <a id='hierarchical-clustering'></a> 2.3.8.2. **Regroupement hi√©rarchique**<br/>([*Hierarchical Clustering*](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering))\n",
    "\n",
    "HDBSCAN peut √™tre consid√©r√© comme un algorithme qui effectue le regroupement DBSCAN* pour toutes les valeurs de $\\varepsilon$. Comme mentionn√© pr√©c√©demment, cela √©quivaut √† trouver les composantes connexes des graphes d'accessibilit√© mutuelle pour toutes les valeurs de $\\varepsilon$. Pour ce faire de mani√®re efficace, HDBSCAN extrait d'abord un arbre de recouvrement minimal (MST, _Minimum Spanning Tree_) du graphe d'accessibilit√© mutuelle totalement connexe, puis coupe de mani√®re gloutonne les ar√™tes ayant le poids le plus √©lev√©. Un aper√ßu de l'algorithme HDBSCAN est le suivant :\n",
    "\n",
    "1. Extraire le MST de $G_{ms}$\n",
    "2. √âtendre le MST en ajoutant une ¬´ ar√™te propre ¬ª pour chaque sommet, avec un poids √©gal √† la distance centrale de l'√©chantillon sous-jacent.\n",
    "3. Initialiser une seule grappe et une √©tiquette pour le MST.\n",
    "4. Supprimer l'ar√™te ayant le poids le plus √©lev√© du MST (les √©galit√©s sont supprim√©es simultan√©ment).\n",
    "5. Attribuer des √©tiquettes de grappe aux composantes connexes contenant les extr√©mit√©s de l'ar√™te maintenant supprim√©e. Si la composante n'a pas au moins une ar√™te, elle re√ßoit √† la place une √©tiquette \"nulle\" la marquant comme du bruit.\n",
    "6. R√©p√©ter 4-5 jusqu'√† ce qu'il n'y ait plus de composantes connect√©es.\n",
    "\n",
    "HDBSCAN est donc capable d'obtenir toutes les partitions possibles r√©alisables par DBSCAN* pour un choix fixe de `min_samples` de mani√®re hi√©rarchique. En effet, cela permet √† HDBSCAN d'effectuer le regroupement sur de multiples densit√©s et, en tant que tel, il n'a plus besoin de $\\varepsilon$ comme hyperparam√®tre. Il d√©pend uniquement du choix de `min_samples`, qui tend √† √™tre un hyperparam√®tre plus robuste.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_hdbscan_005.png\"\n",
    "    alt=\"True number of clusters: 4\"\n",
    "    style=\"max-width: 75%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_hdbscan_007.png\"\n",
    "    alt=\"Estimated number of clusters: 4\"\n",
    "    style=\"max-width: 75%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "HDBSCAN peut √™tre adouci avec un hyperparam√®tre suppl√©mentaire `min_cluster_size`, qui sp√©cifie que pendant le regroupement hi√©rarchique, les composantes avec moins de `minimum_cluster_size` √©chantillons soient consid√©r√©es comme du bruit. En pratique, on peut d√©finir `minimum_cluster_size = min_samples` pour coupler les param√®tres et simplifier l'espace des hyperparam√®tres.\n",
    "\n",
    "#### References\n",
    "\n",
    "Z üî¨ [CM2013]Campello, R.J.G.B., Moulavi, D., Sander, J. (2013). [‚Äú**Density-Based Clustering Based on Hierarchical Density Estimates‚Äù**](https://link.springer.com/chapter/10.1007/978-3-642-37456-2_14). In: Pei, J., Tseng, V.S., Cao, L., Motoda, H., Xu, G. (eds) Advances in Knowledge Discovery and Data Mining. PAKDD 2013. Lecture Notes in Computer Science(), vol 7819. Springer, Berlin, Heidelberg. \n",
    "\n",
    "Z üî¨ [LJ2017] L. McInnes and J. Healy, (2017). [‚Äú**Accelerated Hierarchical Density Based Clustering‚Äù**](https://arxiv.org/pdf/1705.07321.pdf). In: IEEE International Conference on Data Mining Workshops (ICDMW), 2017, pp. 33-42. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='optics'></a> 2.3.9. **OPTICS**<br/>([*OPTICS*](https://scikit-learn.org/stable/modules/clustering.html#optics))\n",
    "\n",
    "L'algorithme [**`OPTICS`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.OPTICS.html) partage de nombreux points communs avec l'algorithme [**`DBSCAN`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) et peut √™tre consid√©r√© comme une g√©n√©ralisation de celui-ci qui assouplit l'exigence d'un `eps` d'une valeur unique √† une plage de valeurs. La principale diff√©rence entre DBSCAN et OPTICS est que l'algorithme OPTICS construit un graphe d'_accessibilit√©_, qui attribue √† chaque √©chantillon √† la fois une distance de `reachability_` et une position dans l'attribut `ordering_` (_ordonnancement_) de la grappe ; ces deux attributs sont attribu√©s lors de l'ajustement du mod√®le et sont utilis√©s pour d√©terminer l'appartenance √† la grappe. Si OPTICS est ex√©cut√© avec `max_eps` d√©fini √† la valeur par d√©faut `inf`, l'extraction de grappe de type DBSCAN peut √™tre effectu√©e de mani√®re r√©p√©t√©e en temps lin√©aire pour toute valeur donn√©e de `eps` en utilisant la m√©thode `cluster_optics_dbscan`. D√©finir `max_eps` sur une valeur inf√©rieure entra√Ænera des temps d'ex√©cution plus courts et peut √™tre consid√©r√© comme le rayon de voisinage maximal de chaque point pour trouver d'autres points potentiellement accessibles.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_optics_001.png\"\n",
    "    alt=\"Reachability Plot, Automatic Clustering OPTICS\"\n",
    "    style=\"max-width: 75%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Les distances d'_accessibilit√©_ g√©n√©r√©es par OPTICS permettent une extraction √† densit√© variable de grappes pour un ensemble de donn√©es _donn√©_. Comme le montre le graphique ci-dessus, la combinaison des distances d'*accessibilit√©* et de l'`ordering_` (ordonnancement) des ensembles de donn√©es produit un _graphique d'accessibilit√©_, o√π la densit√© de points est repr√©sent√©e sur l'axe Y, et o√π les points sont ordonn√©s de telle sorte que les points proches soient adjacents. \"Couper\" le graphique d'accessibilit√© √† une seule valeur produit des r√©sultats de type DBSCAN¬†; tous les points au-dessus de la ¬´coupe¬ª sont class√©s comme du bruit, et chaque interruption lors de la lecture de gauche √† droite correspond √† une nouvelle grappe. L'extraction de grappe par d√©faut avec OPTICS examine les pentes raides dans le graphe pour trouver des grappes, et l'utilisateur peut contr√¥ler la raideur de pente avec le param√®tre `xi`. Il existe d'autres possibilit√©s d'analyse sur le graphe lui-m√™me, telles que la g√©n√©ration de repr√©sentations hi√©rarchiques des donn√©es via des dendrogrammes d'accessibilit√©, et la hi√©rarchie des grappes d√©tect√©es par l'algorithme est accessible via le param√®tre `cluster_hierarchy_`. Le trac√© ci-dessus a √©t√© cod√© par couleur afin que les couleurs des grappes dans l'espace plan correspondent aux grappes de segments lin√©aires du graphique d'accessibilit√©. Notez que les grappes bleu et rouge sont adjacentes dans le graphique d'accessibilit√© et peuvent √™tre repr√©sent√©s hi√©rarchiquement comme des enfants d'une grappe parente plus large.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**D√©mo de l'algorithme de regroupement OPTICS**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_3_cluster/plot_optics.ipynb)<br/>([*Demo of OPTICS clustering algorithm*](https://scikit-learn.org/stable/auto_examples/cluster/plot_optics.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison avec DBSCAN\n",
    "\n",
    "Les r√©sultats de la m√©thode OPTICS `cluster_optics_dbscan` et de DBSCAN sont tr√®s similaires, mais pas toujours identiques¬†; plus pr√©cis√©ment, l'√©tiquetage de la p√©riph√©rie et des points de bruit. Cela s'explique en partie par le fait que les premiers √©chantillons de chaque zone dense trait√©e par OPTICS ont une grande valeur d'accessibilit√© tout en √©tant proches d'autres points de leur zone, et seront donc parfois marqu√©s comme \"bruit\" plut√¥t que comme \"p√©riph√©rie\". Cela affecte les points adjacents lorsqu'ils sont consid√©r√©s comme des candidats pour √™tre marqu√©s comme \"p√©riph√©rie\" ou \"bruit\".\n",
    "\n",
    "Notez que pour toute valeur unique d'`eps`, DBSCAN aura tendance √† avoir un temps d'ex√©cution plus court que OPTICS¬†; cependant, pour des ex√©cutions r√©p√©t√©es √† diff√©rentes valeurs `eps`, une seule ex√©cution d'OPTICS peut n√©cessiter moins de temps d'ex√©cution cumul√© que DBSCAN. Il est √©galement important de noter que la sortie d'OPTICS est proche de celle de DBSCAN uniquement si `eps` et `max_eps` sont proches.\n",
    "\n",
    "### Complexit√© algorithmique\n",
    "\n",
    "Les arbres d'indexation spatiale sont utilis√©s pour √©viter de calculer la matrice de distance compl√®te et permettent une utilisation efficace de la m√©moire sur de grands ensembles d'√©chantillons. Diff√©rentes m√©triques de distance peuvent √™tre fournies via le mot-cl√© `metric`.\n",
    "\n",
    "Pour les grands ensembles de donn√©es, des r√©sultats similaires (mais pas identiques) peuvent √™tre obtenus via [**`HDBSCAN`**](https://hdbscan.readthedocs.io/en/latest/). L'impl√©mentation HDBSCAN est multithread√©e et a une meilleure complexit√© algorithmique qu'OPTICS, au prix d'une plus mauvaise mise √† l'√©chelle de la m√©moire. Pour les ensembles de donn√©es particuli√®rement volumineux qui √©puisent la m√©moire du syst√®me √† l'aide de HDBSCAN, OPTICS conservera une complexit√© spatiale (m√©moire) de d'ordre $n$ (contre $n^2$)¬†; cependant, le r√©glage du param√®tre `max_eps` devra probablement √™tre utilis√© pour permettre de produire une solution dans un d√©lai raisonnable.\n",
    "\n",
    "### R√©f√©rences:\n",
    "\n",
    "üî¨ Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel et J√∂rg Sander., [**‚ÄúOPTICS: Ordering Points To Identify the Clustering Structure‚Äù**](https://www.dbs.ifi.lmu.de/Publikationen/Papers/OPTICS.pdf), In ACM Sigmod Record, vol. 28, non. 2, p. 49-60. AMC, 1999."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.9. BIRCH\n",
    "\n",
    "Le [**`Birch`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.Birch.html) construit un arbre appel√© Clustering Feature Tree (CFT) pour les donn√©es donn√©es. Les donn√©es sont essentiellement compress√©es avec perte vers un ensemble de n≈ìuds de caract√©ristiques de clustering (n≈ìuds CF). Les n≈ìuds CF ont un certain nombre de sous-clusters appel√©s sous-clusters de caract√©ristiques de clustering (sous-clusters CF) et ces sous-clusters CF situ√©s dans les n≈ìuds CF non terminaux peuvent avoir des n≈ìuds CF comme enfants.\n",
    "\n",
    "Les sous-grappes CF contiennent les informations n√©cessaires au regroupement, ce qui √©vite d'avoir √† conserver toutes les donn√©es d'entr√©e en m√©moire. Ces informations comprennent¬†:\n",
    "* Nombre d'√©chantillons dans un sous-cluster.\n",
    "* Somme lin√©aire - Un vecteur √† n dimensions contenant la somme de tous les √©chantillons\n",
    "* Somme au carr√© - Somme de la norme L2 au carr√© de tous les √©chantillons.\n",
    "* Centro√Ødes - Pour √©viter le recalcul somme lin√©aire / n_samples.\n",
    "* Norme au carr√© des centro√Ødes.\n",
    "\n",
    "L'algorithme BIRCH a deux param√®tres, le seuil et le facteur de branchement. Le facteur de branchement limite le nombre de sous-grappes dans un n≈ìud et le seuil limite la distance entre l'√©chantillon entrant et les sous-grappes existantes.\n",
    "\n",
    "Cet algorithme peut √™tre consid√©r√© comme une instance ou une m√©thode de r√©duction de donn√©es, car il r√©duit les donn√©es d'entr√©e √† un ensemble de sous-clusters qui sont obtenus directement √† partir des feuilles du CFT. Ces donn√©es r√©duites peuvent √™tre trait√©es ult√©rieurement en les introduisant dans un cluster global. Ce clusterer global peut √™tre d√©fini par `n_clusters`. Si `n_clusters` est d√©fini sur `None`, les sous-clusters des feuilles sont directement lus, sinon une √©tape de clustering global √©tiquette ces sous-clusters en clusters globaux (√©tiquettes) et les √©chantillons sont mapp√©s sur l'√©tiquette globale du sous-cluster le plus proche."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description de l'algorithme¬†:\n",
    "\n",
    "* Un nouvel √©chantillon est ins√©r√© dans la racine de l'arborescence CF qui est un n≈ìud CF. Il est ensuite fusionn√© avec le sous-cluster de la racine, qui a le plus petit rayon apr√®s fusion, contraint par les conditions de seuil et de facteur de branchement. Si le sous-cluster a un n≈ìud enfant, cela est r√©p√©t√© jusqu'√† ce qu'il atteigne une feuille. Apr√®s avoir trouv√© le sous-cluster le plus proche dans la feuille, les propri√©t√©s de ce sous-cluster et des sous-clusters parents sont mises √† jour de mani√®re r√©cursive.\n",
    "* Si le rayon du sous-groupe obtenu en fusionnant le nouvel √©chantillon et le sous-groupe le plus proche est sup√©rieur au carr√© du seuil et si le nombre de sous-groupes est sup√©rieur au facteur de branchement, alors un espace est temporairement allou√© √† ce nouvel √©chantillon. Les deux sous-clusters les plus √©loign√©s sont pris et les sous-clusters sont divis√©s en deux groupes sur la base de la distance entre ces sous-clusters.\n",
    "* Si ce n≈ìud divis√© a un sous-cluster parent et qu'il y a de la place pour un nouveau sous-cluster, alors le parent est divis√© en deux. S'il n'y a pas de place, ce n≈ìud est √† nouveau divis√© en deux et le processus se poursuit de mani√®re r√©cursive, jusqu'√† ce qu'il atteigne la racine.\n",
    "\n",
    "### BIRCH ou MiniBatchKMeans¬†?\n",
    "\n",
    "* BIRCH ne s'adapte pas tr√®s bien aux donn√©es de grande dimension. En r√®gle g√©n√©rale, si `n_features` est sup√©rieur √† vingt, il est g√©n√©ralement pr√©f√©rable d'utiliser MiniBatchKMeans.\n",
    "* Si le nombre d'instances de donn√©es doit √™tre r√©duit, ou si l'on souhaite un grand nombre de sous-clusters, soit comme √©tape de pr√©traitement, soit autrement, BIRCH est plus utile que MiniBatchKMeans.\n",
    "\n",
    "### Comment utiliser partial_fit¬†?\n",
    "\n",
    "Pour √©viter le calcul du clustering global, pour chaque appel de `partial_fit` il est conseill√© de :\n",
    "1. D√©finir initialement `n_clusters=None`\n",
    "2. Entra√Æner toutes les donn√©es par plusieurs appels √† `partial_fit`.\n",
    "3. D√©finir `n_clusters` sur une valeur requise √† l'aide de `brc.set_params(n_clusters=n_clusters)`.\n",
    "4. Appeler enfin `partial_fit` sans arguments, c'est-√†-dire `brc.partial_fit()` qui effectue le clustering global.\n",
    "\n",
    "<img alt=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_birch_vs_minibatchkmeans_001.png\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_birch_vs_minibatchkmeans_001.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**Compare BIRCH and MiniBatchKMeans**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_3_cluster/plot_birch_vs_minibatchkmeans.ipynb)<br/>([*Comparez BIRCH et MiniBatchKMeans*](https://scikit-learn.org/stable/auto_examples/cluster/plot_birch_vs_minibatchkmeans.html))\n",
    "\n",
    "### R√©f√©rences\n",
    "\n",
    "[?] Tian Zhang, Raghu Ramakrishnan, Maron Livny [‚Äú**BIRCH: An efficient data clustering method for large databases**](https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf)[‚Äù](https://drive.google.com/file/d/1n8-C65wdcXpHHrErzrAswXeuuBc24uas/view?usp=share_link)\n",
    "\n",
    "[?] Roberto Perdisci JBirch - [Java implementation of BIRCH clustering algorithm](https://code.google.com/archive/p/jbirch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.10. √âvaluation des performances de clustering\n",
    "\n",
    "√âvaluer les performances d'un algorithme de clustering n'est pas aussi simple que compter le nombre d'erreurs ou la pr√©cision et le rappel d'un algorithme de classification supervis√©e. En particulier, plut√¥t que de prendre en compte les valeurs absolues des √©tiquettes de cluster, une m√©trique d'√©valuation meure si ce regroupement d√©finit des s√©parations des donn√©es similaires √† un ensemble de classes de v√©rit√© terrain ou satisfaisant √† une hypoth√®se telle que les membres appartenant √† la m√™me classe sont plus similaires que les membres de classes diff√©rentes selon une m√©trique de similarit√©.\n",
    "\n",
    "### 2.3.10.1. Indice Rand\n",
    "\n",
    "Compte tenu de la connaissance des affectations de classe de v√©rit√© terrain `labels_true` et de nos affectations d'algorithme de regroupement des m√™mes √©chantillons `labels_pred`, l'**indice Rand (ajust√© ou non ajust√©)** est une fonction qui mesure la **similarit√©** des deux affectations, en ignorant les permutations¬†:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "labels_true = [0, 0, 0, 1, 1, 1]\n",
    "labels_pred = [0, 0, 1, 1, 2, 2]\n",
    "metrics.rand_score(labels_true, labels_pred)\n",
    "# 0.66..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'indice Rand ne garantit pas d'obtenir une valeur proche de 0,0 pour un √©tiquetage al√©atoire. L'indice Rand ajust√© **corrige le hasard** et donnera une telle valeur de r√©f√©rence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24242424242424243"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.adjusted_rand_score(labels_true, labels_pred)\n",
    "# 0.24..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme pour toutes les m√©triques de clustering, on peut permuter 0 et 1 dans les libell√©s pr√©dits, renommer 2 en 3 et obtenir le m√™me score¬†:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24242424242424243"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_pred = [1, 1, 0, 0, 3, 3]\n",
    "metrics.rand_score(labels_true, labels_pred)\n",
    "# 0.66...\n",
    "metrics.adjusted_rand_score(labels_true, labels_pred)\n",
    "# 0.24..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De plus, [**`rand_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.rand_score.html) et [**`ajusted_rand_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html) sont tous deux **sym√©triques**¬†: l'√©change d'arguments ne modifie pas les scores. Ils peuvent ainsi √™tre utilis√©s comme **mesures de consensus** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24242424242424243"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.rand_score(labels_pred, labels_true)\n",
    "# 0.66...\n",
    "metrics.adjusted_rand_score(labels_pred, labels_true)\n",
    "# 0.24..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'√©tiquetage parfait est not√© 1,0¬†:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_pred = labels_true[:]\n",
    "metrics.rand_score(labels_true, labels_pred)\n",
    "# 1.0\n",
    "metrics.adjusted_rand_score(labels_true, labels_pred)\n",
    "# 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les √©tiquettes peu concordantes (par exemple, les √©tiquettes ind√©pendantes) ont des scores inf√©rieurs, et pour l'indice Rand ajust√©, le score sera n√©gatif ou proche de z√©ro. Cependant, pour l'indice Rand non ajust√©, le score, bien qu'inf√©rieur, ne sera pas n√©cessairement proche de z√©ro¬†:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.07207207207207207"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_true = [0, 0, 0, 0, 0, 0, 1, 1]\n",
    "labels_pred = [0, 1, 2, 3, 4, 5, 5, 6]\n",
    "metrics.rand_score(labels_true, labels_pred)\n",
    "# 0.39...\n",
    "metrics.adjusted_rand_score(labels_true, labels_pred)\n",
    "# -0.07..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avantages\n",
    "\n",
    "* **Interpr√©tabilit√©**¬†: l'indice Rand non ajust√© est proportionnel au nombre de paires d'√©chantillons dont les √©tiquettes sont identiques dans `labels_pred` et `labels_true`, ou sont diff√©rentes dans les deux.\n",
    "* **Les attributions d'√©tiquettes al√©atoires (uniformes)** ont un score d'indice Rand ajust√© proche de 0,0 pour toute valeur de `n_clusters` et `n_samples` (ce qui n'est pas le cas pour l'indice Rand non ajust√© ou la V-mesure par exemple).\n",
    "* **Plage d√©limit√©e**¬†: les valeurs inf√©rieures indiquent des √©tiquetages diff√©rents, des regroupements similaires ont un indice Rand √©lev√© (ajust√© ou non ajust√©), 1,0 est le score de correspondance parfaite. La plage de score est [0, 1] pour l'indice Rand non ajust√© et [-1, 1] pour l'indice Rand ajust√©.\n",
    "* **Aucune hypoth√®se n'est faite sur la structure du cluster**¬†: l'indice Rand (ajust√© ou non ajust√©) peut √™tre utilis√© pour comparer toutes sortes d'algorithmes de clustering, et peut √™tre utilis√© pour comparer des algorithmes de clustering tels que k-means qui suppose des formes de blob isotropes avec des r√©sultats d'analyse spectrale. algorithmes de clustering qui peuvent trouver des clusters avec des formes \"pli√©es\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inconv√©nients\n",
    "\n",
    "* Contrairement √† l'inertie, l'**indice Rand (ajust√© ou non) n√©cessite une connaissance des classes de v√©rit√© terrain** qui n'est presque jamais disponible dans la pratique ou n√©cessite une affectation manuelle par des annotateurs humains (comme dans le cadre de l'apprentissage supervis√©).\n",
    "Cependant, l'indice Rand (ajust√© ou non ajust√©) peut √©galement √™tre utile dans un cadre purement non supervis√© en tant que bloc de construction d'un indice de consensus pouvant √™tre utilis√© pour la s√©lection de mod√®les de clustering (TODO).\n",
    "\n",
    "* L'**indice Rand non ajust√© est souvent proche de 1,0** m√™me si les regroupements eux-m√™mes diff√®rent consid√©rablement. Cela peut √™tre compris lors de l'interpr√©tation de l'indice Rand comme la pr√©cision de l'√©tiquetage des paires d'√©l√©ments r√©sultant des regroupements¬†: dans la pratique, il y a souvent une majorit√© de paires d'√©l√©ments qui se voient attribuer l'√©tiquette de paire diff√©rente sous le regroupement pr√©dit et la v√©rit√© terrain, ce qui entra√Æne un forte proportion d'√©tiquettes de paires concordantes, ce qui conduit par la suite √† un score √©lev√©."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**Ajustement pour le hasard dans l'√©valuation des performances de clustering**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_3_cluster/plot_adjusted_for_chance_measures.ipynb)<br/>([*Adjustment for chance in clustering performance evaluation*](https://scikit-learn.org/stable/auto_examples/cluster/plot_adjusted_for_chance_measures.html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulation math√©matique\n",
    "\n",
    "Si C est une affectation de classe de v√©rit√© terrain et K le regroupement, d√©finissons $a$ et $b$ comme:\n",
    "* $a$, le nombre de paires d'√©l√©ments qui sont dans le m√™me ensemble en C et dans le m√™me ensemble en K\n",
    "* $b$, le nombre de paires d'√©l√©ments qui sont dans des ensembles diff√©rents dans C et dans des ensembles diff√©rents dans K\n",
    "\n",
    "L'indice Rand non ajust√© est alors donn√© par :\n",
    "\n",
    "$$\\text{RI} = \\frac{a + b}{C_2^{n_{samples}}}$$\n",
    "\n",
    "o√π $C_2^{n_{samples}}$ est le nombre total de paires possibles dans le jeu de donn√©es. Peu importe si le calcul est effectu√© sur des paires ordonn√©es ou des paires non ordonn√©es tant que le calcul est effectu√© de mani√®re coh√©rente.\n",
    "\n",
    "Cependant, l'indice Rand ne garantit pas que les attributions al√©atoires d'√©tiquettes obtiendront une valeur proche de z√©ro (surtout si le nombre de grappes est du m√™me ordre de grandeur que le nombre d'√©chantillons).\n",
    "\n",
    "Pour contrer cet effet, nous pouvons actualiser le RI attendu $E[\\text{RI}]$ d'√©tiquetages al√©atoires en d√©finissant l'indice Rand ajust√© comme suit¬†:\n",
    "\n",
    "$$\\text{ARI} = \\frac{\\text{RI} - E[\\text{RI}]}{\\max(\\text{RI}) - E[\\text{RI}]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R√©f√©rences\n",
    "\n",
    "[?] L. Hubert and P. Arabie, ‚Äú**Comparing Partitions**‚Äù, Journal of Classification, 1985[.](https://drive.google.com/file/d/1tLtEWnFFD6PZ_uv8uwI_UNTBvZQTa0og/view?usp=share_link)\n",
    "\n",
    "[?] D. Steinley, ‚Äú**Properties of the Hubert-Arabie adjusted Rand index**‚Äù, Psychological Methods, 2004[.](https://drive.google.com/file/d/14PJfGnsbax-DtX_guyQw6UDeBbn-O458/view?usp=share_link)\n",
    "\n",
    "[?] [Wikipedia entry for the **Rand index**](https://en.wikipedia.org/wiki/Rand_index)\n",
    "\n",
    "[?] [Wikipedia entry for the **adjusted Rand index**](https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.10.2. **Scores bas√©s sur l'information mutuelle**<br/>([Mutual Information based scores](https://scikit-learn.org/stable/modules/clustering.html#mutual-information-based-scores))\n",
    "\n",
    "Compte tenu de la connaissance des affectations de classe de v√©rit√© terrain `labels_true` et de nos affectations d'algorithme de regroupement des m√™mes √©chantillons `labels_pred`, l'**information mutuelle** est une fonction qui mesure l'**accord** des deux affectations, en ignorant les permutations. Deux versions normalis√©es diff√©rentes de cette mesure sont disponibles, l'**information mutuelle normalis√©e (NMI)** et l'**information mutuelle ajust√©e (AMI)**. NMI est souvent utilis√© dans la litt√©rature, tandis que AMI a √©t√© propos√© plus r√©cemment et est **normalis√© contre le hasard**¬†:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2987924581708901"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "labels_true = [0, 0, 0, 1, 1, 1]\n",
    "labels_pred = [0, 0, 1, 1, 2, 2]\n",
    "\n",
    "metrics.adjusted_mutual_info_score(labels_true, labels_pred)\n",
    "# 0.22504..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut permuter 0 et 1 dans les √©tiquettes pr√©dites, renommer 2 en 3 et obtenir le m√™me score :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2987924581708901"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_pred = [1, 1, 0, 0, 3, 3]\n",
    "metrics.adjusted_mutual_info_score(labels_true, labels_pred)  \n",
    "# 0.22504..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`mutual_info_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mutual_info_score.html), [**`adjusted_mutual_info_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html) et [**`normalized_mutual_info_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html) sont tous sym√©triques : √©changer l'argument ne change pas le score. Ainsi, ils peuvent √™tre utilis√©s comme **mesure consensuelle** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2987924581708903"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.adjusted_mutual_info_score(labels_pred, labels_true)  \n",
    "# 0.22504..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'√©tiquetage parfait est not√© 1,0¬†:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_pred = labels_true[:]\n",
    "metrics.adjusted_mutual_info_score(labels_true, labels_pred)  \n",
    "# 1.0\n",
    "\n",
    "metrics.normalized_mutual_info_score(labels_true, labels_pred)  \n",
    "# 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce n'est pas vrai pour `mutual_info_score`, qui est donc plus difficile √† juger¬†:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.693147180559945"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.mutual_info_score(labels_true, labels_pred)  \n",
    "# 0.69..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les mauvais (par exemple, les √©tiquetages ind√©pendants) ont des scores non positifs¬†:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.16666666666666655"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_true = [0, 1, 2, 0, 3, 4, 5, 1]\n",
    "labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\n",
    "metrics.adjusted_mutual_info_score(labels_true, labels_pred)  \n",
    "# -0.10526..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avantages\n",
    "\n",
    "* **Les attributions d'√©tiquettes al√©atoires (uniformes) ont un score AMI proche de 0,0** pour toute valeur de `n_clusters` et `n_samples` (ce qui n'est pas le cas pour l'information mutuelle brute ou la V-mesure par exemple).\n",
    "\n",
    "* **Limite sup√©rieure de 1**¬†: les valeurs proches de z√©ro indiquent deux attributions d'√©tiquettes largement ind√©pendantes, tandis que les valeurs proches de un indiquent un accord significatif. De plus, un AMI d'exactement 1 indique que les deux attributions d'√©tiquettes sont √©gales (avec ou sans permutation).\n",
    "\n",
    "### Inconv√©nients\n",
    "\n",
    "* Contrairement √† l'inertie, **les mesures bas√©es sur l'IM n√©cessitent la connaissance des classes de v√©rit√© terrain** alors qu'elles ne sont presque jamais disponibles dans la pratique ou n√©cessitent une affectation manuelle par des annotateurs humains (comme dans le cadre de l'apprentissage supervis√©).<br/>Cependant, les mesures bas√©es sur l'IM peuvent √©galement √™tre utiles dans un cadre purement non supervis√© en tant que bloc de construction pour un indice de consensus qui peut √™tre utilis√© pour la s√©lection de mod√®les de regroupement.\n",
    "* NMI et MI ne sont pas ajust√©s au hasard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**Ajustement pour le hasard dans l'√©valuation des performances de clustering**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_3_cluster/plot_adjusted_for_chance_measures.ipynb)<br/>([*Adjustment for chance in clustering performance evaluation*](https://scikit-learn.org/stable/auto_examples/cluster/plot_adjusted_for_chance_measures.html))\n",
    "\n",
    "Analyse de l'impact de la taille de l'ensemble de donn√©es sur la valeur des mesures de regroupement pour les affectations al√©atoires. Cet exemple inclut √©galement l'indice Rand ajust√©."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulation math√©matique\n",
    "\n",
    "Supposons deux affectations d'√©tiquettes (des m√™mes $N$ objets), $U$ et $V$. Leur entropie est la quantit√© d'incertitude pour un ensemble de partitions, d√©finie par¬†:\n",
    "\n",
    "$$H(U) = - \\sum_{i=1}^{|U|}P(i)\\log(P(i))$$\n",
    "\n",
    "o√π $P(i) = |U_i| / N$ est la probabilit√© qu'un objet choisi au hasard dans $U$ tombe dans la classe $U_i$. De m√™me pour $V$ :\n",
    "\n",
    "$$H(V) = - \\sum_{j=1}^{|V|}P'(j)\\log(P'(j))$$\n",
    "\n",
    "Avec $P'(j) = |V_j| / N$. L'information mutuelle (MI) entre $U$ et $V$ est calcul√©e par :\n",
    "\n",
    "$$\\text{MI}(U, V) = \\sum_{i=1}^{|U|}\\sum_{j=1}^{|V|}P(i, j)\\log\\left(\\frac{P(i,j)}{P(i)P'(j)}\\right)$$\n",
    "\n",
    "o√π $P(i, j) = |U_i \\cap V_j| / N$ est la probabilit√© qu'un objet pris au hasard tombe dans les deux classes $U_i$ et $V_j$\n",
    "\n",
    "Il peut √©galement √™tre exprim√© en formulation de cardinalit√© d√©finie¬†:\n",
    "\n",
    "$$\\text{MI}(U, V) = \\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i \\cap V_j|}{N}\\log\\left(\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}\\right)$$\n",
    "\n",
    "L'information mutuelle normalis√©e est d√©finie comme\n",
    "\n",
    "$$\\text{NMI}(U, V) = \\frac{\\text{MI}(U, V)}{\\text{mean}(H(U), H(V))}$$\n",
    "\n",
    "Cette valeur de l'information mutuelle et √©galement la variante normalis√©e n'est pas ajust√©e au hasard et aura tendance √† augmenter √† mesure que le nombre d'√©tiquettes diff√©rentes (grappes) augmente, quelle que soit la quantit√© r√©elle ¬´d'informations mutuelles¬ª entre les attributions d'√©tiquettes.\n",
    "\n",
    "La valeur attendue de l'information mutuelle peut √™tre calcul√©e √† l'aide de l'√©quation suivante [VEB2009]. Dans cette √©quation, $a_i = |U_i|$ (le nombre d'√©l√©ments dans $U_i$) et $b_j = |V_j|$ (le nombre d'√©l√©ments dans $V_j$).\n",
    "\n",
    "$$E[\\text{MI}(U,V)]=\\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\sum_{n_{ij}=(a_i+b_j-N)^+\n",
    "}^{\\min(a_i, b_j)} \\frac{n_{ij}}{N}\\log \\left( \\frac{ N.n_{ij}}{a_i b_j}\\right)\n",
    "\\frac{a_i!b_j!(N-a_i)!(N-b_j)!}{N!n_{ij}!(a_i-n_{ij})!(b_j-n_{ij})!\n",
    "(N-a_i-b_j+n_{ij})!}$$\n",
    "\n",
    "A partir de la valeur attendue, l'information mutuelle ajust√©e peut alors √™tre calcul√©e selon une forme similaire √† celle de l'indice Rand ajust√© :\n",
    "\n",
    "$$\\text{AMI} = \\frac{\\text{MI} - E[\\text{MI}]}{\\text{mean}(H(U), H(V)) - E[\\text{MI}]}$$\n",
    "\n",
    "Pour les informations mutuelles normalis√©es et les informations mutuelles ajust√©es, la valeur de normalisation est g√©n√©ralement une moyenne g√©n√©ralis√©e des entropies de chaque regroupement. Divers moyens g√©n√©ralis√©s existent, et aucune r√®gle ferme n'existe pour en pr√©f√©rer un aux autres. La d√©cision est en grande partie une base champ par champ; par exemple, dans la d√©tection communautaire, la moyenne arithm√©tique est la plus courante. Chaque m√©thode de normalisation fournit des ¬´¬†comportements qualitativement similaires¬†¬ª [YAT2016]. Dans notre impl√©mentation, ceci est contr√¥l√© par le param√®tre `average_method`.\n",
    "\n",
    "Vinh et al. (2010) ont nomm√© des variantes de NMI et AMI par leur m√©thode de moyenne [VEB2010]. Leurs moyennes ¬´ sqrt ¬ª et ¬´ sum ¬ª sont les moyennes g√©om√©triques et arithm√©tiques ; nous utilisons ces noms plus largement communs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R√©f√©rences\n",
    "\n",
    "[?] Strehl, Alexander, and Joydeep Ghosh (2002). [‚Äú**Cluster ensembles ‚Äì a knowledge reuse framework for combining multiple partitions**](https://dl.acm.org/doi/pdf/10.1162/153244303321897735)[‚Äù](https://drive.google.com/file/d/1BQxDsFWFFabMlAwrmDwjFaEqIiiTMbVA/view?usp=share_link). Journal of Machine Learning Research 3: 583‚Äì617. doi:10.1162/153244303321897735.\n",
    "\n",
    "[Wikipedia entry for the **(normalized) Mutual Information**](https://en.wikipedia.org/wiki/Mutual_information)\n",
    "\n",
    "[Wikipedia entry for the **Adjusted Mutual Information**](https://en.wikipedia.org/wiki/Adjusted_mutual_information)\n",
    "\n",
    "[VEB2009] Vinh, Epps, and Bailey, (2009). [‚Äú**Information theoretic measures for clusterings comparison**](https://dl.acm.org/doi/10.1145/1553374.1553511)[‚Äù](https://drive.google.com/file/d/1TWusEcciRGg1XklzyM8P8jKVr0JwCUYf/view?usp=share_link). Proceedings of the 26th Annual International Conference on Machine Learning - ICML ‚Äò09. doi:10.1145/1553374.1553511. ISBN 9781605585161.\n",
    "\n",
    "[VEB2010] Vinh, Epps, and Bailey, (2010). [‚Äú**Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance**](https://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf)[‚Äù](https://drive.google.com/file/d/144opdgyDLoPavOrxlMO9eh8e5TIi1hCV/view?usp=share_link). JMLR\n",
    "\n",
    "[YAT2016] Yang, Algesheimer, and Tessone, (2016). [‚Äú**A comparative analysis of community detection algorithms on artificial networks**](https://zora.uzh.ch/id/eprint/127494/1/srep30750.pdf)[‚Äù](https://drive.google.com/file/d/1Ckhyhx2Fx0lk4i94dYOwdSMnqf_hADM1/view?usp=share_link). Scientific Reports 6: 30750. doi:10.1038/srep30750.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.10.3. **Homog√©n√©it√©, compl√©tude et V-mesure**<br/>([Homogeneity, completeness and V-measure](https://scikit-learn.org/stable/modules/clustering.html#homogeneity-completeness-and-v-measure)) \n",
    "\n",
    "Compte tenu de la connaissance des affectations de classe de v√©rit√© terrain des √©chantillons, il est possible de d√©finir une m√©trique intuitive en utilisant l'analyse d'entropie conditionnelle.\n",
    "\n",
    "En particulier, Rosenberg et Hirschberg (2007) d√©finissent les deux objectifs souhaitables suivants pour toute affectation de cluster¬†:\n",
    "* **homog√©n√©it√©** : chaque cluster ne contient que des membres d'une seule classe.\n",
    "* **compl√©tude** : tous les membres d'une classe donn√©e sont affect√©s au m√™me cluster.\n",
    "\n",
    "Nous pouvons transformer ces concepts en scores [**`homogeneity_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html) et [**`completeness_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.completeness_score.html). Les deux sont d√©limit√©s en dessous par 0,0 et au-dessus par 1,0 (plus c'est haut, mieux c'est) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "labels_true = [0, 0, 0, 1, 1, 1]\n",
    "labels_pred = [0, 0, 1, 1, 2, 2]\n",
    "\n",
    "metrics.homogeneity_score(labels_true, labels_pred)\n",
    "# 0.66...\n",
    "\n",
    "metrics.completeness_score(labels_true, labels_pred)\n",
    "# 0.42..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leur moyenne harmonique appel√©e **V-measure** est calcul√©e par [**`v_measure_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html)¬†:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4999999999999998"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.v_measure_score(labels_true, labels_pred)\n",
    "# 0.51..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La formule de cette fonction est la suivante :\n",
    "\n",
    "$$v = \\frac{(1 + \\beta) \\times \\text{homogeneity} \\times \\text{completeness}}{(\\beta \\times \\text{homogeneity} + \\text{completeness})}$$\n",
    "\n",
    "`beta` prend par d√©faut la valeur 1.0, mais pour utiliser une valeur inf√©rieure √† 1 pour beta¬†:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4705882352941175"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.v_measure_score(labels_true, labels_pred, beta=0.6)\n",
    "# 0.54..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plus de poids sera attribu√© √† l'exhaustivit√©.\n",
    "\n",
    "La V-mesure est en fait √©quivalente √† l'information mutuelle (NMI) discut√©e ci-dessus, la fonction d'agr√©gation √©tant la moyenne arithm√©tique [B2011].\n",
    "\n",
    "L'homog√©n√©it√©, l'exhaustivit√© et la V-mesure peuvent √™tre calcul√©es en une fois en utilisant [**`homogeneity_completeness_v_measure`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_completeness_v_measure.html) comme suit¬†:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)\n",
    "# (0.66..., 0.42..., 0.51...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'affectation de clustering suivante est l√©g√®rement meilleure, car elle est homog√®ne mais pas compl√®te¬†:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred = [0, 0, 0, 1, 2, 2]\n",
    "metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)\n",
    "# (1.0, 0.68..., 0.81...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : [**`v_measure_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html) est **sym√©trique**¬†: il peut √™tre utilis√© pour √©valuer l'accord de deux affectations ind√©pendantes sur le m√™me jeu de donn√©es.\n",
    "\n",
    "Ce n'est pas le cas pour [**`completeness_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.completeness_score.html) et [**`homogeneity_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html) : tous deux sont li√©s par la relation :\n",
    "\n",
    "    homogeneity_score(a, b) == completeness_score(b, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avantages\n",
    "\n",
    "* **Scores limit√©s**¬†: 0,0 est aussi mauvais que possible, 1,0 est un score parfait.\n",
    "* Interpr√©tation intuitive¬†: le regroupement avec une mauvaise V-mesure peut √™tre **analys√© qualitativement en termes d'homog√©n√©it√© et d'exhaustivit√©** pour mieux sentir quel ¬´¬†type¬†¬ª d'erreurs est commis par l'affectation.\n",
    "* **Aucune hypoth√®se n'est faite sur la structure du cluster**¬†: peut √™tre utilis√© pour comparer des algorithmes de clustering tels que k-means qui suppose des formes de blobs isotropes avec les r√©sultats d'algorithmes de clustering spectraux qui peuvent trouver des clusters avec des formes \"pli√©es\".\n",
    "\n",
    "### Inconv√©nients\n",
    "\n",
    "* Les m√©triques introduites pr√©c√©demment ne **sont pas normalis√©es en ce qui concerne l'√©tiquetage al√©atoire**¬†: cela signifie qu'en fonction du nombre d'√©chantillons, de grappes et de classes de v√©rit√© terrain, un √©tiquetage compl√®tement al√©atoire ne donnera pas toujours les m√™mes valeurs d'homog√©n√©it√©, d'exhaustivit√© et donc de v-mesure. En particulier, l'**√©tiquetage al√©atoire ne donnera pas de scores nuls, surtout lorsque le nombre de grappes est important**.<br/>Ce probl√®me peut √™tre ignor√© en toute s√©curit√© lorsque le nombre d'√©chantillons est sup√©rieur √† mille et que le nombre de grappes est inf√©rieur √† 10. **Pour des tailles d'√©chantillon plus petites ou un plus grand nombre de grappes, il est plus s√ªr d'utiliser un indice ajust√© tel que l'indice Rand ajust√© (ARI)**.\n",
    "\n",
    "<img alt=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_adjusted_for_chance_measures_001.png\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_adjusted_for_chance_measures_001.png\" style=\"width: 640.0px; height: 480.0px;\" />\n",
    "\n",
    "* Ces m√©triques **n√©cessitent la connaissance des classes de v√©rit√© terrain** alors qu'elles ne sont presque jamais disponibles dans la pratique ou n√©cessitent une affectation manuelle par des annotateurs humains (comme dans le cadre de l'apprentissage supervis√©).\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Ajustement pour le hasard dans l'√©valuation des performances de clustering**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_3_cluster/plot_adjusted_for_chance_measures.ipynb)<br/>([*Adjustment for chance in clustering performance evaluation*](https://scikit-learn.org/stable/auto_examples/cluster/plot_adjusted_for_chance_measures.html))\n",
    "\n",
    "Analyse de l'impact de la taille de l'ensemble de donn√©es sur la valeur des mesures de regroupement pour les affectations al√©atoires.\n",
    "\n",
    "### Formulation math√©matique\n",
    "\n",
    "Les scores d'homog√©n√©it√© et de compl√©tude sont formellement donn√©s par :\n",
    "\n",
    "$$h = 1 - \\frac{H(C|K)}{H(C)}$$\n",
    "\n",
    "$$c = 1 - \\frac{H(K|C)}{H(K)}$$\n",
    "\n",
    "o√π $H(C|K)$ est l'**entropie conditionnelle des classes compte tenu des affectations de cluster** et est donn√©e par¬†:\n",
    "\n",
    "$$H(C|K) = - \\sum_{c=1}^{|C|} \\sum_{k=1}^{|K|} \\frac{n_{c,k}}{n}\n",
    "\\cdot \\log\\left(\\frac{n_{c,k}}{n_k}\\right)$$\n",
    "\n",
    "et $H(C)$ est l'**entropie des classes** et est donn√©e par :\n",
    "\n",
    "$$H(C) = - \\sum_{c=1}^{|C|} \\frac{n_c}{n} \\cdot \\log\\left(\\frac{n_c}{n}\\right)$$\n",
    "\n",
    "avec $n$ le nombre total d'√©chantillons, $n_c$ et $n_k$ le nombre d'√©chantillons appartenant respectivement √† la classe $c$ et √† la grappe $k$, et enfin $n_{c, k}$ le nombre d'√©chantillons de la classe $c$ affect√©e √† la grappe $k$.\n",
    "\n",
    "L'**entropie conditionnelle des clusters d'une classe donn√©e** $H(K|C)$ et l'**entropie des clusters** $H(K)$ sont d√©finis de mani√®re sym√©trique.\n",
    "\n",
    "Rosenberg et Hirschberg d√©finissent en outre la **V-mesure** comme la **moyenne harmonique de l'homog√©n√©it√© et de l'exhaustivit√©**¬†:\n",
    "\n",
    "$$v = 2 \\cdot \\frac{h \\cdot c}{h + c}$$\n",
    "\n",
    "### R√©f√©rences\n",
    "\n",
    "[?] [‚Äú**V-Measure: A conditional entropy-based external cluster evaluation measure**](https://aclanthology.org/D07-1043.pdf)[‚Äù](https://drive.google.com/file/d/1ycDJJmQqTMHp3LCR6iI1c8QjqXmYtBQn/view?usp=share_link) Andrew Rosenberg and Julia Hirschberg, 2007\n",
    "\n",
    "[B2011] [‚Äú**Identication and Characterization of Events in Social Media**](http://www.cs.columbia.edu/~hila/hila-thesis-distributed.pdf)[‚Äù](https://drive.google.com/file/d/1Sbt6Qu-zE_wPaEBVefL6hLb4krGyuFr-/view?usp=share_link), Hila Becker, PhD Thesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.10.4. Scores de Fowlkes-Mallows\n",
    "\n",
    "L'indice Fowlkes-Mallows ([**`sklearn.metrics.fowlkes_mallows_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fowlkes_mallows_score.html)) peut √™tre utilis√© lorsque les affectations de classe de v√©rit√© terrain des √©chantillons sont connues. Le score Fowlkes-Mallows FMI est d√©fini comme la moyenne g√©om√©trique de la pr√©cision et du rappel par paire¬†:\n",
    "\n",
    "$$\\text{FMI} = \\frac{\\text{TP}}{\\sqrt{(\\text{TP} + \\text{FP}) (\\text{TP} + \\text{FN})}}$$\n",
    "\n",
    "O√π `TP` est le nombre de **vrais positifs** (c'est-√†-dire le nombre de paires de points qui appartiennent aux m√™mes clusters dans les √©tiquettes vraies et les √©tiquettes pr√©dites), `FP` est le nombre de **faux positifs** (c'est-√†-dire le nombre de paires de points qui appartiennent aux m√™mes clusters dans les vraies √©tiquettes et non dans les √©tiquettes pr√©dites) et FN est le nombre de **faux n√©gatifs** (c'est-√†-dire le nombre de paires de points qui appartiennent aux m√™mes grappes dans les √©tiquettes pr√©dites et non dans les vraies √©tiquettes).\n",
    "\n",
    "Le score varie de 0 √† 1. Une valeur √©lev√©e indique une bonne similarit√© entre deux clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4714045207910317"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "labels_true = [0, 0, 0, 1, 1, 1]\n",
    "labels_pred = [0, 0, 1, 1, 2, 2]\n",
    "metrics.fowlkes_mallows_score(labels_true, labels_pred)\n",
    "# 0.47140..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut permuter 0 et 1 dans les √©tiquettes pr√©dites, renommer 2 en 3 et obtenir le m√™me score :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4714045207910317"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_pred = [1, 1, 0, 0, 3, 3]\n",
    "metrics.fowlkes_mallows_score(labels_true, labels_pred)\n",
    "# 0.47140..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'√©tiquetage parfait est not√© 1,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_pred = labels_true[:]\n",
    "metrics.fowlkes_mallows_score(labels_true, labels_pred)\n",
    "# 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les mauvais (par exemple, les √©tiquetages ind√©pendants) ont des scores nuls¬†:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_true = [0, 1, 2, 0, 3, 4, 5, 1]\n",
    "labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\n",
    "metrics.fowlkes_mallows_score(labels_true, labels_pred)\n",
    "0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avantages\n",
    "\n",
    "* **Les attributions d'√©tiquettes al√©atoires (uniformes) ont un score FMI proche de 0,0** pour toute valeur de `n_clusters` et `n_samples` (ce qui n'est pas le cas pour l'information mutuelle brute ou la V-mesure par exemple).\n",
    "* **Borne sup√©rieure √† 1**: les valeurs proches de z√©ro indiquent deux attributions d'√©tiquettes largement ind√©pendantes, tandis que les valeurs proches de un indiquent un accord significatif. De plus, des valeurs d'exactement 0 indiquent des attributions d'√©tiquettes **purement** ind√©pendantes et un FMI d'exactement 1 indique que les deux attributions d'√©tiquettes sont √©gales (avec ou sans permutation).\n",
    "* **Aucune hypoth√®se n'est faite sur la structure du cluster**¬†: peut √™tre utilis√© pour comparer des algorithmes de clustering tels que k-means qui suppose des formes de blobs isotropes avec les r√©sultats d'algorithmes de clustering spectraux qui peuvent trouver des clusters avec des formes \"pli√©es\".\n",
    "\n",
    "### Inconv√©nients\n",
    "\n",
    "* Contrairement √† l'inertie, **les mesures bas√©es sur FMI n√©cessitent la connaissance des classes de v√©rit√© terrain** alors qu'elles ne sont presque jamais disponibles dans la pratique ou n√©cessitent une affectation manuelle par des annotateurs humains (comme dans le cadre de l'apprentissage supervis√©).\n",
    "\n",
    "### R√©f√©rences\n",
    "\n",
    "E. B. Fowkles and C. L. Mallows, 1983. [‚Äú**A method for comparing two hierarchical clusterings**](https://www.semanticscholar.org/paper/A-Method-for-Comparing-Two-Hierarchical-Clusterings-Fowlkes-Mallows/ededd54b4f7578802ce81fe7a34c05d93e5e09ee)[‚Äù](https://drive.google.com/file/d/1-KzvRHfFIDsQ_zyPUhO3uXhUJuLjDJrL/view?usp=share_link). Journal of the American Statistical Association.\n",
    "\n",
    "[Wikipedia entry for the **Fowlkes-Mallows Index**](https://en.wikipedia.org/wiki/Fowlkes%E2%80%93Mallows_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.10.5. Coefficient de silhouette\n",
    "\n",
    "Si les √©tiquettes de v√©rit√© terrain ne sont pas connues, l'√©valuation doit √™tre effectu√©e √† l'aide du mod√®le lui-m√™me. Le coefficient de silhouette ([**`sklearn.metrics.silhouette_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)) est un exemple d'une telle √©valuation, o√π un score de coefficient de silhouette plus √©lev√© se rapporte √† un mod√®le avec des clusters mieux d√©finis. Le Coefficient Silhouette est d√©fini pour chaque √©chantillon et est compos√© de deux scores :\n",
    "* **a** : La distance moyenne entre un √©chantillon et tous les autres points de la m√™me classe.\n",
    "* **b** : La distance moyenne entre un √©chantillon et tous les autres points de la grappe la plus proche.\n",
    "\n",
    "Le coefficient de silhouette $s$ pour un seul √©chantillon est alors donn√© par¬†:\n",
    "\n",
    "$$s = \\frac{b - a}{max(a, b)}$$\n",
    "\n",
    "Le coefficient de silhouette pour un ensemble d'√©chantillons est donn√© comme la moyenne du coefficient de silhouette pour chaque √©chantillon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn import datasets\n",
    "X, y = datasets.load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisation normale, le coefficient de silhouette est appliqu√© aux r√©sultats d'une analyse de cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5528190123564095"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)\n",
    "labels = kmeans_model.labels_\n",
    "metrics.silhouette_score(X, labels, metric='euclidean')\n",
    "# 0.55..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avantages\n",
    "\n",
    "* Le score est compris entre -1 pour un clustering incorrect et +1 pour un clustering tr√®s dense. Les scores autour de z√©ro indiquent des clusters qui se chevauchent.\n",
    "* Le score est plus √©lev√© lorsque les clusters sont denses et bien s√©par√©s, ce qui correspond √† un concept standard de cluster.\n",
    "\n",
    "### Inconv√©nients\n",
    "\n",
    "* Le coefficient de silhouette est g√©n√©ralement plus √©lev√© pour les clusters convexes que pour les autres concepts de clusters, tels que les clusters bas√©s sur la densit√© comme ceux obtenus via DBSCAN.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**S√©lection du nombre de clusters avec analyse de silhouette sur KMeans clustering**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_3_cluster/plot_kmeans_silhouette_analysis.ipynb)<br/>([*Selecting the number of clusters with silhouette analysis on KMeans clustering*](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html))\n",
    "\n",
    "Dans cet exemple l'analyse de silhouette est utilis√©e pour choisir une valeur optimale pour `n_clusters`.\n",
    "\n",
    "### R√©f√©rences\n",
    "\n",
    "Peter J. Rousseeuw (1987). [‚Äú**Silhouettes: a Graphical Aid to the Interpretation and Validation of Cluster Analysis**](https://www.sciencedirect.com/science/article/pii/0377042787901257?via%3Dihub)[‚Äù](https://drive.google.com/file/d/1T-cjvWWtWF_gUIixc3Rcmmq4wOLYhWvQ/view?usp=share_link). Computational and Applied Mathematics 20: 53‚Äì65.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.10.6. Indice de Calinski-Harabasz\n",
    "\n",
    "Si les √©tiquettes de v√©rit√© de terrain ne sont pas connues, l'indice Calinski-Harabasz ([**`sklearn.metrics.calinski_harabasz_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html)) - √©galement connu sous le nom de crit√®re du rapport de variance - peut √™tre utilis√© pour √©valuer le mod√®le, o√π un score Calinski-Harabasz plus √©lev√© se rapporte √† un mod√®le avec clusters mieux d√©finis.\n",
    "\n",
    "L'indice est le rapport de la somme de la dispersion inter-clusters et de la dispersion intra-cluster pour tous les clusters (o√π la dispersion est d√©finie comme la somme des distances au carr√©)¬†:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn import datasets\n",
    "X, y = datasets.load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En usage normal, l'indice de Calinski-Harabasz est appliqu√© aux r√©sultats d'une analyse de cluster :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "561.62775662962"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)\n",
    "labels = kmeans_model.labels_\n",
    "metrics.calinski_harabasz_score(X, labels)\n",
    "# 561.62..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avantages\n",
    "\n",
    "* Le score est plus √©lev√© lorsque les clusters sont denses et bien s√©par√©s, ce qui correspond √† un concept standard de cluster.\n",
    "\n",
    "* Le score est rapide √† calculer.\n",
    "\n",
    "### Inconv√©nients\n",
    "\n",
    "* L'indice Calinski-Harabasz est g√©n√©ralement plus √©lev√© pour les clusters convexes que pour les autres concepts de clusters, tels que les clusters bas√©s sur la densit√© comme ceux obtenus via DBSCAN.\n",
    "\n",
    "### Formulation math√©matique\n",
    "\n",
    "Pour un ensemble de donn√©es $E$ de taille $n_E$ qui a √©t√© regroup√©e en $k$ grappes, le score de Calinski-Harabasz $s$ est d√©fini comme le rapport de la moyenne de dispersion inter-grappes et de la dispersion intra-grappe¬†:\n",
    "\n",
    "$$s = \\frac{\\mathrm{tr}(B_k)}{\\mathrm{tr}(W_k)} \\times \\frac{n_E - k}{k - 1}$$\n",
    "\n",
    "o√π $\\mathrm{tr}(B_k)$ est la trace de la matrice de dispersion inter-groupes et $\\mathrm{tr}(W_k)$ est la trace de la matrice de dispersion intra-grappe d√©finie par :\n",
    "\n",
    "$$W_k = \\sum_{q=1}^k \\sum_{x \\in C_q} (x - c_q) (x - c_q)^\\top$$\n",
    "$$B_k = \\sum_{q=1}^k n_q (c_q - c_E) (c_q - c_E)^\\top$$\n",
    "\n",
    "avec $C_q$ l'ensemble des points dans le cluster $q$, $c_q$ le centre du cluster $q$, $c_E$ le centre de $E$, et $n_q$ le nombre de points dans le cluster $q$.\n",
    "\n",
    "### R√©f√©rences\n",
    "\n",
    "[?] Cali≈Ñski, T., & Harabasz, J. (1974). [‚Äú**A Dendrite Method for Cluster Analysis**](https://www.researchgate.net/publication/233096619_A_Dendrite_Method_for_Cluster_Analysis)[‚Äù](https://drive.google.com/file/d/1wVMKVAYxRrM4Ma9EwqJ5xqv3a_L0hvQT/view?usp=share_link). Communications in Statistics-theory and Methods 3: 1-27.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.10.7. Davies-Bouldin Index\n",
    "\n",
    "If the ground truth labels are not known, the Davies-Bouldin index ([**`sklearn.metrics.davies_bouldin_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html)) can be used to evaluate the model, where a lower Davies-Bouldin index relates to a model with better separation between the clusters.\n",
    "\n",
    "This index signifies the average ‚Äòsimilarity‚Äô between clusters, where the similarity is a measure that compares the distance between clusters with the size of the clusters themselves.\n",
    "\n",
    "Zero is the lowest possible score. Values closer to zero indicate a better partition.\n",
    "\n",
    "In normal usage, the Davies-Bouldin index is applied to the results of a cluster analysis as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "kmeans = KMeans(n_clusters=3, random_state=1).fit(X)\n",
    "labels = kmeans.labels_\n",
    "davies_bouldin_score(X, labels)\n",
    "# 0.6619..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avantages\n",
    "\n",
    "* Le calcul de Davies-Bouldin est plus simple que celui des scores Silhouette.\n",
    "* L'indice est uniquement bas√© sur des quantit√©s et des caract√©ristiques inh√©rentes √† l'ensemble de donn√©es car son calcul n'utilise que des distances ponctuelles.\n",
    "\n",
    "### Inconv√©nients\n",
    "\n",
    "* L'indice Davies-Boulding est g√©n√©ralement plus √©lev√© pour les clusters convexes que pour les autres concepts de clusters, tels que les clusters bas√©s sur la densit√© comme ceux obtenus √† partir de DBSCAN.\n",
    "* L'utilisation de la distance centro√Øde limite la distance m√©trique √† l'espace euclidien.\n",
    "\n",
    "### Formulation math√©matique\n",
    "\n",
    "L'indice est d√©fini comme la similarit√© moyenne entre chaque cluster $C_i$ pour $i = 1, \\cdots, k$ et son $C_j$ le plus similaire. Dans le cadre de cet indice, la similarit√© est d√©finie comme une mesure $R_{ij}$ qui √©quilibre :\n",
    "* $s_i$, la distance moyenne entre chaque point du cluster $i$ et le centro√Øde de ce cluster - √©galement connu sous le nom de diam√®tre de cluster.\n",
    "* $d_{ij}$, la distance entre les centres de gravit√© des clusters $i$ et $j$.\n",
    "\n",
    "Un choix simple pour construire $R_{ij}$ de telle sorte qu'il soit non n√©gatif et sym√©trique est :\n",
    "\n",
    "$$R_{ij} = \\frac{s_i + s_j}{d_{ij}}$$\n",
    "\n",
    "Alors l'indice de Davies-Bouldin est d√©fini comme :\n",
    "\n",
    "$$DB = \\frac{1}{k} \\sum_{i=1}^k \\max_{i \\neq j} R_{ij}$$\n",
    "\n",
    "### R√©f√©rences\n",
    "\n",
    "[?] Davies, David L.; Bouldin, Dld W. (1979). [‚Äú**A Cluster Separation Measure**](https://ieeexplore.ieee.org/document/4766909)[‚Äù](https://drive.google.com/file/d/1UYs_tDk5azMulhvMTpKhj5Agw930cfW_/view?usp=share_link) IEEE Transactions on Pattern Analysis and Machine Intelligence. PAMI-1 (2): 224-227.\n",
    "\n",
    "[?] Halkidi, Maria; Batistakis, Yannis; Vazirgiannis, Michalis (2001). [‚Äú**On Clustering Validation Techniques**](https://link.springer.com/article/10.1023/A:1012801612483)[‚Äù](https://drive.google.com/file/d/1Ku1fxm7MOm5n8ImQ3UoUbN_F84zgjz_8/view?usp=share_link) Journal of Intelligent Information Systems, 17(2-3), 107-145.\n",
    "\n",
    "[?] [Wikipedia entry for **Davies-Bouldin index**](https://en.wikipedia.org/wiki/Davies%E2%80%93Bouldin_index).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.10.8. Matrice de contingence\n",
    "\n",
    "La matrice de contingence ([**`sklearn.metrics.cluster.contingency_matrix`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cluster.contingency_matrix.html)) indique la cardinalit√© d'intersection pour chaque paire de cluster vraie/pr√©dite. La matrice de contingence fournit des statistiques suffisantes pour toutes les m√©triques de clustering o√π les √©chantillons sont ind√©pendants et distribu√©s de mani√®re identique et il n'est pas n√©cessaire de tenir compte du fait que certaines instances ne sont pas regroup√©es.\n",
    "\n",
    "Voici un exemple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 0],\n",
       "       [0, 1, 2]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "x = [\"a\", \"a\", \"a\", \"b\", \"b\", \"b\"]\n",
    "y = [0, 0, 1, 1, 2, 2]\n",
    "contingency_matrix(x, y)\n",
    "# array([[2, 1, 0],\n",
    "#       [0, 1, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La premi√®re ligne du tableau de sortie indique qu'il y a trois √©chantillons dont le vrai cluster est \"a\". Parmi eux, deux sont dans le cluster pr√©dit 0, un est dans 1 et aucun n'est dans 2. Et la deuxi√®me ligne indique qu'il y a trois √©chantillons dont le vrai cluster est \"b\". Parmi eux, aucun n'est dans le cluster pr√©dit 0, un est dans 1 et deux sont dans 2.\n",
    "\n",
    "Une [**matrice de confusion** (3.3.2.6)](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix) pour la classification est une matrice de contingence carr√©e o√π l'ordre des lignes et des colonnes correspond √† une liste de classes.\n",
    "\n",
    "### Avantages\n",
    "\n",
    "* Permet d'examiner la propagation de chaque vrai cluster √† travers les clusters pr√©dits et vice versa.\n",
    "* Le tableau de contingence calcul√© est g√©n√©ralement utilis√© dans le calcul d'une statistique de similarit√© (comme les autres r√©pertori√©es dans ce document) entre les deux regroupements.\n",
    "\n",
    "### Inconv√©nients\n",
    "\n",
    "* La matrice de contingence est facile √† interpr√©ter pour un petit nombre de clusters, mais devient tr√®s difficile √† interpr√©ter pour un grand nombre de clusters.\n",
    "* Il ne donne pas une seule m√©trique √† utiliser comme objectif pour l'optimisation du clustering.\n",
    "\n",
    "### R√©f√©rences\n",
    "\n",
    "[Wikipedia entry for **contingency matrix**](https://en.wikipedia.org/wiki/Contingency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.10.9. Matrice de confusion des paires\n",
    "\n",
    "La matrice de confusion des paires ([**`sklearn.metrics.cluster.pair_confusion_matrix`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cluster.pair_confusion_matrix.html)) est une matrice de similarit√© 2x2\n",
    "\n",
    "$$\\begin{split}C = \\left[\\begin{matrix}\n",
    "C_{00} & C_{01} \\\\\n",
    "C_{10} & C_{11}\n",
    "\\end{matrix}\\right]\\end{split}$$\n",
    "\n",
    "entre deux clusterings calcul√©s en consid√©rant toutes les paires d'√©chantillons et en comptant les paires qui sont affect√©es dans le m√™me cluster ou dans des clusters diff√©rents sous les clusterings vrais et pr√©dits.\n",
    "\n",
    "Il contient les entr√©es suivantes¬†:\n",
    "\n",
    "$C_{00}$ : nombre de paires avec les deux regroupements ayant les √©chantillons non regroup√©s\n",
    "\n",
    "$C_{10}$ : nombre de paires avec le v√©ritable regroupement d'√©tiquettes ayant les √©chantillons regroup√©s mais l'autre regroupement n'ayant pas les √©chantillons regroup√©s\n",
    "\n",
    "$C_{01}$ : nombre de paires avec le vrai regroupement d'√©tiquettes n'ayant pas les √©chantillons regroup√©s mais l'autre regroupement ayant les √©chantillons regroup√©s\n",
    "\n",
    "$C_{11}$ : nombre de paires avec les deux regroupements ayant les √©chantillons regroup√©s\n",
    "\n",
    "Consid√©rant une paire d'√©chantillons regroup√©s en une paire positive, alors, comme dans la classification binaire, le nombre de vrais n√©gatifs est $C_{00}$, les faux n√©gatifs sont $C_{10}$, les vrais positifs sont $C_{11}$ et les faux positifs sont $C_{01}$.\n",
    "\n",
    "Les libell√©s qui correspondent parfaitement ont tous des entr√©es non nulles sur la diagonale, quelles que soient les valeurs r√©elles des libell√©s¬†:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8, 0],\n",
       "       [0, 4]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.cluster import pair_confusion_matrix\n",
    "pair_confusion_matrix([0, 0, 1, 1], [0, 0, 1, 1])\n",
    "# array([[8, 0],\n",
    "#        [0, 4]])\n",
    "pair_confusion_matrix([0, 0, 1, 1], [1, 1, 0, 0])\n",
    "# array([[8, 0],\n",
    "#        [0, 4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les √©tiquetages qui affectent tous les membres de classes aux m√™mes clusters sont complets mais peuvent ne pas toujours √™tre purs, donc p√©nalis√©s, et ont des entr√©es non nulles hors diagonale¬†:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8, 2],\n",
       "       [0, 2]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_confusion_matrix([0, 0, 1, 2], [0, 0, 1, 1])\n",
    "# array([[8, 2],\n",
    "#        [0, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matrice n'est pas sym√©trique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8, 0],\n",
       "       [2, 2]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_confusion_matrix([0, 0, 1, 1], [0, 0, 1, 2])\n",
    "# array([[8, 0],\n",
    "#        [2, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si les membres des classes sont compl√®tement r√©partis sur diff√©rents clusters, l'affectation est totalement incompl√®te, donc la matrice a toutes les entr√©es en diagonale nulle¬†:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0],\n",
       "       [12,  0]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_confusion_matrix([0, 0, 0, 0], [0, 1, 2, 3])\n",
    "# array([[ 0,  0],\n",
    "#        [12,  0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R√©f√©rences\n",
    "\n",
    "[?] L. Hubert and P. Arabie, ‚Äú**Comparing Partitions**‚Äù, Journal of Classification, 1985[.](https://drive.google.com/file/d/1tLtEWnFFD6PZ_uv8uwI_UNTBvZQTa0og/view?usp=share_link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e03b612d84ba21ce95ed447e81b3062e1eb99b56c6d885cdab4aaa12f1b8e240"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
