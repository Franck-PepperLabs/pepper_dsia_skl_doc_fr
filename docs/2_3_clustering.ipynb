{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='unsupervised-learning'></a> 2. [**Apprentissage non supervisé**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_unsupervised_learning.ipynb#model-selection-and-evaluation)</br>([*Unsupervised learning*](https://scikit-learn.org/stable/unsupervised_learning.html#unsupervised-learning))\n",
    "\n",
    "# 2.3. [**Regroupement**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#clustering)<br/>([_Clustering_](https://scikit-learn.org/stable/modules/clustering.html#clustering))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 54 pages, 20 exemples, 40 papiers\n",
    "- 2.3.1. [**Aperçu des méthodes de regroupement**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#overview-of-clustering-methods)<br/>([_Overview of clustering methods_](https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods))\n",
    "- 2.3.2. [**K-moyennes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#k-means)<br/>([_K-means_](https://scikit-learn.org/stable/modules/clustering.html#k-means))\n",
    "    - 2.3.2.1. [**Parallélisme de bas niveau**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#low-level-parallelism)<br/>([_Low-level parallelism_](https://scikit-learn.org/stable/modules/clustering.html#low-level-parallelism))\n",
    "    - 2.3.2.2. [**K-moyennes par mini-lots**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#mini-batch-k-means)<br/>([_Mini Batch K-Means_](https://scikit-learn.org/stable/modules/clustering.html#mini-batch-k-means))\n",
    "- 2.3.3. [**Propagation d'affinité**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#affinity-propagation)<br/>([_Affinity Propagation_](https://scikit-learn.org/stable/modules/clustering.html#affinity-propagation))\n",
    "- 2.3.4. [**Déplacement moyen**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#mean-shift)<br/>([_Mean Shift_](https://scikit-learn.org/stable/modules/clustering.html#mean-shift))\n",
    "- 2.3.5. [**Regroupement spectral**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#spectral-clustering)<br/>([_Spectral clustering_](https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering))\n",
    "    - 2.3.5.1. [**Stratégies d'attribution d'étiquettes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#different-label-assignment-strategies)<br/>([_Different label assignment strategies_](https://scikit-learn.org/stable/modules/clustering.html#different-label-assignment-strategies))\n",
    "    - 2.3.5.2. [**Graphes de regroupement spectral**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#spectral-clustering-graphs)<br/>([_Spectral Clustering Graphs_](https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering-graphs))\n",
    "- 2.3.6. [**Regroupement hiérarchique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#hierarchical-clustering)<br/>([_Hierarchical clustering_](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering))\n",
    "    - 2.3.6.1. [**Types de liaison : Ward, complète, moyenne et liaison simple**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#different-linkage-type-ward-complete-average-and-single-linkage)<br/>([_Different linkage type: Ward, complete, average, and single linkage_](https://scikit-learn.org/stable/modules/clustering.html#different-linkage-type-ward-complete-average-and-single-linkage))\n",
    "    - 2.3.6.2. [**Visualisation de la hiérarchie des groupes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#visualization-of-cluster-hierarchy)<br/>([_Visualization of cluster hierarchy_](https://scikit-learn.org/stable/modules/clustering.html#visualization-of-cluster-hierarchy))\n",
    "    - 2.3.6.3. [**Ajout de contraintes de connectivité**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#adding-connectivity-constraints)<br/>([_Adding connectivity constraints_](https://scikit-learn.org/stable/modules/clustering.html#adding-connectivity-constraints))\n",
    "    - 2.3.6.4. [**Variation de la métrique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#varying-the-metric)<br/>([_Varying the metric_](https://scikit-learn.org/stable/modules/clustering.html#varying-the-metric))\n",
    "    - 2.3.6.5. [**K-moyennes à bissection**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#bisecting-k-means)<br/>([_Bisecting K-Means_](https://scikit-learn.org/stable/modules/clustering.html#bisecting-k-means))\n",
    "- 2.3.7. [**DBSCAN**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#dbscan)<br/>([_DBSCAN_](https://scikit-learn.org/stable/modules/clustering.html#dbscan))\n",
    "- 2.3.8. [**HDBSCAN**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#hdbscan)<br/>([_HDBSCAN_](https://scikit-learn.org/stable/modules/clustering.html#hdbscan))\n",
    "    - 2.3.8.1. [**Graphe d'accessibilité mutuelle**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#mutual-reachability-graph)<br/>([_Mutual Reachability Graph_](https://scikit-learn.org/stable/modules/clustering.html#mutual-reachability-graph))\n",
    "    - 2.3.8.2. [**Regroupement hiérarchique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#hierarchical-clustering)<br/>([_Hierarchical Clustering_](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering))\n",
    "- 2.3.9. [**OPTICS**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#optics)<br/>([_OPTICS_](https://scikit-learn.org/stable/modules/clustering.html#optics))\n",
    "- 2.3.10. [**BIRCH**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#birch)<br/>([_BIRCH_](https://scikit-learn.org/stable/modules/clustering.html#birch))\n",
    "- 2.3.11. [**Évaluation des performances du regroupement**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#clustering-performance-evaluation)<br/>([_Clustering performance evaluation_](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation))\n",
    "    - 2.3.11.1. [**Indice de Rand**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#rand-index)<br/>([_Rand index_](https://scikit-learn.org/stable/modules/clustering.html#rand-index))\n",
    "    - 2.3.11.2. [**Scores basés sur l'information mutuelle**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#mutual-information-based-scores)<br/>([_Mutual Information based scores_](https://scikit-learn.org/stable/modules/clustering.html#mutual-information-based-scores))\n",
    "    - 2.3.11.3. [**Homogénéité, exhaustivité et mesure V**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#homogeneity-completeness-and-v-measure)<br/>([_Homogeneity, completeness and V-measure_](https://scikit-learn.org/stable/modules/clustering.html#homogeneity-completeness-and-v-measure))\n",
    "    - 2.3.11.4. [**Scores Fowlkes-Mallows**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#fowlkes-mallows-scores)<br/>([_Fowlkes-Mallows scores_](https://scikit-learn.org/stable/modules/clustering.html#fowlkes-mallows-scores))\n",
    "    - 2.3.11.5. [**Coefficient de silhouette**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#silhouette-coefficient)<br/>([_Silhouette Coefficient_](https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient))\n",
    "    - 2.3.11.6. [**Indice de Calinski-Harabasz**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#calinski-harabasz-index)<br/>([_Calinski-Harabasz Index_](https://scikit-learn.org/stable/modules/clustering.html#calinski-harabasz-index))\n",
    "    - 2.3.11.7. [**Indice de Davies-Bouldin**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#davies-bouldin-index)<br/>([_Davies-Bouldin Index_](https://scikit-learn.org/stable/modules/clustering.html#davies-bouldin-index))\n",
    "    - 2.3.11.8. [**Matrice de contingence**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#contingency-matrix)<br/>([_Contingency Matrix_](https://scikit-learn.org/stable/modules/clustering.html#contingency-matrix))\n",
    "    - 2.3.11.9. [**Matrice de confusion par paire**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/2_3_clustering.ipynb#pair-confusion-matrix)<br/>([_Pair Confusion Matrix_](https://scikit-learn.org/stable/modules/clustering.html#pair-confusion-matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='clustering'></a> 2.3. **Regroupement**<br/>([_Clustering_](https://scikit-learn.org/stable/modules/clustering.html#clustering))\n",
    "\n",
    "Le [**regroupement de données**](https://en.wikipedia.org/wiki/Cluster_analysis) (*clustering*) non étiquetées peut être effectué avec le module [**`sklearn.cluster`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster).\n",
    "\n",
    "Chaque algorithme de regroupement se décline en deux variantes : une classe, qui implémente la méthode `fit` pour apprendre les groupes sur les données d'entraînement, et une fonction qui, compte tenu des données d'entraînement, renvoie un tableau d'étiquettes entières correspondant aux différents groupes. Pour la classe, les étiquettes sur les données d'regroupement se trouvent dans l'attribut `labels_`.\n",
    "\n",
    "**Données d'entrée**\n",
    "\n",
    "Il est important de noter que les algorithmes implémentés dans ce module peuvent prendre différents types de matrices en entrée. Toutes les méthodes acceptent des matrices de données standard de forme `(n_samples, n_features)`. Celles-ci peuvent être obtenues à partir des classes du module [**`sklearn.feature_extraction`**](https://scikit-learn.org/stable/modules/clustering.html). Pour [**`AffinityPropagation`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html), [**`SpectralClustering`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html) et [**`DBSCAN`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html), il est également possible d'entrer des matrices de similarité de forme `(n_samples, n_samples)`. Celles-ci peuvent être obtenues à partir des fonctions du module [**`sklearn.metrics.pairwise`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics.pairwise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='overview-of-clustering-methods'></a> 2.3.1. **Aperçu des méthodes de regroupement**<br/>([_Overview of clustering methods_](https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods))\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png\"\n",
    "    alt=\"Comparaison des algorithmes de regroupement dans scikit-learn\"\n",
    "    style=\"max-width: 100%; height: auto;\"/>\n",
    "    <p>Comparaison des algorithmes de regroupement dans scikit-learn</p>\n",
    "</div>\n",
    "\n",
    "|Nom de la méthode|Paramètres|Évolutivité|Usage|Géométrie (métrique utilisée)|\n",
    "|-|-|-|-|-|\n",
    "|[**K-moyennes** (2.3.2)](https://scikit-learn.org/stable/modules/clustering.html#k-means)|nombre de groupes|`n_samples` très grand, `n_clusters` moyen avec [**mini-lots** (2.3.2.2)](https://scikit-learn.org/stable/modules/clustering.html#mini-batch-kmeans)|Usage général, taille de groupe uniforme, géométrie plate, pas trop de groupes, inductif|Distances entre les points|\n",
    "|[**Propagation d'affinité** (2.3.3)](https://scikit-learn.org/stable/modules/clustering.html#affinity-propagation)|amortissement, préférence d'échantillon|Non scalable avec `n_samples`|De nombreux groupes, taille de groupe inégale, géométrie non plate, inductif|Distance du graphe (par exemple, graphe des voisins les plus proches)|\n",
    "|[**Déplacement moyen** (2.3.4)](https://scikit-learn.org/stable/modules/clustering.html#mean-shift)|bande passante|Non scalable avec `n_samples`|De nombreux groupes, taille de groupe inégale, géométrie non plate, inductif|Distances entre les points|\n",
    "|[**Regroupement spectral** (2.3.5)](https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering)|nombre de groupes|`n_samples` moyen, petit `n_clusters`|Peu de groupes, taille de groupe uniforme, géométrie non plate, transductif|Distance du graphe (par exemple, graphe des voisins les plus proches)|\n",
    "|[**Regroupement hiérarchique** (2.3.6)](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering)|nombre de groupes ou seuil de distance|Grands `n_samples` et `n_clusters`|De nombreux groupes, éventuellement des contraintes de connectivité, transductif|Distances entre les points|\n",
    "|[**Regroupement agglomératif** (2.3.6)](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering)|nombre de groupes ou seuil de distance, type de liaison, distance|Grands `n_samples` et `n_clusters`|De nombreux groupes, éventuellement des contraintes de connectivité, distances non euclidiennes, transductif|Toute distance par paire|\n",
    "|[**DBSCAN** (2.3.7)](https://scikit-learn.org/stable/modules/clustering.html#dbscan)|taille de voisinage|`n_samples` très grand, `n_clusters` moyen|Géométrie non plate, tailles de groupe inégales, élimination des valeurs aberrantes, transductif|Distances entre les points les plus proches|\n",
    "|[**OPTICS** (2.3.8)](https://scikit-learn.org/stable/modules/clustering.html#optics)|appartenance minimale au groupe|Très grand `n_samples`, grand `n_clusters`|Géométrie non plate, tailles de groupe inégales, densité de groupe variable, élimination des valeurs aberrantes, transductif|Distances entre les points|\n",
    "|[**Modèles de mélange gaussien** (2.1)](https://scikit-learn.org/stable/modules/mixture.html#mixture)|nombreux|Non scalable|Géométrie plate, bon pour l'estimation de densité, inductif|Distances de Mahalanobis aux centres|\n",
    "|[**BIRCH** (2.3.9)](https://scikit-learn.org/stable/modules/clustering.html#birch)|facteur de ramification, seuil, cluster global facultatif|Grands `n_clusters` et `n_samples`|Grand ensemble de données, élimination des valeurs aberrantes, réduction de données, inductif|Distance euclidienne entre les points|\n",
    "|[**K-moyennes à bissection** (2.3.6.5)](https://scikit-learn.org/stable/modules/clustering.html#bisect-k-means)|nombre de groupes|`n_samples` très grand, `n_clusters` moyen|Usage général, taille de groupe uniforme, géométrie plate, pas de groupes vides, inductif, hiérarchique|Distances entre les points|\n",
    "\n",
    "Le regroupement avec une géométrie non plane est utile lorsque les groupes ont une forme spécifique, c'est-à-dire une variété non plane, et que la distance euclidienne standard n'est pas la bonne métrique. Ce cas se présente dans les deux premières lignes de la figure ci-dessus.\n",
    "\n",
    "Les modèles de mélange gaussien, utiles pour le regroupement, sont décrits dans un [**autre chapitre de la documentation dédié aux modèles de mélange** (2.1)](https://scikit-learn.org/stable/modules/mixture.html#mixture). K-moyennes peut être considéré comme un cas particulier de modèle de mélange gaussien avec une covariance égale par composant.\n",
    "\n",
    "Les méthodes de regroupement [**transductives**](https://scikit-learn.org/stable/glossary.html#term-transductive) (par opposition aux méthodes de regroupement [**inductives**](https://scikit-learn.org/stable/glossary.html#term-inductive)) ne sont pas conçues pour être appliquées à de nouvelles données invisibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='k-means'></a> 2.3.2. **K-moyennes**<br/>([*K-means*](https://scikit-learn.org/stable/modules/clustering.html#k-means))\n",
    "\n",
    "L'algorithme [**`KMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) regroupe les données en essayant de séparer les échantillons en $n$ groupes de variance égale, minimisant un critère connu sous le nom d'inertie ou de somme des carrés intra-cluster (voir ci-dessous). Cet algorithme exige que le nombre de groupes soit spécifié. Il fonctionne bien pour un grand nombre d'échantillons et a été utilisé dans une grande variété de domaines d'application dans de nombreux champs différents.\n",
    "\n",
    "L'algorithme des k-moyennes divise un ensemble d'échantillons en $K$ groupes disjoints, chacun décrit par la moyenne $\\mu_j$ des échantillons du groupe. Les moyennes sont communément appelées les centroïdes (barycentres) du groupe; noter qu'ils ne sont pas, en général, des points de $X$, bien qu'ils peuplent le même espace.\n",
    "\n",
    "L'algorithme des k-moyennes vise à choisir les centroïdes qui minimisent l'inertie, ou le critère de somme des carrés intra-cluster :\n",
    "\n",
    "$$\\sum_{i=0}^{n}\\min_{\\mu_j \\in C}(||x_i - \\mu_j||^2)$$\n",
    "\n",
    "L'inertie peut être vue comme une mesure de la cohérence interne des groupes. Cependant, elle présente certaines limitations :\n",
    "\n",
    "* L'inertie suppose que les groupes sont convexes et isotropes, ce qui n'est pas toujours le cas. Elle réagit mal aux groupes allongés ou aux variétés aux formes irrégulières.\n",
    "* L'inertie n'est pas une métrique normalisée : nous savons seulement que des valeurs plus basses sont meilleures et que zéro est optimal. Cependant, dans les espaces de très grande dimension, les distances euclidiennes ont tendance à devenir disproportionnées (c'est un cas particulier de ce qu'on appelle la \"malédiction de la dimension\"). L'utilisation d'un algorithme de réduction de dimension comme l'[**analyse en composantes principales (ACP)** (2.5.1)](https://scikit-learn.org/stable/modules/decomposition.html#pca) avant le regroupement k-moyennes peut atténuer ce problème et accélérer les calculs.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_kmeans_assumptions_002.png\"\n",
    "    alt=\"Groupes K-moyennes inappropriés\"\n",
    "    style=\"max-width: 75%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Les k-moyennes sont souvent appelées algorithme de Lloyd. En termes simples, l'algorithme comporte trois étapes. La première étape consiste à choisir les centroïdes initiaux, la méthode la plus basique étant de choisir $k$ échantillons du jeu de données $X$. Après l'initialisation, les k-moyennes consistent en une boucle entre les deux autres étapes. La première étape attribue chaque échantillon au centroïde le plus proche. La deuxième étape crée de nouveaux centroïdes en prenant la valeur moyenne de tous les échantillons attribués à chaque centroïde précédent. La différence entre les anciens et les nouveaux centroïdes est calculée et l'algorithme répète ces deux dernières étapes jusqu'à ce que cette valeur soit inférieure à un seuil. En d'autres termes, il se répète jusqu'à ce que les centroïdes ne bougent pas significativement.\n",
    "\n",
    "Les k-moyennes sont équivalentes à l'algorithme d'espérance-maximisation avec une petite matrice de covariance diagonale égale pour tous les groupes.\n",
    "\n",
    "L'algorithme peut également être compris à travers le concept de [**diagrammes de Voronoï** (wkpd)](https://en.wikipedia.org/wiki/Voronoi_diagram). Tout d'abord, le diagramme de Voronoï des points est calculé en utilisant les centroïdes actuels. Chaque segment du diagramme de Voronoï devient un groupe distinct. Ensuite, les centroïdes sont mis à jour en prenant la valeur moyenne de chaque segment. L'algorithme répète cela jusqu'à ce qu'un critère d'arrêt soit satisfait. Habituellement, l'algorithme s'arrête lorsque la diminution relative de la fonction objectif entre les itérations est inférieure à la valeur de tolérance donnée. Ce n'est pas le cas dans cette implémentation : l'itération s'arrête lorsque les centroïdes bougent moins que la tolérance.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_kmeans_digits_001.png\"\n",
    "    alt=\"K-moyennes des chiffres manuscrits\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Avec suffisamment de temps, les k-moyennes convergeront toujours, cependant cela peut se faire vers un minimum local. Cela dépend fortement de l'initialisation des centroïdes. En conséquence, le calcul est souvent répété plusieurs fois, avec des initialisations différentes des centroïdes. Une méthode pour remédier à ce problème est le schéma d'initialisation k-moyennes++, qui a été mis en œuvre dans scikit-learn (utilisez le paramètre `init='k-means++'`). Cela initialise les centroïdes pour qu'ils soient (généralement) éloignés les uns des autres, conduisant probablement à de meilleurs résultats qu'une initialisation aléatoire, comme montré dans la référence.\n",
    "\n",
    "L'initialisation k-moyennes++ peut également être appelée indépendamment pour sélectionner des amorces pour d'autres algorithmes de regroupement, voir [**`sklearn.cluster.kmeans_plusplus`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.kmeans_plusplus.html) pour plus de détails et des exemples d'utilisation.\n",
    "\n",
    "L'algorithme supporte les poids d'échantillon, qui peuvent être spécifiés via le paramètre `sample_weight`. Cela permet d'attribuer plus de poids à certains échantillons lors du calcul des centroïdes de groupe et des valeurs d'inertie. Par exemple, attribuer un poids de 2 à un échantillon équivaut à ajouter une copie de cet échantillon à l'ensemble de données $X$.\n",
    "\n",
    "Les k-moyennes peuvent être utilisées pour la quantification vectorielle. Cela est accompli en utilisant la méthode `transform` d'un modèle entraîné de [**`KMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='low-level-parallelism'></a> 2.3.2.1. **Parallélisme de bas niveau**<br/>([*Low-level parallelism*](https://scikit-learn.org/stable/modules/clustering.html#low-level-parallelism))\n",
    "\n",
    "[**`KMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) bénéficie du parallélisme basé sur OpenMP via Cython. De petits morceaux de données (256 échantillons) sont traités en parallèle, ce qui permet en outre de réduire la consommation de mémoire. Pour plus de détails sur la manière de contrôler le nombre de threads, veuillez vous référer à nos notes sur le [**parallélisme** (8.3.1)](https://scikit-learn.org/stable/computing/parallelism.html#parallelism).\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Démonstration des hypothèses de k-moyennes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_3_cluster/plot_kmeans_assumptions.ipynb)<br/>([_Demonstration of k-means assumptions_](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html))\n",
    "\n",
    "Démonstration de quand les k-moyennes fonctionnent de manière intuitive et quand ce n'est pas le cas.\n",
    "\n",
    "##### [**Une démo du clustering K-moyennes sur les données des chiffres manuscrits**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_3_cluster/plot_kmeans_digits.ipynb)<br/>([_A demo of K-Means clustering on the handwritten digits data_](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html))\n",
    "\n",
    "Clustering des chiffres manuscrits.\n",
    "\n",
    "#### Références\n",
    "\n",
    "🔬 Arthur, David and Sergei Vassilvitskii [**“`k-means++`: The Advantages of Careful Seeding“**](http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf), Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, Society for Industrial and Applied Mathematics (2007)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='mini-batch-k-means'></a> 2.3.2.2. **K-moyennes par mini-lots**<br/>([*Mini Batch K-Means*](https://scikit-learn.org/stable/modules/clustering.html#mini-batch-k-means))\n",
    "\n",
    "Le [**`MiniBatchKMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html) est une variante de l'algorithme [**`KMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) qui utilise des mini-lots pour réduire le temps de calcul, tout en essayant d'optimiser la même fonction objectif. Les mini-lots sont des sous-ensembles des données d'entrée, échantillonnés de manière aléatoire à chaque itération d'apprentissage. Ces mini-lots réduisent considérablement la quantité de calculs nécessaires pour converger vers une solution locale. Contrairement à d'autres algorithmes qui réduisent le temps de convergence des k-moyennes, les mini-batch k-means produisent des résultats qui ne sont généralement que légèrement moins bons que l'algorithme standard.\n",
    "\n",
    "L'algorithme itère entre deux étapes principales, similaires aux k-moyennes classiques. Dans la première étape, des échantillons sont tirés au hasard de l'ensemble de données pour former un mini-lot. Ces échantillons sont ensuite attribués au centroïde le plus proche. Dans la deuxième étape, les centroïdes sont mis à jour. Contrairement aux k-moyennes, cela se fait sur une base par échantillon. Pour chaque échantillon du mini-lot, le centroïde attribué est mis à jour en calculant la moyenne continue de l'échantillon et de tous les échantillons précédents attribués à ce centroïde. Cela a pour effet de réduire le taux de changement d'un centroïde au fil du temps. Ces étapes sont exécutées jusqu'à la convergence ou jusqu'à ce qu'un nombre prédéterminé d'itérations soit atteint.\n",
    "\n",
    "Le [**`MiniBatchKMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html) converge plus rapidement que [**`KMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), mais la qualité des résultats est réduite. En pratique, cette différence de qualité peut être assez faible, comme le montrent l'exemple et la référence citée.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_mini_batch_kmeans_001.png\"\n",
    "    alt=\"Comparaison des algorithmes de regroupement K-Means et MiniBatchKMeans\"\n",
    "    style=\"max-width: 75%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Comparaison des algorithmes de regroupement `KMeans` et `MiniBatchKMeans`**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_3_cluster/plot_mini_batch_kmeans.ipynb)<br/>([*Comparison of the `KMeans` and `MiniBatchKMeans` clustering algorithms*](https://scikit-learn.org/stable/auto_examples/cluster/plot_mini_batch_kmeans.html))\n",
    "\n",
    "Comparaison des algorithmes [**`KMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) et [**`MiniBatchKMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html).\n",
    "\n",
    "##### [**Regroupement de documents texte à l'aide de k-means**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/text/plot_document_clustering.ipynb)<br/>([*Clustering text documents using k-means*](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html))\n",
    "\n",
    "Regroupement de documents texte à l'aide de MiniBatchKMeans pour données éparses.\n",
    "\n",
    "##### [**Apprentissage en ligne d'un dictionnaire de parties de visages**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_3_cluster/plot_dict_face_patches.ipynb)<br/>([*Online learning of a dictionary of parts of faces*](https://scikit-learn.org/stable/auto_examples/cluster/plot_dict_face_patches.html))\n",
    "\n",
    "#### Références\n",
    "\n",
    "🔬 D. Sculley [**“Web Scale K-Means Clustering”**](https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf), Proceedings of the 19th international conference on World wide web (2010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='affinity-propagation'></a> 2.3.3. **Propagation d'affinité**<br/>([*Affinity Propagation*](https://scikit-learn.org/stable/modules/clustering.html#affinity-propagation))\n",
    "\n",
    "La **Propagation d'affinité** réalisée par la classe [**`AffinityPropagation`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html) crée des grappes en envoyant des messages entre des paires d'échantillons jusqu'à convergence. Un ensemble de données est ensuite décrit à l'aide d'un petit nombre d'exemplaires, qui sont identifiés comme les plus représentatifs des autres échantillons. Les messages envoyés entre les paires représentent l'adéquation pour qu'un échantillon soit l'exemplaire de l'autre, et ces valeurs sont mises à jour en réponse aux valeurs des autres paires. Cette mise à jour se fait de manière itérative jusqu'à la convergence, moment où les exemplaires finaux sont sélectionnés et, par conséquent, le regroupement final est obtenu.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_affinity_propagation_001.png\"\n",
    "    alt=\"Démo de l'algorithme de clustering par propagation d'affinité\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "La Propagation d'affinité peut être intéressante car elle choisit le nombre de grappes en fonction des données fournies. À cette fin, les deux paramètres importants sont la _préférence_, qui contrôle le nombre d'exemplaires utilisés, et le _facteur d'amortissement_ qui atténue les messages de responsabilité et de disponibilité pour éviter les oscillations numériques lors de la mise à jour de ces messages.\n",
    "\n",
    "Le principal inconvénient de la Propagation d'affinité est sa complexité. L'algorithme a une complexité temporelle de l'ordre de $\\mathcal{O}(N^2 T)$, où $N$ est le nombre d'échantillons et $T$ est le nombre d'itérations jusqu'à la convergence. De plus, la complexité mémoire est de l'ordre de $\\mathcal{O}(N^2)$ si une matrice de similarité dense est utilisée, mais elle est réduite si une matrice de similarité creuse est utilisée. Cela rend la Propagation d'affinité plus appropriée pour les ensembles de données de petite à moyenne taille.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Démo de l'algorithme de clustering par propagation d'affinité**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_3_cluster/plot_affinity_propagation.ipynb)<br/>([*Demo of affinity propagation clustering algorithm*](https://scikit-learn.org/stable/auto_examples/cluster/plot_affinity_propagation.html))\n",
    "\n",
    "Propagation d'affinité sur un jeu de données 2D synthétique avec 3 classes.\n",
    "\n",
    "#### [**Visualisation de la structure du marché boursier**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/applications/plot_stock_market.ipynb)<br/>([*Visualizing the stock market structure*](https://scikit-learn.org/stable/auto_examples/applications/plot_stock_market.html))\n",
    "\n",
    "Propagation d'affinité sur des séries chronologiques financières pour identifier des groupes d'entreprises.\n",
    "\n",
    "### Références\n",
    "\n",
    "🔬 Brendan J. Frey, Delbert Dueck, [**“Clustering by Passing Messages Between Data Points”**](https://www.science.org/cms/asset/231f2875-11f5-44cd-98dc-b3e583f9fddd/pap.pdf), Science Feb. 2007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description de l'algorithme\n",
    "\n",
    "Les messages envoyés entre les points appartiennent à l'une des deux catégories. Le premier est la responsabilité $r(i, k)$, qui représente les preuves accumulées que l'échantillon $k$ devrait être le représentant de l'échantillon $i$. Le second est la disponibilité $a(i, k)$ qui représente les preuves accumulées que l'échantillon $i$ devrait choisir l'échantillon $k$ comme son représentant, et considère les valeurs de tous les autres échantillons dont $k$ devrait être le représentant. De cette manière, les représentants sont choisis par des échantillons s'ils sont (1) suffisamment similaires à de nombreux échantillons et (2) choisis par de nombreux échantillons pour être représentatifs d'eux-mêmes.\n",
    "\n",
    "Plus formellement, la responsabilité d'un échantillon $k$ pour être le représentant de l'échantillon $i$ est donné par :\n",
    "\n",
    "$$r(i, k) \\leftarrow s(i, k) - max [ a(i, k') + s(i, k') \\forall k' \\neq k ]$$\n",
    "\n",
    "Où $s(i, k)$ est la similarité entre les échantillons $i$ et $k$. La disponibilité de l'échantillon $k$ pour être le représentant de l'échantillon $i$ est donné par:\n",
    "\n",
    "$$a(i, k) \\leftarrow min [0, r(k, k) + \\sum_{i'~s.t.~i' \\notin \\{i, k\\}}{r(i', k)}]$$\n",
    "\n",
    "Pour commencer, toutes les valeurs de $r$ et $a$ sont mises à zéro, et le calcul de chacun itère jusqu'à convergence. Comme discuté ci-dessus, afin d'éviter les oscillations numériques lors de la mise à jour des messages, le facteur d'amortissement $\\lambda$ est introduit au processus d'itération :\n",
    "\n",
    "$$r_{t+1}(i, k) = \\lambda\\cdot r_{t}(i, k) + (1-\\lambda)\\cdot r_{t+1}(i, k)$$\n",
    "$$a_{t+1}(i, k) = \\lambda\\cdot a_{t}(i, k) + (1-\\lambda)\\cdot a_{t+1}(i, k)$$\n",
    "\n",
    "où $t$ indique les temps d'itération."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='mean-shift'></a> 2.3.4. **Déplacement moyen**<br/>([*Mean Shift*](https://scikit-learn.org/stable/modules/clustering.html#mean-shift))\n",
    "\n",
    "Le regroupement [**`MeanShift`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html) vise à découvrir des _blobs_ dans une densité lisse d'échantillons. Il s'agit d'un algorithme basé sur le centroïde, qui fonctionne en mettant à jour les candidats pour les centroïdes afin qu'ils soient la moyenne des points dans une région donnée. Ces candidats sont ensuite filtrés dans une étape de post-traitement pour éliminer les quasi-doublons pour former l'ensemble final de centroïdes.\n",
    "\n",
    "La position des candidats pour les centroïdes est ajustée de manière itérative à l'aide d'une technique appelée _hill climbing_ (escalade de colline), qui recherche les maxima locaux de la densité de probabilité estimée. Étant donné un candidat pour le centroïde $x_i$ à l'itération $t$, le candidat est mis à jour selon l'équation suivante :\n",
    "\n",
    "$$x_i^{t+1} = m(x_i^t)$$\n",
    "\n",
    "Où $N(x_i)$ représente le voisinage des échantillons à une certaines distance de $x_i$ et $m$ est le vecteur de *déplacement moyen* qui est calculé pour chaque centroïde et pointe vers une région d'augmentation maximale de la densité de points. Cela est calculé à l'aide de l'équation suivante, mettant efficacement à jour un centroïde pour qu'il corresponde à la moyenne des échantillons dans son voisinage :\n",
    "\n",
    "$$m(x_i) = \\frac{\\sum_{x_j \\in N(x_i)}K(x_j - x_i)x_j}{\\sum_{x_j \\in N(x_i)}K(x_j - x_i)}$$\n",
    "\n",
    "L'algorithme détermine automatiquement le nombre de grappes au lieu de dépendre d'un paramètre de largeur de bande `bandwidth`, qui contrôle la taille de la région à explorer. Ce paramètre peut être fixé manuellement, mais il peut être également estimé à l'aide de la fonction `estimate_bandwidth` fournie, qui est appelée si la largeur de bande n'est pas définie.\n",
    "\n",
    "L'algorithme n'est pas très évolutif, car il nécessite plusieurs recherches des plus proches voisins pendant son exécution. Toutefois, l'algorithme converge nécessairement, et il cesse d'itérer lorsque le changement dans les centroïdes devient faible.\n",
    "\n",
    "L'étiquetage d'un nouvel échantillon est effectué en trouvant le centroïde le plus proche pour un échantillon donné.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_mean_shift_001.png\"\n",
    "    alt=\"Estimated number of clusters: 3\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Une démonstration de l'algorithme de regroupement par déplacement moyen**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_3_cluster/plot_mean_shift.ipynb)<br/>([*A demo of the mean-shift clustering algorithm*](https://scikit-learn.org/stable/auto_examples/cluster/plot_mean_shift.html))\n",
    "\n",
    "Regroupement par déplacement moyen sur un jeu de données 2D synthétique avec 3 classes.\n",
    "\n",
    "### Références\n",
    "\n",
    "🔬 Dorin Comaniciu, Peter Meer, [**“Mean Shift: A Robust Approach Toward Feature Space Analysis”**](https://ieeexplore.ieee.org/document/1000236). IEEE Transactions on Pattern Analysis and Machine Intelligence. 2002. pp. 603-619."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='spectral-clustering'></a> 2.3.5. **Regroupement spectral**<br/>([*Spectral clustering*](https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering))\n",
    "\n",
    "[**`SpectralClustering`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html) effectue un plongement de faible dimension de la matrice d'affinité entre les échantillons, suivie d'un regroupement, par exemple, par KMeans, des composantes des vecteurs propres dans l'espace de faible dimension. Il est particulièrement efficace en termes de calcul si la matrice d'affinité est creuse et que le solveur `amg` est utilisé pour le problème des valeurs propres (notez que le solveur `amg` nécessite que le module [**`pyamg`**](https://github.com/pyamg/pyamg) soit installé.)\n",
    "\n",
    "La version actuelle de [**`SpectralClustering`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html) nécessite que le nombre de grappes soit spécifié à l'avance. Cela fonctionne bien pour un petit nombre de grappes, mais n'est pas conseillé pour de nombreux grappes.\n",
    "\n",
    "Pour deux grappes, [**`SpectralClustering`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html) résout une relaxation convexe du problème des [coupes normalisées](https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf) sur le graphe de similarité : il s'agit de diviser le graphe en deux parties de manière à ce que le poids des arêtes coupées soit faible par rapport aux poids des arêtes à l'intérieur de chaque grappe. Ce critère est particulièrement intéressant en traitement d'images, où les sommets du graphe représentent les pixels et les poids des arêtes du graphe de similarité sont calculés en fonction du gradient de l'image.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_segmentation_toy_001.png\"\n",
    "    alt=\"Spectral clustering for image segmentation\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_segmentation_toy_002.png\"\n",
    "    alt=\"Spectral clustering for image segmentation\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "### Avertissement : Transformer la distance en similitudes bien élevées\n",
    "\n",
    "> Notez que si les valeurs de votre matrice de similarité ne sont pas correctement réparties, par exemple avec des valeurs négatives ou avec une matrice de distances plutôt qu'une matrice de similarités, le problème spectral sera singulier et insoluble. Dans ce cas, il est recommandé d'appliquer une transformation aux entrées de la matrice. Par exemple, dans le cas d'une matrice de distances signées, il est courant d'appliquer un noyau de chaleur :\n",
    ">\n",
    "> ```similarité = np.exp(-beta * distance / distance.std())```\n",
    ">\n",
    "> Consultez les exemples pour une telle application.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Regroupement spectral pour la segmentation d'images**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_3_cluster/plot_segmentation_toy.ipynb)<br/>([*Spectral clustering for image segmentation*](https://scikit-learn.org/stable/auto_examples/cluster/plot_segmentation_toy.html))\n",
    "\n",
    "Segmentation d'objets à partir d'un arrière-plan bruyant en utilisant le regroupement spectral.\n",
    "\n",
    "#### [**Segmenter en régions l'image des pièces de monnaie grecques**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_3_cluster/plot_coin_segmentation.ipynb)<br/>([*Segmenting the picture of greek coins in regions*](https://scikit-learn.org/stable/auto_examples/cluster/plot_coin_segmentation.html))\n",
    "\n",
    "Regroupement spectral pour diviser l'image des pièces de monnaie grecques en régions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='different-label-assignment-strategies'></a> 2.3.5.1. **Stratégies d'attribution d'étiquettes**<br/>([*Different label assignment strategies*](https://scikit-learn.org/stable/modules/clustering.html#different-label-assignment-strategies))\n",
    "\n",
    "Différentes stratégies d'attribution d'étiquettes peuvent être utilisées, correspondant au paramètre `assign_labels` de [**`SpectralClustering`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html). La stratégie `\"kmeans\"` peut correspondre à des détails plus fins, mais peut être instable. En particulier, à moins que vous ne contrôliez le `random_state`, il peut ne pas être reproductible d'une exécution à l'autre, car il dépend de l'initialisation aléatoire. La stratégie alternative `\"discretize\"` est reproductible à 100%, mais tend à créer des parcelles de forme assez régulière et géométrique. L'option `\"cluster_qr\"` récemment ajoutée est une alternative déterministe qui tend à créer le meilleur partitionnement visuel sur l'exemple d'application ci-dessous.\n",
    "\n",
    "<table class=\"docutils align-default\">\n",
    "<thead>\n",
    "<tr class=\"row-odd\"><th class=\"head\"><p><code class=\"docutils literal notranslate\"><span class=\"pre\">assign_labels=&quot;kmeans&quot;</span></code></p></th>\n",
    "<th class=\"head\"><p><code class=\"docutils literal notranslate\"><span class=\"pre\">assign_labels=&quot;discretize&quot;</span></code></p></th>\n",
    "<th class=\"head\"><p><code class=\"docutils literal notranslate\"><span class=\"pre\">assign_labels=&quot;cluster_qr&quot;</span></code></p></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr class=\"row-even\"><td><p><a class=\"reference external\" href=\"https://scikit-learn.org/stable/auto_examples/cluster/plot_coin_segmentation.html\"><img alt=\"coin_kmeans\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_coin_segmentation_001.png\" style=\"width: 175.0px; height: 175.0px;\" /></a></p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://scikit-learn.org/stable/auto_examples/cluster/plot_coin_segmentation.html\"><img alt=\"coin_discretize\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_coin_segmentation_002.png\" style=\"width: 175.0px; height: 175.0px;\" /></a></p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://scikit-learn.org/stable/auto_examples/cluster/plot_coin_segmentation.html\"><img alt=\"coin_cluster_qr\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_coin_segmentation_003.png\" style=\"width: 175.0px; height: 175.0px;\" /></a></p></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "##### Références\n",
    "\n",
    "🔬 Stella X. Yu, Jianbo Shi, [**“Multiclass spectral clustering”**](http://lear.inrialpes.fr/people/triggs/events/iccv03/cdrom/iccv03/0313_yu.pdf), 2003\n",
    "\n",
    "🔬 Anil Damle, Victor Minden, Lexing Ying, [**“Simple, direct, and efficient multi-way spectral clustering”**](https://doi.org/10.1093/imaiai/iay008), 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='spectral-clustering-graphs'></a> 2.3.5.2. **Graphes de regroupement spectral**<br/>([*Spectral Clustering Graphs*](https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering-graphs))\n",
    "\n",
    "Le regroupement spectral peut également être utilisé pour partitionner des graphes via leurs plongements spectraux. Dans ce cas, la matrice d'affinité est la matrice d'adjacence du graphe, et SpectralClustering est initialisé avec `affinity='precomputed'` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "sc = SpectralClustering(3, affinity='precomputed', n_init=100,\n",
    "                        assign_labels='discretize')\n",
    "sc.fit_predict(adjacency_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Références\n",
    "\n",
    "🔬 Ulrike von Luxburg,  [**“A Tutorial on Spectral Clustering”**](https://arxiv.org/pdf/0711.0189.pdf), 2007\n",
    "\n",
    "🔬 Jianbo Shi, Jitendra Malik, [**“Normalized cuts and image segmentation”**](https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf), 2000\n",
    "\n",
    "🔬 Marina Meila, Jianbo Shi, [**“A Random Walks View of Spectral Segmentation”**](https://www.ri.cmu.edu/pub_files/pub3/maila_marina_2001_2/maila_marina_2001_2.pdf), 2001\n",
    "\n",
    "🔬 Andrew Y. Ng, Michael I. Jordan, Yair Weiss, [**“On Spectral Clustering: Analysis and an algorithm”**](https://proceedings.neurips.cc/paper/2001/file/801272ee79cfde7fa5960571fee36b9b-Paper.pdf), 2001\n",
    "\n",
    "🔬 David Zhuzhunashvili, Andrew Knyazev, [**“Preconditioned Spectral Clustering for Stochastic Block Partition Streaming Graph Challenge”**](https://arxiv.org/pdf/1708.07481.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='hierarchical-clustering'></a> 2.3.6. **Regroupement hiérarchique**<br/>([*Hierarchical clustering*](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering))\n",
    "\n",
    "Le regroupement hiérarchique est une famille générale d'algorithmes de regroupement qui construisent des grappes imbriquées en les fusionnant ou en les divisant successivement. Cette hiérarchie de grappes est représentée sous forme d'arbre (ou dendrogramme). La racine de l'arbre est la grappe unique qui rassemble tous les échantillons, les feuilles étant les grappes à un seul échantillon. Consulter la [**page Wikipédia**](https://en.wikipedia.org/wiki/Hierarchical_clustering) pour plus de détails.\n",
    "\n",
    "L'objet [**`AgglomerativeClustering`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) effectue un regroupement hiérarchique en utilisant une approche ascendante : chaque observation commence dans sa propre grappe, et les grappes sont successivement fusionnées. Les critères de liaison déterminent la métrique utilisée pour la stratégie de fusion :\n",
    "\n",
    "* **Ward** minimise la somme des différences au carré dans toutes les grappes. Il s'agit d'une approche de minimisation de la variance et, à cet égard, elle est similaire à la fonction objectif k-moyennes, mais abordée avec une approche hiérarchique agglomérative.\n",
    "* **La liaison maximale** ou **complète** minimise la distance maximale entre les observations de paires de grappes.\n",
    "* **Le couplage moyen** minimise la moyenne des distances entre toutes les observations de paires de grappes.\n",
    "* **Le couplage unique** minimise la distance entre les observations les plus proches des paires de grappes.\n",
    "\n",
    "L'[**`AgglomerativeClustering`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) peut également évoluer vers un grand nombre d'échantillons lorsqu'il est utilisé conjointement avec une matrice de connectivité, mais il est coûteux en calcul lorsque aucune contrainte de connectivité n'est ajoutée entre les échantillons : il considère à chaque étape toutes les fusions possibles.\n",
    "\n",
    "[**`FeatureAgglomeration`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)\n",
    "\n",
    "Le [**`FeatureAgglomeration`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) utilise le regroupement agglomératif pour regrouper les caractéristiques qui semblent très similaires, réduisant ainsi le nombre de caractéristiques. C'est un outil de réduction de dimensionnalité, voir [**Réduction de dimensionnalité non supervisée** (6.5)](https://scikit-learn.org/stable/modules/unsupervised_reduction.html#data-reduction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='different-linkage-type-ward-complete-average-and-single-linkage'></a> 2.3.6.1. **Types de liaison : Ward, complète, moyenne et liaison simple**<br/>([_Different linkage type: Ward, complete, average, and single linkage_](https://scikit-learn.org/stable/modules/clustering.html#different-linkage-type-ward-complete-average-and-single-linkage))\n",
    "\n",
    "L'[**`AgglomerativeClustering`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) prend en charge les stratégies de liaison Ward, unique, moyenne et complète.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_linkage_comparison_001.png\"\n",
    "    alt=\"Single Linkage, Average Linkage, Complete Linkage, Ward Linkage\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Le regroupement agglomératif présente un comportement de type \"le riche devient plus riche\" qui conduit à des tailles de grappes inégales. À cet égard, la liaison simple est la stratégie la moins performante, tandis que Ward permet d'obtenir des tailles de grappes plus régulières. Cependant, avec Ward, l'affinité (ou la distance utilisée dans le regroupement) ne peut pas être modifiée. Ainsi, pour les mesures non euclidiennes, la liaison moyenne est une bonne alternative. La liaison simple, bien qu'elle ne soit pas robuste face aux données bruitées, peut être calculée très efficacement, ce qui la rend utile pour effectuer un regroupement hiérarchique sur de grands ensembles de données. La liaison simple peut également bien fonctionner sur des données non globulaires.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Divers regroupement agglomératifs sur un plongement 2D de chiffres**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_3_cluster/plot_digits_linkage.ipynb)<br/>([*Various Agglomerative Clustering on a 2D embedding of digits*](https://scikit-learn.org/stable/auto_examples/cluster/plot_digits_linkage.html))\n",
    "\n",
    "Exploration des différentes stratégies de liaison dans un jeu de données réel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='visualization-of-cluster-hierarchy'></a> 2.3.6.2. **Visualisation de la hiérarchie des groupes**<br/>([_Visualization of cluster hierarchy_](https://scikit-learn.org/stable/modules/clustering.html#visualization-of-cluster-hierarchy))\n",
    "\n",
    "Il est possible de visualiser l'arbre représentant la fusion hiérarchique des grappes sous forme de dendrogramme. L'inspection visuelle peut souvent être utile pour comprendre la structure des données, mais plus encore dans le cas d'échantillons de petite taille.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_dendrogram_001.png\"\n",
    "    alt=\"Hierarchical Clustering Dendrogram\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='adding-connectivity-constraints'></a> 2.3.6.3. **Ajout de contraintes de connectivité**<br/>([*Adding connectivity constraints*](https://scikit-learn.org/stable/modules/clustering.html#adding-connectivity-constraints))\n",
    "\n",
    "Un aspect intéressant de l'[**`AgglomerativeClustering`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) est la possibilité d'ajouter des contraintes de connectivité à cet algorithme (seuls les groupes adjacents peuvent être fusionnés), via une matrice de connectivité qui définit pour chaque échantillon les échantillons voisins suivant une structure donnée des données. Par exemple, dans l'exemple du \"swiss roll\" ci-dessous, les contraintes de connectivité interdisent la fusion des points qui ne sont pas adjacents sur le \"swiss roll\", évitant ainsi la formation de groupes qui s'étendent sur des plis superposés du \"roll\".\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_ward_structured_vs_unstructured_001.png\"\n",
    "    alt=\"Without connectivity constraints\"\n",
    "    style=\"max-width: 45%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_ward_structured_vs_unstructured_002.png\"\n",
    "    alt=\"With connectivity constraints\"\n",
    "    style=\"max-width: 45%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Ces contraintes sont utiles pour imposer une certaine structure locale, mais elles rendent aussi l'algorithme plus rapide, surtout lorsque le nombre d'échantillons est élevé.\n",
    "\n",
    "Les contraintes de connectivité sont imposées via une matrice de connectivité : une matrice creuse de Scipy qui ne contient d'éléments qu'à l'intersection d'une ligne et d'une colonne ayant des indices de l'ensemble de données qui doivent être connectés. Cette matrice peut être construite à partir d'informations a priori : par exemple, vous pouvez souhaiter regrouper des pages Web en ne fusionnant que les pages avec un lien pointant de l'une à l'autre. Il peut également être appris à partir des données, par exemple en utilisant [**`sklearn.neighbors.kneighbors_graph`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.kneighbors_graph.html) pour restreindre la fusion aux voisins les plus proches, comme dans [**cet exemple**](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_clustering.html), ou en utilisant [**`sklearn.feature_extraction.image.grid_to_graph`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.image.grid_to_graph.html) pour permettre uniquement la fusion des pixels voisins sur une image, comme dans l'[**exemple de la pièce**](https://scikit-learn.org/stable/auto_examples/cluster/plot_coin_ward_segmentation.html).\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "#### [**Démo de regroupement hiérarchique Ward structuré sur une image de pièces de monnaie**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_3_cluster/plot_coin_ward_segmentation.ipynb)<br/>([*A demo of structured Ward hierarchical clustering on an image of coins*](https://scikit-learn.org/stable/auto_examples/cluster/plot_coin_ward_segmentation.html))\n",
    "\n",
    "Utilisation de l'algorithme Ward pour diviser l'image de pièces de monnaie en régions.\n",
    "\n",
    "#### [**Regroupement hiérarchique : ward structuré vs non structuré**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_3_cluster/plot_coin_ward_segmentation.ipynb)<br/>([*Hierarchical clustering: structured vs unstructured ward*](https://scikit-learn.org/stable/auto_examples/cluster/plot_coin_ward_segmentation.html))\n",
    "\n",
    "Exemple de l'algorithme de Ward sur un \"swiss roll\", comparaison entre les approches structurées et non structurées.\n",
    "\n",
    "#### [**Agglomération de caractéristiques vs sélection univariée**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_3_cluster/plot_feature_agglomeration_vs_univariate_selection.ipynb)<br/>([*Feature agglomeration vs. univariate selection*](https://scikit-learn.org/stable/auto_examples/cluster/plot_feature_agglomeration_vs_univariate_selection.html))\n",
    "\n",
    "Exemple de réduction de dimensionnalité avec agglomération de caractéristiques basée sur le regroupement hiérarchique de Ward.\n",
    "\n",
    "#### [**Agglomération avec et sans structure**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_3_cluster/plot_agglomerative_clustering.ipynb)<br/>([*Agglomerative clustering with and without structure*](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_clustering.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avertissement : **Contraintes de connectivité avec liaison simple, moyenne et complète**\n",
    "\n",
    "> Les contraintes de connectivité et les liaisons simples, complètes ou moyennes peuvent renforcer l'aspect \"le riche devient plus riche\" du regroupement agglomératif, en particulier si elles sont construites avec [**`sklearn.neighbors.kneighbors_graph`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.kneighbors_graph.html). Avec un petit nombre de grappes, elles ont tendance à produire quelques grappes très peuplées et d'autres presque vides sur le plan macroscopique. (voir la discussion dans [**Agglomération avec et sans structure**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_3_cluster/plot_agglomerative_clustering.ipynb)). La liaison unique est l'option de liaison la plus vulnérable à cet égard.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_clustering_001.png\"\n",
    "    alt=\"n_cluster=30, connectivity=False, linkage=average\"\n",
    "    style=\"max-width: 45%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_clustering_002.png\"\n",
    "    alt=\"n_cluster=3, connectivity=False, linkage=average\"\n",
    "    style=\"max-width: 45%; height: auto;\"/>\n",
    "</div>\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_clustering_003.png\"\n",
    "    alt=\"n_cluster=30, connectivity=True, linkage=average\"\n",
    "    style=\"max-width: 45%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_clustering_004.png\"\n",
    "    alt=\"n_cluster=3, connectivity=True, linkage=average\"\n",
    "    style=\"max-width: 45%; height: auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='varying-the-metric'></a> 2.3.6.4. **Variation de la métrique**<br/>([*Varying the metric*](https://scikit-learn.org/stable/modules/clustering.html#varying-the-metric))\n",
    "\n",
    "Une liaison simple, moyenne et complète peut être utilisée avec une variété de distances (ou affinités), en particulier la distance euclidienne ($\\ell_2$), la distance de Manhattan (ou Cityblock, ou $\\ell_1$), la distance cosinus ou toute matrice d'affinité précalculée.\n",
    "* La distance $\\ell_1$ est souvent bien adaptée pour des caractéristiques creuses ou du bruit creux: c'est-à-dire lorsque bon nombre des caractéristiques sont nulles, comme dans l'exploration de texte utilisant des occurrences de mots rares.\n",
    "* La distance cosinus est intéressante car elle est invariante par rapport aux mises à l'échelle globales du signal.\n",
    "\n",
    "Les directives pour choisir une métrique sconsistent à utiliser celle qui maximise la distance entre les échantillons de différentes classes et minimise cette distance au sein de chaque classe.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_clustering_metrics_005.png\"\n",
    "    alt=\"AgglomerativeClustering(metric=cosine)\"\n",
    "    style=\"max-width: 30%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_clustering_metrics_006.png\"\n",
    "    alt=\"AgglomerativeClustering(metric=euclidean)\"\n",
    "    style=\"max-width: 30%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_clustering_metrics_007.png\"\n",
    "    alt=\"AgglomerativeClustering(metric=cityblock)\"\n",
    "    style=\"max-width: 30%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Regroupement agglomératif avec différentes métriques**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_3_cluster/plot_agglomerative_clustering_metrics.ipynb)<br/>([*Agglomerative clustering with different metrics*](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_clustering_metrics.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='bisecting-k-means'></a> 2.3.6.5. **K-moyennes à bissection**<br/>([*Bisecting K-Means*](https://scikit-learn.org/stable/modules/clustering.html#bisecting-k-means))\n",
    "\n",
    "Le [**`BisectingKMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.BisectingKMeans.html) est une variante itérative de l'algorithme [**`KMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), qui utilise le regroupement hiérarchique de division. Au lieu de créer tous les centroïdes en une seule fois, les centroïdes sont sélectionnés progressivement en fonction d'un regroupement précédent : une grappe est divisée en deux nouvelles grappes à plusieurs reprises jusqu'à ce que le nombre cible de grappes soit atteint.\n",
    "\n",
    "Le [**`BisectingKMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.BisectingKMeans.html) est plus efficace que l'algorithme [**`KMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) lorsque le nombre de grappes est important, car il ne travaille que sur un sous-ensemble de données à chaque division, tandis que [**`KMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) travaille toujours sur l'ensemble du jeu de données.\n",
    "\n",
    "Bien que le [**`BisectingKMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.BisectingKMeans.html) ne puisse pas bénéficier des avantages de l'initialisation `\"k-means++\"` par conception, il produira néanmoins des résultats comparables à `KMeans(init=\"k-means++\")` en termes d'inertie, tout en étant plus économe en coût de calcul, et il produira probablement de meilleurs résultats que `KMeans` avec une initialisation aléatoire.\n",
    "\n",
    "Cette variante est plus efficace pour le regroupement agglomératif si le nombre de grappes est marginal par rapport au nombre de points de données.\n",
    "\n",
    "Cette variante ne produit pas non plus de grappes vides.\n",
    "\n",
    "**Il existe deux stratégies pour sélectionner la grappe à diviser :**\n",
    "* `bisecting_strategy=\"largest_cluster\"` sélectionne la grappe ayant le plus de points\n",
    "* `bisecting_strategy=\"biggest_inertia\"` sélectionne la grappe avec la plus grande inertie (c'est-à-dire la grappe avec la plus grande somme des carrés d'erreurs)\n",
    "\n",
    "La sélection en fonction du plus grand nombre de points de données produit généralement des résultats aussi précis que la sélection basée sur l'inertie, et elle est plus rapide (surtout pour un grand nombre de points de données, où le calcul de l'erreur peut être coûteux).\n",
    "\n",
    "La sélection basée sur le plus grand nombre de points de données est également susceptible de produire des clusters de tailles similaires, tandis que `KMeans` est connu pour générer des clusters de tailles différentes.\n",
    "\n",
    "La différence entre Bisecting K-Means et Regular K-Means peut être vue sur l'exemple [**Comparaison des performances entre le Bisecting K-Means et le Regular K-Means **](https://scikit-learn.org/stable/auto_examples/cluster/plot_bisect_kmeans.html). Alors que l'algorithme K-Means régulier a tendance à créer des grappes non liées, les grappes de Bisecting K-Means sont bien ordonnées et créent une hiérarchie assez visible.\n",
    "\n",
    "#### Références\n",
    "\n",
    "🔬 Michael Steinbach, George Karypis and Vipin Kumar, [**“A Comparison of Document Clustering Techniques”**](http://www.philippe-fournier-viger.com/spmf/bisectingkmeans.pdf), Department of Computer Science and Egineering, University of Minnesota (June 2000)\n",
    "\n",
    "🔬 K.Abirami and Dr.P.Mayilvahanan, [**“Performance Analysis of K-Means and Bisecting K-Means Algorithms in Weblog Data”**](https://ijeter.everscience.org/Manuscripts/Volume-4/Issue-8/Vol-4-issue-8-M-23.pdf), International Journal of Emerging Technologies in Engineering Research (IJETER) Volume 4, Issue 8, (August 2016)\n",
    "\n",
    "🔬 Jian Di, Xinyue Gou, [**“Bisecting K-means Algorithm Based on K-valued Self-determining and Clustering Center Optimization”**](http://www.jcomputers.us/vol13/jcp1306-01.pdf), School of Control and Computer Engineering, North China Electric Power University, Baoding, Hebei, China (August 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='dbscan'></a> 2.3.7. **DBSCAN**<br/>([*DBSCAN*](https://scikit-learn.org/stable/modules/clustering.html#dbscan))\n",
    "\n",
    "L'algorithme [**`DBSCAN`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) considère les clusters comme des zones de haute densité séparées par des zones de faible densité. En raison de cette vue plutôt générique, les clusters trouvés par DBSCAN peuvent avoir n'importe quelle forme, contrairement à k-means qui suppose que les clusters sont de forme convexe. L'élément central du DBSCAN est le concept d'**échantillons centraux** (*core samples*), qui sont des échantillons qui se trouvent dans des zones à haute densité. Une grappe est donc un ensemble d'échantillons centraux, chacun proche l'un de l'autre (mesuré par une mesure de distance) et un ensemble d'échantillons non centraux qui sont proches d'un échantillon central (mais ne sont pas eux-mêmes des échantillons centraux). Il y a deux paramètres dans l'algorithme, `min_samples` et `eps`, qui définissent formellement ce que nous voulons dire quand nous disons *dense*. Des `min_samples` plus élevés ou des `eps` plus faibles indiquent une densité plus élevée nécessaire pour former un cluster.\n",
    "\n",
    "Plus formellement, nous définissons un échantillon central comme étant un échantillon dans l'ensemble de données tel qu'il existe `min_samples` d'autres échantillons à une distance de `eps`, qui sont définis comme voisins de l'échantillon central. Cela nous indique que l'échantillon central se trouve dans une zone dense de l'espace vectoriel. Un cluster est un ensemble d'échantillons centraux qui peuvent être construits en prenant de manière récursive un échantillon central, en trouvant tous ses voisins qui sont des échantillons centraux, en trouvant tous leurs voisins qui sont des échantillons centraux, etc. Un cluster a également un ensemble d'échantillons non centraux, qui sont des échantillons voisins d'un échantillon central dans le cluster mais qui ne sont pas eux-mêmes des échantillons centraux. Intuitivement, ces échantillons sont en marge d'un cluster.\n",
    "\n",
    "Tout échantillon central fait partie d'une grappe, par définition. Tout échantillon qui n'est pas un échantillon central et qui se trouve à une distance d'au moins `eps` de tout échantillon central est considéré comme une valeur aberrante par l'algorithme.\n",
    "\n",
    "Alors que le paramètre `min_samples` contrôle principalement la tolérance de l'algorithme au bruit (sur des ensembles de données bruyants et volumineux, il peut être souhaitable d'augmenter ce paramètre), le paramètre `eps` est *crucial pour choisir de manière appropriée*  l'ensemble de données et la fonction de distance et ne peut généralement pas être laissé à sa valeur par défaut. Il contrôle le voisinage local des points. Lorsque cette valeur est trop petite, la plupart des données ne seront pas du tout regroupées (et étiquetées à `-1` pour « bruit »). Lorsqu'elle est trop grande, celà provoque la fusion des clusters proches en un seul cluster, et finalement le retour de l'ensemble de données complet en un seul cluster. Certaines heuristiques pour choisir ce paramètre ont été discutées dans la littérature, par exemple basées sur un genou dans le tracé des distances du plus proche voisin (comme discuté dans les références ci-dessous).\n",
    "\n",
    "Dans la figure ci-dessous, la couleur indique l'appartenance au cluster, avec de grands cercles indiquant les échantillons centraux trouvés par l'algorithme. Les cercles plus petits sont des échantillons non centraux qui font toujours partie d'un cluster. De plus, les valeurs aberrantes sont indiquées par des points noirs ci-dessous.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_dbscan_002.png\"\n",
    "    alt=\"Estimated number of clusters: 3\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "### Exemple\n",
    "\n",
    "#### [**Démo de l'algorithme de regroupement DBSCAN**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_3_cluster/plot_dbscan.ipynb)<br/>([*Demo of DBSCAN clustering algorithm*](https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation\n",
    "\n",
    "L'algorithme DBSCAN est déterministe, générant toujours les mêmes grappes pour les mêmes données dans le même ordre. Ces résultats peuvent néanmoins différer si les données sont fournies dans un ordre différent. D'abord, même si les échantillons centraux seront toujours affectés aux mêmes grappes, les étiquettes de ces grappes dépendent de l'ordre dans lequel ces échantillons sont rencontrés dans les données. Ensuite, et plus important encore, les grappes auxquelles les échantillons non centraux sont affectés peuvent différer selon l'ordre des données. Cela se produit lorsqu'un échantillon non central a une distance inférieure à `eps` à deux échantillons centraux dans des grappes différentes. En raison de l'inégalité triangulaire, ces deux échantillons centraux doivent être plus éloignés que `eps` l'un de l'autre, sinon ils seraient dans la même grappe. L'échantillon non central est affecté à la première grappe générée lors d'un passage à travers les données, et les résultats dépendent donc de l'ordre des données.\n",
    "\n",
    "L'implémentation actuelle utilise des arbres à billes et des kd-arbres pour déterminer le voisinage des points, ce qui évite de calculer la matrice de distance complète (comme cela était fait dans les versions scikit-learn antérieures à 0.14). La possibilité d'utiliser des métriques personnalisées est conservée ; pour plus de détails, voir `NearestNeighbors`.\n",
    "\n",
    "### Consommation mémoire pour les échantillons de grande taille\n",
    "\n",
    "Par défaut, cette implémentation n'est pas particulièrement efficace en termes de mémoire, car elle construit une matrice de similarité complète en termes de paires lorsque les arbres kd ou les arbres de billes ne peuvent pas être utilisés (par exemple, avec des matrices creuses). Cette matrice consommera $n^2$ nombres flottants. Il existe cependant quelques mécanismes pour contourner ce problème :\n",
    "* Utiliser le clustering [**OPTICS** (2.3.8)](https://scikit-learn.org/stable/modules/clustering.html#optics) conjointement avec la méthode `extract_dbscan`. Le regroupement OPTICS calcule également la matrice de similarité complète en termes de paires, mais ne conserve qu'une seule ligne en mémoire à la fois (complexité mémoire de l'ordre de $n$).\n",
    "* Un graphe de voisinage à rayon creux (où les entrées manquantes sont supposées être en dehors `eps`) peut être précalculé d'une manière efficace en termes de mémoire et DBSCAN peut être exécuté dessus avec l'argument `metric='precomputed'`. Voir [**`sklearn.neighbors.NearestNeighbors.radius_neighbors_graph`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html).\n",
    "* L'ensemble de données peut être compressé, soit en supprimant les doublons exacts s'ils existent dans vos données, soit en utilisant l'algorithme BIRCH. Dans ce cas, vous disposez d'un nombre relativement restreint de représentants pour un grand nombre de points. Vous pouvez ensuite fournir un poids d'échantillon (`sample_weight`) lors de l'ajustement de DBSCAN.\n",
    "\n",
    "### Références\n",
    "\n",
    "🔬 Ester, M., H. P. Kriegel, J. Sander, and X. Xu, [**“A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise”**](https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf), In Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, OR, AAAI Press, pp. 226–231. 1996\n",
    "\n",
    "🔬 Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X., [**“DBSCAN revisited, revisited: why and how you should (still) use DBSCAN”**](https://www.ccs.neu.edu/home/vip/teach/DMcourse/2_cluster_EM_mixt/notes_slides/revisitofrevisitDBSCAN.pdf), In ACM Transactions on Database Systems (TODS), 42(3), 19, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='hdbscan'></a> 2.3.8. **HDBSCAN**<br/>([*HDBSCAN*](https://scikit-learn.org/stable/modules/clustering.html#hdbscan))\n",
    "\n",
    "L'algorithme [**`HDBSCAN`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.HDBSCAN.html) peut être considéré comme une extension de [**`DBSCAN`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) et [**`OPTICS`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.OPTICS.html). Plus précisément, [**`DBSCAN`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) suppose que le critère de regroupement (c'est-à-dire l'exigence de densité) est _globalement homogène_. En d'autres termes, [**`DBSCAN`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) peut avoir du mal à capturer avec succès des grappes de densités différentes. [**`HDBSCAN`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.HDBSCAN.html) atténue cette hypothèse et explore toutes les échelles de densité possibles en construisant une représentation alternative du problème de regroupement.\n",
    "\n",
    "**Note:** Cette implémentation est adaptée de l'implémentation originale de HDBSCAN, [scikit-learn-contrib/hdbscan](https://github.com/scikit-learn-contrib/hdbscan), basée sur [LJ2017]. \n",
    "\n",
    "### <a id='mutual-reachability-graph'></a> 2.3.8.1. **Graphe d'accessibilité mutuelle**<br/>([*Mutual Reachability Graph*](https://scikit-learn.org/stable/modules/clustering.html#mutual-reachability-graph))\n",
    "\n",
    "HDBSCAN définit d'abord $d_c(x_p)$, la _distance centrale_ d'un échantillon $x_p$, comme la distance jusqu'à son `min_samples`-ème voisin le plus proche (en se comptant lui-même). Par exemple, si `min_samples=5` et que $x_*$ est le 5ème voisin le plus proche de $x_p$, alors la distance centrale est :\n",
    "\n",
    "$$d_c(x_p) = d(x_p, x_*)$$\n",
    "\n",
    "Ensuite, il définit $d_m(x_p, x_q)$, la _distance d'accessibilité mutuelle_ de deux points $x_p, x_q$, comme :\n",
    "\n",
    "$$d_m(x_p, x_q) = \\max\\{d_c(x_p), d_c(x_q), d(x_p, x_q)\\}$$\n",
    "\n",
    "Ces deux notions nous permettent de construire le _graphe d'accessibilité mutuelle_ $G_{ms}$ défini pour un choix donné de `min_samples` en associant chaque échantillon $x_p$ à un sommet du graphe, et donc les arêtes entre les points $x_p, x_q$ sont leur distance d'accessibilité mutuelle $d_m(x_p, x_q)$. Nous pouvons construire des sous-ensembles de ce graphe, notés $G_{ms,\\varepsilon}$, en supprimant toutes les arêtes dont la valeur est supérieure à $\\varepsilon$. À ce stade, tous les points dont la distance centrale est inférieure à $\\varepsilon$ sont marqués comme du bruit. Les points restants sont ensuite regroupés en trouvant les composantes connexes de ce graphe réduit.\n",
    "\n",
    "> **Note:** Prendre les composantes connexes d'un graphe réduit $G_{ms,\\varepsilon}$ équivaut à exécuter DBSCAN* avec `min_samples` et $\\varepsilon$. DBSCAN* est une version légèrement modifiée de DBSCAN mentionnée dans [CM2013].\n",
    "\n",
    "\n",
    "### <a id='hierarchical-clustering'></a> 2.3.8.2. **Regroupement hiérarchique**<br/>([*Hierarchical Clustering*](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering))\n",
    "\n",
    "HDBSCAN peut être considéré comme un algorithme qui effectue le regroupement DBSCAN* pour toutes les valeurs de $\\varepsilon$. Comme mentionné précédemment, cela équivaut à trouver les composantes connexes des graphes d'accessibilité mutuelle pour toutes les valeurs de $\\varepsilon$. Pour ce faire de manière efficace, HDBSCAN extrait d'abord un arbre de recouvrement minimal (MST, _Minimum Spanning Tree_) du graphe d'accessibilité mutuelle totalement connexe, puis coupe de manière gloutonne les arêtes ayant le poids le plus élevé. Un aperçu de l'algorithme HDBSCAN est le suivant :\n",
    "\n",
    "1. Extraire le MST de $G_{ms}$\n",
    "2. Étendre le MST en ajoutant une « arête propre » pour chaque sommet, avec un poids égal à la distance centrale de l'échantillon sous-jacent.\n",
    "3. Initialiser une seule grappe et une étiquette pour le MST.\n",
    "4. Supprimer l'arête ayant le poids le plus élevé du MST (les égalités sont supprimées simultanément).\n",
    "5. Attribuer des étiquettes de grappe aux composantes connexes contenant les extrémités de l'arête maintenant supprimée. Si la composante n'a pas au moins une arête, elle reçoit à la place une étiquette \"nulle\" la marquant comme du bruit.\n",
    "6. Répéter 4-5 jusqu'à ce qu'il n'y ait plus de composantes connectées.\n",
    "\n",
    "HDBSCAN est donc capable d'obtenir toutes les partitions possibles réalisables par DBSCAN* pour un choix fixe de `min_samples` de manière hiérarchique. En effet, cela permet à HDBSCAN d'effectuer le regroupement sur de multiples densités et, en tant que tel, il n'a plus besoin de $\\varepsilon$ comme hyperparamètre. Il dépend uniquement du choix de `min_samples`, qui tend à être un hyperparamètre plus robuste.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_hdbscan_005.png\"\n",
    "    alt=\"True number of clusters: 4\"\n",
    "    style=\"max-width: 75%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_hdbscan_007.png\"\n",
    "    alt=\"Estimated number of clusters: 4\"\n",
    "    style=\"max-width: 75%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "HDBSCAN peut être adouci avec un hyperparamètre supplémentaire `min_cluster_size`, qui spécifie que pendant le regroupement hiérarchique, les composantes avec moins de `minimum_cluster_size` échantillons soient considérées comme du bruit. En pratique, on peut définir `minimum_cluster_size = min_samples` pour coupler les paramètres et simplifier l'espace des hyperparamètres.\n",
    "\n",
    "#### References\n",
    "\n",
    "Z 🔬 [CM2013]Campello, R.J.G.B., Moulavi, D., Sander, J. (2013). [“**Density-Based Clustering Based on Hierarchical Density Estimates”**](https://link.springer.com/chapter/10.1007/978-3-642-37456-2_14). In: Pei, J., Tseng, V.S., Cao, L., Motoda, H., Xu, G. (eds) Advances in Knowledge Discovery and Data Mining. PAKDD 2013. Lecture Notes in Computer Science(), vol 7819. Springer, Berlin, Heidelberg. \n",
    "\n",
    "Z 🔬 [LJ2017] L. McInnes and J. Healy, (2017). [“**Accelerated Hierarchical Density Based Clustering”**](https://arxiv.org/pdf/1705.07321.pdf). In: IEEE International Conference on Data Mining Workshops (ICDMW), 2017, pp. 33-42. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='optics'></a> 2.3.9. **OPTICS**<br/>([*OPTICS*](https://scikit-learn.org/stable/modules/clustering.html#optics))\n",
    "\n",
    "L'algorithme [**`OPTICS`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.OPTICS.html) partage de nombreux points communs avec l'algorithme [**`DBSCAN`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) et peut être considéré comme une généralisation de celui-ci qui assouplit l'exigence d'un `eps` d'une valeur unique à une plage de valeurs. La principale différence entre DBSCAN et OPTICS est que l'algorithme OPTICS construit un graphe d'_accessibilité_, qui attribue à chaque échantillon à la fois une distance de `reachability_` et une position dans l'attribut `ordering_` (_ordonnancement_) de la grappe ; ces deux attributs sont attribués lors de l'ajustement du modèle et sont utilisés pour déterminer l'appartenance à la grappe. Si OPTICS est exécuté avec `max_eps` défini à la valeur par défaut `inf`, l'extraction de grappe de type DBSCAN peut être effectuée de manière répétée en temps linéaire pour toute valeur donnée de `eps` en utilisant la méthode `cluster_optics_dbscan`. Définir `max_eps` sur une valeur inférieure entraînera des temps d'exécution plus courts et peut être considéré comme le rayon de voisinage maximal de chaque point pour trouver d'autres points potentiellement accessibles.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_optics_001.png\"\n",
    "    alt=\"Reachability Plot, Automatic Clustering OPTICS\"\n",
    "    style=\"max-width: 75%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Les distances d'_accessibilité_ générées par OPTICS permettent une extraction à densité variable de grappes pour un ensemble de données _donné_. Comme le montre le graphique ci-dessus, la combinaison des distances d'*accessibilité* et de l'`ordering_` (ordonnancement) des ensembles de données produit un _graphique d'accessibilité_, où la densité de points est représentée sur l'axe Y, et où les points sont ordonnés de telle sorte que les points proches soient adjacents. \"Couper\" le graphique d'accessibilité à une seule valeur produit des résultats de type DBSCAN ; tous les points au-dessus de la «coupe» sont classés comme du bruit, et chaque interruption lors de la lecture de gauche à droite correspond à une nouvelle grappe. L'extraction de grappe par défaut avec OPTICS examine les pentes raides dans le graphe pour trouver des grappes, et l'utilisateur peut contrôler la raideur de pente avec le paramètre `xi`. Il existe d'autres possibilités d'analyse sur le graphe lui-même, telles que la génération de représentations hiérarchiques des données via des dendrogrammes d'accessibilité, et la hiérarchie des grappes détectées par l'algorithme est accessible via le paramètre `cluster_hierarchy_`. Le tracé ci-dessus a été codé par couleur afin que les couleurs des grappes dans l'espace plan correspondent aux grappes de segments linéaires du graphique d'accessibilité. Notez que les grappes bleu et rouge sont adjacentes dans le graphique d'accessibilité et peuvent être représentés hiérarchiquement comme des enfants d'une grappe parente plus large.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Démo de l'algorithme de regroupement OPTICS**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_3_cluster/plot_optics.ipynb)<br/>([*Demo of OPTICS clustering algorithm*](https://scikit-learn.org/stable/auto_examples/cluster/plot_optics.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison avec DBSCAN\n",
    "\n",
    "Les résultats de la méthode OPTICS `cluster_optics_dbscan` et de DBSCAN sont très similaires, mais pas toujours identiques ; plus précisément, l'étiquetage de la périphérie et des points de bruit. Cela s'explique en partie par le fait que les premiers échantillons de chaque zone dense traitée par OPTICS ont une grande valeur d'accessibilité tout en étant proches d'autres points de leur zone, et seront donc parfois marqués comme \"bruit\" plutôt que comme \"périphérie\". Cela affecte les points adjacents lorsqu'ils sont considérés comme des candidats pour être marqués comme \"périphérie\" ou \"bruit\".\n",
    "\n",
    "Notez que pour toute valeur unique d'`eps`, DBSCAN aura tendance à avoir un temps d'exécution plus court que OPTICS ; cependant, pour des exécutions répétées à différentes valeurs `eps`, une seule exécution d'OPTICS peut nécessiter moins de temps d'exécution cumulé que DBSCAN. Il est également important de noter que la sortie d'OPTICS est proche de celle de DBSCAN uniquement si `eps` et `max_eps` sont proches.\n",
    "\n",
    "### Complexité algorithmique\n",
    "\n",
    "Les arbres d'indexation spatiale sont utilisés pour éviter de calculer la matrice de distance complète et permettent une utilisation efficace de la mémoire sur de grands ensembles d'échantillons. Différentes métriques de distance peuvent être fournies via le mot-clé `metric`.\n",
    "\n",
    "Pour les grands ensembles de données, des résultats similaires (mais pas identiques) peuvent être obtenus via [**`HDBSCAN`**](https://hdbscan.readthedocs.io/en/latest/). L'implémentation HDBSCAN est multithreadée et a une meilleure complexité algorithmique qu'OPTICS, au prix d'une plus mauvaise mise à l'échelle de la mémoire. Pour les ensembles de données particulièrement volumineux qui épuisent la mémoire du système à l'aide de HDBSCAN, OPTICS conservera une complexité spatiale (mémoire) de d'ordre $n$ (contre $n^2$) ; cependant, le réglage du paramètre `max_eps` devra probablement être utilisé pour permettre de produire une solution dans un délai raisonnable.\n",
    "\n",
    "### Références:\n",
    "\n",
    "🔬 Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel et Jörg Sander., [**“OPTICS: Ordering Points To Identify the Clustering Structure”**](https://www.dbs.ifi.lmu.de/Publikationen/Papers/OPTICS.pdf), In ACM Sigmod Record, vol. 28, non. 2, p. 49-60. AMC, 1999."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='birch'></a> 2.3.10. **BIRCH**<br/>([*BIRCH*](https://scikit-learn.org/stable/modules/clustering.html#birch))\n",
    "\n",
    "L'algorithme [**`Birch`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.Birch.html) construit un arbre appelé _Arbre de Caractéristiques de Regroupement_ (CFT - _Clustering Feature Tree_) pour les données fournies. Les données sont essentiellement compressées avec perte pour former un ensemble de nœuds de caractéristiques de regroupement (nœuds CF - _Clustering Feature nodes_). Les nœuds CF ont un certain nombre de sous-grappes appelées _sous-grappes de caractéristiques de regroupement_ (_sous-grappes CF_) et ces sous-grappes CF situées dans les nœuds CF non terminaux peuvent avoir des nœuds CF comme enfants.\n",
    "\n",
    "Les sous-grappes CF contiennent les informations nécessaires au regroupement, évitant ainsi de stocker l'intégralité des données d'entrée en mémoire. Ces informations comprennent :\n",
    "* Le nombre d'échantillons dans une sous-grappe.\n",
    "* La somme linéaire - un vecteur à n dimensions contenant la somme de tous les échantillons\n",
    "* La somme au carré - la somme de la norme $\\ell_2$ au carré de tous les échantillons.\n",
    "* Les centroïdes - Pour éviter de recalculer la somme linéaire / n_samples.\n",
    "* La norme au carré des centroïdes.\n",
    "\n",
    "L'algorithme BIRCH comporte deux paramètres, le seuil et le facteur de ramification. Le facteur de ramification limite le nombre de sous-grappes dans un nœud, et le seuil limite la distance entre l'échantillon entrant et les sous-grappes existantes.\n",
    "\n",
    "Cet algorithme peut être envisagé comme une méthode de réduction d'instance ou de données, car il réduit les données d'entrée à un ensemble de sous-grappes obtenues directement à partir des feuilles de l'arbre CFT. Ces données réduites peuvent ensuite être traitées en les soumettant à une grappe globale. Cette grappe globale peut être défini par `n_clusters`. Si `n_clusters` est défini sur `None`, les sous-grappes des feuilles sont directement extraites. Sinon, une étape de regroupement global étiquette ces sous-grappes en grappes globales (étiquettes) et les échantillons sont associés à l'étiquette globale de la sous-grappe la plus proche.\n",
    "\n",
    "### Description de l'algorithme\n",
    "\n",
    "* Un nouvel échantillon est inséré dans la racine de l'arborescence CF qui est un nœud CF. Il est ensuite fusionné avec la sous-grappe de la racine possédant le plus petit rayon après fusion, tout en respectant les conditions de seuil et de facteur de ramification. Si la sous-grappe a un nœud enfant, le processus est répété jusqu'à ce qu'il atteigne une feuille. Une fois la sous-grappe la plus proche de la feuille identifiée, les propriétés de cette sous-grappe ainsi que celle des sous-grappes parentes sont mises à jour de manière récursive.\n",
    "* Si le rayon de la sous-grappe obtenue en fusionnant le nouvel échantillon et la sous-grappe la plus proche est supérieur au carré du seuil, et que le nombre de sous-grappes est supérieur au facteur de ramification, alors un espace est temporairement alloué à ce nouvel échantillon. Les deux sous-grappes les plus éloignées sont sélectionnées et les sous-grappes sont divisés en deux grappes sur la base de la distance entre ces sous-grappes.\n",
    "* Si ce nœud divisé possède une sous-grappe parente et qu'il y a de la place pour une nouvelle sous-grappe, alors le parent est divisé en deux. S'il aucun espace n'est plus disponible, ce nœud est à nouveau divisé en deux et le processus se poursuit de manière récursive, jusqu'à ce qu'il atteigne la racine.\n",
    "\n",
    "### BIRCH ou MiniBatchKMeans ?\n",
    "\n",
    "* BIRCH ne s'adapte pas très bien aux données de grande dimension. En règle générale, si `n_features` est supérieur à vingt, il est généralement préférable d'utiliser `MiniBatchKMeans`.\n",
    "* Si le nombre d'instances de données doit être réduit, ou si l'on souhaite obtenir un grand nombre de sous-grappes, que ce soit comme étape de prétraitement ou autrement, BIRCH est plus approprié que `MiniBatchKMeans`.\n",
    "\n",
    "### Comment utiliser partial_fit ?\n",
    "\n",
    "Pour éviter le calcul du regroupement global, à chaque appel de `partial_fit`, il est conseillé de :\n",
    "1. Définir initialement `n_clusters=None`\n",
    "2. Entraîner toutes les données par plusieurs appels à `partial_fit`.\n",
    "3. Définir `n_clusters` sur une valeur requise à l'aide de `brc.set_params(n_clusters=n_clusters)`.\n",
    "4. Enfin, appeler `partial_fit` sans arguments, c'est-à-dire `brc.partial_fit()`, ce qui effectuera le regroupement global.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_birch_vs_minibatchkmeans_001.png\"\n",
    "    alt=\"BIRCH without global clustering, BIRCH with global clustering, MiniBatchKMeans\"\n",
    "    style=\"max-width: 100%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Comparaison entre BIRCH et MiniBatchKMeans**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_3_cluster/plot_birch_vs_minibatchkmeans.ipynb)<br/>([*Comparez BIRCH et MiniBatchKMeans*](https://scikit-learn.org/stable/auto_examples/cluster/plot_birch_vs_minibatchkmeans.html))\n",
    "\n",
    "### Références\n",
    "\n",
    "🔬 Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel et Jörg Sander., [**“OPTICS: Ordering Points To Identify the Clustering Structure”**](https://www.dbs.ifi.lmu.de/Publikationen/Papers/OPTICS.pdf), In ACM Sigmod Record, vol. 28, non. 2, p. 49-60. AMC, 1999.\n",
    "\n",
    "Roberto Perdisci JBirch - [**Réalisation Java de l'algorithme de regroupement BIRCH**](https://code.google.com/archive/p/jbirch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='clustering-performance-evaluation'></a> 2.3.11. **Évaluation des performances du regroupement**<br/>([*Clustering performance evaluation*](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation))\n",
    "\n",
    "Évaluer les performances d'un algorithme de regroupement n'est pas aussi simple que compter le nombre d'erreurs ou la précision et le rappel d'un algorithme de classification supervisée. En particulier, une bonne métrique d'évaluation ne devrait pas tenir compte des valeurs absolues des étiquettes de grappe, mais plutôt si ce regroupement définit des séparations des données similaires à un ensemble de classes de vérité terrain ou satisfaisant à une hypothèse telle que les membres appartenant à la même classe sont plus similaires que les membres de classes différentes selon une métrique de similarité.\n",
    "\n",
    "### <a id='rand-index'></a> 2.3.11.1. **Indice de Rand**<br/>([*Rand index*](https://scikit-learn.org/stable/modules/clustering.html#rand-index))\n",
    "\n",
    "Étant données les affectations de classe de vérité terrain (`labels_true`) et celles ( `labels_pred`) produites par l'algorithme de regroupement, l'**indice Rand (ajusté ou non)** est une fonction qui mesure la **similarité** entre les deux affectations, en ignorant les permutations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "labels_true = [0, 0, 0, 1, 1, 1]\n",
    "labels_pred = [0, 0, 1, 1, 2, 2]\n",
    "metrics.rand_score(labels_true, labels_pred)\n",
    "# 0.66..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'indice de Rand ne garantit pas une valeur proche de 0,0 pour un étiquetage aléatoire. En revanche, l'indice de Rand ajusté **corrige le hasard** et fournit une telle valeur de référence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24242424242424243"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.adjusted_rand_score(labels_true, labels_pred)\n",
    "# 0.24..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme pour toutes les métriques de regroupement, on peut permuter 0 et 1 dans les étiquettes prédites, renommer 2 en 3 et obtenir pourtant le même score :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24242424242424243"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_pred = [1, 1, 0, 0, 3, 3]\n",
    "metrics.rand_score(labels_true, labels_pred)\n",
    "# 0.66...\n",
    "metrics.adjusted_rand_score(labels_true, labels_pred)\n",
    "# 0.24..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De plus, [**`rand_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.rand_score.html) et [**`ajusted_rand_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html) sont tous deux **symétriques** : échanger les arguments ne modifie pas les scores. Ils peuvent donc être utilisés en tant que **mesures de consensus** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24242424242424243"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.rand_score(labels_pred, labels_true)\n",
    "# 0.66...\n",
    "metrics.adjusted_rand_score(labels_pred, labels_true)\n",
    "# 0.24..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un étiquetage parfait obtient un score de 1,0 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_pred = labels_true[:]\n",
    "metrics.rand_score(labels_true, labels_pred)\n",
    "# 1.0\n",
    "metrics.adjusted_rand_score(labels_true, labels_pred)\n",
    "# 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Des étiquettes peu concordantes (par exemple, des étiquettes indépendantes) ont des scores inférieurs, et pour l'indice de Rand ajusté, le score sera négatif ou proche de zéro. Cependant, pour l'indice de Rand non ajusté, le score, bien qu'inférieur, ne sera pas nécessairement proche de zéro :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.07207207207207207"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_true = [0, 0, 0, 0, 0, 0, 1, 1]\n",
    "labels_pred = [0, 1, 2, 3, 4, 5, 5, 6]\n",
    "metrics.rand_score(labels_true, labels_pred)\n",
    "# 0.39...\n",
    "metrics.adjusted_rand_score(labels_true, labels_pred)\n",
    "# -0.07..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avantages\n",
    "\n",
    "* **Interprétabilité** : l'indice de Rand non ajusté est proportionnel au nombre de paires d'échantillons dont les étiquettes sont identiques dans `labels_pred` et `labels_true`, ou ayant des étiquettes différentes dans les deux.\n",
    "* **Les attributions d'étiquettes aléatoires (uniformes) ont un score d'indice de Rand ajusté proche de 0,0** pour toutes les valeurs de `n_clusters` et `n_samples` (ce qui n'est pas le cas pour l'indice de Rand non ajusté ou la V-mesure, par exemple).\n",
    "* **Plage bornée** : les valeurs plus basses indiquent des étiquetages différents, l regroupements similaires ont un indice de Rand élevé (ajusté ou non ajusté), 1,0 est le score de correspondance parfaite. La plage de score est [0, 1] pour l'indice de Rand non ajusté et [-1, 1] pour l'indice de Rand ajusté.\n",
    "* **Aucune hypothèse n'est faite sur la structure de la grappe** : l'indice de Rand (ajusté ou non ajusté) peut être utilisé pour comparer toutes sortes d'algorithmes de regroupement, et peut être utilisé pour comparer des algorithmes de regroupement tels que k-moyennes qui suppose des formes isotropes en forme de tâches, avec les résultats d'algorithmes de regroupement spectral qui peuvent trouver des grappes avec des formes \"pliées\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inconvénients\n",
    "\n",
    "* Contrairement à l'inertie, l'**indice de Rand (ajusté ou non) nécessite une connaissance des classes de vérité terrain**, ce qui n'est presque jamais disponible en pratique ou nécessite une attribution manuelle par des annotateurs humains (comme dans le cadre de l'apprentissage supervisé).\n",
    "\n",
    "Cependant, l'indice Rand (ajusté ou non ajusté) peut également être utile dans un cadre purement non supervisé en tant que composant d'un indice de consensus pouvant qui peut être utilisé pour la sélection de modèles de regroupement.\n",
    "\n",
    "* L'**indice Rand non ajusté est souvent proche de 1,0** même si les regroupements eux-mêmes diffèrent significativement. Cela peut être compris en interprétant l'indice de Rand comme la précision de l'étiquetage des paires d'éléments résultant des regroupements : en pratique, il existe souvent une majorité de paires d'éléments auxquelles des étiquettes de paires différentes sont attribuées à la fois dans le regroupement prédit et dans la vérité terrain, ce qui conduit à une forte proportion d'étiquettes de paires concordantes, ce qui entraîne par la suite un score élevé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**Ajustement pour le hasard dans l'évaluation des performances du regroupement**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_3_cluster/plot_adjusted_for_chance_measures.ipynb)<br/>([*Adjustment for chance in clustering performance evaluation*](https://scikit-learn.org/stable/auto_examples/cluster/plot_adjusted_for_chance_measures.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulation mathématique\n",
    "\n",
    "Si $C$ est une affectation de classe de vérité terrain et $K$ le regroupement, définissons $a$ et $b$ comme:\n",
    "* $a$, le nombre de paires d'éléments qui sont dans le même ensemble dans $C$ et dans le même ensemble dans $K$\n",
    "* $b$, le nombre de paires d'éléments qui sont dans des ensembles différents dans $C$ et dans des ensembles différents dans $K$\n",
    "\n",
    "L'indice de Rand non ajusté est alors donné par :\n",
    "\n",
    "$$\\text{RI} = \\frac{a + b}{C_2^{n_{samples}}}$$\n",
    "\n",
    "où $C_2^{n_{samples}}$ est le nombre total de paires possibles dans le jeu de données. Peu importe que le calcul soit effectué sur des paires ordonnées ou non ordonnées, pourvu que le calcul soit cohérent.\n",
    "\n",
    "Cependant, l'indice de Rand ne garantit pas qu'un étiquetage aléatoires obtiendra une valeur proche de zéro (en particulier si le nombre de grappes est du même ordre de grandeur que le nombre d'échantillons).\n",
    "\n",
    "Pour contrer cet effet, nous pouvons réduire l'indice de Rand attendu $E[\\text{RI}]$ des étiquetages aléatoires en définissant l'indice de Rand ajusté comme suit :\n",
    "\n",
    "$$\\text{ARI} = \\frac{\\text{RI} - E[\\text{RI}]}{\\max(\\text{RI}) - E[\\text{RI}]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Références\n",
    "\n",
    "Z 🔬 L. Hubert and P. Arabie, [**“Comparing Partitions”**](https://link.springer.com/article/10.1007/BF01908075), Journal of Classification, 1985\n",
    "\n",
    "Z 🔬 D. Steinley, [“**Properties of the Hubert-Arabie Adjusted Rand Index”**](https://psycnet.apa.org/record/2004-17801-007), Psychological Methods, 2004[.](https://drive.google.com/file/d/14PJfGnsbax-DtX_guyQw6UDeBbn-O458/view?usp=share_link)\n",
    "\n",
    "[Wikipedia entry for the **Rand index**](https://en.wikipedia.org/wiki/Rand_index)\n",
    "\n",
    "[Wikipedia entry for the **adjusted Rand index**](https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='mutual-information-based-scores'></a> 2.3.11.2. **Scores basés sur l'information mutuelle**<br/>([*Mutual Information based scores*](https://scikit-learn.org/stable/modules/clustering.html#mutual-information-based-scores))\n",
    "\n",
    "Compte tenu de la connaissance des affectations `labels_true` de classe de vérité terrain, et des affectations `labels_pred` de notre algorithme de regroupement pour ces mêmes échantillons, l'**information mutuelle** est une fonction qui mesure l'**accord** entre les deux affectations, abstraction faite des permutations. Deux versions normalisées de cette mesure sont disponibles, l'**information mutuelle normalisée (NMI)** et l'**information mutuelle ajustée (AMI)**. NMI est souvent utilisée dans la littérature, tandis que AMI a été proposée plus récemment comme mesure **normalisée par rapport au hasard** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2987924581708901"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "labels_true = [0, 0, 0, 1, 1, 1]\n",
    "labels_pred = [0, 0, 1, 1, 2, 2]\n",
    "\n",
    "metrics.adjusted_mutual_info_score(labels_true, labels_pred)\n",
    "# 0.22504..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut permuter 0 et 1 dans les étiquettes prédites, renommer 2 en 3 et obtenir le même score :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2987924581708901"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_pred = [1, 1, 0, 0, 3, 3]\n",
    "metrics.adjusted_mutual_info_score(labels_true, labels_pred)  \n",
    "# 0.22504..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fonctions [**`mutual_info_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mutual_info_score.html), [**`adjusted_mutual_info_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html) et [**`normalized_mutual_info_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html) sont toutes symétriques : échanger les arguments est sans effet sur le score. Ainsi, elles peuvent être utilisées comme **mesures de consensus** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2987924581708903"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.adjusted_mutual_info_score(labels_pred, labels_true)  \n",
    "# 0.22504..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un étiquetage parfait est noté 1,0 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_pred = labels_true[:]\n",
    "metrics.adjusted_mutual_info_score(labels_true, labels_pred)  \n",
    "# 1.0\n",
    "\n",
    "metrics.normalized_mutual_info_score(labels_true, labels_pred)  \n",
    "# 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela n'est pas vrai pour `mutual_info_score`, qui est donc plus difficile à apprécier :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599452"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.mutual_info_score(labels_true, labels_pred)  \n",
    "# 0.69..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les affectations erronées (par exemple, les étiquetages indépendants) ont des scores non positifs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.16666666666666655"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_true = [0, 1, 2, 0, 3, 4, 5, 1]\n",
    "labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\n",
    "metrics.adjusted_mutual_info_score(labels_true, labels_pred)  \n",
    "# -0.10526..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avantages\n",
    "\n",
    "* **Les affectations d'étiquettes aléatoires (uniformes) ont un score AMI proche de 0,0** pour toute valeur de `n_clusters` et `n_samples` (ce qui n'est pas le cas pour l'information mutuelle brute ou la V-mesure par exemple).\n",
    "\n",
    "* **La limite supérieure de 1** : les valeurs proches de zéro indiquent deux affectations d'étiquettes largement indépendantes, tandis que les valeurs proches de un indiquent un accord significatif. De plus, un AMI d'exactement 1 indique que les deux affectations d'étiquettes sont égales (avec ou sans permutation).\n",
    "\n",
    "### Inconvénients\n",
    "\n",
    "* Contrairement à l'inertie, **les mesures basées sur l'information mutuelle nécessitent la connaissance des classes de vérité terrain** alors qu'elles sont rarement disponibles dans la pratique, ou nécessitent une affectation manuelle par des annotateurs humains (comme dans le cadre de l'apprentissage supervisé).<br/>Cependant, les mesures basées sur l'IM peuvent également être utiles dans un cadre purement non supervisé en tant que composante d'un indice de consensus qui peut être utilisé pour la sélection de modèles de regroupement.\n",
    "* NMI et MI ne sont pas ajustés par rapport au hasard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**Ajustement pour le hasard dans l'évaluation des performances de regroupement**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_3_cluster/plot_adjusted_for_chance_measures.ipynb)<br/>([*Adjustment for chance in clustering performance evaluation*](https://scikit-learn.org/stable/auto_examples/cluster/plot_adjusted_for_chance_measures.html))\n",
    "\n",
    "Analyse de l'impact de la taille de l'ensemble de données sur la valeur des mesures de regroupement pour des affectations aléatoires. Cet exemple inclut également l'indice de Rand ajusté."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulation mathématique\n",
    "\n",
    "Supposons deux affectations d'étiquettes (des mêmes $N$ objets), $U$ et $V$. Leur entropie est la quantité d'incertitude pour un ensemble de partitions, définie par :\n",
    "\n",
    "$$H(U) = - \\sum_{i=1}^{|U|}P(i)\\log(P(i))$$\n",
    "\n",
    "où $P(i) = |U_i| / N$ est la probabilité qu'un objet choisi au hasard dans $U$ tombe dans la classe $U_i$. De même pour $V$ :\n",
    "\n",
    "$$H(V) = - \\sum_{j=1}^{|V|}P'(j)\\log(P'(j))$$\n",
    "\n",
    "Avec $P'(j) = |V_j| / N$. L'information mutuelle (MI) entre $U$ et $V$ est calculée par :\n",
    "\n",
    "$$\\text{MI}(U, V) = \\sum_{i=1}^{|U|}\\sum_{j=1}^{|V|}P(i, j)\\log\\left(\\frac{P(i,j)}{P(i)P'(j)}\\right)$$\n",
    "\n",
    "où $P(i, j) = |U_i \\cap V_j| / N$ est la probabilité qu'un objet choisi au hasard tombe dans les deux classes $U_i$ et $V_j$\n",
    "\n",
    "On peut également l'exprimer sous la forme de la cardinalité des ensembles :\n",
    "\n",
    "$$\\text{MI}(U, V) = \\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i \\cap V_j|}{N}\\log\\left(\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}\\right)$$\n",
    "\n",
    "L'information mutuelle normalisée est définie comme :\n",
    "\n",
    "$$\\text{NMI}(U, V) = \\frac{\\text{MI}(U, V)}{\\text{mean}(H(U), H(V))}$$\n",
    "\n",
    "Cette valeur de l'information mutuelle ainsi que de sa variante normalisée ne sont pas ajustées par rapport au hasard et auront tendance à augmenter lorsque le nombre d'étiquettes différentes (grappes) augmente, indépendamment de la quantité réelle «d'informations mutuelles» entre les attributions d'étiquettes.\n",
    "\n",
    "La valeur attendue de l'information mutuelle peut être calculée à l'aide de l'équation suivante [VEB2009]. Dans cette équation, $a_i = |U_i|$ (le nombre d'éléments dans $U_i$) et $b_j = |V_j|$ (le nombre d'éléments dans $V_j$).\n",
    "\n",
    "$$E[\\text{MI}(U,V)]=\\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\sum_{n_{ij}=(a_i+b_j-N)^+\n",
    "}^{\\min(a_i, b_j)} \\frac{n_{ij}}{N}\\log \\left( \\frac{ N.n_{ij}}{a_i b_j}\\right)\n",
    "\\frac{a_i!b_j!(N-a_i)!(N-b_j)!}{N!n_{ij}!(a_i-n_{ij})!(b_j-n_{ij})!\n",
    "(N-a_i-b_j+n_{ij})!}$$\n",
    "\n",
    "A partir de la valeur attendue, l'information mutuelle ajustée peut alors être calculée selon une forme similaire à celle de l'indice Rand ajusté :\n",
    "\n",
    "$$\\text{AMI} = \\frac{\\text{MI} - E[\\text{MI}]}{\\text{mean}(H(U), H(V)) - E[\\text{MI}]}$$\n",
    "\n",
    "Pour l'information mutuelle normalisée et l'information mutuelle ajustée, la valeur de normalisation est généralement une moyenne généralisée des entropies de chaque regroupement. Divers moyennes généralisées existent, et aucune règle stricte n'en fait prévaloir une sur les autres. Le choix dépend largement du domaine ; par exemple, dans la détection de communautés, la moyenne arithmétique est la plus couramment utilisée. Chaque méthode de normalisation offre des « comportements qualitativement similaires » [YAT2016]. Dans notre implémentation, cela est contrôlé par le paramètre `average_method`.\n",
    "\n",
    "Vinh et al. (2010) ont nommé les variantes de NMI et AMI par leur méthode de moyenne [VEB2010]. Leurs moyennes « sqrt » et « sum » correspondent aux moyennes géométrique et arithmétique ; nous utilisons ces noms plus communément admis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Références\n",
    "\n",
    "🔬 Strehl, Alexander, and Joydeep Ghosh (2002). [**“Cluster ensembles – A Knowledge Reuse Framework for Combining Multiple Partitions”**](https://dl.acm.org/doi/pdf/10.1162/153244303321897735). Journal of Machine Learning Research 3: 583–617.\n",
    "\n",
    "[Wikipedia entry for the **(normalized) Mutual Information**](https://en.wikipedia.org/wiki/Mutual_information)\n",
    "\n",
    "[Wikipedia entry for the **Adjusted Mutual Information**](https://en.wikipedia.org/wiki/Adjusted_mutual_information)\n",
    "\n",
    "🔬 [VEB2009] Vinh, Epps, and Bailey, (2009). [**“Information Theoretic Measures for Clusterings Comparison: Is a Correction for Chance Necessary?”**](https://dl.acm.org/doi/10.1145/1553374.1553511). Proceedings of the 26th Annual International Conference on Machine Learning - ICML ‘09.\n",
    "\n",
    "🔬 [VEB2010] Vinh, Epps, and Bailey, (2010). [**“Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance”**](https://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf). JMLR\n",
    "\n",
    "🔬 [YAT2016] Yang, Algesheimer, and Tessone, (2016). [**“A Comparative Analysis of Community Detection Algorithms on Artificial Networks”**](https://zora.uzh.ch/id/eprint/127494/1/srep30750.pdf). Scientific Reports 6: 30750."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='homogeneity-completeness-and-v-measure'></a> 2.3.11.3. **Homogénéité, exhaustivité et mesure V**<br/>([*Homogeneity, completeness and V-measure*](https://scikit-learn.org/stable/modules/clustering.html#homogeneity-completeness-and-v-measure))\n",
    "\n",
    "Avec la connaissance des affectations de classe de vérité terrain des échantillons, il est possible de définir des métriques intuitives en utilisant l'analyse d'entropie conditionnelle.\n",
    "\n",
    "En particulier, Rosenberg et Hirschberg (2007) définissent les deux objectifs souhaitables suivants pour toute affectation de grappes :\n",
    "* **Homogénéité** : chaque grappe ne contient que des membres d'une seule classe.\n",
    "* **Exhaustivité** : tous les membres d'une classe donnée sont affectés au même cluster.\n",
    "\n",
    "Ces concepts peuvent être transformés en scores [**`homogeneity_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html) et [**`completeness_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.completeness_score.html). Les deux scores sont bornés entre 0,0 et 1,0 (plus le score est élevé, mieux c'est) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.420619835714305"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "labels_true = [0, 0, 0, 1, 1, 1]\n",
    "labels_pred = [0, 0, 1, 1, 2, 2]\n",
    "\n",
    "metrics.homogeneity_score(labels_true, labels_pred)\n",
    "# 0.66...\n",
    "\n",
    "metrics.completeness_score(labels_true, labels_pred)\n",
    "# 0.42..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leur moyenne harmonique appelée **mesure V** est calculée par [**`v_measure_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5158037429793889"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.v_measure_score(labels_true, labels_pred)\n",
    "# 0.51..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La formule de cette fonction est la suivante :\n",
    "\n",
    "$$v = \\frac{(1 + \\beta) \\times \\text{homogeneity} \\times \\text{completeness}}{(\\beta \\times \\text{homogeneity} + \\text{completeness})}$$\n",
    "\n",
    "La valeur par défaut de `beta` est de 1.0, mais pour utiliser une valeur inférieure à 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5467344787062375"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.v_measure_score(labels_true, labels_pred, beta=0.6)\n",
    "# 0.54..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plus de poids sera attribué à l'exhaustivité.\n",
    "\n",
    "La V-mesure est en fait équivalente à l'information mutuelle (NMI) discutée précédemment, la fonction d'agrégation étant la moyenne arithmétique [B2011].\n",
    "\n",
    "L'homogénéité, l'exhaustivité et la mesure V peuvent être calculées en une seule fois en utilisant [**`homogeneity_completeness_v_measure`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_completeness_v_measure.html) comme suit :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6666666666666669, 0.420619835714305, 0.5158037429793889)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)\n",
    "# (0.66..., 0.42..., 0.51...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'affectation de regroupement suivante est légèrement meilleure, car elle est homogène mais pas exhaustive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.6853314789615865, 0.8132898335036762)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_pred = [0, 0, 0, 1, 2, 2]\n",
    "metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)\n",
    "# (1.0, 0.68..., 0.81...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : La fontion [**`v_measure_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html) est **symétrique** : elle peut être utilisée pour évaluer l'accord entre les deux affectations indépendantes sur le même jeu de données.\n",
    "\n",
    "Ce n'est pas le cas pour [**`completeness_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.completeness_score.html) et [**`homogeneity_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html) : les deux sont liés par la relation :\n",
    "\n",
    "    homogeneity_score(a, b) == completeness_score(b, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avantages\n",
    "\n",
    "* **Scores bornés** : 0,0 est aussi mauvais que possible, 1,0 est un score parfait.\n",
    "* Interprétation intuitive : le regroupement avec une mauvaise mesure V peut être **analysé qualitativement en termes d'homogénéité et d'exhaustivité** pour mieux comprendre le « type » d'erreurs est commises par l'affectation.\n",
    "* **Aucune hypothèse n'est faite sur la structure des grappes** : peut être utilisé pour comparer des algorithmes de regroupement tels que les k-moyennes, qui suppose des formes de tâches isotropes, avec les résultats d'algorithmes de regroupement spectral qui peuvent identifier des grappes avec des formes plus complexes.\n",
    "\n",
    "### Inconvénients\n",
    "\n",
    "* Les métriques précédemment introduites ne **sont pas normalisées par rapport à l'étiquetage aléatoire** : cela signifie qu'en fonction du nombre d'échantillons, de grappes et de classes de vérité terrain, un étiquetage complètement aléatoire ne donnera pas toujours les mêmes valeurs d'homogénéité, d'exhaustivité et donc de mesure V. En particulier, un **étiquetage aléatoire ne donnera pas de scores nuls, surtout lorsque le nombre de grappes est élevé**.<br/>Ce problème peut être ignoré en toute sécurité lorsque le nombre d'échantillons est supérieur à mille et le nombre de grappes inférieur à 10. **Pour des tailles d'échantillon plus petites ou un plus grand nombre de grappes, il est plus sûr d'utiliser un indice ajusté tel que l'indice de Rand ajusté (ARI)**.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_adjusted_for_chance_measures_001.png\"\n",
    "    alt=\"Clustering measures for 2 random uniform labelings with equal number of clusters\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "* Ces métriques **nécessitent la connaissance des classes de vérité terrain**, qui est rarement disponible en pratique ou nécessite une affectation manuelle par des annotateurs humains (comme dans le cadre de l'apprentissage supervisé).\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Ajustement pour le hasard dans l'évaluation des performances de regroupement**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_3_cluster/plot_adjusted_for_chance_measures.ipynb)<br/>([*Adjustment for chance in clustering performance evaluation*](https://scikit-learn.org/stable/auto_examples/cluster/plot_adjusted_for_chance_measures.html))\n",
    "\n",
    "Analyse de l'impact de la taille de l'ensemble de données sur la valeur des mesures de regroupement pour des affectations aléatoires.\n",
    "\n",
    "### Formulation mathématique\n",
    "\n",
    "Les scores d'homogénéité et d'exhaustivité sont formellement donnés par :\n",
    "\n",
    "$$h = 1 - \\frac{H(C|K)}{H(C)}$$\n",
    "\n",
    "$$c = 1 - \\frac{H(K|C)}{H(K)}$$\n",
    "\n",
    "où $H(C|K)$ est l'**entropie conditionnelle des classes compte tenu des affectations de grappes** et est donnée par :\n",
    "\n",
    "$$H(C|K) = - \\sum_{c=1}^{|C|} \\sum_{k=1}^{|K|} \\frac{n_{c,k}}{n}\n",
    "\\cdot \\log\\left(\\frac{n_{c,k}}{n_k}\\right)$$\n",
    "\n",
    "et $H(C)$ est l'**entropie des classes** et est donnée par :\n",
    "\n",
    "$$H(C) = - \\sum_{c=1}^{|C|} \\frac{n_c}{n} \\cdot \\log\\left(\\frac{n_c}{n}\\right)$$\n",
    "\n",
    "où $n$ le nombre total d'échantillons, $n_c$ et $n_k$ le nombre d'échantillons appartenant respectivement à la classe $c$ et à la grappe $k$, et enfin $n_{c, k}$ le nombre d'échantillons de la classe $c$ affectée à la grappe $k$.\n",
    "\n",
    "L'**entropie conditionnelle des grappes d'une classe donnée** $H(K|C)$ et l'**entropie des grappes** $H(K)$ sont définis de manière symétrique.\n",
    "\n",
    "Rosenberg et Hirschberg définissent en outre la **mesure V** comme la **moyenne harmonique de l'homogénéité et de l'exhaustivité** :\n",
    "\n",
    "$$v = 2 \\cdot \\frac{h \\cdot c}{h + c}$$\n",
    "\n",
    "### Références\n",
    "\n",
    "🔬 [**“V-Measure: A conditional entropy-based external cluster evaluation measure”**](https://aclanthology.org/D07-1043.pdf) Andrew Rosenberg and Julia Hirschberg, 2007\n",
    "\n",
    "📚 [B2011] [**“Identication and Characterization of Events in Social Media”**](http://www.cs.columbia.edu/~hila/hila-thesis-distributed.pdf), Hila Becker, PhD Thesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='fowlkes-mallows-scores'></a> 2.3.11.4. **Score de Fowlkes-Mallows**<br/>([*Fowlkes-Mallows scores*](https://scikit-learn.org/stable/modules/clustering.html#fowlkes-mallows-scores))\n",
    "\n",
    "L'indice de Fowlkes-Mallows ([**`sklearn.metrics.fowlkes_mallows_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fowlkes_mallows_score.html)) peut être utilisé lorsque les affectations de classe de vérité terrain des échantillons sont connues. Le score de Fowlkes-Mallows (FMI) est défini comme la moyenne géométrique de la précision et du rappel par paire :\n",
    "\n",
    "$$\\text{FMI} = \\frac{\\text{TP}}{\\sqrt{(\\text{TP} + \\text{FP}) (\\text{TP} + \\text{FN})}}$$\n",
    "\n",
    "Où `TP` est le nombre de **vrais positifs** (c'est-à-dire le nombre de paires de points qui appartiennent aux mêmes grappes dans les étiquettes vraies et prédites), `FP` est le nombre de **faux positifs** (c'est-à-dire le nombre de paires de points qui appartiennent aux mêmes grappes dans les étiquettes vraies et non dans les étiquettes prédites) et FN est le nombre de **faux négatifs** (c'est-à-dire le nombre de paires de points qui appartiennent aux mêmes grappes dans les étiquettes prédites et non dans les étiquettes vraies).\n",
    "\n",
    "Le score varie de 0 à 1. Une valeur élevée indique une bonne similarité entre deux grappes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4714045207910317"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "labels_true = [0, 0, 0, 1, 1, 1]\n",
    "labels_pred = [0, 0, 1, 1, 2, 2]\n",
    "metrics.fowlkes_mallows_score(labels_true, labels_pred)\n",
    "# 0.47140..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut permuter 0 et 1 dans les étiquettes prédites, renommer 2 en 3 et obtenir le même score :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4714045207910317"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_pred = [1, 1, 0, 0, 3, 3]\n",
    "metrics.fowlkes_mallows_score(labels_true, labels_pred)\n",
    "# 0.47140..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un étiquetage parfait obtient un score de 1,0 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_pred = labels_true[:]\n",
    "metrics.fowlkes_mallows_score(labels_true, labels_pred)\n",
    "# 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les étiquetage mauvais (par exemple, les étiquetages indépendants) ont des scores nuls :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_true = [0, 1, 2, 0, 3, 4, 5, 1]\n",
    "labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\n",
    "metrics.fowlkes_mallows_score(labels_true, labels_pred)\n",
    "0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avantages\n",
    "\n",
    "* **Les affectations d'étiquettes aléatoires (uniformes) ont un score FMI proche de 0,0** pour toute valeur de `n_clusters` et `n_samples` (ce qui n'est pas le cas pour l'information mutuelle brute ou la mesure V par exemple).\n",
    "* **Borne supérieure à 1**: les valeurs proches de zéro indiquent deux affectations d'étiquettes largement indépendantes, tandis que les valeurs proches de un indiquent un accord significatif. De plus, des valeurs exactement égales à 0 indiquent des affectations d'étiquettes **purement** indépendantes, et un FMI exactement égal à 1 indique que les deux affectations d'étiquettes sont égales (avec ou sans permutation).\n",
    "* **Aucune hypothèse n'est faite sur la structure des grappes** : cette mesure peut être utilisé pour comparer des algorithmes de regroupement tels que les k-moyennes, qui suppose des formes de tâches isotropes, avec les résultats d'algorithmes de regroupement spectral, qui peuvent identifier des grappes ayant des formes \"pliées\".\n",
    "\n",
    "### Inconvénients\n",
    "\n",
    "* Contrairement à l'inertie, **les mesures basées sur le FMI nécessitent la connaissance des classes de vérité terrain** alors qu'elles ne sont presque jamais disponibles en pratique ou nécessitent une affectation manuelle par des annotateurs humains (comme dans le cadre de l'apprentissage supervisé).\n",
    "\n",
    "### Références\n",
    "\n",
    "Z 🔬 E. B. Fowkles and C. L. Mallows, 1983. [**“A Method for Comparing Two Hierarchical Clusterings”**](https://www.semanticscholar.org/paper/A-Method-for-Comparing-Two-Hierarchical-Clusterings-Fowlkes-Mallows/ededd54b4f7578802ce81fe7a34c05d93e5e09ee). Journal of the American Statistical Association.\n",
    "\n",
    "[Wikipedia entry for the **Fowlkes-Mallows Index**](https://en.wikipedia.org/wiki/Fowlkes%E2%80%93Mallows_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='silhouette-coefficient'></a> 2.3.11.5. **Coefficient de silhouette**<br/>([*Silhouette Coefficient*](https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient))\n",
    "\n",
    "Si les étiquettes de vérité terrain ne sont pas connues, l'évaluation doit être effectuée à l'aide du modèle lui-même. Le coefficient de silhouette ([**`sklearn.metrics.silhouette_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)) est un exemple d'une telle évaluation, où un score de coefficient de silhouette plus élevé est associé à un modèle avec des grappes mieux définies. Le coefficient silhouette est défini pour chaque échantillon et il est composé de deux scores :\n",
    "* $a$ : La distance moyenne entre un échantillon et tous les autres points de la même classe.\n",
    "* $b$ : La distance moyenne entre un échantillon et tous les autres points de la grappe la plus proche.\n",
    "\n",
    "Le coefficient de Silhouette $s$ pour un seul échantillon est alors :\n",
    "\n",
    "$$s = \\frac{b - a}{max(a, b)}$$\n",
    "\n",
    "Le coefficient de silhouette pour un ensemble d'échantillons est donné comme la moyenne du coefficient de silhouette pour chacun d'entre eux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn import datasets\n",
    "X, y = datasets.load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisation normale, le coefficient de silhouette est appliqué aux résultats d'une analyse de grappes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5528190123564095"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)\n",
    "labels = kmeans_model.labels_\n",
    "metrics.silhouette_score(X, labels, metric='euclidean')\n",
    "# 0.55..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avantages\n",
    "\n",
    "* Le score est borné par -1 pour un regroupement incorrect et par +1 pour un regroupement très dense. Les scores proches de zéro indiquent des grappes qui se chevauchent.\n",
    "* Le score est plus élevé lorsque les grappes sont denses et bien séparées, ce qui correspond à un concept standard de grappe.\n",
    "\n",
    "### Inconvénients\n",
    "\n",
    "* Le coefficient de silhouette est généralement plus élevé pour les grappes convexes que pour les autres concepts de grappes, tels que les grappes basées sur la densité comme celles obtenues via DBSCAN.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Sélection du nombre de grappes avec analyse de silhouette sur le regroupement KMeans**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_3_cluster/plot_kmeans_silhouette_analysis.ipynb)<br/>([*Selecting the number of clusters with silhouette analysis on KMeans clustering*](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html))\n",
    "\n",
    "Dans cet exemple l'analyse de silhouette est utilisée pour choisir une valeur optimale pour `n_clusters`.\n",
    "\n",
    "### Références\n",
    "\n",
    "🔬 Peter J. Rousseeuw (1987). [**“Silhouettes: a Graphical Aid to the Interpretation and Validation of Cluster Analysis”**](https://www.sciencedirect.com/science/article/pii/0377042787901257?via%3Dihub). Computational and Applied Mathematics 20: 53–65."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='calinski-harabasz-index'></a> 2.3.11.6. **Indice de Calinski-Harabasz**<br/>([*Calinski-Harabasz Index*](https://scikit-learn.org/stable/modules/clustering.html#calinski-harabasz-index))\n",
    "\n",
    "Si les étiquettes de vérité de terrain ne sont pas connues, l'indice de Calinski-Harabasz ([**`sklearn.metrics.calinski_harabasz_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html)) - également connu sous le nom de _critère du rapport de variance_ - peut être utilisé pour évaluer le modèle, où un score élevé de Calinski-Harabasz se rapporte à un modèle avec des grappes mieux définies.\n",
    "\n",
    "L'indice est le ratio de la somme de la dispersion entre les grappes et de la dispersion à l'intérieur des grappes pour l'ensemble des grappes (où la dispersion est définie comme la somme des distances au carré) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn import datasets\n",
    "X, y = datasets.load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisation normale, l'indice de Calinski-Harabasz est appliqué aux résultats d'une analyse de grappes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "561.62775662962"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)\n",
    "labels = kmeans_model.labels_\n",
    "metrics.calinski_harabasz_score(X, labels)\n",
    "# 561.62..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avantages\n",
    "\n",
    "* Le score est plus élevé lorsque les grappes sont denses et bien séparées, ce qui correspond à un concept standard de grappe.\n",
    "\n",
    "* Le score est rapide à calculer.\n",
    "\n",
    "### Inconvénients\n",
    "\n",
    "* L'indice Calinski-Harabasz est généralement plus élevé pour les grappes convexes que pour d'autres concepts de grappes, tels que les grappes basées sur la densité comme celles obtenues via DBSCAN.\n",
    "\n",
    "### Formulation mathématique\n",
    "\n",
    "Pour un ensemble de données $E$ de taille $n_E$ qui a été regroupé en $k$ grappes, le score de Calinski-Harabasz $s$ est défini comme le rapport de la moyenne de dispersion inter-grappes et de la dispersion intra-grappe :\n",
    "\n",
    "$$s = \\frac{\\mathrm{tr}(B_k)}{\\mathrm{tr}(W_k)} \\times \\frac{n_E - k}{k - 1}$$\n",
    "\n",
    "où $\\mathrm{tr}(B_k)$ est la trace de la matrice de dispersion inter-grappes et $\\mathrm{tr}(W_k)$ est la trace de la matrice de dispersion intra-grappe définie par :\n",
    "\n",
    "$$W_k = \\sum_{q=1}^k \\sum_{x \\in C_q} (x - c_q) (x - c_q)^\\top$$\n",
    "$$B_k = \\sum_{q=1}^k n_q (c_q - c_E) (c_q - c_E)^\\top$$\n",
    "\n",
    "avec $C_q$ l'ensemble des points dans la grappe $q$, $c_q$ le centre de la grappe $q$, $c_E$ le centre de $E$, et $n_q$ le nombre de points dans la grappe $q$.\n",
    "\n",
    "### Références\n",
    "\n",
    "🔬 Caliński, T., & Harabasz, J. (1974). [**“A Dendrite Method for Cluster Analysis”**](https://www.researchgate.net/publication/233096619_A_Dendrite_Method_for_Cluster_Analysis). Communications in Statistics-theory and Methods 3: 1-27."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='davies-bouldin-index'></a> 2.3.11.7. **Indice de Davies-Bouldin**<br/>([*Davies-Bouldin Index*](https://scikit-learn.org/stable/modules/clustering.html#davies-bouldin-index))\n",
    "\n",
    "Si les étiquettes de vérité terrain ne sont pas connues, l'indice de Davies-Bouldin ([**`sklearn.metrics.davies_bouldin_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html)) peut être utilisé pour évaluer le modèle. Un indice de Davies-Bouldin plus bas est associé à un modèle avec une meilleure séparation entre les clusters.\n",
    "\n",
    "Cet indice représente la « similarité » moyenne entre les clusters, où la similarité est une mesure qui compare la distance entre les clusters avec la taille des clusters eux-mêmes.\n",
    "\n",
    "Zéro est le score le plus bas possible. Des valeurs plus proches de zéro indiquent une meilleure partition.\n",
    "\n",
    "Dans l'utilisation normale, l'indice de Davies-Bouldin est appliqué aux résultats d'une analyse de cluster de la manière suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6619715465007465"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "kmeans = KMeans(n_clusters=3, random_state=1).fit(X)\n",
    "labels = kmeans.labels_\n",
    "davies_bouldin_score(X, labels)\n",
    "# 0.6619..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avantages\n",
    "\n",
    "* Le calcul de Davies-Bouldin est plus simple que celui des scores Silhouette.\n",
    "* L'indice est uniquement basé sur des quantités et des caractéristiques inhérentes à l'ensemble de données car son calcul n'utilise que des distances entre les points.\n",
    "\n",
    "### Inconvénients\n",
    "\n",
    "* L'indice de Davies-Boulding est généralement plus élevé pour les grappes convexes que pour les autres concepts de grappes, tels que les grappes basées sur la densité comme celles obtenues à partir de DBSCAN.\n",
    "* L'utilisation de la distance au centroïde limite la distance métrique à l'espace euclidien.\n",
    "\n",
    "### Formulation mathématique\n",
    "\n",
    "L'indice est défini comme la similarité moyenne entre chaque grappe $C_i$ pour $i = 1, \\cdots, k$ et sa grappe $C_j$ la plus similaire. Dans le cadre de cet indice, la similarité est définie comme une mesure $R_{ij}$ qui équilibre :\n",
    "* $s_i$, la distance moyenne entre chaque point de la grappe $i$ et le centroïde de cette grappe - également connu sous le nom de _diamètre de la grappe_.\n",
    "* $d_{ij}$, la distance entre les centres de gravité des grappes $i$ et $j$.\n",
    "\n",
    "Un choix simple pour construire $R_{ij}$ de telle sorte qu'il soit non négatif et symétrique est :\n",
    "\n",
    "$$R_{ij} = \\frac{s_i + s_j}{d_{ij}}$$\n",
    "\n",
    "Alors l'indice de Davies-Bouldin est défini comme :\n",
    "\n",
    "$$DB = \\frac{1}{k} \\sum_{i=1}^k \\max_{i \\neq j} R_{ij}$$\n",
    "\n",
    "### Références\n",
    "\n",
    "🔬 Davies, David L.; Bouldin, Dld W. (1979). [**“A Cluster Separation Measure”**](https://ieeexplore.ieee.org/document/4766909) IEEE Transactions on Pattern Analysis and Machine Intelligence. PAMI-1 (2): 224-227.\n",
    "\n",
    "🔬 Halkidi, Maria; Batistakis, Yannis; Vazirgiannis, Michalis (2001). [**“On Clustering Validation Techniques”**](https://link.springer.com/article/10.1023/A:1012801612483) Journal of Intelligent Information Systems, 17(2-3), 107-145.\n",
    "\n",
    "[Wikipedia entry for **Davies-Bouldin index**](https://en.wikipedia.org/wiki/Davies%E2%80%93Bouldin_index)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='contingency-matrix'></a> 2.3.11.8. **Matrice de contingence**<br/>([*Contingency Matrix*](https://scikit-learn.org/stable/modules/clustering.html#contingency-matrix))\n",
    "\n",
    "La matrice de contingence ([**`sklearn.metrics.cluster.contingency_matrix`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cluster.contingency_matrix.html)) rapporte la cardinalité d'intersection pour chaque paire de grappes vraies/prédites. La matrice de contingence fournit des statistiques suffisantes pour toutes les métriques de regroupement où les échantillons sont indépendants et distribués de manière identique, et où il n'est pas nécessaire de tenir compte du fait que certaines instances ne sont pas regroupées.\n",
    "\n",
    "Voici un exemple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 0],\n",
       "       [0, 1, 2]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "x = [\"a\", \"a\", \"a\", \"b\", \"b\", \"b\"]\n",
    "y = [0, 0, 1, 1, 2, 2]\n",
    "contingency_matrix(x, y)\n",
    "# array([[2, 1, 0],\n",
    "#       [0, 1, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première ligne du tableau de sortie indique qu'il y a trois échantillons dont la vraie grappe est \"a\". Parmi eux, deux sont dans la grappe prédite 0, un est dans 1 et aucun n'est dans 2. Et la deuxième ligne indique qu'il y a trois échantillons dont la vraie grappe est \"b\". Parmi eux, aucun n'est dans la grappe prédite 0, un est dans 1 et deux sont dans 2.\n",
    "\n",
    "Une [**matrice de confusion** (3.3.2.6)](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix) pour la classification est une matrice de contingence carrée où l'ordre des lignes et des colonnes correspond à une liste de classes.\n",
    "\n",
    "### Avantages\n",
    "\n",
    "* Permet d'examiner la répartition de chaque grappe vraie à travers les grappes prédites et vice versa.\n",
    "* La matrice de contingence calculée est généralement utilisée dans le calcul d'une statistique de similarité (comme les autres listées dans ce document) entre les deux regroupements.\n",
    "\n",
    "### Inconvénients\n",
    "\n",
    "* La matrice de contingence est facile à interpréter pour un petit nombre de grappes, mais devient très difficile à interpréter pour un grand nombre de grappes.\n",
    "* Elle ne fournit pas une seule métrique à utiliser comme objectif pour l'optimisation du regroupement.\n",
    "\n",
    "### Références\n",
    "\n",
    "[Wikipedia entry for **contingency matrix**](https://en.wikipedia.org/wiki/Contingency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='pair-confusion-matrix'></a> 2.3.11.9. **Matrice de confusion par paire**<br/>([*Pair Confusion Matrix*](https://scikit-learn.org/stable/modules/clustering.html#pair-confusion-matrix))\n",
    "\n",
    "La matrice de confusion par paire ([**`sklearn.metrics.cluster.pair_confusion_matrix`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cluster.pair_confusion_matrix.html)) est une matrice de similarité 2x2\n",
    "\n",
    "$$\\begin{split}C = \\left[\\begin{matrix}\n",
    "C_{00} & C_{01} \\\\\n",
    "C_{10} & C_{11}\n",
    "\\end{matrix}\\right]\\end{split}$$\n",
    "\n",
    "entre deux regroupements calculés en considérant toutes les paires d'échantillons et en comptant les paires qui sont affectées dans la même grappe ou dans des grappes différentes sous les regroupements vrais et prédits.\n",
    "\n",
    "Elle contient les entrées suivantes :\n",
    "\n",
    "$C_{00}$ : le nombre de paires avec les deux regroupements où les échantillons ne sont pas regroupés ensemble.\n",
    "\n",
    "$C_{10}$ : le nombre de paires avec le regroupement d'étiquettes vraies où les échantillons sont regroupés ensemble, mais où l'autre regroupement ne les regroupe pas.\n",
    "\n",
    "$C_{01}$ : le nombre de paires avec le regroupement d'étiquettes vraies où les échantillons ne sont pas regroupés, mais où l'autre regroupement les regroupe.\n",
    "\n",
    "$C_{11}$ : le nombre de paires avec les deux regroupements où les échantillons sont regroupés ensemble.\n",
    "\n",
    "En considérant une paire d'échantillons regroupés ensemble comme une paire positive, alors, comme dans la classification binaire, le nombre de vrais négatifs est $C_{00}$, les faux négatifs sont $C_{10}$, les vrais positifs sont $C_{11}$ et les faux positifs sont $C_{01}$.\n",
    "\n",
    "Si les membres des classes sont complètement répartis sur différents clusters, l'affectation est totalement incomplète, donc la matrice a toutes les entrées en diagonale nulle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8, 0],\n",
       "       [0, 4]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.cluster import pair_confusion_matrix\n",
    "pair_confusion_matrix([0, 0, 1, 1], [0, 0, 1, 1])\n",
    "# array([[8, 0],\n",
    "#        [0, 4]])\n",
    "pair_confusion_matrix([0, 0, 1, 1], [1, 1, 0, 0])\n",
    "# array([[8, 0],\n",
    "#        [0, 4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les regroupements qui assignent tous les membres de classes aux mêmes grappes sont complets mais peuvent ne pas toujours être purs, donc pénalisés, et ont certaines entrées non diagonales non nulles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8, 2],\n",
       "       [0, 2]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_confusion_matrix([0, 0, 1, 2], [0, 0, 1, 1])\n",
    "# array([[8, 2],\n",
    "#        [0, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matrice n'est pas symétrique :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8, 0],\n",
       "       [2, 2]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_confusion_matrix([0, 0, 1, 1], [0, 0, 1, 2])\n",
    "# array([[8, 0],\n",
    "#        [2, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si les membres des classes sont complètement répartis à travers différentes grappes, l'affectation est totalement incomplète, donc la matrice a toutes les entrées diagonales nulles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0],\n",
       "       [12,  0]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_confusion_matrix([0, 0, 0, 0], [0, 1, 2, 3])\n",
    "# array([[ 0,  0],\n",
    "#        [12,  0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Références\n",
    "\n",
    "🔬 L. Hubert and P. Arabie, [**“Comparing Partitions”**](https://link.springer.com/article/10.1007/BF01908075), Journal of Classification, 1985."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e03b612d84ba21ce95ed447e81b3062e1eb99b56c6d885cdab4aaa12f1b8e240"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
