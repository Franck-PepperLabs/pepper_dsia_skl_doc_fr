{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='model-selection-and-evaluation'></a> 3. [**Sélection et évaluation de modèle**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/3_model_selection_and_evaluation.ipynb#model-selection-and-evaluation)</br>([*Model selection and evaluation*](https://scikit-learn.org/stable/model_selection.html#model-selection-and-evaluation))\n",
    "\n",
    "# 3.3. [**Métriques et scoring : quantifier la qualité des prédictions**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/3_model_selection_and_evaluation.ipynb#metrics-and-scoring-quantifying-the-quality-of-predictions)<br/>([_Metrics and scoring: quantifying the quality of predictions_](https://scikit-learn.org/stable/model_selection.html#metrics-and-scoring-quantifying-the-quality-of-predictions))\n",
    "\n",
    "Il existe 3 API différents pour évaluer la qualité des prédictions d'un modèle :\n",
    "\n",
    "- **Méthode de score de l'estimateur :** Les estimateurs possèdent une méthode `score` fournissant un critère d'évaluation par défaut pour le problème auquel ils sont conçus pour résoudre. Cela n'est pas discuté sur cette page, mais dans la documentation de chaque estimateur.\n",
    "\n",
    "- **Paramètre `scoring` :** Les outils d'évaluation du modèle utilisant la [**validation croisée** (3.1)](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) (comme [**`model_selection.cross_val_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) et [**`model_selection.GridSearchCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)) reposent sur une stratégie de score interne. Cela est expliqué dans la section [**Paramètre `scoring` : définir les règles d'évaluation du modèle** (3.3.1)](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter).\n",
    "\n",
    "- **Fonctions de métriques :** Le module [**`sklearn.metrics`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) implémente des fonctions permettant d'évaluer l'erreur de prédiction à des fins spécifiques. Ces métriques sont détaillées dans les sections sur les [**Métriques de classification** (3.3.2)](https://scikit-learn.org/stable/modules/model_evaluation.html), les [**Métriques de classement multi-étiquettes** (3.3.3)](https://scikit-learn.org/stable/modules/model_evaluation.html#multilabel-ranking-metrics), les [**Métriques de régression** (3.3.4)](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics) et les [**Métriques de regroupement** (3.3.5)](https://scikit-learn.org/stable/modules/model_evaluation.html#clustering-metrics).\n",
    "\n",
    "Enfin, les [**Estimateurs factices** (3.3.6)](https://scikit-learn.org/stable/modules/model_evaluation.html#dummy-estimators) sont utiles pour obtenir une valeur de référence pour ces métriques pour des prédictions aléatoires.\n",
    "\n",
    "> **Voir aussi :** Pour les métriques \"pair à pair\", entre les _échantillons_ et non les estimateurs ou les prédictions, voir la section [**Métriques pair à pair, affinités et noyaux** (6.8)](https://scikit-learn.org/stable/modules/metrics.html#metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 63 pages, 19 exemples, 24 papiers\n",
    "- 3.3.1. [**Le paramètre `scoring` : définir les règles d'évaluation du modèle**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/3_model_selection_and_evaluation.ipynb#the-scoring-parameter-defining-model-evaluation-rules)<br/>([_The `scoring` parameter: defining model evaluation rules_](https://scikit-learn.org/stable/model_selection.html#the-scoring-parameter-defining-model-evaluation-rules))\n",
    "- 3.3.2. [**Métriques de classification**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/3_model_selection_and_evaluation.ipynb#classification-metrics)<br/>([_Classification metrics_](https://scikit-learn.org/stable/model_selection.html#classification-metrics))\n",
    "- 3.3.3. [**Métriques de classement multi-étiquettes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/3_model_selection_and_evaluation.ipynb#multilabel-ranking-metrics)<br/>([_Multilabel ranking metrics_](https://scikit-learn.org/stable/model_selection.html#multilabel-ranking-metrics))\n",
    "- 3.3.4. [**Métriques de régression**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/3_model_selection_and_evaluation.ipynb#regression-metrics)<br/>([_Regression metrics_](https://scikit-learn.org/stable/model_selection.html#regression-metrics))\n",
    "- 3.3.5. [**Métriques de clustering**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/3_model_selection_and_evaluation.ipynb#clustering-metrics)<br/>([_Clustering metrics_](https://scikit-learn.org/stable/model_selection.html#clustering-metrics))\n",
    "- 3.3.6. [**Estimateurs factices**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/3_model_selection_and_evaluation.ipynb#dummy-estimators)<br/>([_Dummy estimators_](https://scikit-learn.org/stable/model_selection.html#dummy-estimators))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='the-scoring-parameter-defining-model-evaluation-rules'></a> 3.3.1. Le paramètre `scoring` : définir les règles d'évaluation du modèle\n",
    "\n",
    "La sélection et l'évaluation des modèles à l'aide d'outils tels que [**`model_selection.GridSearchCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) et [**`model_selection.cross_val_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score) nécessitent un paramètre `scoring` qui contrôle la métrique appliquée aux estimateurs évalués.\n",
    "\n",
    "### <a id='common-cases-predefined-values'></a> 3.3.1.1. Cas courants : valeurs prédéfinies\n",
    "\n",
    "Pour les cas d'utilisation les plus courants, vous pouvez désigner un objet de métrique avec le paramètre `scoring`. Le tableau ci-dessous montre toutes les valeurs possibles. Tous les objets de métrique suivent la convention selon laquelle **des valeurs de retour plus élevées sont meilleures que des valeurs de retour plus basses**. Ainsi, les métriques qui mesurent la distance entre le modèle et les données, comme [**`metrics.mean_squared_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html), sont disponibles sous la forme `neg_mean_squared_error`, qui renvoie la valeur négative de la métrique.\n",
    "\n",
    "| Métrique                             | Fonction                                                        | Commentaire                 |\n",
    "|:-------------------------------------|:----------------------------------------------------------------|:----------------------------|\n",
    "| **Classification**                   |                                                                 |                             |\n",
    "| ‘accuracy’                           | [**`metrics.accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)                |                             |\n",
    "| ‘balanced_accuracy’                  | [**`metrics.balanced_accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html)       |                             |\n",
    "| ‘top_k_accuracy’                     | [**`metrics.top_k_accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.top_k_accuracy_score.html)             |                             |\n",
    "| ‘average_precision’                  | [**`metrics.average_precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html)       |                             |\n",
    "| ‘neg_brier_score’                    | [**`metrics.brier_score_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html)                    |                             |\n",
    "| ‘f1’                                 | [**`metrics.f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)                                  | pour les cibles binaires   |\n",
    "| ‘f1_micro’                           | [**`metrics.f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)                                  | micro-average              |\n",
    "| ‘f1_macro’                           | [**`metrics.f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)                                  | macro-average              |\n",
    "| ‘f1_weighted’                        | [**`metrics.f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)                                  | moyenne pondérée           |\n",
    "| ‘f1_samples’                         | [**`metrics.f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)                                  | par échantillon multilabel |\n",
    "| ‘neg_log_loss’                       | [**`metrics.log_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html)                                  | nécessite le support de predict_proba|\n",
    "| ‘precision’ etc.                     | [**`metrics.precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)                    | suffixes comme ‘f1’        |\n",
    "| ‘recall’ etc.                        | [**`metrics.recall_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)                          | suffixes comme ‘f1’        |\n",
    "| ‘jaccard’ etc.                       | [**`metrics.jaccard_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html)                         | suffixes comme ‘f1’        |\n",
    "| ‘roc_auc’                            | [**`metrics.roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)                           |                           |\n",
    "| ‘roc_auc_ovr’                        | [**`metrics.roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)                           |                           |\n",
    "| ‘roc_auc_ovo’                        | [**`metrics.roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)                           |                           |\n",
    "| ‘roc_auc_ovr_weighted’               | [**`metrics.roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)                           |                           |\n",
    "| ‘roc_auc_ovo_weighted’               | [**`metrics.roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)                           |                           |\n",
    "| **Clustering**                       |                                                                |                           |\n",
    "| ‘adjusted_mutual_info_score’         | [**`metrics.adjusted_mutual_info_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html) |                           |\n",
    "| ‘adjusted_rand_score’                | [**`metrics.adjusted_rand_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html)           |                           |\n",
    "| ‘completeness_score’                 | [**`metrics.completeness_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.completeness_score.html)             |                           |\n",
    "| ‘fowlkes_mallows_score’              | [**`metrics.fowlkes_mallows_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fowlkes_mallows_score.html)       |                           |\n",
    "| ‘homogeneity_score’                  | [**`metrics.homogeneity_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html)               |                           |\n",
    "| ‘mutual_info_score’                  | [**`metrics.mutual_info_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mutual_info_score.html)               |                           |\n",
    "| ‘normalized_mutual_info_score’       | [**`metrics.normalized_mutual_info_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html) |                           |\n",
    "| ‘rand_score’                         | [**`metrics.rand_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.rand_score.html)                               |                           |\n",
    "| ‘v_measure_score’                    | [**`metrics.v_measure_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html)                     |                           |\n",
    "| **Regression**                       |                                                                |                           |\n",
    "| ‘explained_variance’                 | [**`metrics.explained_variance_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.explained_variance_score.html)   |                           |\n",
    "| ‘max_error’                          | [**`metrics.max_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.max_error.html)                                 |                           |\n",
    "| ‘neg_mean_absolute_error’            | [**`metrics.mean_absolute_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html)           |                           |\n",
    "| ‘neg_mean_squared_error’             | [**`metrics.mean_squared_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)             |                           |\n",
    "| ‘neg_root_mean_squared_error’        | [**`metrics.mean_squared_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)             |                           |\n",
    "| ‘neg_mean_squared_log_error’         | [**`metrics.mean_squared_log_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_log_error.html)     |                           |\n",
    "| ‘neg_median_absolute_error’          | [**`metrics.median_absolute_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.median_absolute_error.html)       |                           |\n",
    "| ‘r2’                                 | [**`metrics.r2_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)                                   |                           |\n",
    "| ‘neg_mean_poisson_deviance’          | [**`metrics.mean_poisson_deviance`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_poisson_deviance.html)       |                           |\n",
    "| ‘neg_mean_gamma_deviance’            | [**`metrics.mean_gamma_deviance`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_gamma_deviance.html)           |                           |\n",
    "| ‘neg_mean_absolute_percentage_error’ | [**`metrics.mean_absolute_percentage_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_percentage_error.html) |                           |\n",
    "| ‘d2_absolute_error_score’            | [**`metrics.d2_absolute_error_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.d2_absolute_error_score.html)       |                           |\n",
    "| ‘d2_pinball_score’                   | [**`metrics.d2_pinball_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.d2_pinball_score.html)                 |                           |\n",
    "| ‘d2_tweedie_score’                   | [**`metrics.d2_tweedie_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.d2_tweedie_score.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemples d'utilisation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96666667, 0.96666667, 0.96666667, 0.93333333, 1.        ])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "clf = svm.SVC(random_state=0)\n",
    "cross_val_score(clf, X, y, cv=5, scoring='recall_macro')\n",
    "# array([0.96..., 0.96..., 0.96..., 0.93..., 1.        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** Si un nom de métrique incorrect est passé, une `InvalidParameterError` est levée. Vous pouvez récupérer les noms de toutes les métriques disponibles en appelant [**`get_scorer_names`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.get_scorer_names.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='defining-your-scoring-strategy-from-metric-functions'></a> 3.3.1.2. Définir votre stratégie d'évaluation à partir de fonctions de métriques\n",
    "\n",
    "Le module [**`sklearn.metrics`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) expose également un ensemble de fonctions simples mesurant l'erreur de prédiction en fonction des vérités terrain (ground truth) et des prédictions :\n",
    "- Les fonctions se terminant par `_score` renvoient une valeur à maximiser, donc plus élevée est meilleure.\n",
    "- Les fonctions se terminant par `_error` ou `_loss` renvoient une valeur à minimiser, donc plus faible est meilleure. Lors de la conversion en objet de métrique en utilisant [**`make_scorer`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html), vous pouvez fixer le paramètre `greater_is_better` à `False` (`True` par défaut ; voir la description du paramètre ci-dessous).\n",
    "\n",
    "Les métriques disponibles pour diverses tâches d'apprentissage automatique sont détaillées dans les sections ci-dessous.\n",
    "\n",
    "De nombreuses métriques n'ont pas de noms spécifiques à utiliser comme valeurs de `scoring`, parfois parce qu'elles nécessitent des paramètres supplémentaires, tels que [**`fbeta_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html). Dans de tels cas, vous devez générer un objet de métrique approprié. La façon la plus simple de générer un objet callable pour la métrique est d'utiliser [**`make_scorer`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html). Cette fonction convertit les métriques en objets callable pouvant être utilisés pour l'évaluation du modèle.\n",
    "\n",
    "Un cas d'utilisation typique consiste à envelopper une fonction de métrique existante de la bibliothèque avec des valeurs autres que celles par défaut pour ses paramètres, tels que le paramètre `beta` pour la fonction [**`fbeta_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "grid = GridSearchCV(LinearSVC(dual=\"auto\"), param_grid={'C': [1, 10]},\n",
    "                    scoring=ftwo_scorer, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le deuxième cas d'utilisation consiste à créer un objet de métrique complètement personnalisé à partir d'une simple fonction Python en utilisant [**`make_scorer`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html), qui peut prendre plusieurs paramètres :\n",
    "- la fonction Python que vous souhaitez utiliser (`my_custom_loss_func` dans l'exemple ci-dessous)\n",
    "- si la fonction Python renvoie un score (`greater_is_better=True`, la valeur par défaut) ou une perte (`greater_is_better=False`). S'il s'agit d'une perte, la sortie de la fonction Python est inversée par l'objet de métrique, conformément à la convention de validation croisée selon laquelle les métriques renvoient des valeurs plus élevées pour les meilleurs modèles.\n",
    "- pour les métriques de classification uniquement : si la fonction Python que vous avez fournie nécessite des certitudes de décision continues (`needs_threshold=True`). La valeur par défaut est False.\n",
    "- tous les paramètres supplémentaires, tels que `beta` ou `labels` dans [**`f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score).\n",
    "\n",
    "Voici un exemple de création de métriques personnalisées et d'utilisation du paramètre `greater_is_better` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6931471805599453"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def my_custom_loss_func(y_true, y_pred):\n",
    "    diff = np.abs(y_true - y_pred).max()\n",
    "    return np.log1p(diff)\n",
    "\n",
    "# score will negate the return value of my_custom_loss_func,\n",
    "# which will be np.log(2), 0.693, given the values for X\n",
    "# and y defined below.\n",
    "score = make_scorer(my_custom_loss_func, greater_is_better=False)\n",
    "X = [[1], [1]]\n",
    "y = [0, 1]\n",
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier(strategy='most_frequent', random_state=0)\n",
    "clf = clf.fit(X, y)\n",
    "my_custom_loss_func(y, clf.predict(X))\n",
    "# 0.69...\n",
    "score(clf, X, y)\n",
    "# -0.69..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='implementing-your-own-scoring-object'></a> 3.3.1.2. Implémentation de votre propre objet de métrique\n",
    "\n",
    "Vous pouvez créer des métriques de modèle encore plus flexibles en construisant votre propre objet de métrique à partir de zéro, sans utiliser la fabrique [**`make_scorer`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html).\n",
    "\n",
    "**Comment créer une métrique à partir de zéro**\n",
    "\n",
    "Pour qu'un callable soit une métrique, il doit respecter le protocole spécifié par les deux règles suivantes :\n",
    "- Il peut être appelé avec les paramètres `(estimator, X, y)`, où `estimator` est le modèle qui doit être évalué, `X` est la donnée de validation et `y` est la vérité terrain pour `X` (dans le cas supervisé) ou `None` (dans le cas non supervisé).\n",
    "- Il doit renvoyer un nombre à virgule flottante qui quantifie la qualité de prédiction de `estimator` sur `X`, par rapport à `y`. Encore une fois, selon la convention, les nombres plus élevés sont meilleurs. Donc, si votre métrique renvoie une perte, cette valeur doit être négative.\n",
    "- **Avancé** : Si cela nécessite des métadonnées supplémentaires à lui passer, il doit exposer une méthode `get_metadata_routing` renvoyant les métadonnées demandées. L'utilisateur doit pouvoir définir les métadonnées demandées via une méthode `set_score_request`. Veuillez consulter [**Routage de métadonnées** (User Guide _exp)](https://scikit-learn.org/stable/metadata_routing.html#metadata-routing) et [**Routage de métadonnées** (Developer Guide _exp)](https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_metadata_routing.html#sphx-glr-auto-examples-miscellaneous-plot-metadata-routing-py) pour plus de détails.\n",
    "\n",
    "> **Note:** **Utilisation de métriques personnalisées dans des fonctions avec `n_jobs > 1`**\n",
    ">\n",
    "> Bien que la définition de la fonction de métrique personnalisée aux côtés de la fonction d'appel fonctionne par défaut avec le backend joblib (loky), l'importer à partir d'un autre module sera une approche plus robuste et fonctionnera indépendamment du backend joblib.\n",
    ">\n",
    "> Par exemple, pour utiliser `n_jobs` supérieur à 1 dans l'exemple ci-dessous, la fonction `custom_scoring_function` est sauvegardée dans un module créé par l'utilisateur (`custom_scorer_module.py`) et importée :\n",
    "> ```python\n",
    "> >>> from custom_scorer_module import custom_scoring_function \n",
    "> >>> cross_val_score(model,\n",
    "> ...  X_train,\n",
    "> ...  y_train,\n",
    "> ...  scoring=make_scorer(custom_scoring_function, greater_is_better=False),\n",
    "> ...  cv=5,\n",
    "> ...  n_jobs=-1) \n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='using-multiple-metric-evaluation'></a> 3.3.1.4. Utilisation de plusieurs métriques d'évaluation\n",
    "\n",
    "Scikit-learn permet également l'évaluation de plusieurs métriques dans `GridSearchCV`, `RandomizedSearchCV` et `cross_validate`.\n",
    "\n",
    "Il existe trois façons de spécifier plusieurs métriques `scoring` pour le paramètre `scoring` :\n",
    "\n",
    "#### Comme un ensemble de métriques sous forme de chaînes de caractères"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['accuracy', 'precision']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comme un `dict` associant le nom de l'évaluateur à la fonction de mesure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "           'prec': 'precision'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez que les valeurs du dictionnaire peuvent être des fonctions de mesure ou l'une des chaînes prédéfinies de métriques.\n",
    "\n",
    "#### Comme une fonction appelable qui renvoie un dictionnaire de scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10  9  8  7  8]\n",
      "[0 1 2 3 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# A sample toy binary classification dataset\n",
    "X, y = datasets.make_classification(n_classes=2, random_state=0)\n",
    "svm = LinearSVC(dual=\"auto\", random_state=0)\n",
    "def confusion_matrix_scorer(clf, X, y):\n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    return {'tn': cm[0, 0], 'fp': cm[0, 1],\n",
    "            'fn': cm[1, 0], 'tp': cm[1, 1]}\n",
    "cv_results = cross_validate(svm, X, y, cv=5,\n",
    "                            scoring=confusion_matrix_scorer)\n",
    "# Getting the test set true positive scores\n",
    "print(cv_results['test_tp'])\n",
    "# [10  9  8  7  8]\n",
    "# Getting the test set false negative scores\n",
    "print(cv_results['test_fn'])\n",
    "# [0 1 2 3 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='classification-metrics'></a> 3.3.2. Métriques de classification\n",
    "\n",
    "Le module [**`sklearn.metrics`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) implémente plusieurs fonctions de perte, de score et d'utilité pour mesurer les performances de classification. Certaines métriques peuvent nécessiter des estimations de probabilité de la classe positive, des valeurs de confiance ou des valeurs de décision binaires. La plupart des implémentations permettent à chaque échantillon de contribuer de manière pondérée au score global, grâce au paramètre `sample_weight`.\n",
    "\n",
    "### Tableau 1: Métriques de classification pour les cas de classification binaire\n",
    "\n",
    "Certaines de ces métriques sont restreintes au cas de classification binaire :\n",
    "\n",
    "| Fonction                                | Description                                                                                |\n",
    "|:----------------------------------------|:-------------------------------------------------------------------------------------------|\n",
    "| [**`precision_recall_curve`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve)(y_true, probas_pred, *) | Calcule les couples précision-rappel pour différentes valeurs de seuil de probabilité. |\n",
    "| [**`roc_curve`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve)(y_true, y_score, *[, pos_label, ...]) | Calcule la courbe de caractéristique de fonctionnement du récepteur (ROC). |\n",
    "| [**`class_likelihood_ratios`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.class_likelihood_ratios.html#sklearn.metrics.class_likelihood_ratios)(y_true, y_pred, *[, ...]) | Calcule les rapports de probabilités positifs et négatifs de classification binaire. |\n",
    "| [**`det_curve`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.det_curve.html#sklearn.metrics.det_curve)(y_true, y_score[, pos_label, ...]) | Calcule les taux d'erreur pour différentes valeurs de seuil de probabilité. |\n",
    "\n",
    "### Tableau 2: Métriques de classification pour les cas de classification multiclasse\n",
    "\n",
    "D'autres fonctionnent également dans le cas de classification multiclasse :\n",
    "\n",
    "| Fonction                                | Description                                                                                |\n",
    "|:----------------------------------------|:-------------------------------------------------------------------------------------------|\n",
    "| [**`balanced_accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html)(y_true, y_pred, *[, ...]) | Calcule la précision équilibrée. |\n",
    "| [**`cohen_kappa_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html)(y1, y2, *[, labels, ...]) | Calcule le kappa de Cohen : une statistique mesurant l'accord entre annotateurs. |\n",
    "| [**`confusion_matrix`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)(y_true, y_pred, *[, ...]) | Calcule la matrice de confusion pour évaluer l'exactitude d'une classification. |\n",
    "| [**`hinge_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hinge_loss.html)(y_true, pred_decision, *[, ...]) | Perte de charnière moyenne (non régularisée). |\n",
    "| [**`matthews_corrcoef`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)(y_true, y_pred, *[, ...]) | Calcule le coefficient de corrélation de Matthews (MCC). |\n",
    "| [**`roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)(y_true, y_score, *[, average, ...]) | Calcule l'aire sous la courbe ROC (ROC AUC) à partir des scores de prédiction. |\n",
    "| [**`top_k_accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.top_k_accuracy_score.html)(y_true, y_score, *[, ...]) | Score de classification Top-k Accuracy. |\n",
    "\n",
    "### Tableau 3: Métriques de classification pour les cas de classification multilabel\n",
    "\n",
    "Certaines fonctionnent également dans le cas de classification multilabel :\n",
    "\n",
    "| Fonction                                | Description                                                                                |\n",
    "|:----------------------------------------|:-------------------------------------------------------------------------------------------|\n",
    "| [**`accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score)(y_true, y_pred, *[, ...]) | Score de classification Accuracy. |\n",
    "| [**`classification_report`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report)(y_true, y_pred, *[, ...]) | Génère un rapport texte présentant les principales métriques de classification. |\n",
    "| [**`f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score)(y_true, y_pred, *[, labels, ...]) | Calcule le score F1, également connu sous le nom de F-score équilibré ou mesure F. |\n",
    "| [**`fbeta_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score)(y_true, y_pred, *, beta[, ...]) | Calcule le score F-beta. |\n",
    "| [**`hamming_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html#sklearn.metrics.hamming_loss)(y_true, y_pred, *[, sample_weight]) | Calcule la perte de Hamming moyenne. |\n",
    "| [**`jaccard_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html#sklearn.metrics.jaccard_score)(y_true, y_pred, *[, labels, ...]) | Score de similarité de Jaccard. |\n",
    "| [**`log_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss)(y_true, y_pred, *[, eps, ...]) | Perte logarithmique, également appelée perte logistique ou perte d'entropie croisée. |\n",
    "| [**`multilabel_confusion_matrix`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html#sklearn.metrics.multilabel_confusion_matrix)(y_true, y_pred, *) | Calcule une matrice de confusion pour chaque classe ou échantillon. |\n",
    "| [**`precision_recall_fscore_support`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support)(y_true, ...) | Calcule la précision, le rappel, le score F et le support pour chaque classe. |\n",
    "| [**`precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)(y_true, y_pred, *[, labels, ...]) | Calcule la précision. |\n",
    "| [**`recall_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)(y_true, y_pred, *[, labels, ...]) | Calcule le rappel. |\n",
    "\n",
    "### Tableau 4: Métriques de classification pour les cas de classification binaire et multilabel (mais pas multiclasse)\n",
    "\n",
    "Et certaines fonctionnent pour les problèmes binaires et multilabel (mais pas multiclasse) :\n",
    "\n",
    "| Fonction                                | Description                                                                                |\n",
    "|:----------------------------------------|:-------------------------------------------------------------------------------------------|\n",
    "| [**`average_precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html)(y_true, y_score, *) | Calcule la précision moyenne (AP) à partir des scores de prédiction. |\n",
    "\n",
    "Dans les sous-sections suivantes, nous décrirons chacune de ces fonctions, précédées de quelques notes sur l'API commune et la définition des métriques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='classification-metrics'></a> 3.3.2.1. De la classification binaire à la classification multiclasse et multilabel\n",
    "\n",
    "Certaines métriques sont essentiellement définies pour les tâches de classification binaire (par exemple, [**`f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html), [**`roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)). Dans ces cas, par défaut, seule l'étiquette positive est évaluée, en supposant que la classe positive est étiquetée `1` (bien que cela puisse être configuré via le paramètre `pos_label`).\n",
    "\n",
    "Lors de l'extension d'une métrique binaire aux problèmes de classification multiclasse ou multilabel, les données sont traitées comme une collection de problèmes binaires, un pour chaque classe. Il existe alors plusieurs façons de calculer la moyenne des métriques binaires pour l'ensemble des classes, chacune pouvant être utile dans certains scénarios. Lorsque cela est disponible, vous devez sélectionner l'une de ces options à l'aide du paramètre `average`.\n",
    "\n",
    "- `\"macro\"` calcule simplement la moyenne des métriques binaires, en accordant un poids égal à chaque classe. Dans les problèmes où les classes peu fréquentes sont néanmoins importantes, le calcul macro-averaging peut mettre en évidence leurs performances. D'un autre côté, l'hypothèse selon laquelle toutes les classes sont également importantes est souvent fausse, de sorte que le macro-averaging surestimera généralement les performances généralement faibles d'une classe peu fréquente.\n",
    "- `\"weighted\"` prend en compte le déséquilibre de classe en calculant la moyenne des métriques binaires dans lesquelles le score de chaque classe est pondéré par sa présence dans l'échantillon de données réelles.\n",
    "- `\"micro\"` donne à chaque paire échantillon-classe une contribution égale à la métrique globale (à l'exception du poids de l'échantillon). Au lieu de sommer la métrique par classe, cela additionne les dividendes et les diviseurs qui composent les métriques par classe pour calculer un quotient global. Le micro-averaging peut être préféré dans les paramètres multilabel, y compris la classification multiclasse où une classe majoritaire doit être ignorée.\n",
    "- `\"samples\"` s'applique uniquement aux problèmes multilabel. Il ne calcule pas une mesure par classe, mais calcule plutôt la métrique sur les classes réelles et prédites pour chaque échantillon dans les données d'évaluation, et retourne leur moyenne (`sample_weight-weighted`).\n",
    "- La sélection de `average=None` renverra un tableau avec le score pour chaque classe.\n",
    "\n",
    "Alors que les données multiclasse sont fournies à la métrique, tout comme les cibles binaires, sous forme de tableau d'étiquettes de classe, les données multilabel sont spécifiées sous forme de matrice indicatrice, dans laquelle la cellule `[i, j]` a la valeur 1 si l'échantillon `i` a l'étiquette `j` et la valeur 0 sinon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='accuracy-score'></a> 3.3.2.2. Score d'exactitude (Accuracy score)\n",
    "\n",
    "La fonction [**`accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) calcule l'[**exactitude**](https://en.wikipedia.org/wiki/Accuracy_and_precision) soit en tant que fraction (par défaut), soit en tant que nombre (`normalize=False`) de prédictions correctes.\n",
    "\n",
    "Dans la classification multi-étiquette, la fonction renvoie l'exactitude du sous-ensemble. Si l'ensemble entier des étiquettes prédites pour un échantillon correspond strictement à l'ensemble réel des étiquettes, alors l'exactitude du sous-ensemble est de 1.0 ; sinon, elle est de 0.0.\n",
    "\n",
    "Si $\\hat{y}_i$ représente la valeur prédite du $i$-ème échantillon et $y_i$ représente la valeur réelle correspondante, alors la fraction de prédictions correctes sur $n_\\text{samples}$ est définie comme suit :\n",
    "\n",
    "$$\\texttt{accuracy}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples}-1} 1(\\hat{y}_i = y_i)$$\n",
    "\n",
    "où $1(x)$ est la [**fonction indicatrice**](https://en.wikipedia.org/wiki/Indicator_function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = [0, 2, 1, 3]\n",
    "y_true = [0, 1, 2, 3]\n",
    "accuracy_score(y_true, y_pred)\n",
    "# 0.5\n",
    "accuracy_score(y_true, y_pred, normalize=False)\n",
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas multi-étiquette avec des indicateurs d'étiquettes binaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemple\n",
    "\n",
    "##### [**Test par permutations de la signifiance d'un score de classification**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/3_model_selection/plot_permutation_tests_for_classification.ipynb)<br/>([_Test with permutations the significance of a classification score_](https://scikit-learn.org/stable/auto_examples/model_selection/plot_permutation_tests_for_classification.html))\n",
    "\n",
    "Exemple d'utilisation du score d'exactitude en utilisant des permutations de l'ensemble de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='top-k-accuracy-score'></a> 3.3.2.3. Top-k accuracy score\n",
    "\n",
    "The [**`top_k_accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.top_k_accuracy_score.html) function is a generalization of [**`accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html). The difference is that a prediction is considered correct as long as the true label is associated with one of the `k` highest predicted scores. [**`accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) is the special case of `k = 1`.\n",
    "\n",
    "The function covers the binary and multiclass classification cases but not the multilabel case.\n",
    "\n",
    "If $\\hat{f}_{i,j}$ is the predicted class for the $i$-th sample corresponding to the $j$-th largest predicted score and $y_i$ is the corresponding true value, then the fraction of correct predictions over $n_\\text{samples}$ is defined as\n",
    "\n",
    "$$\\texttt{top-k accuracy}(y, \\hat{f}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples}-1} \\sum_{j=1}^{k} 1(\\hat{f}_{i,j} = y_i)$$\n",
    "\n",
    "where $k$ is the number of guesses allowed and $1(x)$ is the [**indicator function**](https://en.wikipedia.org/wiki/Indicator_function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='top-k-accuracy-score'></a> 3.3.2.3. Score d'exactitude Top-k (Top-k accuracy score)\n",
    "\n",
    "La fonction [**`top_k_accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.top_k_accuracy_score.html) est une généralisation du [**`accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html). La différence est qu'une prédiction est considérée comme correcte tant que l'étiquette réelle est associée à l'une des `k` valeurs de prédiction les plus élevées. [**`accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) est le cas spécial où `k = 1`.\n",
    "\n",
    "La fonction couvre les cas de classification binaire et multiclasse, mais pas le cas multi-étiquette.\n",
    "\n",
    "Si $\\hat{f}_{i,j}$ représente la classe prédite pour le $i$-ème échantillon correspondant au $j$-ème score de prédiction le plus élevé et $y_i$ représente la valeur réelle correspondante, alors la fraction de prédictions correctes sur $n_\\text{samples}$ est définie comme suit :\n",
    "\n",
    "$$\\texttt{top-k accuracy}(y, \\hat{f}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples}-1} \\sum_{j=1}^{k} 1(\\hat{f}_{i,j} = y_i)$$\n",
    "\n",
    "où $k$ est le nombre de prédictions autorisées et $1(x)$ est la [**fonction indicatrice**](https://en.wikipedia.org/wiki/Indicator_function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "y_true = np.array([0, 1, 2, 2])\n",
    "y_score = np.array([[0.5, 0.2, 0.2],\n",
    "                    [0.3, 0.4, 0.2],\n",
    "                    [0.2, 0.4, 0.3],\n",
    "                    [0.7, 0.2, 0.1]])\n",
    "top_k_accuracy_score(y_true, y_score, k=2)\n",
    "# 0.75\n",
    "# Not normalizing gives the number of \"correctly\" classified samples\n",
    "top_k_accuracy_score(y_true, y_score, k=2, normalize=False)\n",
    "# 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='balanced-accuracy-score'></a> 3.3.2.4. Score d'exactitude équilibrée (Balanced accuracy score)\n",
    "\n",
    "La fonction [**`balanced_accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html) calcule l'[**exactitude équilibrée**](https://en.wikipedia.org/wiki/Accuracy_and_precision), ce qui évite les estimations de performance biaisées sur des ensembles de données déséquilibrés. C'est la macro-moyenne des scores de rappel par classe, ou, de manière équivalente, l'exactitude brute où chaque échantillon est pondéré en fonction de l'inverse de la prévalence de sa classe réelle. Ainsi, pour les ensembles de données équilibrés, le score est égal à l'exactitude.\n",
    "\n",
    "Dans le cas binaire, l'exactitude équilibrée est égale à la moyenne arithmétique de la [**sensibilité**](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) (taux de vrais positifs) et de la [**spécificité**](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) (taux de vrais négatifs), ou la surface sous la courbe ROC avec des prédictions binaires plutôt que des scores :\n",
    "\n",
    "$$\\texttt{balanced-accuracy} = \\frac{1}{2}\\left( \\frac{TP}{TP + FN} + \\frac{TN}{TN + FP}\\right )$$\n",
    "\n",
    "Si le classifieur fonctionne également bien pour les deux classes, ce terme se réduit à l'exactitude conventionnelle (c'est-à-dire le nombre de prédictions correctes divisé par le nombre total de prédictions).\n",
    "\n",
    "En revanche, si l'exactitude conventionnelle est supérieure au hasard uniquement parce que le classifieur profite d'un ensemble de test déséquilibré, alors l'exactitude équilibrée, le cas échéant, se réduira à $\\frac{1}{n\\_classes}$.\n",
    "\n",
    "Le score varie de 0 à 1, ou lorsqu'on utilise `adjusted=True`, il est recalé dans la plage $\\frac{1}{1 - n\\_classes}$ à 1, inclusivement, avec une performance aléatoire obtenant un score de 0.\n",
    "\n",
    "Si $y_i$ est la vraie valeur du $i$-ème échantillon, et $w_i$ est le poids d'échantillon correspondant, alors nous ajustons le poids d'échantillon de la manière suivante :\n",
    "\n",
    "$$\\hat{w}_i = \\frac{w_i}{\\sum_j{1(y_j = y_i) w_j}}$$\n",
    "\n",
    "où $1(x)$ est la [**fonction indicatrice**](https://en.wikipedia.org/wiki/Indicator_function). Étant donné la prédiction $\\hat{y}_i$ pour l'échantillon $i$, l'exactitude équilibrée est définie comme suit :\n",
    "\n",
    "$$\\texttt{balanced-accuracy}(y, \\hat{y}, w) = \\frac{1}{\\sum{\\hat{w}_i}} \\sum_i 1(\\hat{y}_i = y_i) \\hat{w}_i$$\n",
    "\n",
    "Avec `adjusted=True`, l'exactitude équilibrée rapporte l'augmentation relative à $\\texttt{balanced-accuracy}(y, \\mathbf{0}, w) = \\frac{1}{n\\_classes}$. Dans le cas binaire, cela est également connu sous le nom de [**statistique J de Youden**](https://en.wikipedia.org/wiki/Youden's_J_statistic), ou _informedness_. \n",
    "\n",
    "> **Note:** La définition multiclasse ici semble être l'extension la plus raisonnable de la métrique utilisée en classification binaire, bien qu'il n'y ait pas de consensus certain dans la littérature :\n",
    "> - Notre définition : [Mosley2013], [Kelleher2015] et [Guyon2015], où [Guyon2015] adopte la version ajustée pour s'assurer que les prédictions aléatoires ont un score de $0$ et les prédictions parfaites ont un score de $1$.\n",
    "> - Exactitude équilibrée de classe comme décrite dans [Mosley2013] : le minimum entre la précision et le rappel pour chaque classe est calculé. Ces valeurs sont ensuite moyennées sur le nombre total de classes pour obtenir l'exactitude équilibrée.\n",
    "> - Exactitude équilibrée comme décrite dans [Urbanowicz2015] : la moyenne de la sensibilité et de la spécificité est calculée pour chaque classe, puis moyennée sur le nombre total de classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Références\n",
    "\n",
    "🔬 [Guyon2015] (1,2) I. Guyon, K. Bennett, G. Cawley, H.J. Escalante, S. Escalera, T.K. Ho, N. Macià, B. Ray, M. Saeed, A.R. Statnikov, E. Viegas, [**“Design of the 2015 ChaLearn AutoML Challenge”**](http://haralick.org/ML/automl_ijcnn15.pdf), IJCNN 2015.\n",
    "\n",
    "🔬 [Mosley2013] (1,2) L. Mosley, [**“A balanced approach to the multi-class imbalance problem”**](https://dr.lib.iastate.edu/server/api/core/bitstreams/7af0eb70-76f0-4411-9b1b-ce9254e64062/content), IJCV 2010.\n",
    "\n",
    "📚 [Kelleher2015] John. D. Kelleher, Brian Mac Namee, Aoife D’Arcy, [**“Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies”**](https://mitpress.mit.edu/9780262029445/), 2015.\n",
    "\n",
    "🔬 [Urbanowicz2015] Urbanowicz R.J., Moore, J.H. [**“ExSTraCS 2.0: description and evaluation of a scalable learning classifier system”**](https://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC4583133&blobtype=pdf), Evol. Intel. (2015) 8: 89."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='cohen-s-kappa'></a> 3.3.2.5. Statistique de Cohen (Cohen’s kappa)\n",
    "\n",
    "La fonction [**`cohen_kappa_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html) calcule la [**statistique de Cohen**](https://en.wikipedia.org/wiki/Cohen's_kappa). Cette mesure vise à comparer les annotations par différents annotateurs humains, et non un classifieur par rapport à une vérité terrain.\n",
    "\n",
    "Le score kappa (voir la docstring) est un nombre compris entre -1 et 1. Les scores supérieurs à 0,8 sont généralement considérés comme une bonne concordance ; zéro ou moins signifie aucune concordance (étiquettes pratiquement aléatoires).\n",
    "\n",
    "Les scores kappa peuvent être calculés pour les problèmes binaires ou multiclasse, mais pas pour les problèmes multi-étiquette (sauf en calculant manuellement un score par étiquette) et pas pour plus de deux annotateurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4285714285714286"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "y_true = [2, 0, 2, 2, 0, 1]\n",
    "y_pred = [0, 0, 2, 2, 0, 2]\n",
    "cohen_kappa_score(y_true, y_pred)\n",
    "# 0.4285714285714286"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='confusion-matrix'></a> 3.3.2.6. Matrice de confusion\n",
    "\n",
    "La fonction [**`confusion_matrix`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix) évalue l'exactitude de la classification en calculant la [**matrice de confusion**](https://en.wikipedia.org/wiki/Confusion_matrix) dont chaque ligne correspond à la classe réelle (Wikipedia et d'autres références peuvent utiliser une convention différente pour les axes).\n",
    "\n",
    "Par définition, l'entrée $i, j$ dans une matrice de confusion est le nombre d'observations réellement dans le groupe $i$, mais prédit comme étant dans le groupe $j$. Voici un exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 2]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = [2, 0, 2, 2, 0, 1]\n",
    "y_pred = [0, 0, 2, 2, 0, 2]\n",
    "confusion_matrix(y_true, y_pred)\n",
    "# array([[2, 0, 0],\n",
    "#        [0, 0, 1],\n",
    "#        [1, 0, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`ConfusionMatrixDisplay`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html) peut être utilisé pour représenter visuellement une matrice de confusion comme illustré dans l'exemple **Confusion matrix**, qui crée la figure suivante :\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_confusion_matrix_001.png)\n",
    "\n",
    "Le paramètre `normalize` permet de rapporter des ratios au lieu des décomptes. La matrice de confusion peut être normalisée de 3 manières différentes : `'pred'`, `'true'` et `'all'`, qui diviseront les décomptes par la somme de chaque colonne, ligne ou la matrice entière, respectivement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25 , 0.125],\n",
       "       [0.25 , 0.375]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [0, 0, 0, 1, 1, 1, 1, 1]\n",
    "y_pred = [0, 1, 0, 1, 0, 1, 0, 1]\n",
    "confusion_matrix(y_true, y_pred, normalize='all')\n",
    "# array([[0.25 , 0.125],\n",
    "#        [0.25 , 0.375]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les problèmes binaires, nous pouvons obtenir les décomptes de vrais négatifs, faux positifs, faux négatifs et vrais positifs de la manière suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 2, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [0, 0, 0, 1, 1, 1, 1, 1]\n",
    "y_pred = [0, 1, 0, 1, 0, 1, 0, 1]\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "tn, fp, fn, tp\n",
    "# (2, 1, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples\n",
    "\n",
    "##### [**Matrice de confusion**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/3_model_selection/plot_confusion_matrix.ipynb)<br/>([_Confusion matrix_](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html))\n",
    "\n",
    "Exemple d'utilisation d'une matrice de confusion pour évaluer la qualité de la sortie d'un classifieur.\n",
    "\n",
    "##### [**Reconnaissance de chiffres manuscrits**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/classification/plot_digits_classification.ipynb)<br/>([_Recognizing hand-written digits_](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html))\n",
    "\n",
    "Exemple d'utilisation d'une matrice de confusion pour classifier des chiffres manuscrits.\n",
    "\n",
    "##### [**Classification de documents textuels à l'aide de caractéristiques creuses**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/text/plot_document_classification_20newsgroups.ipynb)<br/>([*Classification of text documents using sparse features*](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html))\n",
    "\n",
    "Exemple d'utilisation d'une matrice de confusion pour classifier des documents textuels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='classification-report'></a> 3.3.2.7. Rapport de classification\n",
    "\n",
    "La fonction [**`classification_report`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report) génère un rapport textuel montrant les principales métriques de classification. Voici un petit exemple avec des `target_names` personnalisées et des étiquettes inférées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.67      1.00      0.80         2\n",
      "     class 1       0.00      0.00      0.00         1\n",
      "     class 2       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.56      0.50      0.49         5\n",
      "weighted avg       0.67      0.60      0.59         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true = [0, 1, 2, 2, 0]\n",
    "y_pred = [0, 0, 2, 1, 0]\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "#               precision    recall  f1-score   support\n",
    "# \n",
    "#      class 0       0.67      1.00      0.80         2\n",
    "#      class 1       0.00      0.00      0.00         1\n",
    "#      class 2       1.00      0.50      0.67         2\n",
    "# \n",
    "#     accuracy                           0.60         5\n",
    "#    macro avg       0.56      0.50      0.49         5\n",
    "# weighted avg       0.67      0.60      0.59         5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemples\n",
    "\n",
    "##### [**Reconnaissance de chiffres manuscrits**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/classification/plot_digits_classification.ipynb)<br/>([_Recognizing hand-written digits_](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html))\n",
    "\n",
    "Exemple d'utilisation du rapport de classification pour des chiffres manuscrits.\n",
    "\n",
    "##### [**Stratégie personnalisée de réajustement d'une recherche en grille avec validation croisée**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/3_model_selection/plot_grid_search_digits.ipynb)<br/>([_Custom refit strategy of a grid search with cross-validation_](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html))\n",
    "\n",
    "Exemple d'utilisation du rapport de classification pour une recherche en grille avec validation croisée imbriquée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='hamming-loss'></a> 3.3.2.8. Perte de Hamming\n",
    "\n",
    "La fonction [**`hamming_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html#sklearn.metrics.hamming_loss) calcule la perte de Hamming moyenne ou [**distance de Hamming**](https://en.wikipedia.org/wiki/Hamming_distance) entre deux ensembles d'échantillons.\n",
    "\n",
    "Si $\\hat{y}_{i,j}$ est la valeur prédite pour la $j$-ème étiquette d'un échantillon donné $i$, $y_{i,j}$ est la valeur correspondante vraie, $n_\\text{samples}$ est le nombre d'échantillons et $n_\\text{labels}$ est le nombre d'étiquettes, alors la perte de Hamming $L_{Hamming}$ est définie comme suit :\n",
    "\n",
    "$$L_{Hamming}(y, \\hat{y}) = \\frac{1}{n_\\text{samples} * n_\\text{labels}} \\sum_{i=0}^{n_\\text{samples}-1} \\sum_{j=0}^{n_\\text{labels} - 1} 1(\\hat{y}_{i,j} \\not= y_{i,j})$$\n",
    "\n",
    "où $1(x)$ est la [**fonction indicatrice**](https://en.wikipedia.org/wiki/Indicator_function).\n",
    "\n",
    "L'équation ci-dessus ne s'applique pas au cas de classification multiclasse. Veuillez vous référer à la note ci-dessous pour plus d'informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "y_pred = [1, 2, 3, 4]\n",
    "y_true = [2, 2, 3, 4]\n",
    "hamming_loss(y_true, y_pred)\n",
    "# 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas multilabel avec des indicateurs d'étiquettes binaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))\n",
    "# 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note :** En classification multiclasse, la perte de Hamming correspond à la distance de Hamming entre `y_true` et `y_pred`, ce qui est similaire à la fonction de perte [**Zero one loss** (3.3.2.17)](https://scikit-learn.org/stable/modules/model_evaluation.html#zero-one-loss). Cependant, alors que la perte zero-one pénalise les ensembles de prédictions qui ne correspondent pas strictement aux ensembles réels, la perte de Hamming pénalise les étiquettes individuelles. Ainsi, la perte de Hamming, bornée par la perte zero-one, est toujours comprise entre zéro et un, inclusivement ; et prédire un sous-ensemble ou un sur-ensemble approprié des étiquettes réelles donnera une perte de Hamming comprise entre zéro et un, exclusivement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='hamming-loss'></a> 3.3.2.9. Précision, rappel et scores F\n",
    "\n",
    "De manière intuitive, la [**précision**](https://fr.wikipedia.org/wiki/Pr%C3%A9cision_et_rappel) représente la capacité du classifieur à ne pas étiqueter comme positive un échantillon qui est négatif, et le rappel représente la capacité du classifieur à trouver tous les échantillons positifs.\n",
    "\n",
    "Le [**score F**](https://fr.wikipedia.org/wiki/F-mesure) ($F_\\beta$ et $F_1$) peut être interprété comme une moyenne harmonique pondérée de la précision et du rappel. Un score F atteint sa meilleure valeur à 1 et son pire score à 0. Avec $\\beta = 1$, le score F et le score F1 sont équivalents, et le rappel et la précision sont tout aussi importants.\n",
    "\n",
    "La fonction [**`precision_recall_curve`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html) calcule une courbe de précision-rappel à partir des étiquettes de vérité terrain et d'un score donné par le classifieur en faisant varier un seuil de décision.\n",
    "\n",
    "La fonction [**`average_precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html) calcule la [**précision moyenne**](https://fr.wikipedia.org/w/index.php?title=Information_retrieval&oldid=793358396#Average_precision) (AP) à partir des scores de prédiction. La valeur est comprise entre 0 et 1, et plus elle est élevée, mieux c'est. L'AP est définie comme suit :\n",
    "\n",
    "$$\\text{AP} = \\sum_n (R_n - R_{n-1}) P_n$$\n",
    "\n",
    "où $P_n$ et $R_n$ sont la précision et le rappel au n-ième seuil. Avec des prédictions aléatoires, l'AP est la fraction d'échantillons positifs.\n",
    "\n",
    "Les références [Manning2008] et [Everingham2010] présentent des variantes alternatives de l'AP qui permettent d'interpoler la courbe précision-rappel. Actuellement, [**`average_precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html) n'implémente aucune variante interpolée. Les références [Davis2006] et [Flach2015] expliquent pourquoi une interpolation linéaire des points sur la courbe précision-rappel donne une mesure excessivement optimiste des performances du classifieur. Cette interpolation linéaire est utilisée lors du calcul de l'aire sous la courbe avec la règle du trapèze dans la fonction [**`auc`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html).\n",
    "\n",
    "Plusieurs fonctions vous permettent d'analyser la précision, le rappel et le score F :\n",
    "\n",
    "| Fonction                                | Description                                                                                |\n",
    "|:----------------------------------------|:-------------------------------------------------------------------------------------------|\n",
    "| [**`average_precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html)(y_true, y_score, *) | Calcule la précision moyenne (AP) à partir des scores de prédiction. |\n",
    "| [**`f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)(y_true, y_pred, *[, labels, ...]) | Calcule le score F1, également connu sous le nom de F-score équilibré ou F-mesure. |\n",
    "| [**`fbeta_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html)(y_true, y_pred, *, beta[, ...]) | Calcule le score F-beta. |\n",
    "| [**`precision_recall_curve`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html)(y_true, probas_pred, *) | Calcule les paires précision-rappel pour différents seuils de probabilité. |\n",
    "| [**`precision_recall_fscore_support`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html) (y_true, ...) | Calcule la précision, le rappel, le score F et le support pour chaque classe. |\n",
    "| [**`precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)(y_true, y_pred, *[, labels, ...]) | Calcule la précision. |\n",
    "| [**`recall_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)(y_true, y_pred, *[, labels, ...]) | Calcule le rappel. |\n",
    "\n",
    "Notez que la fonction [**`precision_recall_curve`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html) est limitée au cas binaire. La fonction [**`average_precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html) prend en charge les formats multiclasse et multilabel en calculant le score de chaque classe de manière \"One-vs-the-rest\" (OvR) et en les moyennant ou non en fonction de la valeur de l'argument `average`. \n",
    "\n",
    "Les fonctions `PredictionRecallDisplay.from_estimator` et `PredictionRecallDisplay.from_predictions` afficheront la courbe précision-rappel comme suit :\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_precision_recall_001.png)\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Stratégie personnalisée de réajustement d'une recherche en grille avec validation croisée**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/3_model_selection/plot_grid_search_digits.ipynb)<br/>([_Custom refit strategy of a grid search with cross-validation_](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html))\n",
    "\n",
    "Exemple d'utilisation des fonctions [**`precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) et [**`recall_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html) pour estimer les paramètres à l'aide d'une recherche par grille avec validation croisée imbriquée.\n",
    "\n",
    "##### [**Précision-Rappel**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/3_model_selection/plot_precision_recall.ipynb)<br/>([_Precision-Recall_](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html))\n",
    "\n",
    "Exemple d'utilisation de la fonction [**`precision_recall_curve`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html) pour évaluer la qualité de la sortie du classifieur.\n",
    "\n",
    "#### Références\n",
    "\n",
    "Z 📚 [Manning2008] C.D. Manning, P. Raghavan, H. Schütze, [**“Introduction to Information Retrieval”**](https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf), 2009.\n",
    "\n",
    "Z 🔬 [Everingham2010] M. Everingham, L. Van Gool, C.K.I. Williams, J. Winn, A. Zisserman, [**“The Pascal Visual Object Classes (VOC) Challenge”**](https://www.pure.ed.ac.uk/ws/files/20017166/ijcv_voc14.pdf), IJCV 2010.\n",
    "\n",
    "Z 🔬 [Davis2006] J. Davis, M. Goadrich, [**“The Relationship Between Precision-Recall and ROC Curves”**](https://www.biostat.wisc.edu/~page/rocpr.pdf), ICML 2006.\n",
    "\n",
    "Z 🔬 [Flach2015] P.A. Flach, M. Kull, [**“Precision-Recall-Gain Curves: PR Analysis Done Right”**](https://proceedings.neurips.cc/paper_files/paper/2015/file/33e8075e9970de0cfea955afd4644bb2-Paper.pdf), NIPS 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='binary-classification'></a> Classification binaire\n",
    "\n",
    "Dans une tâche de classification binaire, les termes \"positif\" et \"négatif\" font référence à la prédiction du classifieur, tandis que les termes \"vrai\" et \"faux\" font référence à la correspondance de cette prédiction avec le jugement externe (parfois appelé \"observation\"). Avec ces définitions, nous pouvons formuler le tableau suivant :\n",
    "\n",
    "<table>\n",
    "<tbody>\n",
    "<tr><td></td>\n",
    "<td colspan=\"2\"><p>Classe réelle (observation)</p></td>\n",
    "</tr>\n",
    "<tr><td rowspan=\"2\"><p>Classe prédite\n",
    "(expectation)</p></td>\n",
    "<td><p>tp (vrai positif)\n",
    "Résultat correct</p></td>\n",
    "<td><p>fp (faux positif)\n",
    "Résultat inattendu</p></td>\n",
    "</tr>\n",
    "<tr><td><p>fn (faux négatif)\n",
    "Résultat manquant</p></td>\n",
    "<td><p>tn (vrai négatif)\n",
    "Absence de résultat correct</p></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "Dans ce contexte, nous pouvons définir les notions de précision, rappel et F-mesure :\n",
    "\n",
    "$$P = \\frac{tp}{tp + fp},$$\n",
    "$$R = \\frac{tp}{tp + fn},$$\n",
    "$$F_\\beta = (1 + \\beta^2) \\frac{P \\times R}{\\beta^2 P + R}.$$\n",
    "\n",
    "Parfois, le rappel est également appelé \"sensibilité\".\n",
    "\n",
    "Voici quelques petits exemples de classification binaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333333"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "y_pred = [0, 1, 0, 0]\n",
    "y_true = [0, 1, 0, 1]\n",
    "metrics.precision_score(y_true, y_pred)\n",
    "# 1.0\n",
    "metrics.recall_score(y_true, y_pred)\n",
    "# 0.5\n",
    "metrics.f1_score(y_true, y_pred)\n",
    "# 0.66...\n",
    "metrics.fbeta_score(y_true, y_pred, beta=0.5)\n",
    "# 0.83...\n",
    "metrics.fbeta_score(y_true, y_pred, beta=1)\n",
    "# 0.66...\n",
    "metrics.fbeta_score(y_true, y_pred, beta=2)\n",
    "# 0.55...\n",
    "metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5)\n",
    "# (array([0.66..., 1.        ]), array([1. , 0.5]), array([0.71..., 0.83...]), array([2, 2]))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "y_true = np.array([0, 0, 1, 1])\n",
    "y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "precision, recall, threshold = precision_recall_curve(y_true, y_scores)\n",
    "precision\n",
    "# array([0.5       , 0.66..., 0.5       , 1.        , 1.        ])\n",
    "recall\n",
    "# array([1. , 1. , 0.5, 0.5, 0. ])\n",
    "threshold\n",
    "# array([0.1 , 0.35, 0.4 , 0.8 ])\n",
    "average_precision_score(y_true, y_scores)\n",
    "# 0.83..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='multiclass-and-multilabel-classification'></a> Classification multiclasse et multilabel\n",
    "\n",
    "Dans une tâche de classification multiclasse et multilabel, les notions de précision, de rappel et de F-mesure peuvent être appliquées à chaque étiquette indépendamment. Il existe quelques façons de combiner les résultats entre les étiquettes, spécifiées par l'argument `average` des fonctions [**`average_precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html), [**`f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html), [**`fbeta_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html), [**`precision_recall_fscore_support`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html), [**`precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) et [**`recall_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html), comme décrit ci-dessus. Notez que si toutes les étiquettes sont incluses, le \"micro-averaging\" dans un contexte multiclasse produira des valeurs de précision, de rappel et de F qui sont toutes identiques à l'exactitude (accuracy). Notez également que le \"weighted-averaging\" peut produire un score F qui n'est pas nécessairement compris entre la précision et le rappel.\n",
    "\n",
    "Pour clarifier cela, considérons la notation suivante :\n",
    "\n",
    "- $y$ est l'ensemble des paires $(échantillon, étiquette)$ réelles\n",
    "- $\\hat{y}$ est l'ensemble des paires $(échantillon, étiquette)$ prédites\n",
    "- $L$ est l'ensemble des étiquettes\n",
    "- $S$ est l'ensemble des échantillons\n",
    "- $y_s$ est le sous-ensemble de $y$ contenant l'échantillon $s$, c'est-à-dire $y_s := \\left\\{(s', l) \\in y | s' = s\\right\\}$\n",
    "- $y_l$ est le sous-ensemble de $y$ contenant l'étiquette $l$\n",
    "- de même, $\\hat{y}_s$ et $\\hat{y}_l$ sont des sous-ensembles de $\\hat{y}$\n",
    "- $P(A, B) := \\frac{\\left \\vert A \\cap B \\right \\vert}{\\left \\vert B \\right \\vert}$ pour certains ensembles $A$ et $B$\n",
    "- $R(A, B) := \\frac{\\left \\vert A \\cap B \\right \\vert}{\\left \\vert A \\right \\vert}$ (Les conventions peuvent varier sur le traitement de $A = \\emptyset$ ; cette implémentation utilise $R(A, B):=0$, et de manière similaire pour $P$.)\n",
    "- $F_\\beta(A, B) := \\left(1 + \\beta^2\\right) \\frac{P(A, B) \\times R(A, B)}{\\beta^2 P(A, B) + R(A, B)}$\n",
    "\n",
    "Les métriques sont ensuite définies comme suit :\n",
    "\n",
    "| Moyenne | Précision | Rappel | F_beta |\n",
    "|-|-|-|-|\n",
    "|`\"micro\"`|$P(y, \\hat{y})$|$R(y, \\hat{y})$|$F_\\beta(y, \\hat{y})$|\n",
    "|`\"samples\"`|$\\frac{1}{\\left \\vert S \\right \\vert} \\sum_{s \\in S} P(y_s, \\hat{y}_s)$|$\\frac{1}{\\left \\vert S \\right \\vert} \\sum_{s \\in S} R(y_s, \\hat{y}_s)$|$\\frac{1}{\\left \\vert S \\right \\vert} \\sum_{s \\in S} F_\\beta(y_s, \\hat{y}_s)$|\n",
    "|`\"macro\"`|$\\frac{1}{\\left \\vert L \\right \\vert} \\sum_{l \\in L} P(y_l, \\hat{y}_l)$|$\\frac{1}{\\left \\vert L \\right \\vert} \\sum_{l \\in L} R(y_l, \\hat{y}_l)$|$\\frac{1}{\\left \\vert L \\right \\vert} \\sum_{l \\in L} F_\\beta(y_l, \\hat{y}_l)$|\n",
    "|`\"weighted\"`|$\\frac{1}{\\sum_{l \\in L} \\left \\vert y_l \\right \\vert} \\sum_{l \\in L} \\left \\vert y_l \\right \\vert P(y_l, \\hat{y}_l)$|$\\frac{1}{\\sum_{l \\in L} \\left \\vert y_l \\right \\vert} \\sum_{l \\in L} \\left \\vert y_l \\right \\vert R(y_l, \\hat{y}_l)$|$\\frac{1}{\\sum_{l \\in L} \\left \\vert y_l \\right \\vert} \\sum_{l \\in L} \\left \\vert y_l \\right \\vert F_\\beta(y_l, \\hat{y}_l)$|\n",
    "|`None`|$\\langle P(y_l, \\hat{y}_l) \\vert l \\in L \\rangle$|$\\langle R(y_l, \\hat{y}_l) \\vert l \\in L \\rangle$|$\\langle F_\\beta(y_l, \\hat{y}_l) \\vert l \\in L \\rangle$|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.66666667, 0.        , 0.        ]),\n",
       " array([1., 0., 0.]),\n",
       " array([0.71428571, 0.        , 0.        ]),\n",
       " array([2, 2, 2], dtype=int64))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "y_true = [0, 1, 2, 0, 1, 2]\n",
    "y_pred = [0, 2, 1, 0, 0, 1]\n",
    "metrics.precision_score(y_true, y_pred, average='macro')\n",
    "# 0.22...\n",
    "metrics.recall_score(y_true, y_pred, average='micro')\n",
    "# 0.33...\n",
    "metrics.f1_score(y_true, y_pred, average='weighted')\n",
    "# 0.26...\n",
    "metrics.fbeta_score(y_true, y_pred, average='macro', beta=0.5)\n",
    "# 0.23...\n",
    "metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5, average=None)\n",
    "# (array([0.66..., 0.        , 0.        ]), array([1., 0., 0.]), array([0.71..., 0.        , 0.        ]), array([2, 2, 2]...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la classification multiclasse avec une \"classe négative\", il est possible d'exclure certaines étiquettes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.recall_score(y_true, y_pred, labels=[1, 2], average='micro')\n",
    "# excluding 0, no labels were correctly recalled\n",
    "# 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De même, les étiquettes non présentes dans l'échantillon de données peuvent être prises en compte dans l'approche de la macro-moyenne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_score(y_true, y_pred, labels=[0, 1, 2, 3], average='macro')\n",
    "# 0.166..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
