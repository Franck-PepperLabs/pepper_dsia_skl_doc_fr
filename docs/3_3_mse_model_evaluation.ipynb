{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='model-selection-and-evaluation'></a> 3. [**Sélection et évaluation de modèle**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/3_model_selection_and_evaluation.ipynb#model-selection-and-evaluation)</br>([*Model selection and evaluation*](https://scikit-learn.org/stable/model_selection.html#model-selection-and-evaluation))\n",
    "\n",
    "# 3.3. [**Métriques et scoring : quantifier la qualité des prédictions**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/3_model_selection_and_evaluation.ipynb#metrics-and-scoring-quantifying-the-quality-of-predictions)<br/>([_Metrics and scoring: quantifying the quality of predictions_](https://scikit-learn.org/stable/model_selection.html#metrics-and-scoring-quantifying-the-quality-of-predictions))\n",
    "\n",
    "Il existe 3 API différents pour évaluer la qualité des prédictions d'un modèle :\n",
    "\n",
    "- **Méthode de score de l'estimateur :** Les estimateurs possèdent une méthode `score` fournissant un critère d'évaluation par défaut pour le problème auquel ils sont conçus pour résoudre. Cela n'est pas discuté sur cette page, mais dans la documentation de chaque estimateur.\n",
    "\n",
    "- **Paramètre `scoring` :** Les outils d'évaluation du modèle utilisant la [**validation croisée** (3.1)](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) (comme [**`model_selection.cross_val_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) et [**`model_selection.GridSearchCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)) reposent sur une stratégie de score interne. Cela est expliqué dans la section [**Paramètre `scoring` : définir les règles d'évaluation du modèle** (3.3.1)](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter).\n",
    "\n",
    "- **Fonctions de métriques :** Le module [**`sklearn.metrics`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) implémente des fonctions permettant d'évaluer l'erreur de prédiction à des fins spécifiques. Ces métriques sont détaillées dans les sections sur les [**Métriques de classification** (3.3.2)](https://scikit-learn.org/stable/modules/model_evaluation.html), les [**Métriques de classement multi-étiquettes** (3.3.3)](https://scikit-learn.org/stable/modules/model_evaluation.html#multilabel-ranking-metrics), les [**Métriques de régression** (3.3.4)](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics) et les [**Métriques de regroupement** (3.3.5)](https://scikit-learn.org/stable/modules/model_evaluation.html#clustering-metrics).\n",
    "\n",
    "Enfin, les [**Estimateurs factices** (3.3.6)](https://scikit-learn.org/stable/modules/model_evaluation.html#dummy-estimators) sont utiles pour obtenir une valeur de référence pour ces métriques pour des prédictions aléatoires.\n",
    "\n",
    "> **Voir aussi :** Pour les métriques \"pair à pair\", entre les _échantillons_ et non les estimateurs ou les prédictions, voir la section [**Métriques pair à pair, affinités et noyaux** (6.8)](https://scikit-learn.org/stable/modules/metrics.html#metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 63 pages, 19 exemples, 24 papiers\n",
    "- 3.3.1. [**Le paramètre `scoring` : définir les règles d'évaluation du modèle**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/3_model_selection_and_evaluation.ipynb#the-scoring-parameter-defining-model-evaluation-rules)<br/>([_The `scoring` parameter: defining model evaluation rules_](https://scikit-learn.org/stable/model_selection.html#the-scoring-parameter-defining-model-evaluation-rules))\n",
    "- 3.3.2. [**Métriques de classification**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/3_model_selection_and_evaluation.ipynb#classification-metrics)<br/>([_Classification metrics_](https://scikit-learn.org/stable/model_selection.html#classification-metrics))\n",
    "- 3.3.3. [**Métriques de classement multi-étiquettes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/3_model_selection_and_evaluation.ipynb#multilabel-ranking-metrics)<br/>([_Multilabel ranking metrics_](https://scikit-learn.org/stable/model_selection.html#multilabel-ranking-metrics))\n",
    "- 3.3.4. [**Métriques de régression**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/3_model_selection_and_evaluation.ipynb#regression-metrics)<br/>([_Regression metrics_](https://scikit-learn.org/stable/model_selection.html#regression-metrics))\n",
    "- 3.3.5. [**Métriques de clustering**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/3_model_selection_and_evaluation.ipynb#clustering-metrics)<br/>([_Clustering metrics_](https://scikit-learn.org/stable/model_selection.html#clustering-metrics))\n",
    "- 3.3.6. [**Estimateurs factices**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/3_model_selection_and_evaluation.ipynb#dummy-estimators)<br/>([_Dummy estimators_](https://scikit-learn.org/stable/model_selection.html#dummy-estimators))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='the-scoring-parameter-defining-model-evaluation-rules'></a> 3.3.1. Le paramètre `scoring` : définir les règles d'évaluation du modèle\n",
    "\n",
    "La sélection et l'évaluation des modèles à l'aide d'outils tels que [**`model_selection.GridSearchCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) et [**`model_selection.cross_val_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score) nécessitent un paramètre `scoring` qui contrôle la métrique appliquée aux estimateurs évalués.\n",
    "\n",
    "### <a id='common-cases-predefined-values'></a> 3.3.1.1. Cas courants : valeurs prédéfinies\n",
    "\n",
    "Pour les cas d'utilisation les plus courants, vous pouvez désigner un objet de métrique avec le paramètre `scoring`. Le tableau ci-dessous montre toutes les valeurs possibles. Tous les objets de métrique suivent la convention selon laquelle **des valeurs de retour plus élevées sont meilleures que des valeurs de retour plus basses**. Ainsi, les métriques qui mesurent la distance entre le modèle et les données, comme [**`metrics.mean_squared_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html), sont disponibles sous la forme `neg_mean_squared_error`, qui renvoie la valeur négative de la métrique.\n",
    "\n",
    "| Métrique                             | Fonction                                                        | Commentaire                 |\n",
    "|:-------------------------------------|:----------------------------------------------------------------|:----------------------------|\n",
    "| **Classification**                   |                                                                 |                             |\n",
    "| ‘accuracy’                           | [**`metrics.accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)                |                             |\n",
    "| ‘balanced_accuracy’                  | [**`metrics.balanced_accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html)       |                             |\n",
    "| ‘top_k_accuracy’                     | [**`metrics.top_k_accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.top_k_accuracy_score.html)             |                             |\n",
    "| ‘average_precision’                  | [**`metrics.average_precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html)       |                             |\n",
    "| ‘neg_brier_score’                    | [**`metrics.brier_score_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html)                    |                             |\n",
    "| ‘f1’                                 | [**`metrics.f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)                                  | pour les cibles binaires   |\n",
    "| ‘f1_micro’                           | [**`metrics.f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)                                  | micro-average              |\n",
    "| ‘f1_macro’                           | [**`metrics.f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)                                  | macro-average              |\n",
    "| ‘f1_weighted’                        | [**`metrics.f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)                                  | moyenne pondérée           |\n",
    "| ‘f1_samples’                         | [**`metrics.f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)                                  | par échantillon multilabel |\n",
    "| ‘neg_log_loss’                       | [**`metrics.log_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html)                                  | nécessite le support de predict_proba|\n",
    "| ‘precision’ etc.                     | [**`metrics.precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)                    | suffixes comme ‘f1’        |\n",
    "| ‘recall’ etc.                        | [**`metrics.recall_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)                          | suffixes comme ‘f1’        |\n",
    "| ‘jaccard’ etc.                       | [**`metrics.jaccard_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html)                         | suffixes comme ‘f1’        |\n",
    "| ‘roc_auc’                            | [**`metrics.roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)                           |                           |\n",
    "| ‘roc_auc_ovr’                        | [**`metrics.roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)                           |                           |\n",
    "| ‘roc_auc_ovo’                        | [**`metrics.roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)                           |                           |\n",
    "| ‘roc_auc_ovr_weighted’               | [**`metrics.roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)                           |                           |\n",
    "| ‘roc_auc_ovo_weighted’               | [**`metrics.roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)                           |                           |\n",
    "| **Clustering**                       |                                                                |                           |\n",
    "| ‘adjusted_mutual_info_score’         | [**`metrics.adjusted_mutual_info_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html) |                           |\n",
    "| ‘adjusted_rand_score’                | [**`metrics.adjusted_rand_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html)           |                           |\n",
    "| ‘completeness_score’                 | [**`metrics.completeness_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.completeness_score.html)             |                           |\n",
    "| ‘fowlkes_mallows_score’              | [**`metrics.fowlkes_mallows_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fowlkes_mallows_score.html)       |                           |\n",
    "| ‘homogeneity_score’                  | [**`metrics.homogeneity_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html)               |                           |\n",
    "| ‘mutual_info_score’                  | [**`metrics.mutual_info_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mutual_info_score.html)               |                           |\n",
    "| ‘normalized_mutual_info_score’       | [**`metrics.normalized_mutual_info_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html) |                           |\n",
    "| ‘rand_score’                         | [**`metrics.rand_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.rand_score.html)                               |                           |\n",
    "| ‘v_measure_score’                    | [**`metrics.v_measure_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html)                     |                           |\n",
    "| **Regression**                       |                                                                |                           |\n",
    "| ‘explained_variance’                 | [**`metrics.explained_variance_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.explained_variance_score.html)   |                           |\n",
    "| ‘max_error’                          | [**`metrics.max_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.max_error.html)                                 |                           |\n",
    "| ‘neg_mean_absolute_error’            | [**`metrics.mean_absolute_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html)           |                           |\n",
    "| ‘neg_mean_squared_error’             | [**`metrics.mean_squared_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)             |                           |\n",
    "| ‘neg_root_mean_squared_error’        | [**`metrics.mean_squared_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)             |                           |\n",
    "| ‘neg_mean_squared_log_error’         | [**`metrics.mean_squared_log_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_log_error.html)     |                           |\n",
    "| ‘neg_median_absolute_error’          | [**`metrics.median_absolute_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.median_absolute_error.html)       |                           |\n",
    "| ‘r2’                                 | [**`metrics.r2_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)                                   |                           |\n",
    "| ‘neg_mean_poisson_deviance’          | [**`metrics.mean_poisson_deviance`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_poisson_deviance.html)       |                           |\n",
    "| ‘neg_mean_gamma_deviance’            | [**`metrics.mean_gamma_deviance`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_gamma_deviance.html)           |                           |\n",
    "| ‘neg_mean_absolute_percentage_error’ | [**`metrics.mean_absolute_percentage_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_percentage_error.html) |                           |\n",
    "| ‘d2_absolute_error_score’            | [**`metrics.d2_absolute_error_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.d2_absolute_error_score.html)       |                           |\n",
    "| ‘d2_pinball_score’                   | [**`metrics.d2_pinball_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.d2_pinball_score.html)                 |                           |\n",
    "| ‘d2_tweedie_score’                   | [**`metrics.d2_tweedie_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.d2_tweedie_score.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemples d'utilisation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96666667, 0.96666667, 0.96666667, 0.93333333, 1.        ])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "clf = svm.SVC(random_state=0)\n",
    "cross_val_score(clf, X, y, cv=5, scoring='recall_macro')\n",
    "# array([0.96..., 0.96..., 0.96..., 0.93..., 1.        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** Si un nom de métrique incorrect est passé, une `InvalidParameterError` est levée. Vous pouvez récupérer les noms de toutes les métriques disponibles en appelant [**`get_scorer_names`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.get_scorer_names.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='defining-your-scoring-strategy-from-metric-functions'></a> 3.3.1.2. Définir votre stratégie d'évaluation à partir de fonctions de métriques\n",
    "\n",
    "Le module [**`sklearn.metrics`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) expose également un ensemble de fonctions simples mesurant l'erreur de prédiction en fonction des vérités terrain (ground truth) et des prédictions :\n",
    "- Les fonctions se terminant par `_score` renvoient une valeur à maximiser, donc plus élevée est meilleure.\n",
    "- Les fonctions se terminant par `_error` ou `_loss` renvoient une valeur à minimiser, donc plus faible est meilleure. Lors de la conversion en objet de métrique en utilisant [**`make_scorer`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html), vous pouvez fixer le paramètre `greater_is_better` à `False` (`True` par défaut ; voir la description du paramètre ci-dessous).\n",
    "\n",
    "Les métriques disponibles pour diverses tâches d'apprentissage automatique sont détaillées dans les sections ci-dessous.\n",
    "\n",
    "De nombreuses métriques n'ont pas de noms spécifiques à utiliser comme valeurs de `scoring`, parfois parce qu'elles nécessitent des paramètres supplémentaires, tels que [**`fbeta_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html). Dans de tels cas, vous devez générer un objet de métrique approprié. La façon la plus simple de générer un objet callable pour la métrique est d'utiliser [**`make_scorer`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html). Cette fonction convertit les métriques en objets callable pouvant être utilisés pour l'évaluation du modèle.\n",
    "\n",
    "Un cas d'utilisation typique consiste à envelopper une fonction de métrique existante de la bibliothèque avec des valeurs autres que celles par défaut pour ses paramètres, tels que le paramètre `beta` pour la fonction [**`fbeta_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "grid = GridSearchCV(LinearSVC(dual=\"auto\"), param_grid={'C': [1, 10]},\n",
    "                    scoring=ftwo_scorer, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le deuxième cas d'utilisation consiste à créer un objet de métrique complètement personnalisé à partir d'une simple fonction Python en utilisant [**`make_scorer`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html), qui peut prendre plusieurs paramètres :\n",
    "- la fonction Python que vous souhaitez utiliser (`my_custom_loss_func` dans l'exemple ci-dessous)\n",
    "- si la fonction Python renvoie un score (`greater_is_better=True`, la valeur par défaut) ou une perte (`greater_is_better=False`). S'il s'agit d'une perte, la sortie de la fonction Python est inversée par l'objet de métrique, conformément à la convention de validation croisée selon laquelle les métriques renvoient des valeurs plus élevées pour les meilleurs modèles.\n",
    "- pour les métriques de classification uniquement : si la fonction Python que vous avez fournie nécessite des certitudes de décision continues (`needs_threshold=True`). La valeur par défaut est False.\n",
    "- tous les paramètres supplémentaires, tels que `beta` ou `labels` dans [**`f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score).\n",
    "\n",
    "Voici un exemple de création de métriques personnalisées et d'utilisation du paramètre `greater_is_better` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6931471805599453"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def my_custom_loss_func(y_true, y_pred):\n",
    "    diff = np.abs(y_true - y_pred).max()\n",
    "    return np.log1p(diff)\n",
    "\n",
    "# score will negate the return value of my_custom_loss_func,\n",
    "# which will be np.log(2), 0.693, given the values for X\n",
    "# and y defined below.\n",
    "score = make_scorer(my_custom_loss_func, greater_is_better=False)\n",
    "X = [[1], [1]]\n",
    "y = [0, 1]\n",
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier(strategy='most_frequent', random_state=0)\n",
    "clf = clf.fit(X, y)\n",
    "my_custom_loss_func(y, clf.predict(X))\n",
    "# 0.69...\n",
    "score(clf, X, y)\n",
    "# -0.69..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='implementing-your-own-scoring-object'></a> 3.3.1.2. Implémentation de votre propre objet de métrique\n",
    "\n",
    "Vous pouvez créer des métriques de modèle encore plus flexibles en construisant votre propre objet de métrique à partir de zéro, sans utiliser la fabrique [**`make_scorer`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html).\n",
    "\n",
    "**Comment créer une métrique à partir de zéro**\n",
    "\n",
    "Pour qu'un callable soit une métrique, il doit respecter le protocole spécifié par les deux règles suivantes :\n",
    "- Il peut être appelé avec les paramètres `(estimator, X, y)`, où `estimator` est le modèle qui doit être évalué, `X` est la donnée de validation et `y` est la vérité terrain pour `X` (dans le cas supervisé) ou `None` (dans le cas non supervisé).\n",
    "- Il doit renvoyer un nombre à virgule flottante qui quantifie la qualité de prédiction de `estimator` sur `X`, par rapport à `y`. Encore une fois, selon la convention, les nombres plus élevés sont meilleurs. Donc, si votre métrique renvoie une perte, cette valeur doit être négative.\n",
    "- **Avancé** : Si cela nécessite des métadonnées supplémentaires à lui passer, il doit exposer une méthode `get_metadata_routing` renvoyant les métadonnées demandées. L'utilisateur doit pouvoir définir les métadonnées demandées via une méthode `set_score_request`. Veuillez consulter [**Routage de métadonnées** (User Guide _exp)](https://scikit-learn.org/stable/metadata_routing.html#metadata-routing) et [**Routage de métadonnées** (Developer Guide _exp)](https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_metadata_routing.html#sphx-glr-auto-examples-miscellaneous-plot-metadata-routing-py) pour plus de détails.\n",
    "\n",
    "> **Note:** **Utilisation de métriques personnalisées dans des fonctions avec `n_jobs > 1`**\n",
    ">\n",
    "> Bien que la définition de la fonction de métrique personnalisée aux côtés de la fonction d'appel fonctionne par défaut avec le backend joblib (loky), l'importer à partir d'un autre module sera une approche plus robuste et fonctionnera indépendamment du backend joblib.\n",
    ">\n",
    "> Par exemple, pour utiliser `n_jobs` supérieur à 1 dans l'exemple ci-dessous, la fonction `custom_scoring_function` est sauvegardée dans un module créé par l'utilisateur (`custom_scorer_module.py`) et importée :\n",
    "> ```python\n",
    "> >>> from custom_scorer_module import custom_scoring_function \n",
    "> >>> cross_val_score(model,\n",
    "> ...  X_train,\n",
    "> ...  y_train,\n",
    "> ...  scoring=make_scorer(custom_scoring_function, greater_is_better=False),\n",
    "> ...  cv=5,\n",
    "> ...  n_jobs=-1) \n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='using-multiple-metric-evaluation'></a> 3.3.1.4. Utilisation de plusieurs métriques d'évaluation\n",
    "\n",
    "Scikit-learn permet également l'évaluation de plusieurs métriques dans `GridSearchCV`, `RandomizedSearchCV` et `cross_validate`.\n",
    "\n",
    "Il existe trois façons de spécifier plusieurs métriques `scoring` pour le paramètre `scoring` :\n",
    "\n",
    "#### Comme un ensemble de métriques sous forme de chaînes de caractères"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['accuracy', 'precision']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comme un `dict` associant le nom de l'évaluateur à la fonction de mesure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "           'prec': 'precision'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez que les valeurs du dictionnaire peuvent être des fonctions de mesure ou l'une des chaînes prédéfinies de métriques.\n",
    "\n",
    "#### Comme une fonction appelable qui renvoie un dictionnaire de scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10  9  8  7  8]\n",
      "[0 1 2 3 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# A sample toy binary classification dataset\n",
    "X, y = datasets.make_classification(n_classes=2, random_state=0)\n",
    "svm = LinearSVC(dual=\"auto\", random_state=0)\n",
    "def confusion_matrix_scorer(clf, X, y):\n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    return {'tn': cm[0, 0], 'fp': cm[0, 1],\n",
    "            'fn': cm[1, 0], 'tp': cm[1, 1]}\n",
    "cv_results = cross_validate(svm, X, y, cv=5,\n",
    "                            scoring=confusion_matrix_scorer)\n",
    "# Getting the test set true positive scores\n",
    "print(cv_results['test_tp'])\n",
    "# [10  9  8  7  8]\n",
    "# Getting the test set false negative scores\n",
    "print(cv_results['test_fn'])\n",
    "# [0 1 2 3 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='classification-metrics'></a> 3.3.2. Métriques de classification\n",
    "\n",
    "Le module [**`sklearn.metrics`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) implémente plusieurs fonctions de perte, de score et d'utilité pour mesurer les performances de classification. Certaines métriques peuvent nécessiter des estimations de probabilité de la classe positive, des valeurs de confiance ou des valeurs de décision binaires. La plupart des implémentations permettent à chaque échantillon de contribuer de manière pondérée au score global, grâce au paramètre `sample_weight`.\n",
    "\n",
    "### Tableau 1: Métriques de classification pour les cas de classification binaire\n",
    "\n",
    "Certaines de ces métriques sont restreintes au cas de classification binaire :\n",
    "\n",
    "| Fonction                                | Description                                                                                |\n",
    "|:----------------------------------------|:-------------------------------------------------------------------------------------------|\n",
    "| [**`precision_recall_curve`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve)(y_true, probas_pred, *) | Calcule les couples précision-rappel pour différentes valeurs de seuil de probabilité. |\n",
    "| [**`roc_curve`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve)(y_true, y_score, *[, pos_label, ...]) | Calcule la courbe de caractéristique de fonctionnement du récepteur (ROC). |\n",
    "| [**`class_likelihood_ratios`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.class_likelihood_ratios.html#sklearn.metrics.class_likelihood_ratios)(y_true, y_pred, *[, ...]) | Calcule les rapports de probabilités positifs et négatifs de classification binaire. |\n",
    "| [**`det_curve`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.det_curve.html#sklearn.metrics.det_curve)(y_true, y_score[, pos_label, ...]) | Calcule les taux d'erreur pour différentes valeurs de seuil de probabilité. |\n",
    "\n",
    "### Tableau 2: Métriques de classification pour les cas de classification multiclasse\n",
    "\n",
    "D'autres fonctionnent également dans le cas de classification multiclasse :\n",
    "\n",
    "| Fonction                                | Description                                                                                |\n",
    "|:----------------------------------------|:-------------------------------------------------------------------------------------------|\n",
    "| [**`balanced_accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html)(y_true, y_pred, *[, ...]) | Calcule la précision équilibrée. |\n",
    "| [**`cohen_kappa_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html)(y1, y2, *[, labels, ...]) | Calcule le kappa de Cohen : une statistique mesurant l'accord entre annotateurs. |\n",
    "| [**`confusion_matrix`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)(y_true, y_pred, *[, ...]) | Calcule la matrice de confusion pour évaluer l'exactitude d'une classification. |\n",
    "| [**`hinge_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hinge_loss.html)(y_true, pred_decision, *[, ...]) | Perte de charnière moyenne (non régularisée). |\n",
    "| [**`matthews_corrcoef`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)(y_true, y_pred, *[, ...]) | Calcule le coefficient de corrélation de Matthews (MCC). |\n",
    "| [**`roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)(y_true, y_score, *[, average, ...]) | Calcule l'aire sous la courbe ROC (ROC AUC) à partir des scores de prédiction. |\n",
    "| [**`top_k_accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.top_k_accuracy_score.html)(y_true, y_score, *[, ...]) | Score de classification Top-k Accuracy. |\n",
    "\n",
    "### Tableau 3: Métriques de classification pour les cas de classification multilabel\n",
    "\n",
    "Certaines fonctionnent également dans le cas de classification multilabel :\n",
    "\n",
    "| Fonction                                | Description                                                                                |\n",
    "|:----------------------------------------|:-------------------------------------------------------------------------------------------|\n",
    "| [**`accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score)(y_true, y_pred, *[, ...]) | Score de classification Accuracy. |\n",
    "| [**`classification_report`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report)(y_true, y_pred, *[, ...]) | Génère un rapport texte présentant les principales métriques de classification. |\n",
    "| [**`f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score)(y_true, y_pred, *[, labels, ...]) | Calcule le score F1, également connu sous le nom de F-score équilibré ou mesure F. |\n",
    "| [**`fbeta_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score)(y_true, y_pred, *, beta[, ...]) | Calcule le score F-beta. |\n",
    "| [**`hamming_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html#sklearn.metrics.hamming_loss)(y_true, y_pred, *[, sample_weight]) | Calcule la perte de Hamming moyenne. |\n",
    "| [**`jaccard_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html#sklearn.metrics.jaccard_score)(y_true, y_pred, *[, labels, ...]) | Score de similarité de Jaccard. |\n",
    "| [**`log_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss)(y_true, y_pred, *[, eps, ...]) | Perte logarithmique, également appelée perte logistique ou perte d'entropie croisée. |\n",
    "| [**`multilabel_confusion_matrix`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html#sklearn.metrics.multilabel_confusion_matrix)(y_true, y_pred, *) | Calcule une matrice de confusion pour chaque classe ou échantillon. |\n",
    "| [**`precision_recall_fscore_support`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support)(y_true, ...) | Calcule la précision, le rappel, le score F et le support pour chaque classe. |\n",
    "| [**`precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)(y_true, y_pred, *[, labels, ...]) | Calcule la précision. |\n",
    "| [**`recall_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)(y_true, y_pred, *[, labels, ...]) | Calcule le rappel. |\n",
    "\n",
    "### Tableau 4: Métriques de classification pour les cas de classification binaire et multilabel (mais pas multiclasse)\n",
    "\n",
    "Et certaines fonctionnent pour les problèmes binaires et multilabel (mais pas multiclasse) :\n",
    "\n",
    "| Fonction                                | Description                                                                                |\n",
    "|:----------------------------------------|:-------------------------------------------------------------------------------------------|\n",
    "| [**`average_precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html)(y_true, y_score, *) | Calcule la précision moyenne (AP) à partir des scores de prédiction. |\n",
    "\n",
    "Dans les sous-sections suivantes, nous décrirons chacune de ces fonctions, précédées de quelques notes sur l'API commune et la définition des métriques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='classification-metrics'></a> 3.3.2.1. De la classification binaire à la classification multiclasse et multilabel\n",
    "\n",
    "Certaines métriques sont essentiellement définies pour les tâches de classification binaire (par exemple, [**`f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html), [**`roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)). Dans ces cas, par défaut, seule l'étiquette positive est évaluée, en supposant que la classe positive est étiquetée `1` (bien que cela puisse être configuré via le paramètre `pos_label`).\n",
    "\n",
    "Lors de l'extension d'une métrique binaire aux problèmes de classification multiclasse ou multilabel, les données sont traitées comme une collection de problèmes binaires, un pour chaque classe. Il existe alors plusieurs façons de calculer la moyenne des métriques binaires pour l'ensemble des classes, chacune pouvant être utile dans certains scénarios. Lorsque cela est disponible, vous devez sélectionner l'une de ces options à l'aide du paramètre `average`.\n",
    "\n",
    "- `\"macro\"` calcule simplement la moyenne des métriques binaires, en accordant un poids égal à chaque classe. Dans les problèmes où les classes peu fréquentes sont néanmoins importantes, le calcul macro-averaging peut mettre en évidence leurs performances. D'un autre côté, l'hypothèse selon laquelle toutes les classes sont également importantes est souvent fausse, de sorte que le macro-averaging surestimera généralement les performances généralement faibles d'une classe peu fréquente.\n",
    "- `\"weighted\"` prend en compte le déséquilibre de classe en calculant la moyenne des métriques binaires dans lesquelles le score de chaque classe est pondéré par sa présence dans l'échantillon de données réelles.\n",
    "- `\"micro\"` donne à chaque paire échantillon-classe une contribution égale à la métrique globale (à l'exception du poids de l'échantillon). Au lieu de sommer la métrique par classe, cela additionne les dividendes et les diviseurs qui composent les métriques par classe pour calculer un quotient global. Le micro-averaging peut être préféré dans les paramètres multilabel, y compris la classification multiclasse où une classe majoritaire doit être ignorée.\n",
    "- `\"samples\"` s'applique uniquement aux problèmes multilabel. Il ne calcule pas une mesure par classe, mais calcule plutôt la métrique sur les classes réelles et prédites pour chaque échantillon dans les données d'évaluation, et retourne leur moyenne (`sample_weight-weighted`).\n",
    "- La sélection de `average=None` renverra un tableau avec le score pour chaque classe.\n",
    "\n",
    "Alors que les données multiclasse sont fournies à la métrique, tout comme les cibles binaires, sous forme de tableau d'étiquettes de classe, les données multilabel sont spécifiées sous forme de matrice indicatrice, dans laquelle la cellule `[i, j]` a la valeur 1 si l'échantillon `i` a l'étiquette `j` et la valeur 0 sinon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='accuracy-score'></a> 3.3.2.2. Score d'exactitude (Accuracy score)\n",
    "\n",
    "La fonction [**`accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) calcule l'[**exactitude**](https://en.wikipedia.org/wiki/Accuracy_and_precision) soit en tant que fraction (par défaut), soit en tant que nombre (`normalize=False`) de prédictions correctes.\n",
    "\n",
    "Dans la classification multi-étiquette, la fonction renvoie l'exactitude du sous-ensemble. Si l'ensemble entier des étiquettes prédites pour un échantillon correspond strictement à l'ensemble réel des étiquettes, alors l'exactitude du sous-ensemble est de 1.0 ; sinon, elle est de 0.0.\n",
    "\n",
    "Si $\\hat{y}_i$ représente la valeur prédite du $i$-ème échantillon et $y_i$ représente la valeur réelle correspondante, alors la fraction de prédictions correctes sur $n_\\text{samples}$ est définie comme suit :\n",
    "\n",
    "$$\\texttt{accuracy}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples}-1} 1(\\hat{y}_i = y_i)$$\n",
    "\n",
    "où $1(x)$ est la [**fonction indicatrice**](https://en.wikipedia.org/wiki/Indicator_function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = [0, 2, 1, 3]\n",
    "y_true = [0, 1, 2, 3]\n",
    "accuracy_score(y_true, y_pred)\n",
    "# 0.5\n",
    "accuracy_score(y_true, y_pred, normalize=False)\n",
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas multi-étiquette avec des indicateurs d'étiquettes binaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemple\n",
    "\n",
    "##### [**Test par permutations de la signifiance d'un score de classification**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/3_model_selection/plot_permutation_tests_for_classification.ipynb)<br/>([_Test with permutations the significance of a classification score_](https://scikit-learn.org/stable/auto_examples/model_selection/plot_permutation_tests_for_classification.html))\n",
    "\n",
    "Exemple d'utilisation du score d'exactitude en utilisant des permutations de l'ensemble de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='top-k-accuracy-score'></a> 3.3.2.3. Top-k accuracy score\n",
    "\n",
    "The [**`top_k_accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.top_k_accuracy_score.html) function is a generalization of [**`accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html). The difference is that a prediction is considered correct as long as the true label is associated with one of the `k` highest predicted scores. [**`accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) is the special case of `k = 1`.\n",
    "\n",
    "The function covers the binary and multiclass classification cases but not the multilabel case.\n",
    "\n",
    "If $\\hat{f}_{i,j}$ is the predicted class for the $i$-th sample corresponding to the $j$-th largest predicted score and $y_i$ is the corresponding true value, then the fraction of correct predictions over $n_\\text{samples}$ is defined as\n",
    "\n",
    "$$\\texttt{top-k accuracy}(y, \\hat{f}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples}-1} \\sum_{j=1}^{k} 1(\\hat{f}_{i,j} = y_i)$$\n",
    "\n",
    "where $k$ is the number of guesses allowed and $1(x)$ is the [**indicator function**](https://en.wikipedia.org/wiki/Indicator_function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='top-k-accuracy-score'></a> 3.3.2.3. Score d'exactitude Top-k (Top-k accuracy score)\n",
    "\n",
    "La fonction [**`top_k_accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.top_k_accuracy_score.html) est une généralisation du [**`accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html). La différence est qu'une prédiction est considérée comme correcte tant que l'étiquette réelle est associée à l'une des `k` valeurs de prédiction les plus élevées. [**`accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) est le cas spécial où `k = 1`.\n",
    "\n",
    "La fonction couvre les cas de classification binaire et multiclasse, mais pas le cas multi-étiquette.\n",
    "\n",
    "Si $\\hat{f}_{i,j}$ représente la classe prédite pour le $i$-ème échantillon correspondant au $j$-ème score de prédiction le plus élevé et $y_i$ représente la valeur réelle correspondante, alors la fraction de prédictions correctes sur $n_\\text{samples}$ est définie comme suit :\n",
    "\n",
    "$$\\texttt{top-k accuracy}(y, \\hat{f}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples}-1} \\sum_{j=1}^{k} 1(\\hat{f}_{i,j} = y_i)$$\n",
    "\n",
    "où $k$ est le nombre de prédictions autorisées et $1(x)$ est la [**fonction indicatrice**](https://en.wikipedia.org/wiki/Indicator_function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "y_true = np.array([0, 1, 2, 2])\n",
    "y_score = np.array([[0.5, 0.2, 0.2],\n",
    "                    [0.3, 0.4, 0.2],\n",
    "                    [0.2, 0.4, 0.3],\n",
    "                    [0.7, 0.2, 0.1]])\n",
    "top_k_accuracy_score(y_true, y_score, k=2)\n",
    "# 0.75\n",
    "# Not normalizing gives the number of \"correctly\" classified samples\n",
    "top_k_accuracy_score(y_true, y_score, k=2, normalize=False)\n",
    "# 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='balanced-accuracy-score'></a> 3.3.2.4. Score d'exactitude équilibrée (Balanced accuracy score)\n",
    "\n",
    "La fonction [**`balanced_accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html) calcule l'[**exactitude équilibrée**](https://en.wikipedia.org/wiki/Accuracy_and_precision), ce qui évite les estimations de performance biaisées sur des ensembles de données déséquilibrés. C'est la macro-moyenne des scores de rappel par classe, ou, de manière équivalente, l'exactitude brute où chaque échantillon est pondéré en fonction de l'inverse de la prévalence de sa classe réelle. Ainsi, pour les ensembles de données équilibrés, le score est égal à l'exactitude.\n",
    "\n",
    "Dans le cas binaire, l'exactitude équilibrée est égale à la moyenne arithmétique de la [**sensibilité**](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) (taux de vrais positifs) et de la [**spécificité**](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) (taux de vrais négatifs), ou la surface sous la courbe ROC avec des prédictions binaires plutôt que des scores :\n",
    "\n",
    "$$\\texttt{balanced-accuracy} = \\frac{1}{2}\\left( \\frac{TP}{TP + FN} + \\frac{TN}{TN + FP}\\right )$$\n",
    "\n",
    "Si le classifieur fonctionne également bien pour les deux classes, ce terme se réduit à l'exactitude conventionnelle (c'est-à-dire le nombre de prédictions correctes divisé par le nombre total de prédictions).\n",
    "\n",
    "En revanche, si l'exactitude conventionnelle est supérieure au hasard uniquement parce que le classifieur profite d'un ensemble de test déséquilibré, alors l'exactitude équilibrée, le cas échéant, se réduira à $\\frac{1}{n\\_classes}$.\n",
    "\n",
    "Le score varie de 0 à 1, ou lorsqu'on utilise `adjusted=True`, il est recalé dans la plage $\\frac{1}{1 - n\\_classes}$ à 1, inclusivement, avec une performance aléatoire obtenant un score de 0.\n",
    "\n",
    "Si $y_i$ est la vraie valeur du $i$-ème échantillon, et $w_i$ est le poids d'échantillon correspondant, alors nous ajustons le poids d'échantillon de la manière suivante :\n",
    "\n",
    "$$\\hat{w}_i = \\frac{w_i}{\\sum_j{1(y_j = y_i) w_j}}$$\n",
    "\n",
    "où $1(x)$ est la [**fonction indicatrice**](https://en.wikipedia.org/wiki/Indicator_function). Étant donné la prédiction $\\hat{y}_i$ pour l'échantillon $i$, l'exactitude équilibrée est définie comme suit :\n",
    "\n",
    "$$\\texttt{balanced-accuracy}(y, \\hat{y}, w) = \\frac{1}{\\sum{\\hat{w}_i}} \\sum_i 1(\\hat{y}_i = y_i) \\hat{w}_i$$\n",
    "\n",
    "Avec `adjusted=True`, l'exactitude équilibrée rapporte l'augmentation relative à $\\texttt{balanced-accuracy}(y, \\mathbf{0}, w) = \\frac{1}{n\\_classes}$. Dans le cas binaire, cela est également connu sous le nom de [**statistique J de Youden**](https://en.wikipedia.org/wiki/Youden's_J_statistic), ou _informedness_. \n",
    "\n",
    "> **Note:** La définition multiclasse ici semble être l'extension la plus raisonnable de la métrique utilisée en classification binaire, bien qu'il n'y ait pas de consensus certain dans la littérature :\n",
    "> - Notre définition : [Mosley2013], [Kelleher2015] et [Guyon2015], où [Guyon2015] adopte la version ajustée pour s'assurer que les prédictions aléatoires ont un score de $0$ et les prédictions parfaites ont un score de $1$.\n",
    "> - Exactitude équilibrée de classe comme décrite dans [Mosley2013] : le minimum entre la précision et le rappel pour chaque classe est calculé. Ces valeurs sont ensuite moyennées sur le nombre total de classes pour obtenir l'exactitude équilibrée.\n",
    "> - Exactitude équilibrée comme décrite dans [Urbanowicz2015] : la moyenne de la sensibilité et de la spécificité est calculée pour chaque classe, puis moyennée sur le nombre total de classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Références\n",
    "\n",
    "🔬 [Guyon2015] (1,2) I. Guyon, K. Bennett, G. Cawley, H.J. Escalante, S. Escalera, T.K. Ho, N. Macià, B. Ray, M. Saeed, A.R. Statnikov, E. Viegas, [**“Design of the 2015 ChaLearn AutoML Challenge”**](http://haralick.org/ML/automl_ijcnn15.pdf), IJCNN 2015.\n",
    "\n",
    "🔬 [Mosley2013] (1,2) L. Mosley, [**“A balanced approach to the multi-class imbalance problem”**](https://dr.lib.iastate.edu/server/api/core/bitstreams/7af0eb70-76f0-4411-9b1b-ce9254e64062/content), IJCV 2010.\n",
    "\n",
    "📚 [Kelleher2015] John. D. Kelleher, Brian Mac Namee, Aoife D’Arcy, [**“Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies”**](https://mitpress.mit.edu/9780262029445/), 2015.\n",
    "\n",
    "🔬 [Urbanowicz2015] Urbanowicz R.J., Moore, J.H. [**“ExSTraCS 2.0: description and evaluation of a scalable learning classifier system”**](https://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC4583133&blobtype=pdf), Evol. Intel. (2015) 8: 89."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='cohen-s-kappa'></a> 3.3.2.5. Statistique de Cohen (Cohen’s kappa)\n",
    "\n",
    "La fonction [**`cohen_kappa_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html) calcule la [**statistique de Cohen**](https://en.wikipedia.org/wiki/Cohen's_kappa). Cette mesure vise à comparer les annotations par différents annotateurs humains, et non un classifieur par rapport à une vérité terrain.\n",
    "\n",
    "Le score kappa (voir la docstring) est un nombre compris entre -1 et 1. Les scores supérieurs à 0,8 sont généralement considérés comme une bonne concordance ; zéro ou moins signifie aucune concordance (étiquettes pratiquement aléatoires).\n",
    "\n",
    "Les scores kappa peuvent être calculés pour les problèmes binaires ou multiclasse, mais pas pour les problèmes multi-étiquette (sauf en calculant manuellement un score par étiquette) et pas pour plus de deux annotateurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4285714285714286"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "y_true = [2, 0, 2, 2, 0, 1]\n",
    "y_pred = [0, 0, 2, 2, 0, 2]\n",
    "cohen_kappa_score(y_true, y_pred)\n",
    "# 0.4285714285714286"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='confusion-matrix'></a> 3.3.2.6. Matrice de confusion\n",
    "\n",
    "La fonction [**`confusion_matrix`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix) évalue l'exactitude de la classification en calculant la [**matrice de confusion**](https://en.wikipedia.org/wiki/Confusion_matrix) dont chaque ligne correspond à la classe réelle (Wikipedia et d'autres références peuvent utiliser une convention différente pour les axes).\n",
    "\n",
    "Par définition, l'entrée $i, j$ dans une matrice de confusion est le nombre d'observations réellement dans le groupe $i$, mais prédit comme étant dans le groupe $j$. Voici un exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 2]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = [2, 0, 2, 2, 0, 1]\n",
    "y_pred = [0, 0, 2, 2, 0, 2]\n",
    "confusion_matrix(y_true, y_pred)\n",
    "# array([[2, 0, 0],\n",
    "#        [0, 0, 1],\n",
    "#        [1, 0, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`ConfusionMatrixDisplay`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html) peut être utilisé pour représenter visuellement une matrice de confusion comme illustré dans l'exemple **Confusion matrix**, qui crée la figure suivante :\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_confusion_matrix_001.png)\n",
    "\n",
    "Le paramètre `normalize` permet de rapporter des ratios au lieu des décomptes. La matrice de confusion peut être normalisée de 3 manières différentes : `'pred'`, `'true'` et `'all'`, qui diviseront les décomptes par la somme de chaque colonne, ligne ou la matrice entière, respectivement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25 , 0.125],\n",
       "       [0.25 , 0.375]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [0, 0, 0, 1, 1, 1, 1, 1]\n",
    "y_pred = [0, 1, 0, 1, 0, 1, 0, 1]\n",
    "confusion_matrix(y_true, y_pred, normalize='all')\n",
    "# array([[0.25 , 0.125],\n",
    "#        [0.25 , 0.375]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les problèmes binaires, nous pouvons obtenir les décomptes de vrais négatifs, faux positifs, faux négatifs et vrais positifs de la manière suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 2, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [0, 0, 0, 1, 1, 1, 1, 1]\n",
    "y_pred = [0, 1, 0, 1, 0, 1, 0, 1]\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "tn, fp, fn, tp\n",
    "# (2, 1, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples\n",
    "\n",
    "##### [**Matrice de confusion**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/3_model_selection/plot_confusion_matrix.ipynb)<br/>([_Confusion matrix_](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html))\n",
    "\n",
    "Exemple d'utilisation d'une matrice de confusion pour évaluer la qualité de la sortie d'un classifieur.\n",
    "\n",
    "##### [**Reconnaissance de chiffres manuscrits**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/classification/plot_digits_classification.ipynb)<br/>([_Recognizing hand-written digits_](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html))\n",
    "\n",
    "Exemple d'utilisation d'une matrice de confusion pour classifier des chiffres manuscrits.\n",
    "\n",
    "##### [**Classification de documents textuels à l'aide de caractéristiques creuses**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/text/plot_document_classification_20newsgroups.ipynb)<br/>([*Classification of text documents using sparse features*](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html))\n",
    "\n",
    "Exemple d'utilisation d'une matrice de confusion pour classifier des documents textuels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='classification-report'></a> 3.3.2.7. Rapport de classification\n",
    "\n",
    "La fonction [**`classification_report`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report) génère un rapport textuel montrant les principales métriques de classification. Voici un petit exemple avec des `target_names` personnalisées et des étiquettes inférées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.67      1.00      0.80         2\n",
      "     class 1       0.00      0.00      0.00         1\n",
      "     class 2       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.56      0.50      0.49         5\n",
      "weighted avg       0.67      0.60      0.59         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true = [0, 1, 2, 2, 0]\n",
    "y_pred = [0, 0, 2, 1, 0]\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "#               precision    recall  f1-score   support\n",
    "# \n",
    "#      class 0       0.67      1.00      0.80         2\n",
    "#      class 1       0.00      0.00      0.00         1\n",
    "#      class 2       1.00      0.50      0.67         2\n",
    "# \n",
    "#     accuracy                           0.60         5\n",
    "#    macro avg       0.56      0.50      0.49         5\n",
    "# weighted avg       0.67      0.60      0.59         5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemples\n",
    "\n",
    "##### [**Reconnaissance de chiffres manuscrits**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/classification/plot_digits_classification.ipynb)<br/>([_Recognizing hand-written digits_](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html))\n",
    "\n",
    "Exemple d'utilisation du rapport de classification pour des chiffres manuscrits.\n",
    "\n",
    "##### [**Stratégie personnalisée de réajustement d'une recherche en grille avec validation croisée**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/3_model_selection/plot_grid_search_digits.ipynb)<br/>([_Custom refit strategy of a grid search with cross-validation_](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html))\n",
    "\n",
    "Exemple d'utilisation du rapport de classification pour une recherche en grille avec validation croisée imbriquée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='hamming-loss'></a> 3.3.2.8. Perte de Hamming\n",
    "\n",
    "La fonction [**`hamming_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html#sklearn.metrics.hamming_loss) calcule la perte de Hamming moyenne ou [**distance de Hamming**](https://en.wikipedia.org/wiki/Hamming_distance) entre deux ensembles d'échantillons.\n",
    "\n",
    "Si $\\hat{y}_{i,j}$ est la valeur prédite pour la $j$-ème étiquette d'un échantillon donné $i$, $y_{i,j}$ est la valeur correspondante vraie, $n_\\text{samples}$ est le nombre d'échantillons et $n_\\text{labels}$ est le nombre d'étiquettes, alors la perte de Hamming $L_{Hamming}$ est définie comme suit :\n",
    "\n",
    "$$L_{Hamming}(y, \\hat{y}) = \\frac{1}{n_\\text{samples} * n_\\text{labels}} \\sum_{i=0}^{n_\\text{samples}-1} \\sum_{j=0}^{n_\\text{labels} - 1} 1(\\hat{y}_{i,j} \\not= y_{i,j})$$\n",
    "\n",
    "où $1(x)$ est la [**fonction indicatrice**](https://en.wikipedia.org/wiki/Indicator_function).\n",
    "\n",
    "L'équation ci-dessus ne s'applique pas au cas de classification multiclasse. Veuillez vous référer à la note ci-dessous pour plus d'informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "y_pred = [1, 2, 3, 4]\n",
    "y_true = [2, 2, 3, 4]\n",
    "hamming_loss(y_true, y_pred)\n",
    "# 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas multilabel avec des indicateurs d'étiquettes binaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))\n",
    "# 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note :** En classification multiclasse, la perte de Hamming correspond à la distance de Hamming entre `y_true` et `y_pred`, ce qui est similaire à la fonction de perte [**Zero one loss** (3.3.2.17)](https://scikit-learn.org/stable/modules/model_evaluation.html#zero-one-loss). Cependant, alors que la perte zero-one pénalise les ensembles de prédictions qui ne correspondent pas strictement aux ensembles réels, la perte de Hamming pénalise les étiquettes individuelles. Ainsi, la perte de Hamming, bornée par la perte zero-one, est toujours comprise entre zéro et un, inclusivement ; et prédire un sous-ensemble ou un sur-ensemble approprié des étiquettes réelles donnera une perte de Hamming comprise entre zéro et un, exclusivement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='hamming-loss'></a> 3.3.2.9. Précision, rappel et scores F\n",
    "\n",
    "De manière intuitive, la [**précision**](https://fr.wikipedia.org/wiki/Pr%C3%A9cision_et_rappel) représente la capacité du classifieur à ne pas étiqueter comme positive un échantillon qui est négatif, et le rappel représente la capacité du classifieur à trouver tous les échantillons positifs.\n",
    "\n",
    "Le [**score F**](https://fr.wikipedia.org/wiki/F-mesure) ($F_\\beta$ et $F_1$) peut être interprété comme une moyenne harmonique pondérée de la précision et du rappel. Un score F atteint sa meilleure valeur à 1 et son pire score à 0. Avec $\\beta = 1$, le score F et le score F1 sont équivalents, et le rappel et la précision sont tout aussi importants.\n",
    "\n",
    "La fonction [**`precision_recall_curve`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html) calcule une courbe de précision-rappel à partir des étiquettes de vérité terrain et d'un score donné par le classifieur en faisant varier un seuil de décision.\n",
    "\n",
    "La fonction [**`average_precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html) calcule la [**précision moyenne**](https://fr.wikipedia.org/w/index.php?title=Information_retrieval&oldid=793358396#Average_precision) (AP) à partir des scores de prédiction. La valeur est comprise entre 0 et 1, et plus elle est élevée, mieux c'est. L'AP est définie comme suit :\n",
    "\n",
    "$$\\text{AP} = \\sum_n (R_n - R_{n-1}) P_n$$\n",
    "\n",
    "où $P_n$ et $R_n$ sont la précision et le rappel au n-ième seuil. Avec des prédictions aléatoires, l'AP est la fraction d'échantillons positifs.\n",
    "\n",
    "Les références [Manning2008] et [Everingham2010] présentent des variantes alternatives de l'AP qui permettent d'interpoler la courbe précision-rappel. Actuellement, [**`average_precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html) n'implémente aucune variante interpolée. Les références [Davis2006] et [Flach2015] expliquent pourquoi une interpolation linéaire des points sur la courbe précision-rappel donne une mesure excessivement optimiste des performances du classifieur. Cette interpolation linéaire est utilisée lors du calcul de l'aire sous la courbe avec la règle du trapèze dans la fonction [**`auc`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html).\n",
    "\n",
    "Plusieurs fonctions vous permettent d'analyser la précision, le rappel et le score F :\n",
    "\n",
    "| Fonction                                | Description                                                                                |\n",
    "|:----------------------------------------|:-------------------------------------------------------------------------------------------|\n",
    "| [**`average_precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html)(y_true, y_score, *) | Calcule la précision moyenne (AP) à partir des scores de prédiction. |\n",
    "| [**`f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)(y_true, y_pred, *[, labels, ...]) | Calcule le score F1, également connu sous le nom de F-score équilibré ou F-mesure. |\n",
    "| [**`fbeta_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html)(y_true, y_pred, *, beta[, ...]) | Calcule le score F-beta. |\n",
    "| [**`precision_recall_curve`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html)(y_true, probas_pred, *) | Calcule les paires précision-rappel pour différents seuils de probabilité. |\n",
    "| [**`precision_recall_fscore_support`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html) (y_true, ...) | Calcule la précision, le rappel, le score F et le support pour chaque classe. |\n",
    "| [**`precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)(y_true, y_pred, *[, labels, ...]) | Calcule la précision. |\n",
    "| [**`recall_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)(y_true, y_pred, *[, labels, ...]) | Calcule le rappel. |\n",
    "\n",
    "Notez que la fonction [**`precision_recall_curve`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html) est limitée au cas binaire. La fonction [**`average_precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html) prend en charge les formats multiclasse et multilabel en calculant le score de chaque classe de manière \"One-vs-the-rest\" (OvR) et en les moyennant ou non en fonction de la valeur de l'argument `average`. \n",
    "\n",
    "Les fonctions `PredictionRecallDisplay.from_estimator` et `PredictionRecallDisplay.from_predictions` afficheront la courbe précision-rappel comme suit :\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_precision_recall_001.png)\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Stratégie personnalisée de réajustement d'une recherche en grille avec validation croisée**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/3_model_selection/plot_grid_search_digits.ipynb)<br/>([_Custom refit strategy of a grid search with cross-validation_](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html))\n",
    "\n",
    "Exemple d'utilisation des fonctions [**`precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) et [**`recall_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html) pour estimer les paramètres à l'aide d'une recherche par grille avec validation croisée imbriquée.\n",
    "\n",
    "##### [**Précision-Rappel**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/3_model_selection/plot_precision_recall.ipynb)<br/>([_Precision-Recall_](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html))\n",
    "\n",
    "Exemple d'utilisation de la fonction [**`precision_recall_curve`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html) pour évaluer la qualité de la sortie du classifieur.\n",
    "\n",
    "#### Références\n",
    "\n",
    "Z 📚 [Manning2008] C.D. Manning, P. Raghavan, H. Schütze, [**“Introduction to Information Retrieval”**](https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf), 2009.\n",
    "\n",
    "Z 🔬 [Everingham2010] M. Everingham, L. Van Gool, C.K.I. Williams, J. Winn, A. Zisserman, [**“The Pascal Visual Object Classes (VOC) Challenge”**](https://www.pure.ed.ac.uk/ws/files/20017166/ijcv_voc14.pdf), IJCV 2010.\n",
    "\n",
    "Z 🔬 [Davis2006] J. Davis, M. Goadrich, [**“The Relationship Between Precision-Recall and ROC Curves”**](https://www.biostat.wisc.edu/~page/rocpr.pdf), ICML 2006.\n",
    "\n",
    "Z 🔬 [Flach2015] P.A. Flach, M. Kull, [**“Precision-Recall-Gain Curves: PR Analysis Done Right”**](https://proceedings.neurips.cc/paper_files/paper/2015/file/33e8075e9970de0cfea955afd4644bb2-Paper.pdf), NIPS 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='binary-classification'></a> Classification binaire\n",
    "\n",
    "Dans une tâche de classification binaire, les termes \"positif\" et \"négatif\" font référence à la prédiction du classifieur, tandis que les termes \"vrai\" et \"faux\" font référence à la correspondance de cette prédiction avec le jugement externe (parfois appelé \"observation\"). Avec ces définitions, nous pouvons formuler le tableau suivant :\n",
    "\n",
    "<table>\n",
    "<tbody>\n",
    "<tr><td></td>\n",
    "<td colspan=\"2\"><p>Classe réelle (observation)</p></td>\n",
    "</tr>\n",
    "<tr><td rowspan=\"2\"><p>Classe prédite\n",
    "(expectation)</p></td>\n",
    "<td><p>tp (vrai positif)\n",
    "Résultat correct</p></td>\n",
    "<td><p>fp (faux positif)\n",
    "Résultat inattendu</p></td>\n",
    "</tr>\n",
    "<tr><td><p>fn (faux négatif)\n",
    "Résultat manquant</p></td>\n",
    "<td><p>tn (vrai négatif)\n",
    "Absence de résultat correct</p></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "Dans ce contexte, nous pouvons définir les notions de précision, rappel et F-mesure :\n",
    "\n",
    "$$P = \\frac{tp}{tp + fp},$$\n",
    "$$R = \\frac{tp}{tp + fn},$$\n",
    "$$F_\\beta = (1 + \\beta^2) \\frac{P \\times R}{\\beta^2 P + R}.$$\n",
    "\n",
    "Parfois, le rappel est également appelé \"sensibilité\".\n",
    "\n",
    "Voici quelques petits exemples de classification binaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333333"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "y_pred = [0, 1, 0, 0]\n",
    "y_true = [0, 1, 0, 1]\n",
    "metrics.precision_score(y_true, y_pred)\n",
    "# 1.0\n",
    "metrics.recall_score(y_true, y_pred)\n",
    "# 0.5\n",
    "metrics.f1_score(y_true, y_pred)\n",
    "# 0.66...\n",
    "metrics.fbeta_score(y_true, y_pred, beta=0.5)\n",
    "# 0.83...\n",
    "metrics.fbeta_score(y_true, y_pred, beta=1)\n",
    "# 0.66...\n",
    "metrics.fbeta_score(y_true, y_pred, beta=2)\n",
    "# 0.55...\n",
    "metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5)\n",
    "# (array([0.66..., 1.        ]), array([1. , 0.5]), array([0.71..., 0.83...]), array([2, 2]))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "y_true = np.array([0, 0, 1, 1])\n",
    "y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "precision, recall, threshold = precision_recall_curve(y_true, y_scores)\n",
    "precision\n",
    "# array([0.5       , 0.66..., 0.5       , 1.        , 1.        ])\n",
    "recall\n",
    "# array([1. , 1. , 0.5, 0.5, 0. ])\n",
    "threshold\n",
    "# array([0.1 , 0.35, 0.4 , 0.8 ])\n",
    "average_precision_score(y_true, y_scores)\n",
    "# 0.83..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='multiclass-and-multilabel-classification'></a> Classification multiclasse et multilabel\n",
    "\n",
    "Dans une tâche de classification multiclasse et multilabel, les notions de précision, de rappel et de F-mesure peuvent être appliquées à chaque étiquette indépendamment. Il existe quelques façons de combiner les résultats entre les étiquettes, spécifiées par l'argument `average` des fonctions [**`average_precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html), [**`f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html), [**`fbeta_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html), [**`precision_recall_fscore_support`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html), [**`precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) et [**`recall_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html), comme décrit ci-dessus. Notez que si toutes les étiquettes sont incluses, le \"micro-averaging\" dans un contexte multiclasse produira des valeurs de précision, de rappel et de F qui sont toutes identiques à l'exactitude (accuracy). Notez également que le \"weighted-averaging\" peut produire un score F qui n'est pas nécessairement compris entre la précision et le rappel.\n",
    "\n",
    "Pour clarifier cela, considérons la notation suivante :\n",
    "\n",
    "- $y$ est l'ensemble des paires $(échantillon, étiquette)$ réelles\n",
    "- $\\hat{y}$ est l'ensemble des paires $(échantillon, étiquette)$ prédites\n",
    "- $L$ est l'ensemble des étiquettes\n",
    "- $S$ est l'ensemble des échantillons\n",
    "- $y_s$ est le sous-ensemble de $y$ contenant l'échantillon $s$, c'est-à-dire $y_s := \\left\\{(s', l) \\in y | s' = s\\right\\}$\n",
    "- $y_l$ est le sous-ensemble de $y$ contenant l'étiquette $l$\n",
    "- de même, $\\hat{y}_s$ et $\\hat{y}_l$ sont des sous-ensembles de $\\hat{y}$\n",
    "- $P(A, B) := \\frac{\\left \\vert A \\cap B \\right \\vert}{\\left \\vert B \\right \\vert}$ pour certains ensembles $A$ et $B$\n",
    "- $R(A, B) := \\frac{\\left \\vert A \\cap B \\right \\vert}{\\left \\vert A \\right \\vert}$ (Les conventions peuvent varier sur le traitement de $A = \\emptyset$ ; cette implémentation utilise $R(A, B):=0$, et de manière similaire pour $P$.)\n",
    "- $F_\\beta(A, B) := \\left(1 + \\beta^2\\right) \\frac{P(A, B) \\times R(A, B)}{\\beta^2 P(A, B) + R(A, B)}$\n",
    "\n",
    "Les métriques sont ensuite définies comme suit :\n",
    "\n",
    "| Moyenne | Précision | Rappel | F_beta |\n",
    "|-|-|-|-|\n",
    "|`\"micro\"`|$P(y, \\hat{y})$|$R(y, \\hat{y})$|$F_\\beta(y, \\hat{y})$|\n",
    "|`\"samples\"`|$\\frac{1}{\\left \\vert S \\right \\vert} \\sum_{s \\in S} P(y_s, \\hat{y}_s)$|$\\frac{1}{\\left \\vert S \\right \\vert} \\sum_{s \\in S} R(y_s, \\hat{y}_s)$|$\\frac{1}{\\left \\vert S \\right \\vert} \\sum_{s \\in S} F_\\beta(y_s, \\hat{y}_s)$|\n",
    "|`\"macro\"`|$\\frac{1}{\\left \\vert L \\right \\vert} \\sum_{l \\in L} P(y_l, \\hat{y}_l)$|$\\frac{1}{\\left \\vert L \\right \\vert} \\sum_{l \\in L} R(y_l, \\hat{y}_l)$|$\\frac{1}{\\left \\vert L \\right \\vert} \\sum_{l \\in L} F_\\beta(y_l, \\hat{y}_l)$|\n",
    "|`\"weighted\"`|$\\frac{1}{\\sum_{l \\in L} \\left \\vert y_l \\right \\vert} \\sum_{l \\in L} \\left \\vert y_l \\right \\vert P(y_l, \\hat{y}_l)$|$\\frac{1}{\\sum_{l \\in L} \\left \\vert y_l \\right \\vert} \\sum_{l \\in L} \\left \\vert y_l \\right \\vert R(y_l, \\hat{y}_l)$|$\\frac{1}{\\sum_{l \\in L} \\left \\vert y_l \\right \\vert} \\sum_{l \\in L} \\left \\vert y_l \\right \\vert F_\\beta(y_l, \\hat{y}_l)$|\n",
    "|`None`|$\\langle P(y_l, \\hat{y}_l) \\vert l \\in L \\rangle$|$\\langle R(y_l, \\hat{y}_l) \\vert l \\in L \\rangle$|$\\langle F_\\beta(y_l, \\hat{y}_l) \\vert l \\in L \\rangle$|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.66666667, 0.        , 0.        ]),\n",
       " array([1., 0., 0.]),\n",
       " array([0.71428571, 0.        , 0.        ]),\n",
       " array([2, 2, 2], dtype=int64))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "y_true = [0, 1, 2, 0, 1, 2]\n",
    "y_pred = [0, 2, 1, 0, 0, 1]\n",
    "metrics.precision_score(y_true, y_pred, average='macro')\n",
    "# 0.22...\n",
    "metrics.recall_score(y_true, y_pred, average='micro')\n",
    "# 0.33...\n",
    "metrics.f1_score(y_true, y_pred, average='weighted')\n",
    "# 0.26...\n",
    "metrics.fbeta_score(y_true, y_pred, average='macro', beta=0.5)\n",
    "# 0.23...\n",
    "metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5, average=None)\n",
    "# (array([0.66..., 0.        , 0.        ]), array([1., 0., 0.]), array([0.71..., 0.        , 0.        ]), array([2, 2, 2]...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la classification multiclasse avec une \"classe négative\", il est possible d'exclure certaines étiquettes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.recall_score(y_true, y_pred, labels=[1, 2], average='micro')\n",
    "# excluding 0, no labels were correctly recalled\n",
    "# 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De même, les étiquettes non présentes dans l'échantillon de données peuvent être prises en compte dans l'approche de la macro-moyenne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_score(y_true, y_pred, labels=[0, 1, 2, 3], average='macro')\n",
    "# 0.166..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='jaccard-similarity-coefficient-score'></a> 3.3.2.10. Coefficient de similarité de Jaccard\n",
    "\n",
    "La fonction [**`jaccard_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html) calcule la moyenne des [**coefficients de similarité de Jaccard**](https://en.wikipedia.org/wiki/Jaccard_index), également appelés indice de Jaccard, entre les paires d'ensembles d'étiquettes.\n",
    "\n",
    "Le coefficient de similarité de Jaccard avec un ensemble d'étiquettes de vérité terrain $y$ et un ensemble d'étiquettes prédites $\\hat{y}$ est défini comme suit :\n",
    "\n",
    "$$J(y, \\hat{y}) = \\frac{|y \\cap \\hat{y}|}{|y \\cup \\hat{y}|}.$$\n",
    "\n",
    "\n",
    "Le [**`jaccard_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html) (tout comme [**`precision_recall_fscore_support`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html)) s'applique nativement aux cibles binaires. En le calculant par ensemble, il peut être étendu pour s'appliquer aux problèmes multilabel et multiclasse en utilisant l'argument `average` (voir [**ci-dessus** (3.3.2.1)](https://scikit-learn.org/stable/modules/model_evaluation.html#average)).\n",
    "\n",
    "Dans le cas binaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_score\n",
    "y_true = np.array([[0, 1, 1],\n",
    "                   [1, 1, 0]])\n",
    "y_pred = np.array([[1, 1, 1],\n",
    "                   [1, 0, 0]])\n",
    "jaccard_score(y_true[0], y_pred[0])\n",
    "# 0.6666..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas de la comparaison 2D (par exemple, la similarité d'images) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_score(y_true, y_pred, average=\"micro\")\n",
    "# 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas multilabel avec des indicateurs binaires d'étiquettes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5, 1. ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_score(y_true, y_pred, average='samples')\n",
    "# 0.5833...\n",
    "jaccard_score(y_true, y_pred, average='macro')\n",
    "# 0.6666...\n",
    "jaccard_score(y_true, y_pred, average=None)\n",
    "# array([0.5, 0.5, 1. ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les problèmes multiclasse sont binarisés et traités de la même manière que les problèmes multilabel correspondants :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [0, 2, 1, 2]\n",
    "y_true = [0, 1, 2, 2]\n",
    "jaccard_score(y_true, y_pred, average=None)\n",
    "# array([1. , 0. , 0.33...])\n",
    "jaccard_score(y_true, y_pred, average='macro')\n",
    "# 0.44...\n",
    "jaccard_score(y_true, y_pred, average='micro')\n",
    "# 0.33..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='hinge-loss'></a> 3.3.2.11. Perte de charnière (Hinge Loss)\n",
    "\n",
    "La fonction [**`hinge_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hinge_loss.html) calcule la distance moyenne entre le modèle et les données en utilisant la [**perte de charnière**](https://en.wikipedia.org/wiki/Hinge_loss), une métrique unilatérale qui ne prend en compte que les erreurs de prédiction. (La perte de charnière est utilisée dans les classificateurs à marge maximale tels que les machines à vecteurs de support.)\n",
    "\n",
    "Si l'étiquette vraie $y_i$ d'une tâche de classification binaire est encodée comme $y_i=\\left\\{-1, +1\\right\\}$ pour chaque échantillon $i$ ; et $w_i$ est la décision prédite correspondante (un tableau de forme `(n_samples,)` en sortie de la méthode `decision_function`), alors la perte de charnière est définie comme suit :\n",
    "\n",
    "$$L_\\text{Hinge}(y, w) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples}-1} \\max\\left\\{1 - w_i y_i, 0\\right\\}$$\n",
    "\n",
    "S'il y a plus de deux étiquettes, [**`hinge_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hinge_loss.html) utilise une variante multiclasse due à Crammer & Singer. [Ici](https://jmlr.csail.mit.edu/papers/volume2/crammer01a/crammer01a.pdf) se trouve l'article qui la décrit.\n",
    "\n",
    "Dans ce cas, la décision prédite est un tableau de forme `(n_samples, n_labels)`. Si $w_{i, y_i}$ est la décision prédite pour la vraie étiquette de l'échantillon $i$ ; et $\\hat{w}_{i, y_i} = \\max\\left\\{w_{i, y_j}~|~y_j \\ne y_i \\right\\}$ est le maximum des décisions prédites pour toutes les autres étiquettes, alors la perte de charnière multiclasse est définie par :\n",
    "\n",
    "$$L_\\text{Hinge}(y, w) = \\frac{1}{n_\\text{samples}}\n",
    "\\sum_{i=0}^{n_\\text{samples}-1} \\max\\left\\{1 + \\hat{w}_{i, y_i}\n",
    "- w_{i, y_i}, 0\\right\\}$$\n",
    "\n",
    "Voici un petit exemple illustrant l'utilisation de la fonction [**`hinge_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hinge_loss.html) avec un classificateur SVM dans un problème de classe binaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30303030303030304"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import hinge_loss\n",
    "X = [[0], [1]]\n",
    "y = [-1, 1]\n",
    "est = svm.LinearSVC(dual=\"auto\", random_state=0)\n",
    "est.fit(X, y)\n",
    "# LinearSVC(dual='auto', random_state=0)\n",
    "pred_decision = est.decision_function([[-2], [3], [0.5]])\n",
    "pred_decision\n",
    "# array([-2.18...,  2.36...,  0.09...])\n",
    "hinge_loss([-1, 1, 1], pred_decision)\n",
    "# 0.3..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un exemple illustrant l'utilisation de la fonction [**`hinge_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hinge_loss.html) avec un classificateur SVM dans un problème multiclasse :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5641025641025641"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0], [1], [2], [3]])\n",
    "Y = np.array([0, 1, 2, 3])\n",
    "labels = np.array([0, 1, 2, 3])\n",
    "est = svm.LinearSVC(dual=\"auto\")\n",
    "est.fit(X, Y)\n",
    "# LinearSVC(dual='auto')\n",
    "pred_decision = est.decision_function([[-1], [2], [3]])\n",
    "y_true = [0, 2, 3]\n",
    "hinge_loss(y_true, pred_decision, labels=labels)\n",
    "# 0.56..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='log-loss'></a> 3.3.2.12. Perte logarithmique (Log Loss)\n",
    "\n",
    "La perte logarithmique, également appelée perte de régression logistique ou entropie croisée, est définie sur les estimations de probabilité. Elle est couramment utilisée en régression logistique (multinomiale) et en réseaux neuronaux, ainsi que dans certaines variantes de l'espérance-maximisation. Elle peut être utilisée pour évaluer les sorties de probabilité (`predict_proba`) d'un classificateur au lieu de ses prédictions discrètes.\n",
    "\n",
    "Pour la classification binaire avec une vraie étiquette $y \\in \\{0,1\\}$ et une estimation de probabilité $p = \\operatorname{Pr}(y = 1)$, la perte logarithmique par échantillon est le négatif du logarithme de vraisemblance du classificateur étant donné la vraie étiquette :\n",
    "\n",
    "$$L_{\\log}(y, p) = -\\log \\operatorname{Pr}(y|p) = -(y \\log (p) + (1 - y) \\log (1 - p))$$\n",
    "\n",
    "Ceci s'étend au cas multiclasse comme suit. Soit les vraies étiquettes pour un ensemble d'échantillons encodées sous forme d'une matrice binaire indicatrice 1-of-K $Y$, c'est-à-dire $y_{i,k} = 1$ si l'échantillon $i$ a l'étiquette $k$ choisie parmi un ensemble de $K$ étiquettes. Soit $P$ une matrice d'estimations de probabilité, avec $p_{i,k} = \\operatorname{Pr}(y_{i,k} = 1)$. Alors, la perte logarithmique de l'ensemble entier est définie par :\n",
    "\n",
    "$$L_{\\log}(Y, P) = -\\log \\operatorname{Pr}(Y|P) = - \\frac{1}{N} \\sum_{i=0}^{N-1} \\sum_{k=0}^{K-1} y_{i,k} \\log p_{i,k}$$\n",
    "\n",
    "Pour voir comment cela généralise la perte logarithmique binaire donnée ci-dessus, notez que dans le cas binaire, $p_{i,0} = 1 - p_{i,1}$ et $y_{i,0} = 1 - y_{i,1}$. Ainsi, en développant la somme interne sur $y_{i,k} \\in \\{0,1\\}$, on obtient la perte logarithmique binaire.\n",
    "\n",
    "La fonction [**`log_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html) calcule la perte logarithmique à partir d'une liste d'étiquettes réelles et d'une matrice de probabilités, telle qu'elle est renvoyée par la méthode `predict_proba` d'un estimateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1738073366910675"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "y_true = [0, 0, 1, 1]\n",
    "y_pred = [[.9, .1], [.8, .2], [.3, .7], [.01, .99]]\n",
    "log_loss(y_true, y_pred)\n",
    "# 0.1738..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le premier `[.9, .1]` dans `y_pred` indique une probabilité de 90% que le premier échantillon ait l'étiquette 0. La perte logarithmique est non négative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='matthews-correlation-coefficient'></a> 3.3.2.13. Coefficient de corrélation de Matthews\n",
    "\n",
    "La fonction [**`matthews_corrcoef`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html) calcule le [**coefficient de corrélation de Matthews (MCC)**](https://en.wikipedia.org/wiki/Phi_coefficient) pour les classes binaires. En citant Wikipedia :\n",
    "\n",
    "_\"Le coefficient de corrélation de Matthews est utilisé en apprentissage automatique comme mesure de la qualité des classifications binaires (à deux classes). Il prend en compte les vrais positifs, les vrais négatifs, les faux positifs et les faux négatifs, et est généralement considéré comme une mesure équilibrée qui peut être utilisée même si les classes sont de tailles très différentes. Le MCC est essentiellement une valeur de coefficient de corrélation entre -1 et +1. Un coefficient de +1 représente une prédiction parfaite, 0 une prédiction aléatoire moyenne et -1 une prédiction inverse. La statistique est également connue sous le nom de coefficient phi.\"_\n",
    "\n",
    "Dans le cas binaire (à deux classes), $tp$, $tn$, $fp$ et $fn$ représentent respectivement le nombre de vrais positifs, de vrais négatifs, de faux positifs et de faux négatifs. Le MCC est défini comme suit :\n",
    "\n",
    "$$MCC = \\frac{tp \\times tn - fp \\times fn}{\\sqrt{(tp + fp)(tp + fn)(tn + fp)(tn + fn)}}.$$\n",
    "\n",
    "Dans le cas multiclasse, le coefficient de corrélation de Matthews peut être [défini](https://rth.dk/resources/rk/introduction/index.html) en termes d'une [**matrice de confusion**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) $C$ pour $K$ classes. Pour simplifier la définition, considérons les variables intermédiaires suivantes :\n",
    "\n",
    "- $t_k=\\sum_{i}^{K} C_{ik}$ le nombre de fois où la classe $k$ est réellement présente,\n",
    "- $p_k=\\sum_{i}^{K} C_{ki}$ le nombre de fois où la classe $k$ a été prédite,\n",
    "- $c=\\sum_{k}^{K} C_{kk}$ le nombre total d'échantillons correctement prédits,\n",
    "- $s=\\sum_{i}^{K} \\sum_{j}^{K} C_{ij}$ le nombre total d'échantillons.\n",
    "\n",
    "Le coefficient de corrélation de Matthews multiclasse est défini comme suit :\n",
    "\n",
    "$$MCC = \\frac{\n",
    "    c \\times s - \\sum_{k}^{K} p_k \\times t_k\n",
    "}{\\sqrt{\n",
    "    (s^2 - \\sum_{k}^{K} p_k^2) \\times\n",
    "    (s^2 - \\sum_{k}^{K} t_k^2)\n",
    "}}$$\n",
    "\n",
    "Lorsqu'il y a plus de deux étiquettes, la valeur du MCC ne se situera plus entre -1 et +1. Au lieu de cela, la valeur minimale sera quelque part entre -1 et 0 en fonction du nombre et de la répartition des étiquettes réelles. La valeur maximale est toujours +1.\n",
    "\n",
    "Voici un petit exemple illustrant l'utilisation de la fonction [**`matthews_corrcoef`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3333333333333333"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "y_true = [+1, +1, +1, -1]\n",
    "y_pred = [+1, -1, +1, +1]\n",
    "matthews_corrcoef(y_true, y_pred)\n",
    "# -0.33..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='multi-label-confusion-matrix'></a> 3.3.2.14. Matrice de confusion multi-étiquettes\n",
    "\n",
    "La fonction [**`multilabel_confusion_matrix`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html) calcule la matrice de confusion multi-étiquettes pour chaque classe (par défaut) ou pour chaque échantillon (samplewise=True) afin d'évaluer l'exactitude d'une classification. La fonction `multilabel_confusion_matrix` traite également les données multiclasse comme si elles étaient multi-étiquettes, car il s'agit d'une transformation couramment appliquée pour évaluer les problèmes multiclasse avec des métriques de classification binaire (telles que la précision, le rappel, etc.).\n",
    "\n",
    "Lors du calcul de la matrice de confusion multi-étiquettes par classe $C$, le nombre de vrais négatifs pour la classe $i$ est $C_{i,0,0}$, le nombre de faux négatifs est $C_{i,1,0}$, le nombre de vrais positifs est $C_{i,1,1}$ et le nombre de faux positifs est $C_{i,0,1}$.\n",
    "\n",
    "Voici un exemple illustrant l'utilisation de la fonction [**`multilabel_confusion_matrix`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html) avec une entrée de [**matrice indicatrice multi-étiquettes**](https://scikit-learn.org/stable/glossary.html#term-multilabel-indicator-matrix) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 0],\n",
       "        [0, 1]],\n",
       "\n",
       "       [[1, 0],\n",
       "        [0, 1]],\n",
       "\n",
       "       [[0, 1],\n",
       "        [1, 0]]], dtype=int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "y_true = np.array([[1, 0, 1],\n",
    "                   [0, 1, 0]])\n",
    "y_pred = np.array([[1, 0, 0],\n",
    "                   [0, 1, 1]])\n",
    "multilabel_confusion_matrix(y_true, y_pred)\n",
    "# array([[[1, 0],\n",
    "#         [0, 1]],\n",
    "# \n",
    "#        [[1, 0],\n",
    "#         [0, 1]],\n",
    "# \n",
    "#        [[0, 1],\n",
    "#         [1, 0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou une matrice de confusion peut être construite pour les étiquettes de chaque échantillon :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 0],\n",
       "        [1, 1]],\n",
       "\n",
       "       [[1, 1],\n",
       "        [0, 1]]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(y_true, y_pred, samplewise=True)\n",
    "# array([[[1, 0],\n",
    "#         [1, 1]],\n",
    "# \n",
    "#        [[1, 1],\n",
    "#         [0, 1]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un exemple illustrant l'utilisation de la fonction [**`multilabel_confusion_matrix`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html) avec une entrée [**multiclasse**](https://scikit-learn.org/stable/glossary.html#term-multiclass) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[3, 1],\n",
       "        [0, 2]],\n",
       "\n",
       "       [[5, 0],\n",
       "        [1, 0]],\n",
       "\n",
       "       [[2, 1],\n",
       "        [1, 2]]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
    "y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
    "multilabel_confusion_matrix(y_true, y_pred,\n",
    "                            labels=[\"ant\", \"bird\", \"cat\"])\n",
    "# array([[[3, 1],\n",
    "#         [0, 2]],\n",
    "# \n",
    "#        [[5, 0],\n",
    "#         [1, 0]],\n",
    "# \n",
    "#        [[2, 1],\n",
    "#         [1, 2]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici quelques exemples illustrant l'utilisation de la fonction [**`multilabel_confusion_matrix`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html) pour calculer le rappel (ou la sensibilité), la spécificité, le taux de faux positifs et le taux de faux négatifs pour chaque classe dans un problème avec une entrée de matrice indicatrice multi-étiquettes.\n",
    "\n",
    "Calcul du [**rappel**](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) (également appelé le taux de vrais positifs ou la sensibilité) pour chaque classe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 0.5, 0. ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.array([[0, 0, 1],\n",
    "                   [0, 1, 0],\n",
    "                   [1, 1, 0]])\n",
    "y_pred = np.array([[0, 1, 0],\n",
    "                   [0, 0, 1],\n",
    "                   [1, 1, 0]])\n",
    "mcm = multilabel_confusion_matrix(y_true, y_pred)\n",
    "tn = mcm[:, 0, 0]\n",
    "tp = mcm[:, 1, 1]\n",
    "fn = mcm[:, 1, 0]\n",
    "fp = mcm[:, 0, 1]\n",
    "tp / (tp + fn)\n",
    "# array([1. , 0.5, 0. ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul de la [**spécificité**](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) (également appelée le taux de vrais négatifs) pour chaque classe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 0. , 0.5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn / (tn + fp)\n",
    "# array([1. , 0. , 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul du [**taux de faux positifs**](https://en.wikipedia.org/wiki/False_positive_rate) pour chaque classe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 1. , 0.5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp / (fp + tn)\n",
    "# array([0. , 1. , 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul du [**taux de faux négatifs**](https://en.wikipedia.org/wiki/False_positives_and_false_negatives) pour chaque classe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0.5, 1. ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn / (fn + tp)\n",
    "# array([0. , 0.5, 1. ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='multi-label-confusion-matrix'></a> 3.3.2.15. Courbe caractéristique de fonctionnement du récepteur (ROC)\n",
    "\n",
    "La fonction [**`roc_curve`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) calcule la [**courbe caractéristique de fonctionnement du récepteur, ou courbe ROC**](https://en.wikipedia.org/wiki/Receiver_operating_characteristic). Pour citer Wikipedia :\n",
    "\n",
    "_\"Une courbe caractéristique de fonctionnement du récepteur (ROC), ou simplement courbe ROC, est un graphique qui illustre la performance d'un système de classification binaire lorsque son seuil de discrimination est varié. Elle est créée en traçant la fraction de vrais positifs parmi les positifs (TRP = taux de vrais positifs) en fonction de la fraction de faux positifs parmi les négatifs (FRP = taux de faux positifs), pour différentes valeurs de seuil. TRP est également connu sous le nom de sensibilité, et FRP est équivalent à un moins la spécificité ou le taux de vrais négatifs.\"_\n",
    "\n",
    "Cette fonction nécessite les vraies valeurs binaires ainsi que les scores cibles, qui peuvent être soit les estimations de probabilité de la classe positive, soit les valeurs de confiance, soit les décisions binaires. Voici un petit exemple de l'utilisation de la fonction [**`roc_curve`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "y = np.array([1, 1, 2, 2])\n",
    "scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "fpr, tpr, thresholds = roc_curve(y, scores, pos_label=2)\n",
    "fpr\n",
    "# array([0. , 0. , 0.5, 0.5, 1. ])\n",
    "tpr\n",
    "# array([0. , 0.5, 0.5, 1. , 1. ])\n",
    "thresholds\n",
    "# array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparé à des métriques telles que la précision de sous-ensemble, la perte de Hamming ou le score F1, la courbe ROC ne nécessite pas d'optimiser un seuil pour chaque étiquette.\n",
    "\n",
    "La fonction [**`roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html), également appelée ROC-AUC ou AUROC, calcule l'aire sous la courbe ROC. Ainsi, les informations de la courbe sont résumées en un seul nombre.\n",
    "\n",
    "La figure suivante montre la courbe ROC et le score ROC-AUC pour un classificateur visant à distinguer la fleur \"virginica\" du reste des espèces dans l'ensemble de données [**Iris plants** (7.1.1)](https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-dataset) :\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_roc_001.png)\n",
    "\n",
    "Pour plus d'informations, consultez l'[**article Wikipedia sur l'aire sous la courbe**](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cas binaire\n",
    "\n",
    "Dans le cas **binaire**, vous pouvez fournir les estimations de probabilité en utilisant la méthode `classifier.predict_proba()`, ou les valeurs de décision non seuillées fournies par la méthode `classifier.decision_function()`. Dans le cas où vous fournissez les estimations de probabilité, la probabilité de la classe avec l'\"étiquette supérieure\" doit être fournie. L'\"étiquette supérieure\" correspond à `classifier.classes_[1]` et donc `classifier.predict_proba(X)[:, 1]`. Par conséquent, le paramètre `y_score` a une taille de `(n_samples,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "clf = LogisticRegression(solver=\"liblinear\").fit(X, y)\n",
    "clf.classes_\n",
    "# array([0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons utiliser les estimations de probabilité correspondant à `clf.classes_[1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9947412927435125"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_score = clf.predict_proba(X)[:, 1]\n",
    "roc_auc_score(y, y_score)\n",
    "# 0.99..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sinon, nous pouvons utiliser les valeurs de décision non seuillées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9947412927435125"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y, clf.decision_function(X))\n",
    "# 0.99..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cas multi-classes\n",
    "\n",
    "La fonction [**`roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) peut également être utilisée pour la **classification multi-classes**. Deux stratégies de moyennage sont actuellement prises en charge : l'algorithme \"un contre un\" calcule la moyenne des scores de l'aire sous la courbe ROC par paires, et l'algorithme \"un contre tous\" calcule la moyenne des scores de l'aire sous la courbe ROC pour chaque classe par rapport à toutes les autres classes. Dans les deux cas, les étiquettes prédites sont fournies dans un tableau avec des valeurs de 0 à `n_classes`, et les scores correspondent aux estimations de probabilité qu'un échantillon appartienne à une classe particulière. Les algorithmes OvO (un contre un) et OvR (un contre tous) prennent en charge un pondérage uniforme (`average='macro'`) et un pondérage par prévalence (`average='weighted'`).\n",
    "\n",
    "**Algorithme un contre un (OvO) :** Calcule la moyenne de toutes les combinaisons possibles de paires de classes. [HT2001] définit une métrique AUC multiclasse pondérée uniformément :\n",
    "\n",
    "$$\\frac{1}{c(c-1)}\\sum_{j=1}^{c}\\sum_{k > j}^c (\\text{AUC}(j | k) +\n",
    "\\text{AUC}(k | j))$$\n",
    "\n",
    "où $c$ est le nombre de classes et $\\text{AUC}(j | k)$ est l'AUC avec la classe $j$ comme classe positive et la classe $k$ comme classe négative. En général, $\\text{AUC}(j | k) \\neq \\text{AUC}(k | j)$ dans le cas multiclasse. Cet algorithme est utilisé en définissant l'argument `multiclass` sur `'ovo'` et `average` sur `'macro'`.\n",
    "\n",
    "La métrique AUC multiclasse de [HT2001] peut être étendue pour être pondérée par la prévalence :\n",
    "\n",
    "$$\\frac{1}{c(c-1)}\\sum_{j=1}^{c}\\sum_{k > j}^c p(j \\cup k)(\n",
    "\\text{AUC}(j | k) + \\text{AUC}(k | j))$$\n",
    "\n",
    "où $c$ est le nombre de classes. Cet algorithme est utilisé en définissant l'argument `multiclass` sur `'ovo'` et `average` sur `'weighted'`. L'option `'weighted'` renvoie une moyenne pondérée par la prévalence comme décrit dans [FC2009].\n",
    "\n",
    "**Algorithme un contre tous (OvR) :** Calcule l'AUC de chaque classe par rapport au reste [PD2000]. L'algorithme est fonctionnellement le même que dans le cas multilabel. Pour activer cet algorithme, définissez l'argument `multiclass` sur `'ovr'`. En plus de la moyenne `'macro'` [F2006] et `'weighted'` [F2001], OvR prend en charge la moyenne `'micro'`.\n",
    "\n",
    "Dans les applications où un taux de faux positifs élevé n'est pas tolérable, le paramètre `max_fpr` de [**`roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) peut être utilisé pour résumer la courbe ROC jusqu'à la limite donnée.\n",
    "\n",
    "La figure suivante montre la courbe ROC micro-pondérée et son score ROC-AUC correspondant pour un classificateur visant à distinguer les différentes espèces dans l'ensemble de données [**Iris plants** (7.1.1)](https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-dataset) :\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_roc_002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cas multi-étiquettes\n",
    "\n",
    "Dans la **classification multi-étiquettes**, la fonction [**`roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) est étendue par la moyenne des étiquettes comme [**ci-dessus** (3.3.2.1)](https://scikit-learn.org/stable/modules/model_evaluation.html#average). Dans ce cas, vous devez fournir un `y_score` de forme `(n_samples, n_classes)`. Ainsi, lors de l'utilisation des estimations de probabilité, il est nécessaire de sélectionner la probabilité de la classe avec l'étiquette supérieure pour chaque sortie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.82664884, 0.86034414, 0.94181818, 0.8502652 , 0.94809095])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "X, y = make_multilabel_classification(random_state=0)\n",
    "inner_clf = LogisticRegression(solver=\"liblinear\", random_state=0)\n",
    "clf = MultiOutputClassifier(inner_clf).fit(X, y)\n",
    "y_score = np.transpose([y_pred[:, 1] for y_pred in clf.predict_proba(X)])\n",
    "roc_auc_score(y, y_score, average=None)\n",
    "# array([0.82..., 0.86..., 0.94..., 0.85... , 0.94...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et les valeurs de décision ne nécessitent pas un tel traitement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81996435, 0.8467387 , 0.93090909, 0.87229702, 0.94422994])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "clf = RidgeClassifierCV().fit(X, y)\n",
    "y_score = clf.decision_function(X)\n",
    "roc_auc_score(y, y_score, average=None)\n",
    "# array([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemples\n",
    "\n",
    "##### [**Courbe de Caractéristique de Fonctionnement du Récepteur (ROC) Multiclasse**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/3_model_selection/plot_roc.ipynb)<br/>([*Multiclass Receiver Operating Characteristic (ROC)*](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html))\n",
    "\n",
    "Exemple de l'utilisation de la courbe ROC pour évaluer la qualité de la sortie d'un classifieur.\n",
    "\n",
    "##### [**Caractéristique de Fonctionnement du Récepteur (Receiver Operating Characteristic - ROC) avec validation croisée**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/3_model_selection/plot_roc_crossval.ipynb)<br/>([*Receiver Operating Characteristic (ROC) with cross validation*](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html))\n",
    "\n",
    "Exemple de l'utilisation de la courbe ROC pour évaluer la qualité de la sortie d'un classifieur, en utilisant une validation croisée.\n",
    "\n",
    "##### Modélisation de la distribution des espèces -> à compléter\n",
    "\n",
    "Exemple de l'utilisation de la courbe ROC pour modéliser la distribution des espèces."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
