{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='model-selection-and-evaluation'></a> 3. [**Sélection et évaluation de modèle**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/3_model_selection_and_evaluation.ipynb#model-selection-and-evaluation)</br>([*Model selection and evaluation*](https://scikit-learn.org/stable/model_selection.html#model-selection-and-evaluation))\n",
    "\n",
    "# 3.3. [**Métriques et scoring : quantifier la qualité des prédictions**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/3_model_selection_and_evaluation.ipynb#metrics-and-scoring-quantifying-the-quality-of-predictions)<br/>([_Metrics and scoring: quantifying the quality of predictions_](https://scikit-learn.org/stable/model_selection.html#metrics-and-scoring-quantifying-the-quality-of-predictions))\n",
    "\n",
    "Il existe 3 API différents pour évaluer la qualité des prédictions d'un modèle :\n",
    "\n",
    "- **Méthode de score de l'estimateur :** Les estimateurs possèdent une méthode `score` fournissant un critère d'évaluation par défaut pour le problème auquel ils sont conçus pour résoudre. Cela n'est pas discuté sur cette page, mais dans la documentation de chaque estimateur.\n",
    "\n",
    "- **Paramètre `scoring` :** Les outils d'évaluation du modèle utilisant la [**validation croisée** (3.1)](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) (comme [**`model_selection.cross_val_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) et [**`model_selection.GridSearchCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)) reposent sur une stratégie de score interne. Cela est expliqué dans la section [**Paramètre `scoring` : définir les règles d'évaluation du modèle** (3.3.1)](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter).\n",
    "\n",
    "- **Fonctions de métriques :** Le module [**`sklearn.metrics`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) implémente des fonctions permettant d'évaluer l'erreur de prédiction à des fins spécifiques. Ces métriques sont détaillées dans les sections sur les [**Métriques de classification** (3.3.2)](https://scikit-learn.org/stable/modules/model_evaluation.html), les [**Métriques de classement multi-étiquettes** (3.3.3)](https://scikit-learn.org/stable/modules/model_evaluation.html#multilabel-ranking-metrics), les [**Métriques de régression** (3.3.4)](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics) et les [**Métriques de regroupement** (3.3.5)](https://scikit-learn.org/stable/modules/model_evaluation.html#clustering-metrics).\n",
    "\n",
    "Enfin, les [**Estimateurs factices** (3.3.6)](https://scikit-learn.org/stable/modules/model_evaluation.html#dummy-estimators) sont utiles pour obtenir une valeur de référence pour ces métriques pour des prédictions aléatoires.\n",
    "\n",
    "> **Voir aussi :** Pour les métriques \"pair à pair\", entre les _échantillons_ et non les estimateurs ou les prédictions, voir la section [**Métriques pair à pair, affinités et noyaux** (6.8)](https://scikit-learn.org/stable/modules/metrics.html#metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 63 pages, 19 exemples, 24 papiers\n",
    "- 3.3.1. [**Le paramètre `scoring` : définir les règles d'évaluation du modèle**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/3_model_selection_and_evaluation.ipynb#the-scoring-parameter-defining-model-evaluation-rules)<br/>([_The `scoring` parameter: defining model evaluation rules_](https://scikit-learn.org/stable/model_selection.html#the-scoring-parameter-defining-model-evaluation-rules))\n",
    "- 3.3.2. [**Métriques de classification**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/3_model_selection_and_evaluation.ipynb#classification-metrics)<br/>([_Classification metrics_](https://scikit-learn.org/stable/model_selection.html#classification-metrics))\n",
    "- 3.3.3. [**Métriques de classement multi-étiquettes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/3_model_selection_and_evaluation.ipynb#multilabel-ranking-metrics)<br/>([_Multilabel ranking metrics_](https://scikit-learn.org/stable/model_selection.html#multilabel-ranking-metrics))\n",
    "- 3.3.4. [**Métriques de régression**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/3_model_selection_and_evaluation.ipynb#regression-metrics)<br/>([_Regression metrics_](https://scikit-learn.org/stable/model_selection.html#regression-metrics))\n",
    "- 3.3.5. [**Métriques de clustering**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/3_model_selection_and_evaluation.ipynb#clustering-metrics)<br/>([_Clustering metrics_](https://scikit-learn.org/stable/model_selection.html#clustering-metrics))\n",
    "- 3.3.6. [**Estimateurs factices**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/3_model_selection_and_evaluation.ipynb#dummy-estimators)<br/>([_Dummy estimators_](https://scikit-learn.org/stable/model_selection.html#dummy-estimators))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='the-scoring-parameter-defining-model-evaluation-rules'></a> 3.3.1. Le paramètre `scoring` : définir les règles d'évaluation du modèle\n",
    "\n",
    "La sélection et l'évaluation des modèles à l'aide d'outils tels que [**`model_selection.GridSearchCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) et [**`model_selection.cross_val_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score) nécessitent un paramètre `scoring` qui contrôle la métrique appliquée aux estimateurs évalués.\n",
    "\n",
    "### <a id='common-cases-predefined-values'></a> 3.3.1.1. Cas courants : valeurs prédéfinies\n",
    "\n",
    "Pour les cas d'utilisation les plus courants, vous pouvez désigner un objet de métrique avec le paramètre `scoring`. Le tableau ci-dessous montre toutes les valeurs possibles. Tous les objets de métrique suivent la convention selon laquelle **des valeurs de retour plus élevées sont meilleures que des valeurs de retour plus basses**. Ainsi, les métriques qui mesurent la distance entre le modèle et les données, comme [**`metrics.mean_squared_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html), sont disponibles sous la forme `neg_mean_squared_error`, qui renvoie la valeur négative de la métrique.\n",
    "\n",
    "| Métrique                             | Fonction                                                        | Commentaire                 |\n",
    "|:-------------------------------------|:----------------------------------------------------------------|:----------------------------|\n",
    "| **Classification**                   |                                                                 |                             |\n",
    "| ‘accuracy’                           | [**`metrics.accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)                |                             |\n",
    "| ‘balanced_accuracy’                  | [**`metrics.balanced_accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html)       |                             |\n",
    "| ‘top_k_accuracy’                     | [**`metrics.top_k_accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.top_k_accuracy_score.html)             |                             |\n",
    "| ‘average_precision’                  | [**`metrics.average_precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html)       |                             |\n",
    "| ‘neg_brier_score’                    | [**`metrics.brier_score_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html)                    |                             |\n",
    "| ‘f1’                                 | [**`metrics.f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)                                  | pour les cibles binaires   |\n",
    "| ‘f1_micro’                           | [**`metrics.f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)                                  | micro-average              |\n",
    "| ‘f1_macro’                           | [**`metrics.f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)                                  | macro-average              |\n",
    "| ‘f1_weighted’                        | [**`metrics.f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)                                  | moyenne pondérée           |\n",
    "| ‘f1_samples’                         | [**`metrics.f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)                                  | par échantillon multilabel |\n",
    "| ‘neg_log_loss’                       | [**`metrics.log_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html)                                  | nécessite le support de predict_proba|\n",
    "| ‘precision’ etc.                     | [**`metrics.precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)                    | suffixes comme ‘f1’        |\n",
    "| ‘recall’ etc.                        | [**`metrics.recall_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)                          | suffixes comme ‘f1’        |\n",
    "| ‘jaccard’ etc.                       | [**`metrics.jaccard_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html)                         | suffixes comme ‘f1’        |\n",
    "| ‘roc_auc’                            | [**`metrics.roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)                           |                           |\n",
    "| ‘roc_auc_ovr’                        | [**`metrics.roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)                           |                           |\n",
    "| ‘roc_auc_ovo’                        | [**`metrics.roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)                           |                           |\n",
    "| ‘roc_auc_ovr_weighted’               | [**`metrics.roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)                           |                           |\n",
    "| ‘roc_auc_ovo_weighted’               | [**`metrics.roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)                           |                           |\n",
    "| **Clustering**                       |                                                                |                           |\n",
    "| ‘adjusted_mutual_info_score’         | [**`metrics.adjusted_mutual_info_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html) |                           |\n",
    "| ‘adjusted_rand_score’                | [**`metrics.adjusted_rand_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html)           |                           |\n",
    "| ‘completeness_score’                 | [**`metrics.completeness_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.completeness_score.html)             |                           |\n",
    "| ‘fowlkes_mallows_score’              | [**`metrics.fowlkes_mallows_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fowlkes_mallows_score.html)       |                           |\n",
    "| ‘homogeneity_score’                  | [**`metrics.homogeneity_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html)               |                           |\n",
    "| ‘mutual_info_score’                  | [**`metrics.mutual_info_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mutual_info_score.html)               |                           |\n",
    "| ‘normalized_mutual_info_score’       | [**`metrics.normalized_mutual_info_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html) |                           |\n",
    "| ‘rand_score’                         | [**`metrics.rand_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.rand_score.html)                               |                           |\n",
    "| ‘v_measure_score’                    | [**`metrics.v_measure_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html)                     |                           |\n",
    "| **Regression**                       |                                                                |                           |\n",
    "| ‘explained_variance’                 | [**`metrics.explained_variance_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.explained_variance_score.html)   |                           |\n",
    "| ‘max_error’                          | [**`metrics.max_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.max_error.html)                                 |                           |\n",
    "| ‘neg_mean_absolute_error’            | [**`metrics.mean_absolute_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html)           |                           |\n",
    "| ‘neg_mean_squared_error’             | [**`metrics.mean_squared_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)             |                           |\n",
    "| ‘neg_root_mean_squared_error’        | [**`metrics.mean_squared_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)             |                           |\n",
    "| ‘neg_mean_squared_log_error’         | [**`metrics.mean_squared_log_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_log_error.html)     |                           |\n",
    "| ‘neg_median_absolute_error’          | [**`metrics.median_absolute_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.median_absolute_error.html)       |                           |\n",
    "| ‘r2’                                 | [**`metrics.r2_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)                                   |                           |\n",
    "| ‘neg_mean_poisson_deviance’          | [**`metrics.mean_poisson_deviance`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_poisson_deviance.html)       |                           |\n",
    "| ‘neg_mean_gamma_deviance’            | [**`metrics.mean_gamma_deviance`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_gamma_deviance.html)           |                           |\n",
    "| ‘neg_mean_absolute_percentage_error’ | [**`metrics.mean_absolute_percentage_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_percentage_error.html) |                           |\n",
    "| ‘d2_absolute_error_score’            | [**`metrics.d2_absolute_error_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.d2_absolute_error_score.html)       |                           |\n",
    "| ‘d2_pinball_score’                   | [**`metrics.d2_pinball_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.d2_pinball_score.html)                 |                           |\n",
    "| ‘d2_tweedie_score’                   | [**`metrics.d2_tweedie_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.d2_tweedie_score.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemples d'utilisation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96666667, 0.96666667, 0.96666667, 0.93333333, 1.        ])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "clf = svm.SVC(random_state=0)\n",
    "cross_val_score(clf, X, y, cv=5, scoring='recall_macro')\n",
    "# array([0.96..., 0.96..., 0.96..., 0.93..., 1.        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** Si un nom de métrique incorrect est passé, une `InvalidParameterError` est levée. Vous pouvez récupérer les noms de toutes les métriques disponibles en appelant [**`get_scorer_names`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.get_scorer_names.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='defining-your-scoring-strategy-from-metric-functions'></a> 3.3.1.2. Définir votre stratégie d'évaluation à partir de fonctions de métriques\n",
    "\n",
    "Le module [**`sklearn.metrics`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) expose également un ensemble de fonctions simples mesurant l'erreur de prédiction en fonction des vérités terrain (ground truth) et des prédictions :\n",
    "- Les fonctions se terminant par `_score` renvoient une valeur à maximiser, donc plus élevée est meilleure.\n",
    "- Les fonctions se terminant par `_error` ou `_loss` renvoient une valeur à minimiser, donc plus faible est meilleure. Lors de la conversion en objet de métrique en utilisant [**`make_scorer`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html), vous pouvez fixer le paramètre `greater_is_better` à `False` (`True` par défaut ; voir la description du paramètre ci-dessous).\n",
    "\n",
    "Les métriques disponibles pour diverses tâches d'apprentissage automatique sont détaillées dans les sections ci-dessous.\n",
    "\n",
    "De nombreuses métriques n'ont pas de noms spécifiques à utiliser comme valeurs de `scoring`, parfois parce qu'elles nécessitent des paramètres supplémentaires, tels que [**`fbeta_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html). Dans de tels cas, vous devez générer un objet de métrique approprié. La façon la plus simple de générer un objet callable pour la métrique est d'utiliser [**`make_scorer`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html). Cette fonction convertit les métriques en objets callable pouvant être utilisés pour l'évaluation du modèle.\n",
    "\n",
    "Un cas d'utilisation typique consiste à envelopper une fonction de métrique existante de la bibliothèque avec des valeurs autres que celles par défaut pour ses paramètres, tels que le paramètre `beta` pour la fonction [**`fbeta_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "grid = GridSearchCV(LinearSVC(dual=\"auto\"), param_grid={'C': [1, 10]},\n",
    "                    scoring=ftwo_scorer, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le deuxième cas d'utilisation consiste à créer un objet de métrique complètement personnalisé à partir d'une simple fonction Python en utilisant [**`make_scorer`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html), qui peut prendre plusieurs paramètres :\n",
    "- la fonction Python que vous souhaitez utiliser (`my_custom_loss_func` dans l'exemple ci-dessous)\n",
    "- si la fonction Python renvoie un score (`greater_is_better=True`, la valeur par défaut) ou une perte (`greater_is_better=False`). S'il s'agit d'une perte, la sortie de la fonction Python est inversée par l'objet de métrique, conformément à la convention de validation croisée selon laquelle les métriques renvoient des valeurs plus élevées pour les meilleurs modèles.\n",
    "- pour les métriques de classification uniquement : si la fonction Python que vous avez fournie nécessite des certitudes de décision continues (`needs_threshold=True`). La valeur par défaut est False.\n",
    "- tous les paramètres supplémentaires, tels que `beta` ou `labels` dans [**`f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score).\n",
    "\n",
    "Voici un exemple de création de métriques personnalisées et d'utilisation du paramètre `greater_is_better` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6931471805599453"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def my_custom_loss_func(y_true, y_pred):\n",
    "    diff = np.abs(y_true - y_pred).max()\n",
    "    return np.log1p(diff)\n",
    "\n",
    "# score will negate the return value of my_custom_loss_func,\n",
    "# which will be np.log(2), 0.693, given the values for X\n",
    "# and y defined below.\n",
    "score = make_scorer(my_custom_loss_func, greater_is_better=False)\n",
    "X = [[1], [1]]\n",
    "y = [0, 1]\n",
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier(strategy='most_frequent', random_state=0)\n",
    "clf = clf.fit(X, y)\n",
    "my_custom_loss_func(y, clf.predict(X))\n",
    "# 0.69...\n",
    "score(clf, X, y)\n",
    "# -0.69..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='implementing-your-own-scoring-object'></a> 3.3.1.2. Implémentation de votre propre objet de métrique\n",
    "\n",
    "Vous pouvez créer des métriques de modèle encore plus flexibles en construisant votre propre objet de métrique à partir de zéro, sans utiliser la fabrique [**`make_scorer`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html).\n",
    "\n",
    "**Comment créer une métrique à partir de zéro**\n",
    "\n",
    "Pour qu'un callable soit une métrique, il doit respecter le protocole spécifié par les deux règles suivantes :\n",
    "- Il peut être appelé avec les paramètres `(estimator, X, y)`, où `estimator` est le modèle qui doit être évalué, `X` est la donnée de validation et `y` est la vérité terrain pour `X` (dans le cas supervisé) ou `None` (dans le cas non supervisé).\n",
    "- Il doit renvoyer un nombre à virgule flottante qui quantifie la qualité de prédiction de `estimator` sur `X`, par rapport à `y`. Encore une fois, selon la convention, les nombres plus élevés sont meilleurs. Donc, si votre métrique renvoie une perte, cette valeur doit être négative.\n",
    "- **Avancé** : Si cela nécessite des métadonnées supplémentaires à lui passer, il doit exposer une méthode `get_metadata_routing` renvoyant les métadonnées demandées. L'utilisateur doit pouvoir définir les métadonnées demandées via une méthode `set_score_request`. Veuillez consulter [**Routage de métadonnées** (User Guide _exp)](https://scikit-learn.org/stable/metadata_routing.html#metadata-routing) et [**Routage de métadonnées** (Developer Guide _exp)](https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_metadata_routing.html#sphx-glr-auto-examples-miscellaneous-plot-metadata-routing-py) pour plus de détails.\n",
    "\n",
    "> **Note:** **Utilisation de métriques personnalisées dans des fonctions avec `n_jobs > 1`**\n",
    ">\n",
    "> Bien que la définition de la fonction de métrique personnalisée aux côtés de la fonction d'appel fonctionne par défaut avec le backend joblib (loky), l'importer à partir d'un autre module sera une approche plus robuste et fonctionnera indépendamment du backend joblib.\n",
    ">\n",
    "> Par exemple, pour utiliser `n_jobs` supérieur à 1 dans l'exemple ci-dessous, la fonction `custom_scoring_function` est sauvegardée dans un module créé par l'utilisateur (`custom_scorer_module.py`) et importée :\n",
    "> ```python\n",
    "> >>> from custom_scorer_module import custom_scoring_function \n",
    "> >>> cross_val_score(model,\n",
    "> ...  X_train,\n",
    "> ...  y_train,\n",
    "> ...  scoring=make_scorer(custom_scoring_function, greater_is_better=False),\n",
    "> ...  cv=5,\n",
    "> ...  n_jobs=-1) \n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='using-multiple-metric-evaluation'></a> 3.3.1.4. Utilisation de plusieurs métriques d'évaluation\n",
    "\n",
    "Scikit-learn permet également l'évaluation de plusieurs métriques dans `GridSearchCV`, `RandomizedSearchCV` et `cross_validate`.\n",
    "\n",
    "Il existe trois façons de spécifier plusieurs métriques `scoring` pour le paramètre `scoring` :\n",
    "\n",
    "#### Comme un ensemble de métriques sous forme de chaînes de caractères"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['accuracy', 'precision']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comme un `dict` associant le nom de l'évaluateur à la fonction de mesure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "           'prec': 'precision'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez que les valeurs du dictionnaire peuvent être des fonctions de mesure ou l'une des chaînes prédéfinies de métriques.\n",
    "\n",
    "#### Comme une fonction appelable qui renvoie un dictionnaire de scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10  9  8  7  8]\n",
      "[0 1 2 3 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# A sample toy binary classification dataset\n",
    "X, y = datasets.make_classification(n_classes=2, random_state=0)\n",
    "svm = LinearSVC(dual=\"auto\", random_state=0)\n",
    "def confusion_matrix_scorer(clf, X, y):\n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    return {'tn': cm[0, 0], 'fp': cm[0, 1],\n",
    "            'fn': cm[1, 0], 'tp': cm[1, 1]}\n",
    "cv_results = cross_validate(svm, X, y, cv=5,\n",
    "                            scoring=confusion_matrix_scorer)\n",
    "# Getting the test set true positive scores\n",
    "print(cv_results['test_tp'])\n",
    "# [10  9  8  7  8]\n",
    "# Getting the test set false negative scores\n",
    "print(cv_results['test_fn'])\n",
    "# [0 1 2 3 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='classification-metrics'></a> 3.3.2. Métriques de classification\n",
    "\n",
    "Le module [**`sklearn.metrics`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) implémente plusieurs fonctions de perte, de score et d'utilité pour mesurer les performances de classification. Certaines métriques peuvent nécessiter des estimations de probabilité de la classe positive, des valeurs de confiance ou des valeurs de décision binaires. La plupart des implémentations permettent à chaque échantillon de contribuer de manière pondérée au score global, grâce au paramètre `sample_weight`.\n",
    "\n",
    "### Tableau 1: Métriques de classification pour les cas de classification binaire\n",
    "\n",
    "Certaines de ces métriques sont restreintes au cas de classification binaire :\n",
    "\n",
    "| Fonction                                | Description                                                                                |\n",
    "|:----------------------------------------|:-------------------------------------------------------------------------------------------|\n",
    "| [**`precision_recall_curve`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve)(y_true, probas_pred, *) | Calcule les couples précision-rappel pour différentes valeurs de seuil de probabilité. |\n",
    "| [**`roc_curve`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve)(y_true, y_score, *[, pos_label, ...]) | Calcule la courbe de caractéristique de fonctionnement du récepteur (ROC). |\n",
    "| [**`class_likelihood_ratios`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.class_likelihood_ratios.html#sklearn.metrics.class_likelihood_ratios)(y_true, y_pred, *[, ...]) | Calcule les rapports de probabilités positifs et négatifs de classification binaire. |\n",
    "| [**`det_curve`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.det_curve.html#sklearn.metrics.det_curve)(y_true, y_score[, pos_label, ...]) | Calcule les taux d'erreur pour différentes valeurs de seuil de probabilité. |\n",
    "\n",
    "### Tableau 2: Métriques de classification pour les cas de classification multiclasse\n",
    "\n",
    "D'autres fonctionnent également dans le cas de classification multiclasse :\n",
    "\n",
    "| Fonction                                | Description                                                                                |\n",
    "|:----------------------------------------|:-------------------------------------------------------------------------------------------|\n",
    "| [**`balanced_accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html)(y_true, y_pred, *[, ...]) | Calcule la précision équilibrée. |\n",
    "| [**`cohen_kappa_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html)(y1, y2, *[, labels, ...]) | Calcule le kappa de Cohen : une statistique mesurant l'accord entre annotateurs. |\n",
    "| [**`confusion_matrix`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)(y_true, y_pred, *[, ...]) | Calcule la matrice de confusion pour évaluer l'exactitude d'une classification. |\n",
    "| [**`hinge_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hinge_loss.html)(y_true, pred_decision, *[, ...]) | Perte de charnière moyenne (non régularisée). |\n",
    "| [**`matthews_corrcoef`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)(y_true, y_pred, *[, ...]) | Calcule le coefficient de corrélation de Matthews (MCC). |\n",
    "| [**`roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)(y_true, y_score, *[, average, ...]) | Calcule l'aire sous la courbe ROC (ROC AUC) à partir des scores de prédiction. |\n",
    "| [**`top_k_accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.top_k_accuracy_score.html)(y_true, y_score, *[, ...]) | Score de classification Top-k Accuracy. |\n",
    "\n",
    "### Tableau 3: Métriques de classification pour les cas de classification multilabel\n",
    "\n",
    "Certaines fonctionnent également dans le cas de classification multilabel :\n",
    "\n",
    "| Fonction                                | Description                                                                                |\n",
    "|:----------------------------------------|:-------------------------------------------------------------------------------------------|\n",
    "| [**`accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score)(y_true, y_pred, *[, ...]) | Score de classification Accuracy. |\n",
    "| [**`classification_report`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report)(y_true, y_pred, *[, ...]) | Génère un rapport texte présentant les principales métriques de classification. |\n",
    "| [**`f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score)(y_true, y_pred, *[, labels, ...]) | Calcule le score F1, également connu sous le nom de F-score équilibré ou mesure F. |\n",
    "| [**`fbeta_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score)(y_true, y_pred, *, beta[, ...]) | Calcule le score F-beta. |\n",
    "| [**`hamming_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html#sklearn.metrics.hamming_loss)(y_true, y_pred, *[, sample_weight]) | Calcule la perte de Hamming moyenne. |\n",
    "| [**`jaccard_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html#sklearn.metrics.jaccard_score)(y_true, y_pred, *[, labels, ...]) | Score de similarité de Jaccard. |\n",
    "| [**`log_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss)(y_true, y_pred, *[, eps, ...]) | Perte logarithmique, également appelée perte logistique ou perte d'entropie croisée. |\n",
    "| [**`multilabel_confusion_matrix`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html#sklearn.metrics.multilabel_confusion_matrix)(y_true, y_pred, *) | Calcule une matrice de confusion pour chaque classe ou échantillon. |\n",
    "| [**`precision_recall_fscore_support`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support)(y_true, ...) | Calcule la précision, le rappel, le score F et le support pour chaque classe. |\n",
    "| [**`precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)(y_true, y_pred, *[, labels, ...]) | Calcule la précision. |\n",
    "| [**`recall_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)(y_true, y_pred, *[, labels, ...]) | Calcule le rappel. |\n",
    "\n",
    "### Tableau 4: Métriques de classification pour les cas de classification binaire et multilabel (mais pas multiclasse)\n",
    "\n",
    "Et certaines fonctionnent pour les problèmes binaires et multilabel (mais pas multiclasse) :\n",
    "\n",
    "| Fonction                                | Description                                                                                |\n",
    "|:----------------------------------------|:-------------------------------------------------------------------------------------------|\n",
    "| [**`average_precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html)(y_true, y_score, *) | Calcule la précision moyenne (AP) à partir des scores de prédiction. |\n",
    "\n",
    "Dans les sous-sections suivantes, nous décrirons chacune de ces fonctions, précédées de quelques notes sur l'API commune et la définition des métriques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='classification-metrics'></a> 3.3.2.1. De la classification binaire à la classification multiclasse et multilabel\n",
    "\n",
    "Certaines métriques sont essentiellement définies pour les tâches de classification binaire (par exemple, [**`f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html), [**`roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)). Dans ces cas, par défaut, seule l'étiquette positive est évaluée, en supposant que la classe positive est étiquetée `1` (bien que cela puisse être configuré via le paramètre `pos_label`).\n",
    "\n",
    "Lors de l'extension d'une métrique binaire aux problèmes de classification multiclasse ou multilabel, les données sont traitées comme une collection de problèmes binaires, un pour chaque classe. Il existe alors plusieurs façons de calculer la moyenne des métriques binaires pour l'ensemble des classes, chacune pouvant être utile dans certains scénarios. Lorsque cela est disponible, vous devez sélectionner l'une de ces options à l'aide du paramètre `average`.\n",
    "\n",
    "- `\"macro\"` calcule simplement la moyenne des métriques binaires, en accordant un poids égal à chaque classe. Dans les problèmes où les classes peu fréquentes sont néanmoins importantes, le calcul macro-averaging peut mettre en évidence leurs performances. D'un autre côté, l'hypothèse selon laquelle toutes les classes sont également importantes est souvent fausse, de sorte que le macro-averaging surestimera généralement les performances généralement faibles d'une classe peu fréquente.\n",
    "- `\"weighted\"` prend en compte le déséquilibre de classe en calculant la moyenne des métriques binaires dans lesquelles le score de chaque classe est pondéré par sa présence dans l'échantillon de données réelles.\n",
    "- `\"micro\"` donne à chaque paire échantillon-classe une contribution égale à la métrique globale (à l'exception du poids de l'échantillon). Au lieu de sommer la métrique par classe, cela additionne les dividendes et les diviseurs qui composent les métriques par classe pour calculer un quotient global. Le micro-averaging peut être préféré dans les paramètres multilabel, y compris la classification multiclasse où une classe majoritaire doit être ignorée.\n",
    "- `\"samples\"` s'applique uniquement aux problèmes multilabel. Il ne calcule pas une mesure par classe, mais calcule plutôt la métrique sur les classes réelles et prédites pour chaque échantillon dans les données d'évaluation, et retourne leur moyenne (`sample_weight-weighted`).\n",
    "- La sélection de `average=None` renverra un tableau avec le score pour chaque classe.\n",
    "\n",
    "Alors que les données multiclasse sont fournies à la métrique, tout comme les cibles binaires, sous forme de tableau d'étiquettes de classe, les données multilabel sont spécifiées sous forme de matrice indicatrice, dans laquelle la cellule `[i, j]` a la valeur 1 si l'échantillon `i` a l'étiquette `j` et la valeur 0 sinon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='accuracy-score'></a> 3.3.2.2. Score d'exactitude (Accuracy score)\n",
    "\n",
    "La fonction [**`accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) calcule l'[**exactitude**](https://en.wikipedia.org/wiki/Accuracy_and_precision) soit en tant que fraction (par défaut), soit en tant que nombre (`normalize=False`) de prédictions correctes.\n",
    "\n",
    "Dans la classification multi-étiquette, la fonction renvoie l'exactitude du sous-ensemble. Si l'ensemble entier des étiquettes prédites pour un échantillon correspond strictement à l'ensemble réel des étiquettes, alors l'exactitude du sous-ensemble est de 1.0 ; sinon, elle est de 0.0.\n",
    "\n",
    "Si $\\hat{y}_i$ représente la valeur prédite du $i$-ème échantillon et $y_i$ représente la valeur réelle correspondante, alors la fraction de prédictions correctes sur $n_\\text{samples}$ est définie comme suit :\n",
    "\n",
    "$$\\texttt{accuracy}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples}-1} 1(\\hat{y}_i = y_i)$$\n",
    "\n",
    "où $1(x)$ est la [**fonction indicatrice**](https://en.wikipedia.org/wiki/Indicator_function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = [0, 2, 1, 3]\n",
    "y_true = [0, 1, 2, 3]\n",
    "accuracy_score(y_true, y_pred)\n",
    "# 0.5\n",
    "accuracy_score(y_true, y_pred, normalize=False)\n",
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas multi-étiquette avec des indicateurs d'étiquettes binaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemple\n",
    "\n",
    "##### [**Test par permutations de la signifiance d'un score de classification**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/3_model_selection/plot_permutation_tests_for_classification.ipynb)<br/>([_Test with permutations the significance of a classification score_](https://scikit-learn.org/stable/auto_examples/model_selection/plot_permutation_tests_for_classification.html))\n",
    "\n",
    "Exemple d'utilisation du score d'exactitude en utilisant des permutations de l'ensemble de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='top-k-accuracy-score'></a> 3.3.2.3. Top-k accuracy score\n",
    "\n",
    "The [**`top_k_accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.top_k_accuracy_score.html) function is a generalization of [**`accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html). The difference is that a prediction is considered correct as long as the true label is associated with one of the `k` highest predicted scores. [**`accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) is the special case of `k = 1`.\n",
    "\n",
    "The function covers the binary and multiclass classification cases but not the multilabel case.\n",
    "\n",
    "If $\\hat{f}_{i,j}$ is the predicted class for the $i$-th sample corresponding to the $j$-th largest predicted score and $y_i$ is the corresponding true value, then the fraction of correct predictions over $n_\\text{samples}$ is defined as\n",
    "\n",
    "$$\\texttt{top-k accuracy}(y, \\hat{f}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples}-1} \\sum_{j=1}^{k} 1(\\hat{f}_{i,j} = y_i)$$\n",
    "\n",
    "where $k$ is the number of guesses allowed and $1(x)$ is the [**indicator function**](https://en.wikipedia.org/wiki/Indicator_function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='top-k-accuracy-score'></a> 3.3.2.3. Score d'exactitude Top-k (Top-k accuracy score)\n",
    "\n",
    "La fonction [**`top_k_accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.top_k_accuracy_score.html) est une généralisation du [**`accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html). La différence est qu'une prédiction est considérée comme correcte tant que l'étiquette réelle est associée à l'une des `k` valeurs de prédiction les plus élevées. [**`accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) est le cas spécial où `k = 1`.\n",
    "\n",
    "La fonction couvre les cas de classification binaire et multiclasse, mais pas le cas multi-étiquette.\n",
    "\n",
    "Si $\\hat{f}_{i,j}$ représente la classe prédite pour le $i$-ème échantillon correspondant au $j$-ème score de prédiction le plus élevé et $y_i$ représente la valeur réelle correspondante, alors la fraction de prédictions correctes sur $n_\\text{samples}$ est définie comme suit :\n",
    "\n",
    "$$\\texttt{top-k accuracy}(y, \\hat{f}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples}-1} \\sum_{j=1}^{k} 1(\\hat{f}_{i,j} = y_i)$$\n",
    "\n",
    "où $k$ est le nombre de prédictions autorisées et $1(x)$ est la [**fonction indicatrice**](https://en.wikipedia.org/wiki/Indicator_function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "y_true = np.array([0, 1, 2, 2])\n",
    "y_score = np.array([[0.5, 0.2, 0.2],\n",
    "                    [0.3, 0.4, 0.2],\n",
    "                    [0.2, 0.4, 0.3],\n",
    "                    [0.7, 0.2, 0.1]])\n",
    "top_k_accuracy_score(y_true, y_score, k=2)\n",
    "# 0.75\n",
    "# Not normalizing gives the number of \"correctly\" classified samples\n",
    "top_k_accuracy_score(y_true, y_score, k=2, normalize=False)\n",
    "# 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='balanced-accuracy-score'></a> 3.3.2.4. Score d'exactitude équilibrée (Balanced accuracy score)\n",
    "\n",
    "La fonction [**`balanced_accuracy_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html) calcule l'[**exactitude équilibrée**](https://en.wikipedia.org/wiki/Accuracy_and_precision), ce qui évite les estimations de performance biaisées sur des ensembles de données déséquilibrés. C'est la macro-moyenne des scores de rappel par classe, ou, de manière équivalente, l'exactitude brute où chaque échantillon est pondéré en fonction de l'inverse de la prévalence de sa classe réelle. Ainsi, pour les ensembles de données équilibrés, le score est égal à l'exactitude.\n",
    "\n",
    "Dans le cas binaire, l'exactitude équilibrée est égale à la moyenne arithmétique de la [**sensibilité**](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) (taux de vrais positifs) et de la [**spécificité**](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) (taux de vrais négatifs), ou la surface sous la courbe ROC avec des prédictions binaires plutôt que des scores :\n",
    "\n",
    "$$\\texttt{balanced-accuracy} = \\frac{1}{2}\\left( \\frac{TP}{TP + FN} + \\frac{TN}{TN + FP}\\right )$$\n",
    "\n",
    "Si le classifieur fonctionne également bien pour les deux classes, ce terme se réduit à l'exactitude conventionnelle (c'est-à-dire le nombre de prédictions correctes divisé par le nombre total de prédictions).\n",
    "\n",
    "En revanche, si l'exactitude conventionnelle est supérieure au hasard uniquement parce que le classifieur profite d'un ensemble de test déséquilibré, alors l'exactitude équilibrée, le cas échéant, se réduira à $\\frac{1}{n\\_classes}$.\n",
    "\n",
    "Le score varie de 0 à 1, ou lorsqu'on utilise `adjusted=True`, il est recalé dans la plage $\\frac{1}{1 - n\\_classes}$ à 1, inclusivement, avec une performance aléatoire obtenant un score de 0.\n",
    "\n",
    "Si $y_i$ est la vraie valeur du $i$-ème échantillon, et $w_i$ est le poids d'échantillon correspondant, alors nous ajustons le poids d'échantillon de la manière suivante :\n",
    "\n",
    "$$\\hat{w}_i = \\frac{w_i}{\\sum_j{1(y_j = y_i) w_j}}$$\n",
    "\n",
    "où $1(x)$ est la [**fonction indicatrice**](https://en.wikipedia.org/wiki/Indicator_function). Étant donné la prédiction $\\hat{y}_i$ pour l'échantillon $i$, l'exactitude équilibrée est définie comme suit :\n",
    "\n",
    "$$\\texttt{balanced-accuracy}(y, \\hat{y}, w) = \\frac{1}{\\sum{\\hat{w}_i}} \\sum_i 1(\\hat{y}_i = y_i) \\hat{w}_i$$\n",
    "\n",
    "Avec `adjusted=True`, l'exactitude équilibrée rapporte l'augmentation relative à $\\texttt{balanced-accuracy}(y, \\mathbf{0}, w) = \\frac{1}{n\\_classes}$. Dans le cas binaire, cela est également connu sous le nom de [**statistique J de Youden**](https://en.wikipedia.org/wiki/Youden's_J_statistic), ou _informedness_. \n",
    "\n",
    "> **Note:** La définition multiclasse ici semble être l'extension la plus raisonnable de la métrique utilisée en classification binaire, bien qu'il n'y ait pas de consensus certain dans la littérature :\n",
    "> - Notre définition : [Mosley2013], [Kelleher2015] et [Guyon2015], où [Guyon2015] adopte la version ajustée pour s'assurer que les prédictions aléatoires ont un score de $0$ et les prédictions parfaites ont un score de $1$.\n",
    "> - Exactitude équilibrée de classe comme décrite dans [Mosley2013] : le minimum entre la précision et le rappel pour chaque classe est calculé. Ces valeurs sont ensuite moyennées sur le nombre total de classes pour obtenir l'exactitude équilibrée.\n",
    "> - Exactitude équilibrée comme décrite dans [Urbanowicz2015] : la moyenne de la sensibilité et de la spécificité est calculée pour chaque classe, puis moyennée sur le nombre total de classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Références\n",
    "\n",
    "🔬 [Guyon2015] (1,2) I. Guyon, K. Bennett, G. Cawley, H.J. Escalante, S. Escalera, T.K. Ho, N. Macià, B. Ray, M. Saeed, A.R. Statnikov, E. Viegas, [**“Design of the 2015 ChaLearn AutoML Challenge”**](http://haralick.org/ML/automl_ijcnn15.pdf), IJCNN 2015.\n",
    "\n",
    "🔬 [Mosley2013] (1,2) L. Mosley, [**“A balanced approach to the multi-class imbalance problem”**](https://dr.lib.iastate.edu/server/api/core/bitstreams/7af0eb70-76f0-4411-9b1b-ce9254e64062/content), IJCV 2010.\n",
    "\n",
    "📚 [Kelleher2015] John. D. Kelleher, Brian Mac Namee, Aoife D’Arcy, [**“Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies”**](https://mitpress.mit.edu/9780262029445/), 2015.\n",
    "\n",
    "🔬 [Urbanowicz2015] Urbanowicz R.J., Moore, J.H. [**“ExSTraCS 2.0: description and evaluation of a scalable learning classifier system”**](https://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC4583133&blobtype=pdf), Evol. Intel. (2015) 8: 89."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='cohen-s-kappa'></a> 3.3.2.5. Statistique de Cohen (Cohen’s kappa)\n",
    "\n",
    "La fonction [**`cohen_kappa_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html) calcule la [**statistique de Cohen**](https://en.wikipedia.org/wiki/Cohen's_kappa). Cette mesure vise à comparer les annotations par différents annotateurs humains, et non un classifieur par rapport à une vérité terrain.\n",
    "\n",
    "Le score kappa (voir la docstring) est un nombre compris entre -1 et 1. Les scores supérieurs à 0,8 sont généralement considérés comme une bonne concordance ; zéro ou moins signifie aucune concordance (étiquettes pratiquement aléatoires).\n",
    "\n",
    "Les scores kappa peuvent être calculés pour les problèmes binaires ou multiclasse, mais pas pour les problèmes multi-étiquette (sauf en calculant manuellement un score par étiquette) et pas pour plus de deux annotateurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4285714285714286"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "y_true = [2, 0, 2, 2, 0, 1]\n",
    "y_pred = [0, 0, 2, 2, 0, 2]\n",
    "cohen_kappa_score(y_true, y_pred)\n",
    "# 0.4285714285714286"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='confusion-matrix'></a> 3.3.2.6. Matrice de confusion\n",
    "\n",
    "La fonction [**`confusion_matrix`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix) évalue l'exactitude de la classification en calculant la [**matrice de confusion**](https://en.wikipedia.org/wiki/Confusion_matrix) dont chaque ligne correspond à la classe réelle (Wikipedia et d'autres références peuvent utiliser une convention différente pour les axes).\n",
    "\n",
    "Par définition, l'entrée $i, j$ dans une matrice de confusion est le nombre d'observations réellement dans le groupe $i$, mais prédit comme étant dans le groupe $j$. Voici un exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 2]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = [2, 0, 2, 2, 0, 1]\n",
    "y_pred = [0, 0, 2, 2, 0, 2]\n",
    "confusion_matrix(y_true, y_pred)\n",
    "# array([[2, 0, 0],\n",
    "#        [0, 0, 1],\n",
    "#        [1, 0, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`ConfusionMatrixDisplay`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html) peut être utilisé pour représenter visuellement une matrice de confusion comme illustré dans l'exemple **Confusion matrix**, qui crée la figure suivante :\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_confusion_matrix_001.png)\n",
    "\n",
    "Le paramètre `normalize` permet de rapporter des ratios au lieu des décomptes. La matrice de confusion peut être normalisée de 3 manières différentes : `'pred'`, `'true'` et `'all'`, qui diviseront les décomptes par la somme de chaque colonne, ligne ou la matrice entière, respectivement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25 , 0.125],\n",
       "       [0.25 , 0.375]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [0, 0, 0, 1, 1, 1, 1, 1]\n",
    "y_pred = [0, 1, 0, 1, 0, 1, 0, 1]\n",
    "confusion_matrix(y_true, y_pred, normalize='all')\n",
    "# array([[0.25 , 0.125],\n",
    "#        [0.25 , 0.375]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les problèmes binaires, nous pouvons obtenir les décomptes de vrais négatifs, faux positifs, faux négatifs et vrais positifs de la manière suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 2, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [0, 0, 0, 1, 1, 1, 1, 1]\n",
    "y_pred = [0, 1, 0, 1, 0, 1, 0, 1]\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "tn, fp, fn, tp\n",
    "# (2, 1, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples\n",
    "\n",
    "##### [**Matrice de confusion**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/3_model_selection/plot_confusion_matrix.ipynb)<br/>([_Confusion matrix_](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html))\n",
    "\n",
    "Exemple d'utilisation d'une matrice de confusion pour évaluer la qualité de la sortie d'un classifieur.\n",
    "\n",
    "##### [**Reconnaissance de chiffres manuscrits**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/classification/plot_digits_classification.ipynb)<br/>([_Recognizing hand-written digits_](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html))\n",
    "\n",
    "Exemple d'utilisation d'une matrice de confusion pour classifier des chiffres manuscrits.\n",
    "\n",
    "##### [**Classification de documents textuels à l'aide de caractéristiques creuses**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/text/plot_document_classification_20newsgroups.ipynb)<br/>([*Classification of text documents using sparse features*](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html))\n",
    "\n",
    "Exemple d'utilisation d'une matrice de confusion pour classifier des documents textuels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='classification-report'></a> 3.3.2.7. Rapport de classification\n",
    "\n",
    "La fonction [**`classification_report`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report) génère un rapport textuel montrant les principales métriques de classification. Voici un petit exemple avec des `target_names` personnalisées et des étiquettes inférées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.67      1.00      0.80         2\n",
      "     class 1       0.00      0.00      0.00         1\n",
      "     class 2       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.56      0.50      0.49         5\n",
      "weighted avg       0.67      0.60      0.59         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true = [0, 1, 2, 2, 0]\n",
    "y_pred = [0, 0, 2, 1, 0]\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "#               precision    recall  f1-score   support\n",
    "# \n",
    "#      class 0       0.67      1.00      0.80         2\n",
    "#      class 1       0.00      0.00      0.00         1\n",
    "#      class 2       1.00      0.50      0.67         2\n",
    "# \n",
    "#     accuracy                           0.60         5\n",
    "#    macro avg       0.56      0.50      0.49         5\n",
    "# weighted avg       0.67      0.60      0.59         5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemples\n",
    "\n",
    "##### [**Reconnaissance de chiffres manuscrits**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/classification/plot_digits_classification.ipynb)<br/>([_Recognizing hand-written digits_](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html))\n",
    "\n",
    "Exemple d'utilisation du rapport de classification pour des chiffres manuscrits.\n",
    "\n",
    "##### [**Stratégie personnalisée de réajustement d'une recherche en grille avec validation croisée**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/3_model_selection/plot_grid_search_digits.ipynb)<br/>([_Custom refit strategy of a grid search with cross-validation_](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html))\n",
    "\n",
    "Exemple d'utilisation du rapport de classification pour une recherche en grille avec validation croisée imbriquée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='hamming-loss'></a> 3.3.2.8. Perte de Hamming\n",
    "\n",
    "La fonction [**`hamming_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html#sklearn.metrics.hamming_loss) calcule la perte de Hamming moyenne ou [**distance de Hamming**](https://en.wikipedia.org/wiki/Hamming_distance) entre deux ensembles d'échantillons.\n",
    "\n",
    "Si $\\hat{y}_{i,j}$ est la valeur prédite pour la $j$-ème étiquette d'un échantillon donné $i$, $y_{i,j}$ est la valeur correspondante vraie, $n_\\text{samples}$ est le nombre d'échantillons et $n_\\text{labels}$ est le nombre d'étiquettes, alors la perte de Hamming $L_{Hamming}$ est définie comme suit :\n",
    "\n",
    "$$L_{Hamming}(y, \\hat{y}) = \\frac{1}{n_\\text{samples} * n_\\text{labels}} \\sum_{i=0}^{n_\\text{samples}-1} \\sum_{j=0}^{n_\\text{labels} - 1} 1(\\hat{y}_{i,j} \\not= y_{i,j})$$\n",
    "\n",
    "où $1(x)$ est la [**fonction indicatrice**](https://en.wikipedia.org/wiki/Indicator_function).\n",
    "\n",
    "L'équation ci-dessus ne s'applique pas au cas de classification multiclasse. Veuillez vous référer à la note ci-dessous pour plus d'informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "y_pred = [1, 2, 3, 4]\n",
    "y_true = [2, 2, 3, 4]\n",
    "hamming_loss(y_true, y_pred)\n",
    "# 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas multilabel avec des indicateurs d'étiquettes binaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))\n",
    "# 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note :** En classification multiclasse, la perte de Hamming correspond à la distance de Hamming entre `y_true` et `y_pred`, ce qui est similaire à la fonction de perte [**Zero one loss** (3.3.2.17)](https://scikit-learn.org/stable/modules/model_evaluation.html#zero-one-loss). Cependant, alors que la perte zero-one pénalise les ensembles de prédictions qui ne correspondent pas strictement aux ensembles réels, la perte de Hamming pénalise les étiquettes individuelles. Ainsi, la perte de Hamming, bornée par la perte zero-one, est toujours comprise entre zéro et un, inclusivement ; et prédire un sous-ensemble ou un sur-ensemble approprié des étiquettes réelles donnera une perte de Hamming comprise entre zéro et un, exclusivement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='hamming-loss'></a> 3.3.2.9. Précision, rappel et scores F\n",
    "\n",
    "De manière intuitive, la [**précision**](https://fr.wikipedia.org/wiki/Pr%C3%A9cision_et_rappel) représente la capacité du classifieur à ne pas étiqueter comme positive un échantillon qui est négatif, et le rappel représente la capacité du classifieur à trouver tous les échantillons positifs.\n",
    "\n",
    "Le [**score F**](https://fr.wikipedia.org/wiki/F-mesure) ($F_\\beta$ et $F_1$) peut être interprété comme une moyenne harmonique pondérée de la précision et du rappel. Un score F atteint sa meilleure valeur à 1 et son pire score à 0. Avec $\\beta = 1$, le score F et le score F1 sont équivalents, et le rappel et la précision sont tout aussi importants.\n",
    "\n",
    "La fonction [**`precision_recall_curve`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html) calcule une courbe de précision-rappel à partir des étiquettes de vérité terrain et d'un score donné par le classifieur en faisant varier un seuil de décision.\n",
    "\n",
    "La fonction [**`average_precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html) calcule la [**précision moyenne**](https://fr.wikipedia.org/w/index.php?title=Information_retrieval&oldid=793358396#Average_precision) (AP) à partir des scores de prédiction. La valeur est comprise entre 0 et 1, et plus elle est élevée, mieux c'est. L'AP est définie comme suit :\n",
    "\n",
    "$$\\text{AP} = \\sum_n (R_n - R_{n-1}) P_n$$\n",
    "\n",
    "où $P_n$ et $R_n$ sont la précision et le rappel au n-ième seuil. Avec des prédictions aléatoires, l'AP est la fraction d'échantillons positifs.\n",
    "\n",
    "Les références [Manning2008] et [Everingham2010] présentent des variantes alternatives de l'AP qui permettent d'interpoler la courbe précision-rappel. Actuellement, [**`average_precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html) n'implémente aucune variante interpolée. Les références [Davis2006] et [Flach2015] expliquent pourquoi une interpolation linéaire des points sur la courbe précision-rappel donne une mesure excessivement optimiste des performances du classifieur. Cette interpolation linéaire est utilisée lors du calcul de l'aire sous la courbe avec la règle du trapèze dans la fonction [**`auc`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html).\n",
    "\n",
    "Plusieurs fonctions vous permettent d'analyser la précision, le rappel et le score F :\n",
    "\n",
    "| Fonction                                | Description                                                                                |\n",
    "|:----------------------------------------|:-------------------------------------------------------------------------------------------|\n",
    "| [**`average_precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html)(y_true, y_score, *) | Calcule la précision moyenne (AP) à partir des scores de prédiction. |\n",
    "| [**`f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)(y_true, y_pred, *[, labels, ...]) | Calcule le score F1, également connu sous le nom de F-score équilibré ou F-mesure. |\n",
    "| [**`fbeta_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html)(y_true, y_pred, *, beta[, ...]) | Calcule le score F-beta. |\n",
    "| [**`precision_recall_curve`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html)(y_true, probas_pred, *) | Calcule les paires précision-rappel pour différents seuils de probabilité. |\n",
    "| [**`precision_recall_fscore_support`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html) (y_true, ...) | Calcule la précision, le rappel, le score F et le support pour chaque classe. |\n",
    "| [**`precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)(y_true, y_pred, *[, labels, ...]) | Calcule la précision. |\n",
    "| [**`recall_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)(y_true, y_pred, *[, labels, ...]) | Calcule le rappel. |\n",
    "\n",
    "Notez que la fonction [**`precision_recall_curve`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html) est limitée au cas binaire. La fonction [**`average_precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html) prend en charge les formats multiclasse et multilabel en calculant le score de chaque classe de manière \"One-vs-the-rest\" (OvR) et en les moyennant ou non en fonction de la valeur de l'argument `average`. \n",
    "\n",
    "Les fonctions `PredictionRecallDisplay.from_estimator` et `PredictionRecallDisplay.from_predictions` afficheront la courbe précision-rappel comme suit :\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_precision_recall_001.png)\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Stratégie personnalisée de réajustement d'une recherche en grille avec validation croisée**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/3_model_selection/plot_grid_search_digits.ipynb)<br/>([_Custom refit strategy of a grid search with cross-validation_](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html))\n",
    "\n",
    "Exemple d'utilisation des fonctions [**`precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) et [**`recall_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html) pour estimer les paramètres à l'aide d'une recherche par grille avec validation croisée imbriquée.\n",
    "\n",
    "##### [**Précision-Rappel**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/3_model_selection/plot_precision_recall.ipynb)<br/>([_Precision-Recall_](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html))\n",
    "\n",
    "Exemple d'utilisation de la fonction [**`precision_recall_curve`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html) pour évaluer la qualité de la sortie du classifieur.\n",
    "\n",
    "#### Références\n",
    "\n",
    "Z 📚 [Manning2008] C.D. Manning, P. Raghavan, H. Schütze, [**“Introduction to Information Retrieval”**](https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf), 2009.\n",
    "\n",
    "Z 🔬 [Everingham2010] M. Everingham, L. Van Gool, C.K.I. Williams, J. Winn, A. Zisserman, [**“The Pascal Visual Object Classes (VOC) Challenge”**](https://www.pure.ed.ac.uk/ws/files/20017166/ijcv_voc14.pdf), IJCV 2010.\n",
    "\n",
    "Z 🔬 [Davis2006] J. Davis, M. Goadrich, [**“The Relationship Between Precision-Recall and ROC Curves”**](https://www.biostat.wisc.edu/~page/rocpr.pdf), ICML 2006.\n",
    "\n",
    "Z 🔬 [Flach2015] P.A. Flach, M. Kull, [**“Precision-Recall-Gain Curves: PR Analysis Done Right”**](https://proceedings.neurips.cc/paper_files/paper/2015/file/33e8075e9970de0cfea955afd4644bb2-Paper.pdf), NIPS 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='binary-classification'></a> Classification binaire\n",
    "\n",
    "Dans une tâche de classification binaire, les termes \"positif\" et \"négatif\" font référence à la prédiction du classifieur, tandis que les termes \"vrai\" et \"faux\" font référence à la correspondance de cette prédiction avec le jugement externe (parfois appelé \"observation\"). Avec ces définitions, nous pouvons formuler le tableau suivant :\n",
    "\n",
    "<table>\n",
    "<tbody>\n",
    "<tr><td></td>\n",
    "<td colspan=\"2\"><p>Classe réelle (observation)</p></td>\n",
    "</tr>\n",
    "<tr><td rowspan=\"2\"><p>Classe prédite\n",
    "(expectation)</p></td>\n",
    "<td><p>tp (vrai positif)\n",
    "Résultat correct</p></td>\n",
    "<td><p>fp (faux positif)\n",
    "Résultat inattendu</p></td>\n",
    "</tr>\n",
    "<tr><td><p>fn (faux négatif)\n",
    "Résultat manquant</p></td>\n",
    "<td><p>tn (vrai négatif)\n",
    "Absence de résultat correct</p></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "Dans ce contexte, nous pouvons définir les notions de précision, rappel et F-mesure :\n",
    "\n",
    "$$P = \\frac{tp}{tp + fp},$$\n",
    "$$R = \\frac{tp}{tp + fn},$$\n",
    "$$F_\\beta = (1 + \\beta^2) \\frac{P \\times R}{\\beta^2 P + R}.$$\n",
    "\n",
    "Parfois, le rappel est également appelé \"sensibilité\".\n",
    "\n",
    "Voici quelques petits exemples de classification binaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333333"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "y_pred = [0, 1, 0, 0]\n",
    "y_true = [0, 1, 0, 1]\n",
    "metrics.precision_score(y_true, y_pred)\n",
    "# 1.0\n",
    "metrics.recall_score(y_true, y_pred)\n",
    "# 0.5\n",
    "metrics.f1_score(y_true, y_pred)\n",
    "# 0.66...\n",
    "metrics.fbeta_score(y_true, y_pred, beta=0.5)\n",
    "# 0.83...\n",
    "metrics.fbeta_score(y_true, y_pred, beta=1)\n",
    "# 0.66...\n",
    "metrics.fbeta_score(y_true, y_pred, beta=2)\n",
    "# 0.55...\n",
    "metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5)\n",
    "# (array([0.66..., 1.        ]), array([1. , 0.5]), array([0.71..., 0.83...]), array([2, 2]))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "y_true = np.array([0, 0, 1, 1])\n",
    "y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "precision, recall, threshold = precision_recall_curve(y_true, y_scores)\n",
    "precision\n",
    "# array([0.5       , 0.66..., 0.5       , 1.        , 1.        ])\n",
    "recall\n",
    "# array([1. , 1. , 0.5, 0.5, 0. ])\n",
    "threshold\n",
    "# array([0.1 , 0.35, 0.4 , 0.8 ])\n",
    "average_precision_score(y_true, y_scores)\n",
    "# 0.83..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='multiclass-and-multilabel-classification'></a> Classification multiclasse et multilabel\n",
    "\n",
    "Dans une tâche de classification multiclasse et multilabel, les notions de précision, de rappel et de F-mesure peuvent être appliquées à chaque étiquette indépendamment. Il existe quelques façons de combiner les résultats entre les étiquettes, spécifiées par l'argument `average` des fonctions [**`average_precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html), [**`f1_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html), [**`fbeta_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html), [**`precision_recall_fscore_support`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html), [**`precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) et [**`recall_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html), comme décrit ci-dessus. Notez que si toutes les étiquettes sont incluses, le \"micro-averaging\" dans un contexte multiclasse produira des valeurs de précision, de rappel et de F qui sont toutes identiques à l'exactitude (accuracy). Notez également que le \"weighted-averaging\" peut produire un score F qui n'est pas nécessairement compris entre la précision et le rappel.\n",
    "\n",
    "Pour clarifier cela, considérons la notation suivante :\n",
    "\n",
    "- $y$ est l'ensemble des paires $(échantillon, étiquette)$ réelles\n",
    "- $\\hat{y}$ est l'ensemble des paires $(échantillon, étiquette)$ prédites\n",
    "- $L$ est l'ensemble des étiquettes\n",
    "- $S$ est l'ensemble des échantillons\n",
    "- $y_s$ est le sous-ensemble de $y$ contenant l'échantillon $s$, c'est-à-dire $y_s := \\left\\{(s', l) \\in y | s' = s\\right\\}$\n",
    "- $y_l$ est le sous-ensemble de $y$ contenant l'étiquette $l$\n",
    "- de même, $\\hat{y}_s$ et $\\hat{y}_l$ sont des sous-ensembles de $\\hat{y}$\n",
    "- $P(A, B) := \\frac{\\left \\vert A \\cap B \\right \\vert}{\\left \\vert B \\right \\vert}$ pour certains ensembles $A$ et $B$\n",
    "- $R(A, B) := \\frac{\\left \\vert A \\cap B \\right \\vert}{\\left \\vert A \\right \\vert}$ (Les conventions peuvent varier sur le traitement de $A = \\emptyset$ ; cette implémentation utilise $R(A, B):=0$, et de manière similaire pour $P$.)\n",
    "- $F_\\beta(A, B) := \\left(1 + \\beta^2\\right) \\frac{P(A, B) \\times R(A, B)}{\\beta^2 P(A, B) + R(A, B)}$\n",
    "\n",
    "Les métriques sont ensuite définies comme suit :\n",
    "\n",
    "| Moyenne | Précision | Rappel | F_beta |\n",
    "|-|-|-|-|\n",
    "|`\"micro\"`|$P(y, \\hat{y})$|$R(y, \\hat{y})$|$F_\\beta(y, \\hat{y})$|\n",
    "|`\"samples\"`|$\\frac{1}{\\left \\vert S \\right \\vert} \\sum_{s \\in S} P(y_s, \\hat{y}_s)$|$\\frac{1}{\\left \\vert S \\right \\vert} \\sum_{s \\in S} R(y_s, \\hat{y}_s)$|$\\frac{1}{\\left \\vert S \\right \\vert} \\sum_{s \\in S} F_\\beta(y_s, \\hat{y}_s)$|\n",
    "|`\"macro\"`|$\\frac{1}{\\left \\vert L \\right \\vert} \\sum_{l \\in L} P(y_l, \\hat{y}_l)$|$\\frac{1}{\\left \\vert L \\right \\vert} \\sum_{l \\in L} R(y_l, \\hat{y}_l)$|$\\frac{1}{\\left \\vert L \\right \\vert} \\sum_{l \\in L} F_\\beta(y_l, \\hat{y}_l)$|\n",
    "|`\"weighted\"`|$\\frac{1}{\\sum_{l \\in L} \\left \\vert y_l \\right \\vert} \\sum_{l \\in L} \\left \\vert y_l \\right \\vert P(y_l, \\hat{y}_l)$|$\\frac{1}{\\sum_{l \\in L} \\left \\vert y_l \\right \\vert} \\sum_{l \\in L} \\left \\vert y_l \\right \\vert R(y_l, \\hat{y}_l)$|$\\frac{1}{\\sum_{l \\in L} \\left \\vert y_l \\right \\vert} \\sum_{l \\in L} \\left \\vert y_l \\right \\vert F_\\beta(y_l, \\hat{y}_l)$|\n",
    "|`None`|$\\langle P(y_l, \\hat{y}_l) \\vert l \\in L \\rangle$|$\\langle R(y_l, \\hat{y}_l) \\vert l \\in L \\rangle$|$\\langle F_\\beta(y_l, \\hat{y}_l) \\vert l \\in L \\rangle$|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.66666667, 0.        , 0.        ]),\n",
       " array([1., 0., 0.]),\n",
       " array([0.71428571, 0.        , 0.        ]),\n",
       " array([2, 2, 2], dtype=int64))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "y_true = [0, 1, 2, 0, 1, 2]\n",
    "y_pred = [0, 2, 1, 0, 0, 1]\n",
    "metrics.precision_score(y_true, y_pred, average='macro')\n",
    "# 0.22...\n",
    "metrics.recall_score(y_true, y_pred, average='micro')\n",
    "# 0.33...\n",
    "metrics.f1_score(y_true, y_pred, average='weighted')\n",
    "# 0.26...\n",
    "metrics.fbeta_score(y_true, y_pred, average='macro', beta=0.5)\n",
    "# 0.23...\n",
    "metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5, average=None)\n",
    "# (array([0.66..., 0.        , 0.        ]), array([1., 0., 0.]), array([0.71..., 0.        , 0.        ]), array([2, 2, 2]...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la classification multiclasse avec une \"classe négative\", il est possible d'exclure certaines étiquettes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.recall_score(y_true, y_pred, labels=[1, 2], average='micro')\n",
    "# excluding 0, no labels were correctly recalled\n",
    "# 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De même, les étiquettes non présentes dans l'échantillon de données peuvent être prises en compte dans l'approche de la macro-moyenne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_score(y_true, y_pred, labels=[0, 1, 2, 3], average='macro')\n",
    "# 0.166..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='jaccard-similarity-coefficient-score'></a> 3.3.2.10. Coefficient de similarité de Jaccard\n",
    "\n",
    "La fonction [**`jaccard_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html) calcule la moyenne des [**coefficients de similarité de Jaccard**](https://en.wikipedia.org/wiki/Jaccard_index), également appelés indice de Jaccard, entre les paires d'ensembles d'étiquettes.\n",
    "\n",
    "Le coefficient de similarité de Jaccard avec un ensemble d'étiquettes de vérité terrain $y$ et un ensemble d'étiquettes prédites $\\hat{y}$ est défini comme suit :\n",
    "\n",
    "$$J(y, \\hat{y}) = \\frac{|y \\cap \\hat{y}|}{|y \\cup \\hat{y}|}.$$\n",
    "\n",
    "\n",
    "Le [**`jaccard_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html) (tout comme [**`precision_recall_fscore_support`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html)) s'applique nativement aux cibles binaires. En le calculant par ensemble, il peut être étendu pour s'appliquer aux problèmes multilabel et multiclasse en utilisant l'argument `average` (voir [**ci-dessus** (3.3.2.1)](https://scikit-learn.org/stable/modules/model_evaluation.html#average)).\n",
    "\n",
    "Dans le cas binaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_score\n",
    "y_true = np.array([[0, 1, 1],\n",
    "                   [1, 1, 0]])\n",
    "y_pred = np.array([[1, 1, 1],\n",
    "                   [1, 0, 0]])\n",
    "jaccard_score(y_true[0], y_pred[0])\n",
    "# 0.6666..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas de la comparaison 2D (par exemple, la similarité d'images) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_score(y_true, y_pred, average=\"micro\")\n",
    "# 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas multilabel avec des indicateurs binaires d'étiquettes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5, 1. ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_score(y_true, y_pred, average='samples')\n",
    "# 0.5833...\n",
    "jaccard_score(y_true, y_pred, average='macro')\n",
    "# 0.6666...\n",
    "jaccard_score(y_true, y_pred, average=None)\n",
    "# array([0.5, 0.5, 1. ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les problèmes multiclasse sont binarisés et traités de la même manière que les problèmes multilabel correspondants :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [0, 2, 1, 2]\n",
    "y_true = [0, 1, 2, 2]\n",
    "jaccard_score(y_true, y_pred, average=None)\n",
    "# array([1. , 0. , 0.33...])\n",
    "jaccard_score(y_true, y_pred, average='macro')\n",
    "# 0.44...\n",
    "jaccard_score(y_true, y_pred, average='micro')\n",
    "# 0.33..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='hinge-loss'></a> 3.3.2.11. Perte de charnière (Hinge Loss)\n",
    "\n",
    "La fonction [**`hinge_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hinge_loss.html) calcule la distance moyenne entre le modèle et les données en utilisant la [**perte de charnière**](https://en.wikipedia.org/wiki/Hinge_loss), une métrique unilatérale qui ne prend en compte que les erreurs de prédiction. (La perte de charnière est utilisée dans les classificateurs à marge maximale tels que les machines à vecteurs de support.)\n",
    "\n",
    "Si l'étiquette vraie $y_i$ d'une tâche de classification binaire est encodée comme $y_i=\\left\\{-1, +1\\right\\}$ pour chaque échantillon $i$ ; et $w_i$ est la décision prédite correspondante (un tableau de forme `(n_samples,)` en sortie de la méthode `decision_function`), alors la perte de charnière est définie comme suit :\n",
    "\n",
    "$$L_\\text{Hinge}(y, w) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples}-1} \\max\\left\\{1 - w_i y_i, 0\\right\\}$$\n",
    "\n",
    "S'il y a plus de deux étiquettes, [**`hinge_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hinge_loss.html) utilise une variante multiclasse due à Crammer & Singer. [Ici](https://jmlr.csail.mit.edu/papers/volume2/crammer01a/crammer01a.pdf) se trouve l'article qui la décrit.\n",
    "\n",
    "Dans ce cas, la décision prédite est un tableau de forme `(n_samples, n_labels)`. Si $w_{i, y_i}$ est la décision prédite pour la vraie étiquette de l'échantillon $i$ ; et $\\hat{w}_{i, y_i} = \\max\\left\\{w_{i, y_j}~|~y_j \\ne y_i \\right\\}$ est le maximum des décisions prédites pour toutes les autres étiquettes, alors la perte de charnière multiclasse est définie par :\n",
    "\n",
    "$$L_\\text{Hinge}(y, w) = \\frac{1}{n_\\text{samples}}\n",
    "\\sum_{i=0}^{n_\\text{samples}-1} \\max\\left\\{1 + \\hat{w}_{i, y_i}\n",
    "- w_{i, y_i}, 0\\right\\}$$\n",
    "\n",
    "Voici un petit exemple illustrant l'utilisation de la fonction [**`hinge_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hinge_loss.html) avec un classificateur SVM dans un problème de classe binaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30303030303030304"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import hinge_loss\n",
    "X = [[0], [1]]\n",
    "y = [-1, 1]\n",
    "est = svm.LinearSVC(dual=\"auto\", random_state=0)\n",
    "est.fit(X, y)\n",
    "# LinearSVC(dual='auto', random_state=0)\n",
    "pred_decision = est.decision_function([[-2], [3], [0.5]])\n",
    "pred_decision\n",
    "# array([-2.18...,  2.36...,  0.09...])\n",
    "hinge_loss([-1, 1, 1], pred_decision)\n",
    "# 0.3..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un exemple illustrant l'utilisation de la fonction [**`hinge_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hinge_loss.html) avec un classificateur SVM dans un problème multiclasse :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5641025641025641"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0], [1], [2], [3]])\n",
    "Y = np.array([0, 1, 2, 3])\n",
    "labels = np.array([0, 1, 2, 3])\n",
    "est = svm.LinearSVC(dual=\"auto\")\n",
    "est.fit(X, Y)\n",
    "# LinearSVC(dual='auto')\n",
    "pred_decision = est.decision_function([[-1], [2], [3]])\n",
    "y_true = [0, 2, 3]\n",
    "hinge_loss(y_true, pred_decision, labels=labels)\n",
    "# 0.56..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='log-loss'></a> 3.3.2.12. Perte logarithmique (Log Loss)\n",
    "\n",
    "La perte logarithmique, également appelée perte de régression logistique ou entropie croisée, est définie sur les estimations de probabilité. Elle est couramment utilisée en régression logistique (multinomiale) et en réseaux neuronaux, ainsi que dans certaines variantes de l'espérance-maximisation. Elle peut être utilisée pour évaluer les sorties de probabilité (`predict_proba`) d'un classificateur au lieu de ses prédictions discrètes.\n",
    "\n",
    "Pour la classification binaire avec une vraie étiquette $y \\in \\{0,1\\}$ et une estimation de probabilité $p = \\operatorname{Pr}(y = 1)$, la perte logarithmique par échantillon est le négatif du logarithme de vraisemblance du classificateur étant donné la vraie étiquette :\n",
    "\n",
    "$$L_{\\log}(y, p) = -\\log \\operatorname{Pr}(y|p) = -(y \\log (p) + (1 - y) \\log (1 - p))$$\n",
    "\n",
    "Ceci s'étend au cas multiclasse comme suit. Soit les vraies étiquettes pour un ensemble d'échantillons encodées sous forme d'une matrice binaire indicatrice 1-of-K $Y$, c'est-à-dire $y_{i,k} = 1$ si l'échantillon $i$ a l'étiquette $k$ choisie parmi un ensemble de $K$ étiquettes. Soit $P$ une matrice d'estimations de probabilité, avec $p_{i,k} = \\operatorname{Pr}(y_{i,k} = 1)$. Alors, la perte logarithmique de l'ensemble entier est définie par :\n",
    "\n",
    "$$L_{\\log}(Y, P) = -\\log \\operatorname{Pr}(Y|P) = - \\frac{1}{N} \\sum_{i=0}^{N-1} \\sum_{k=0}^{K-1} y_{i,k} \\log p_{i,k}$$\n",
    "\n",
    "Pour voir comment cela généralise la perte logarithmique binaire donnée ci-dessus, notez que dans le cas binaire, $p_{i,0} = 1 - p_{i,1}$ et $y_{i,0} = 1 - y_{i,1}$. Ainsi, en développant la somme interne sur $y_{i,k} \\in \\{0,1\\}$, on obtient la perte logarithmique binaire.\n",
    "\n",
    "La fonction [**`log_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html) calcule la perte logarithmique à partir d'une liste d'étiquettes réelles et d'une matrice de probabilités, telle qu'elle est renvoyée par la méthode `predict_proba` d'un estimateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1738073366910675"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "y_true = [0, 0, 1, 1]\n",
    "y_pred = [[.9, .1], [.8, .2], [.3, .7], [.01, .99]]\n",
    "log_loss(y_true, y_pred)\n",
    "# 0.1738..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le premier `[.9, .1]` dans `y_pred` indique une probabilité de 90% que le premier échantillon ait l'étiquette 0. La perte logarithmique est non négative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='matthews-correlation-coefficient'></a> 3.3.2.13. Coefficient de corrélation de Matthews\n",
    "\n",
    "La fonction [**`matthews_corrcoef`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html) calcule le [**coefficient de corrélation de Matthews (MCC)**](https://en.wikipedia.org/wiki/Phi_coefficient) pour les classes binaires. En citant Wikipedia :\n",
    "\n",
    "_\"Le coefficient de corrélation de Matthews est utilisé en apprentissage automatique comme mesure de la qualité des classifications binaires (à deux classes). Il prend en compte les vrais positifs, les vrais négatifs, les faux positifs et les faux négatifs, et est généralement considéré comme une mesure équilibrée qui peut être utilisée même si les classes sont de tailles très différentes. Le MCC est essentiellement une valeur de coefficient de corrélation entre -1 et +1. Un coefficient de +1 représente une prédiction parfaite, 0 une prédiction aléatoire moyenne et -1 une prédiction inverse. La statistique est également connue sous le nom de coefficient phi.\"_\n",
    "\n",
    "Dans le cas binaire (à deux classes), $tp$, $tn$, $fp$ et $fn$ représentent respectivement le nombre de vrais positifs, de vrais négatifs, de faux positifs et de faux négatifs. Le MCC est défini comme suit :\n",
    "\n",
    "$$MCC = \\frac{tp \\times tn - fp \\times fn}{\\sqrt{(tp + fp)(tp + fn)(tn + fp)(tn + fn)}}.$$\n",
    "\n",
    "Dans le cas multiclasse, le coefficient de corrélation de Matthews peut être [défini](https://rth.dk/resources/rk/introduction/index.html) en termes d'une [**matrice de confusion**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) $C$ pour $K$ classes. Pour simplifier la définition, considérons les variables intermédiaires suivantes :\n",
    "\n",
    "- $t_k=\\sum_{i}^{K} C_{ik}$ le nombre de fois où la classe $k$ est réellement présente,\n",
    "- $p_k=\\sum_{i}^{K} C_{ki}$ le nombre de fois où la classe $k$ a été prédite,\n",
    "- $c=\\sum_{k}^{K} C_{kk}$ le nombre total d'échantillons correctement prédits,\n",
    "- $s=\\sum_{i}^{K} \\sum_{j}^{K} C_{ij}$ le nombre total d'échantillons.\n",
    "\n",
    "Le coefficient de corrélation de Matthews multiclasse est défini comme suit :\n",
    "\n",
    "$$MCC = \\frac{\n",
    "    c \\times s - \\sum_{k}^{K} p_k \\times t_k\n",
    "}{\\sqrt{\n",
    "    (s^2 - \\sum_{k}^{K} p_k^2) \\times\n",
    "    (s^2 - \\sum_{k}^{K} t_k^2)\n",
    "}}$$\n",
    "\n",
    "Lorsqu'il y a plus de deux étiquettes, la valeur du MCC ne se situera plus entre -1 et +1. Au lieu de cela, la valeur minimale sera quelque part entre -1 et 0 en fonction du nombre et de la répartition des étiquettes réelles. La valeur maximale est toujours +1.\n",
    "\n",
    "Voici un petit exemple illustrant l'utilisation de la fonction [**`matthews_corrcoef`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3333333333333333"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "y_true = [+1, +1, +1, -1]\n",
    "y_pred = [+1, -1, +1, +1]\n",
    "matthews_corrcoef(y_true, y_pred)\n",
    "# -0.33..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='multi-label-confusion-matrix'></a> 3.3.2.14. Matrice de confusion multi-étiquettes\n",
    "\n",
    "La fonction [**`multilabel_confusion_matrix`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html) calcule la matrice de confusion multi-étiquettes pour chaque classe (par défaut) ou pour chaque échantillon (samplewise=True) afin d'évaluer l'exactitude d'une classification. La fonction `multilabel_confusion_matrix` traite également les données multiclasse comme si elles étaient multi-étiquettes, car il s'agit d'une transformation couramment appliquée pour évaluer les problèmes multiclasse avec des métriques de classification binaire (telles que la précision, le rappel, etc.).\n",
    "\n",
    "Lors du calcul de la matrice de confusion multi-étiquettes par classe $C$, le nombre de vrais négatifs pour la classe $i$ est $C_{i,0,0}$, le nombre de faux négatifs est $C_{i,1,0}$, le nombre de vrais positifs est $C_{i,1,1}$ et le nombre de faux positifs est $C_{i,0,1}$.\n",
    "\n",
    "Voici un exemple illustrant l'utilisation de la fonction [**`multilabel_confusion_matrix`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html) avec une entrée de [**matrice indicatrice multi-étiquettes**](https://scikit-learn.org/stable/glossary.html#term-multilabel-indicator-matrix) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 0],\n",
       "        [0, 1]],\n",
       "\n",
       "       [[1, 0],\n",
       "        [0, 1]],\n",
       "\n",
       "       [[0, 1],\n",
       "        [1, 0]]], dtype=int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "y_true = np.array([[1, 0, 1],\n",
    "                   [0, 1, 0]])\n",
    "y_pred = np.array([[1, 0, 0],\n",
    "                   [0, 1, 1]])\n",
    "multilabel_confusion_matrix(y_true, y_pred)\n",
    "# array([[[1, 0],\n",
    "#         [0, 1]],\n",
    "# \n",
    "#        [[1, 0],\n",
    "#         [0, 1]],\n",
    "# \n",
    "#        [[0, 1],\n",
    "#         [1, 0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou une matrice de confusion peut être construite pour les étiquettes de chaque échantillon :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 0],\n",
       "        [1, 1]],\n",
       "\n",
       "       [[1, 1],\n",
       "        [0, 1]]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(y_true, y_pred, samplewise=True)\n",
    "# array([[[1, 0],\n",
    "#         [1, 1]],\n",
    "# \n",
    "#        [[1, 1],\n",
    "#         [0, 1]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un exemple illustrant l'utilisation de la fonction [**`multilabel_confusion_matrix`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html) avec une entrée [**multiclasse**](https://scikit-learn.org/stable/glossary.html#term-multiclass) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[3, 1],\n",
       "        [0, 2]],\n",
       "\n",
       "       [[5, 0],\n",
       "        [1, 0]],\n",
       "\n",
       "       [[2, 1],\n",
       "        [1, 2]]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
    "y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
    "multilabel_confusion_matrix(y_true, y_pred,\n",
    "                            labels=[\"ant\", \"bird\", \"cat\"])\n",
    "# array([[[3, 1],\n",
    "#         [0, 2]],\n",
    "# \n",
    "#        [[5, 0],\n",
    "#         [1, 0]],\n",
    "# \n",
    "#        [[2, 1],\n",
    "#         [1, 2]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici quelques exemples illustrant l'utilisation de la fonction [**`multilabel_confusion_matrix`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html) pour calculer le rappel (ou la sensibilité), la spécificité, le taux de faux positifs et le taux de faux négatifs pour chaque classe dans un problème avec une entrée de matrice indicatrice multi-étiquettes.\n",
    "\n",
    "Calcul du [**rappel**](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) (également appelé le taux de vrais positifs ou la sensibilité) pour chaque classe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 0.5, 0. ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.array([[0, 0, 1],\n",
    "                   [0, 1, 0],\n",
    "                   [1, 1, 0]])\n",
    "y_pred = np.array([[0, 1, 0],\n",
    "                   [0, 0, 1],\n",
    "                   [1, 1, 0]])\n",
    "mcm = multilabel_confusion_matrix(y_true, y_pred)\n",
    "tn = mcm[:, 0, 0]\n",
    "tp = mcm[:, 1, 1]\n",
    "fn = mcm[:, 1, 0]\n",
    "fp = mcm[:, 0, 1]\n",
    "tp / (tp + fn)\n",
    "# array([1. , 0.5, 0. ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul de la [**spécificité**](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) (également appelée le taux de vrais négatifs) pour chaque classe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 0. , 0.5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn / (tn + fp)\n",
    "# array([1. , 0. , 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul du [**taux de faux positifs**](https://en.wikipedia.org/wiki/False_positive_rate) pour chaque classe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 1. , 0.5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp / (fp + tn)\n",
    "# array([0. , 1. , 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul du [**taux de faux négatifs**](https://en.wikipedia.org/wiki/False_positives_and_false_negatives) pour chaque classe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0.5, 1. ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn / (fn + tp)\n",
    "# array([0. , 0.5, 1. ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='receiver-operating-characteristic-roc'></a> 3.3.2.15. Courbe caractéristique de fonctionnement du récepteur (ROC)\n",
    "\n",
    "La fonction [**`roc_curve`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) calcule la [**courbe caractéristique de fonctionnement du récepteur, ou courbe ROC**](https://en.wikipedia.org/wiki/Receiver_operating_characteristic). Pour citer Wikipedia :\n",
    "\n",
    "_\"Une courbe caractéristique de fonctionnement du récepteur (ROC), ou simplement courbe ROC, est un graphique qui illustre la performance d'un système de classification binaire lorsque son seuil de discrimination est varié. Elle est créée en traçant la fraction de vrais positifs parmi les positifs (TRP = taux de vrais positifs) en fonction de la fraction de faux positifs parmi les négatifs (FRP = taux de faux positifs), pour différentes valeurs de seuil. TRP est également connu sous le nom de sensibilité, et FRP est équivalent à un moins la spécificité ou le taux de vrais négatifs.\"_\n",
    "\n",
    "Cette fonction nécessite les vraies valeurs binaires ainsi que les scores cibles, qui peuvent être soit les estimations de probabilité de la classe positive, soit les valeurs de confiance, soit les décisions binaires. Voici un petit exemple de l'utilisation de la fonction [**`roc_curve`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "y = np.array([1, 1, 2, 2])\n",
    "scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "fpr, tpr, thresholds = roc_curve(y, scores, pos_label=2)\n",
    "fpr\n",
    "# array([0. , 0. , 0.5, 0.5, 1. ])\n",
    "tpr\n",
    "# array([0. , 0.5, 0.5, 1. , 1. ])\n",
    "thresholds\n",
    "# array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparé à des métriques telles que la précision de sous-ensemble, la perte de Hamming ou le score F1, la courbe ROC ne nécessite pas d'optimiser un seuil pour chaque étiquette.\n",
    "\n",
    "La fonction [**`roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html), également appelée ROC-AUC ou AUROC, calcule l'aire sous la courbe ROC. Ainsi, les informations de la courbe sont résumées en un seul nombre.\n",
    "\n",
    "La figure suivante montre la courbe ROC et le score ROC-AUC pour un classificateur visant à distinguer la fleur \"virginica\" du reste des espèces dans l'ensemble de données [**Iris plants** (7.1.1)](https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-dataset) :\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_roc_001.png)\n",
    "\n",
    "Pour plus d'informations, consultez l'[**article Wikipedia sur l'aire sous la courbe**](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cas binaire\n",
    "\n",
    "Dans le cas **binaire**, vous pouvez fournir les estimations de probabilité en utilisant la méthode `classifier.predict_proba()`, ou les valeurs de décision non seuillées fournies par la méthode `classifier.decision_function()`. Dans le cas où vous fournissez les estimations de probabilité, la probabilité de la classe avec l'\"étiquette supérieure\" doit être fournie. L'\"étiquette supérieure\" correspond à `classifier.classes_[1]` et donc `classifier.predict_proba(X)[:, 1]`. Par conséquent, le paramètre `y_score` a une taille de `(n_samples,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "clf = LogisticRegression(solver=\"liblinear\").fit(X, y)\n",
    "clf.classes_\n",
    "# array([0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons utiliser les estimations de probabilité correspondant à `clf.classes_[1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9947412927435125"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_score = clf.predict_proba(X)[:, 1]\n",
    "roc_auc_score(y, y_score)\n",
    "# 0.99..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sinon, nous pouvons utiliser les valeurs de décision non seuillées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9947412927435125"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y, clf.decision_function(X))\n",
    "# 0.99..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cas multi-classes\n",
    "\n",
    "La fonction [**`roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) peut également être utilisée pour la **classification multi-classes**. Deux stratégies de moyennage sont actuellement prises en charge : l'algorithme \"un contre un\" calcule la moyenne des scores de l'aire sous la courbe ROC par paires, et l'algorithme \"un contre tous\" calcule la moyenne des scores de l'aire sous la courbe ROC pour chaque classe par rapport à toutes les autres classes. Dans les deux cas, les étiquettes prédites sont fournies dans un tableau avec des valeurs de 0 à `n_classes`, et les scores correspondent aux estimations de probabilité qu'un échantillon appartienne à une classe particulière. Les algorithmes OvO (un contre un) et OvR (un contre tous) prennent en charge un pondérage uniforme (`average='macro'`) et un pondérage par prévalence (`average='weighted'`).\n",
    "\n",
    "**Algorithme un contre un (OvO) :** Calcule la moyenne de toutes les combinaisons possibles de paires de classes. [HT2001] définit une métrique AUC multiclasse pondérée uniformément :\n",
    "\n",
    "$$\\frac{1}{c(c-1)}\\sum_{j=1}^{c}\\sum_{k > j}^c (\\text{AUC}(j | k) +\n",
    "\\text{AUC}(k | j))$$\n",
    "\n",
    "où $c$ est le nombre de classes et $\\text{AUC}(j | k)$ est l'AUC avec la classe $j$ comme classe positive et la classe $k$ comme classe négative. En général, $\\text{AUC}(j | k) \\neq \\text{AUC}(k | j)$ dans le cas multiclasse. Cet algorithme est utilisé en définissant l'argument `multiclass` sur `'ovo'` et `average` sur `'macro'`.\n",
    "\n",
    "La métrique AUC multiclasse de [HT2001] peut être étendue pour être pondérée par la prévalence :\n",
    "\n",
    "$$\\frac{1}{c(c-1)}\\sum_{j=1}^{c}\\sum_{k > j}^c p(j \\cup k)(\n",
    "\\text{AUC}(j | k) + \\text{AUC}(k | j))$$\n",
    "\n",
    "où $c$ est le nombre de classes. Cet algorithme est utilisé en définissant l'argument `multiclass` sur `'ovo'` et `average` sur `'weighted'`. L'option `'weighted'` renvoie une moyenne pondérée par la prévalence comme décrit dans [FC2009].\n",
    "\n",
    "**Algorithme un contre tous (OvR) :** Calcule l'AUC de chaque classe par rapport au reste [PD2000]. L'algorithme est fonctionnellement le même que dans le cas multilabel. Pour activer cet algorithme, définissez l'argument `multiclass` sur `'ovr'`. En plus de la moyenne `'macro'` [F2006] et `'weighted'` [F2001], OvR prend en charge la moyenne `'micro'`.\n",
    "\n",
    "Dans les applications où un taux de faux positifs élevé n'est pas tolérable, le paramètre `max_fpr` de [**`roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) peut être utilisé pour résumer la courbe ROC jusqu'à la limite donnée.\n",
    "\n",
    "La figure suivante montre la courbe ROC micro-pondérée et son score ROC-AUC correspondant pour un classificateur visant à distinguer les différentes espèces dans l'ensemble de données [**Iris plants** (7.1.1)](https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-dataset) :\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_roc_002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cas multi-étiquettes\n",
    "\n",
    "Dans la **classification multi-étiquettes**, la fonction [**`roc_auc_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) est étendue par la moyenne des étiquettes comme [**ci-dessus** (3.3.2.1)](https://scikit-learn.org/stable/modules/model_evaluation.html#average). Dans ce cas, vous devez fournir un `y_score` de forme `(n_samples, n_classes)`. Ainsi, lors de l'utilisation des estimations de probabilité, il est nécessaire de sélectionner la probabilité de la classe avec l'étiquette supérieure pour chaque sortie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.82664884, 0.86034414, 0.94181818, 0.8502652 , 0.94809095])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "X, y = make_multilabel_classification(random_state=0)\n",
    "inner_clf = LogisticRegression(solver=\"liblinear\", random_state=0)\n",
    "clf = MultiOutputClassifier(inner_clf).fit(X, y)\n",
    "y_score = np.transpose([y_pred[:, 1] for y_pred in clf.predict_proba(X)])\n",
    "roc_auc_score(y, y_score, average=None)\n",
    "# array([0.82..., 0.86..., 0.94..., 0.85... , 0.94...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et les valeurs de décision ne nécessitent pas un tel traitement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81996435, 0.8467387 , 0.93090909, 0.87229702, 0.94422994])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "clf = RidgeClassifierCV().fit(X, y)\n",
    "y_score = clf.decision_function(X)\n",
    "roc_auc_score(y, y_score, average=None)\n",
    "# array([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemples\n",
    "\n",
    "##### [**Courbe de Caractéristique de Fonctionnement du Récepteur (ROC) Multiclasse**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/3_model_selection/plot_roc.ipynb)<br/>([*Multiclass Receiver Operating Characteristic (ROC)*](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html))\n",
    "\n",
    "Exemple de l'utilisation de la courbe ROC pour évaluer la qualité de la sortie d'un classifieur.\n",
    "\n",
    "##### [**Caractéristique de Fonctionnement du Récepteur (Receiver Operating Characteristic - ROC) avec validation croisée**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/3_model_selection/plot_roc_crossval.ipynb)<br/>([*Receiver Operating Characteristic (ROC) with cross validation*](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html))\n",
    "\n",
    "Exemple de l'utilisation de la courbe ROC pour évaluer la qualité de la sortie d'un classifieur, en utilisant une validation croisée.\n",
    "\n",
    "##### [**Modélisation de la distribution des espèces**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/applications/plot_species_distribution_modeling.ipynb)<br/>([_Species distribution modeling_](https://scikit-learn.org/stable/auto_examples/applications/plot_species_distribution_modeling.html))\n",
    "\n",
    "Exemple de l'utilisation de la courbe ROC pour modéliser la distribution des espèces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Références\n",
    "\n",
    "Z 🔬 [HT2001] (1,2) Hand, D.J. and Till, R.J., (2001). [**“A simple generalisation of the area under the ROC curve for multiple class classification problems”**](https://link.springer.com/content/pdf/10.1023/A:1010920819831.pdf). Machine learning, 45(2), pp. 171-186.\n",
    "\n",
    "Z 🔬 [FC2009] Ferri, Cèsar & Hernandez-Orallo, Jose & Modroiu, R. (2009). [**“An Experimental Comparison of Performance Measures for Classification”**](https://www.math.ucdavis.edu/~saito/data/roc/ferri-class-perf-metrics.pdf). Pattern Recognition Letters. 30. 27-38.\n",
    "\n",
    "Z 🔬 [PD2000] Provost, F., Domingos, P. (2000). [**“Well-trained PETs: Improving probability estimation trees”**](https://pages.stern.nyu.edu/~fprovost/Papers/pet-wp.pdf) (Section 6.2), CeDER Working Paper #IS-00-04, Stern School of Business, New York University.\n",
    "\n",
    "Z 🔬 [F2006] Fawcett, T., 2006. [**“An introduction to ROC analysis”**](http://www.decom.ufop.br/menotti/rp112/slides/13-2006-AnIntroductionToROCAnalysis-PRLv27pp861-874.pdf). Pattern Recognition Letters, 27(8), pp. 861-874.\n",
    "\n",
    "x 🔬 [F2001] Fawcett, T., 2001. [**“Using rule sets to maximize ROC performance”**](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=41641e00fe67bb0fd4f2d9cec01d0e35fc89552f) In Data Mining, 2001. Proceedings IEEE International Conference, pp. 131-138"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='detection-error-tradeoff-det'></a> 3.3.2.16. Courbe de trade-off d'erreur de détection (DET)\n",
    "\n",
    "La fonction [**`det_curve`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.det_curve.html) calcule la [**courbe de trade-off d'erreur de détection (DET)**](https://en.wikipedia.org/wiki/Detection_error_tradeoff). Pour citer Wikipedia :\n",
    "\n",
    "_\"Un graphique de trade-off d'erreur de détection (DET) est une représentation graphique des taux d'erreur pour les systèmes de classification binaire, qui trace le taux de rejet faux par rapport au taux d'acceptation faux. Les axes x et y sont mis à l'échelle de manière non linéaire par leurs écarts types normaux (ou simplement par transformation logarithmique), ce qui donne des courbes de trade-off plus linéaires que les courbes ROC et utilise la majeure partie de la zone de l'image pour mettre en évidence les différences d'importance dans la région opérationnelle critique.\"_\n",
    "\n",
    "Les courbes DET sont une variation des courbes ROC où le taux de faux négatifs est tracé sur l'axe des y au lieu du taux de vrais positifs. Les courbes DET sont généralement tracées à l'échelle des écarts normaux par transformation avec $\\phi^{-1}$ (avec $\\phi$ étant la fonction de répartition cumulative). Les courbes de performance résultantes visualisent explicitement le compromis entre les types d'erreurs pour les algorithmes de classification donnés. Voir [Martin1997] pour des exemples et des motivations supplémentaires.\n",
    "\n",
    "Cette figure compare les courbes ROC et DET de deux classificateurs d'exemple sur la même tâche de classification :\n",
    "\n",
    "![Courbes ROC et DET](https://scikit-learn.org/stable/_images/sphx_glr_plot_det_001.png)\n",
    "\n",
    "#### Propriétés\n",
    "\n",
    "- Les courbes DET forment une courbe linéaire à l'échelle des écarts normaux si les scores de détection sont distribués de manière normale (ou proche de la normale). Il a été démontré par [Navratil2007] que l'inverse n'est pas nécessairement vrai et que des distributions encore plus générales peuvent produire des courbes DET linéaires.\n",
    "\n",
    "- La transformation à l'échelle des écarts normaux écarte les points de manière à occuper un espace relativement plus grand sur le graphique. Par conséquent, des courbes avec des performances de classification similaires peuvent être plus faciles à distinguer sur un graphique DET.\n",
    "\n",
    "- Avec le taux de faux négatifs étant \"inverse\" au taux de vrais positifs, le point de perfection pour les courbes DET est l'origine (par opposition au coin supérieur gauche pour les courbes ROC).\n",
    "\n",
    "#### Applications et limites\n",
    "\n",
    "Les courbes DET sont intuitives à lire et permettent donc une évaluation visuelle rapide des performances d'un classificateur. De plus, les courbes DET peuvent être consultées pour une analyse des seuils et la sélection du point de fonctionnement. Cela est particulièrement utile si une comparaison des types d'erreurs est nécessaire.\n",
    "\n",
    "D'autre part, les courbes DET ne fournissent pas leur métrique sous forme d'un seul nombre. Par conséquent, pour une évaluation automatisée ou une comparaison avec d'autres tâches de classification, des métriques telles que l'aire sous la courbe ROC dérivée peuvent être plus adaptées.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Courbe de trade-off d'erreur de détection (DET)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/3_model_selection/plot_det.ipynb)<br/>([_Detection error tradeoff (DET) curve_](https://scikit-learn.org/stable/auto_examples/model_selection/plot_det.html))\n",
    "\n",
    "Exemple de comparaison entre les courbes de caractéristiques de fonctionnement du récepteur (ROC) et les courbes de trade-off d'erreur de détection (DET).\n",
    "\n",
    "#### Références\n",
    "\n",
    "Z 🔬 [Martin1997] A. Martin, G. Doddington, T. Kamm, M. Ordowski, and M. Przybocki, [**“The DET Curve in Assessment of Detection Task Performance”**](http://ccc.inaoep.mx/%7Evillasen/bib/martin97det.pdf), NIST 1997.\n",
    "\n",
    "Z 🔬 [Navratil2007] J. Navractil and D. Klusacek, [**“On Linear DETs”**](https://www.researchgate.net/profile/Jiri-Navratil/publication/4249280_On_Linear_DETs/links/5820e0f108ae40da2cb50dfb/On-Linear-DETs.pdf), 2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP ‘07, Honolulu, HI, 2007, pp. IV-229-IV-232."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='zero-one-loss'></a> 3.3.2.17. Perte zéro-un\n",
    "\n",
    "La fonction [**`zero_one_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.zero_one_loss.html) calcule la somme ou la moyenne de la perte de classification 0-1 ($L_{0-1}$) sur $n_{\\text{samples}}$. Par défaut, la fonction normalise par échantillon. Pour obtenir la somme de $L_{0-1}$, définissez `normalize` sur `False`.\n",
    "\n",
    "Dans la classification multilabel, la fonction [**`zero_one_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.zero_one_loss.html) attribue un score de un à un sous-ensemble si ses étiquettes correspondent strictement aux prédictions, et de zéro s'il y a des erreurs. Par défaut, la fonction renvoie le pourcentage de sous-ensembles mal prédits. Pour obtenir le nombre de ces sous-ensembles, définissez `normalize` sur `False`.\n",
    "\n",
    "Si $\\hat{y}_i$ est la valeur prédite pour le $i$-ème échantillon et $y_i$ est la valeur vraie correspondante, alors la perte 0-1 $L_{0-1}$ est définie comme suit :\n",
    "\n",
    "$$L_{0-1}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples}-1} 1(\\hat{y}_i \\neq y_i)$$\n",
    "\n",
    "où $1(x)$ est la [**fonction indicatrice**](https://en.wikipedia.org/wiki/Indicator_function). La perte zéro-un peut également être calculée comme $zero\\_one\\_loss = 1 - accuracy$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "y_pred = [1, 2, 3, 4]\n",
    "y_true = [2, 2, 3, 4]\n",
    "zero_one_loss(y_true, y_pred)\n",
    "# 0.25\n",
    "zero_one_loss(y_true, y_pred, normalize=False)\n",
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas multilabel avec des indicateurs d'étiquettes binaires, où le premier ensemble d'étiquettes [0,1] comporte une erreur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
    "# 0.5\n",
    "zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)),  normalize=False)\n",
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemple\n",
    "\n",
    "##### [**Élimination récursive des caractéristiques (RFE) avec validation croisée**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_13_feature_selection/plot_rfe_with_cross_validation.ipynb)<br/>([_Recursive Feature Elimination (RFE) with cross-validation_](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html))\n",
    "\n",
    "Exemple d'utilisation de la perte zéro-un pour effectuer une élimination récursive des fonctionnalités avec validation croisée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='brier-score-loss'></a> 3.3.2.18. Score de Brier\n",
    "\n",
    "La fonction [**`brier_score_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html) calcule le [**score de Brier**](https://en.wikipedia.org/wiki/Brier_score) pour les classes binaires [Brier1950]. Pour citer Wikipedia :\n",
    "\n",
    "_« Le score de Brier est une fonction de score appropriée qui mesure l'exactitude des prédictions probabilistes. Il s'applique aux tâches dans lesquelles les prédictions doivent attribuer des probabilités à un ensemble de résultats discrets mutuellement exclusifs. »_\n",
    "\n",
    "Cette fonction renvoie l'erreur quadratique moyenne entre le résultat réel $y \\in \\{0,1\\}$ et l'estimation de probabilité prédite $p = \\operatorname{Pr}(y = 1)$ ([**predict_proba**](https://scikit-learn.org/stable/glossary.html#term-predict_proba)) comme sortie par :\n",
    "\n",
    "$$BS = \\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}} - 1}(y_i - p_i)^2$$\n",
    "\n",
    "Le score de Brier est également compris entre 0 et 1, et plus la valeur est basse (l'écart quadratique moyen est plus petit), plus la prédiction est précise.\n",
    "\n",
    "Voici un petit exemple d'utilisation de cette fonction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import brier_score_loss\n",
    "y_true = np.array([0, 1, 1, 0])\n",
    "y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n",
    "y_prob = np.array([0.1, 0.9, 0.8, 0.4])\n",
    "y_pred = np.array([0, 1, 1, 0])\n",
    "brier_score_loss(y_true, y_prob)\n",
    "# 0.055\n",
    "brier_score_loss(y_true, 1 - y_prob, pos_label=0)\n",
    "# 0.055\n",
    "brier_score_loss(y_true_categorical, y_prob, pos_label=\"ham\")\n",
    "# 0.055\n",
    "brier_score_loss(y_true, y_prob > 0.5)\n",
    "# 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le score de Brier peut être utilisé pour évaluer à quel point un classificateur est calibré. Cependant, un score de Brier plus bas ne signifie pas toujours une meilleure calibration. Cela est dû au fait que, par analogie avec la décomposition biais-variance de l'erreur quadratique moyenne, le score de Brier peut être décomposé en la somme de la perte de calibration et de la perte de raffinement [Bella2012]. La perte de calibration est définie comme l'écart quadratique moyen par rapport aux probabilités empiriques dérivées de la pente des segments de la courbe ROC. La perte de raffinement peut être définie comme la perte optimale attendue mesurée par l'aire sous la courbe de coût optimale. La perte de raffinement peut changer indépendamment de la perte de calibration, donc un score de Brier plus bas ne signifie pas nécessairement un modèle mieux calibré. \"Ce n'est que lorsque la perte de raffinement reste la même qu'un score de Brier plus bas signifie toujours une meilleure calibration\" [Bella2012], [Flach2008].\n",
    "\n",
    "#### Exemple\n",
    "\n",
    "##### [**Calibration des probabilités des classificateurs**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/calibration/plot_calibration.ipynb)<br/>([_Probability calibration of classifiers_](https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration.html))\n",
    "\n",
    "Exemple d'utilisation du score de Brier pour effectuer la calibration des probabilités des classifieurs.\n",
    "\n",
    "#### Références\n",
    "\n",
    "Z 🔬 [Brier1950] G. Brier, [**“Verification of forecasts expressed in terms of probability”**](https://viterbi-web.usc.edu/~shaddin/cs699fa17/docs/Brier50.pdf), Monthly weather review 78.1 (1950)\n",
    "\n",
    "Z 🔬 [Bella2012] (1,2) Bella, Ferri, Hernández-Orallo, and Ramírez-Quintana [**“Calibration of Machine Learning Models”**](http://dmip.webs.upv.es/papers/BFHRHandbook2010.pdf) in Khosrow-Pour, M. “Machine learning: concepts, methodologies, tools and applications.” Hershey, PA: Information Science Reference (2012).\n",
    "\n",
    "Z 🔬 [Flach2008] Flach, Peter, and Edson Matsubara. [**“On classification, ranking, and probability estimation”**](https://drops.dagstuhl.de/opus/volltexte/2008/1382/pdf/07161.FlachPeter.Paper.1382.pdf). Dagstuhl Seminar Proceedings. Schloss Dagstuhl-Leibniz-Zentrum fr Informatik (2008)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='class-likelihood-ratios'></a> 3.3.2.19. Rapports de vraisemblance de classe\n",
    "\n",
    "La fonction [**`class_likelihood_ratios`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.class_likelihood_ratios.html) calcule les [**rapports de vraisemblance positifs et négatifs**](https://en.wikipedia.org/wiki/Likelihood_ratios_in_diagnostic_testing)  $LR_\\pm$ pour les classes binaires, qui peuvent être interprétés comme le rapport des cotes post-test sur pré-test, comme expliqué ci-dessous. En conséquence, cette mesure est invariante par rapport à la prévalence de la classe (le nombre d'échantillons dans la classe positive divisé par le nombre total d'échantillons) et **peut être extrapolée entre les populations indépendamment de tout déséquilibre éventuel des classes**.\n",
    "\n",
    "Les $LR_\\pm$ metrics sont donc très utiles dans les situations où les données disponibles pour apprendre et évaluer un classifieur proviennent d'une population étudiée avec des classes presque équilibrées, comme une étude cas-témoin, tandis que l'application cible, c'est-à-dire la population générale, a une prévalence très faible.\n",
    "\n",
    "Le rapport de vraisemblance positif $LR_+$ est la probabilité d'un classifieur de prédire correctement qu'un échantillon appartient à la classe positive, divisée par la probabilité de prédire la classe positive pour un échantillon appartenant à la classe négative :\n",
    "\n",
    "$$LR_+ = \\frac{\\text{PR}(P+|T+)}{\\text{PR}(P+|T-)}.$$\n",
    "\n",
    "La notation ici fait référence à l'étiquette prédite ($P$) ou réelle ($T$) et le signe $+$ et $-$ fait référence aux classes positive et négative, respectivement, par exemple $P+$ signifie \"prédiction positive\".\n",
    "\n",
    "De manière analogue, le rapport de vraisemblance négatif $LR_-$ est la probabilité qu'un échantillon de la classe positive soit classé comme appartenant à la classe négative, divisée par la probabilité qu'un échantillon de la classe négative soit correctement classé :\n",
    "\n",
    "$$LR_- = \\frac{\\text{PR}(P-|T+)}{\\text{PR}(P-|T-)}.$$\n",
    "\n",
    "Pour les classifieurs supérieurs au hasard, $LR_+$ au-dessus de 1 est **meilleur**, tandis que $LR_-$ varie de 0 à 1 et est **meilleur** si plus petit. Les valeurs de $LR_\\pm \\approx 1$ correspondent à un niveau de chance.\n",
    "\n",
    "Notez que les probabilités diffèrent des dénombrements, par exemple $\\operatorname{PR}(P+|T+)$ n'est pas égal au nombre de vrais positifs `tp` (voir [la page Wikipedia](https://en.wikipedia.org/wiki/Likelihood_ratios_in_diagnostic_testing) pour les formules réelles).\n",
    "\n",
    "#### Interprétation avec une prévalence variable\n",
    "\n",
    "Les deux rapports de vraisemblance de classe sont interprétables en termes de rapport des cotes (pré-test et post-test) :\n",
    "\n",
    "$$\\text{cotes post-test} = \\text{Rapport de vraisemblance} \\times \\text{cotes pré-test}.$$\n",
    "\n",
    "Les cotes sont en général liées aux probabilités via\n",
    "\n",
    "$$\\text{cotes} = \\frac{\\text{probabilité}}{1 - \\text{probabilité}},$$\n",
    "\n",
    "ou de manière équivalente\n",
    "\n",
    "$$\\text{probabilité} = \\frac{\\text{cotes}}{1 + \\text{cotes}}.$$\n",
    "\n",
    "Dans une population donnée, la probabilité pré-test est donnée par la prévalence. En convertissant les cotes en probabilités, les rapports de vraisemblance peuvent être traduits en probabilité d'appartenir véritablement à l'une ou l'autre classe avant et après la prédiction d'un classifieur :\n",
    "\n",
    "$$\\text{cotes post-test} = \\text{Rapport de vraisemblance} \\times\n",
    "\\frac{\\text{probabilité pré-test}}{1 - \\text{probabilité pré-test}},$$\n",
    "$$\\text{probabilité post-test} = \\frac{\\text{cotes post-test}}{1 + \\text{cotes post-test}}.$$\n",
    "\n",
    "#### Divergences mathématiques\n",
    "\n",
    "Le rapport de vraisemblance positif est indéfini lorsque $fp = 0$, ce qui peut être interprété comme le classifieur identifiant parfaitement les cas positifs. Si $fp = 0$ et en plus $tp = 0$, cela entraîne une division par zéro. Cela se produit, par exemple, lors de l'utilisation d'un `DummyClassifier` qui prédit toujours la classe négative, ce qui fait perdre l'interprétation en tant que classifieur parfait.\n",
    "\n",
    "Le rapport de vraisemblance négatif est indéfini lorsque $tn = 0$. Une telle divergence est invalide, car $LR_- > 1$ indiquerait une augmentation des chances qu'un échantillon appartienne à la classe positive après avoir été classifié comme négatif, comme si l'acte de classification avait provoqué la condition positive. Cela inclut le cas d'un `DummyClassifier` qui prédit toujours la classe positive (c'est-à-dire lorsque $tn=fn=0$).\n",
    "\n",
    "Les deux rapports de vraisemblance de classe sont indéfinis lorsque $tp=fn=0$, ce qui signifie qu'aucun échantillon de la classe positive n'était présent dans l'ensemble de test. Cela peut également se produire lors de la validation croisée de données fortement déséquilibrées.\n",
    "\n",
    "Dans tous les cas précédents, la fonction [**`class_likelihood_ratios`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.class_likelihood_ratios.html) déclenche par défaut un message d'avertissement approprié et renvoie `nan` pour éviter toute pollution lors de la moyenne sur les plis de validation croisée.\n",
    "\n",
    "Pour une démonstration détaillée de la fonction [**`class_likelihood_ratios`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.class_likelihood_ratios.html), consultez l'exemple ci-dessous.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Rapports de vraisemblance de classe pour mesurer la performance de classification**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/3_model_selection/plot_likelihood_ratios.ipynb)<br/>([_Class Likelihood Ratios to measure classification performance_](https://scikit-learn.org/stable/auto_examples/model_selection/plot_likelihood_ratios.html))\n",
    "\n",
    "#### Références\n",
    "\n",
    "Z 🔬 Brenner, H., & Gefeller, O. (1997). [**“Variation of sensitivity, specificity, likelihood ratios and predictive values with disease prevalence”**](https://www.floppybunny.org/robin/web/virtualclassroom/stats/basics/articles/odds_risks/odds_sensitivity%29likelihood_ratios_validity_brenner_1997.pdf). Statistics in medicine, 16(9), 981-991"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='multilabel-ranking-metrics'></a> 3.3.3. Mesures de classement multi-étiquettes\n",
    "\n",
    "En apprentissage multi-étiquettes, chaque échantillon peut être associé à un nombre quelconque d'étiquettes réelles. L'objectif est d'attribuer des scores élevés et un meilleur classement aux étiquettes réelles.\n",
    "\n",
    "### <a id='multilabel-ranking-metrics'></a> 3.3.3.1. Erreur de couverture\n",
    "\n",
    "La fonction [**`coverage_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.coverage_error.html) calcule le nombre moyen d'étiquettes qui doivent être incluses dans la prédiction finale pour que toutes les étiquettes réelles soient prédites. Cela est utile si vous souhaitez savoir combien d'étiquettes ayant le meilleur score vous devez prédire en moyenne sans en manquer une seule. La meilleure valeur de cette métrique est donc le nombre moyen d'étiquettes réelles.\n",
    "\n",
    "**Note :** Le score de notre implémentation est supérieur d'une unité à celui donné dans l'article de Tsoumakas et al., 2010. Cela étend la fonction pour gérer le cas dégénéré dans lequel une instance n'a aucune étiquette réelle.\n",
    "\n",
    "Formellement, étant donné une matrice binaire d'indicateurs des étiquettes réelles $y \\in \\left\\{0, 1\\right\\}^{n_\\text{samples} \\times n_\\text{labels}}$ et le score associé à chaque étiquette $\\hat{f} \\in \\mathbb{R}^{n_\\text{samples} \\times n_\\text{labels}}$, la couverture est définie comme suit :\n",
    "\n",
    "$$\\text{couverture}(y, \\hat{f}) = \\frac{1}{n_{\\text{samples}}}\n",
    "  \\sum_{i=0}^{n_{\\text{samples}} - 1} \\max_{j:y_{ij} = 1} \\text{rang}_{ij}$$\n",
    "\n",
    "avec $\\text{rang}_{ij} = \\left|\\left\\{k: \\hat{f}_{ik} \\geq \\hat{f}_{ij} \\right\\}\\right|$. Étant donné la définition du rang, les égalités dans `y_scores` sont résolues en attribuant le rang maximal qui aurait été attribué à toutes les valeurs égales.\n",
    "\n",
    "Voici un petit exemple d'utilisation de cette fonction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import coverage_error\n",
    "y_true = np.array([[1, 0, 0], [0, 0, 1]])\n",
    "y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n",
    "coverage_error(y_true, y_score)\n",
    "# 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='label-ranking-average-precision'></a> 3.3.3.2. Précision moyenne du classement des étiquettes\n",
    "\n",
    "La fonction [**`label_ranking_average_precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.label_ranking_average_precision_score.html) implémente la précision moyenne du classement des étiquettes (LRAP - Label Ranking Average Precision). Cette mesure est liée à la fonction [**`average_precision_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html), mais est basée sur la notion de classement des étiquettes plutôt que sur la précision et le rappel.\n",
    "\n",
    "La précision moyenne du classement des étiquettes (LRAP) calcule en moyenne sur les échantillons la réponse à la question suivante : pour chaque étiquette réelle, quelle fraction des étiquettes mieux classées étaient des étiquettes réelles ? Cette mesure de performance sera plus élevée si vous êtes capable de donner un meilleur classement aux étiquettes associées à chaque échantillon. Le score obtenu est toujours strictement supérieur à 0, et la meilleure valeur est 1. S'il y a exactement une étiquette pertinente par échantillon, la précision moyenne du classement des étiquettes est équivalente au [**taux de rang réciproque moyen**](https://en.wikipedia.org/wiki/Mean_reciprocal_rank).\n",
    "\n",
    "Formellement, étant donné une matrice binaire d'indicateurs des étiquettes réelles $y \\in \\left\\{0, 1\\right\\}^{n_\\text{samples} \\times n_\\text{labels}}$ et le score associé à chaque étiquette $\\hat{f} \\in \\mathbb{R}^{n_\\text{samples} \\times n_\\text{labels}}$, la précision moyenne est définie comme suit :\n",
    "\n",
    "$$LRAP(y, \\hat{f}) = \\frac{1}{n_{\\text{samples}}}\n",
    "  \\sum_{i=0}^{n_{\\text{samples}} - 1} \\frac{1}{||y_i||_0}\n",
    "  \\sum_{j:y_{ij} = 1} \\frac{|\\mathcal{L}_{ij}|}{\\text{rang}_{ij}}$$\n",
    "\n",
    "où $\\mathcal{L}_{ij} = \\left\\{k: y_{ik} = 1, \\hat{f}_{ik} \\geq \\hat{f}_{ij} \\right\\}$, $|\\cdot|$, calcule le cardinal de l'ensemble (c'est-à-dire le nombre d'éléments dans l'ensemble), et $||\\cdot||_0$ est la \"norme\" $\\ell_0$ (qui calcule le nombre d'éléments non nuls dans un vecteur).\n",
    "\n",
    "Voici un petit exemple d'utilisation de cette fonction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41666666666666663"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "y_true = np.array([[1, 0, 0], [0, 0, 1]])\n",
    "y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n",
    "label_ranking_average_precision_score(y_true, y_score)\n",
    "# 0.416..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='ranking-loss'></a> 3.3.3.3. Perte de classement\n",
    "\n",
    "La fonction [**`label_ranking_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.label_ranking_loss.html) calcule la perte de classement, qui représente en moyenne sur les échantillons le nombre de paires d'étiquettes incorrectement ordonnées, c'est-à-dire les étiquettes réelles ayant un score inférieur aux étiquettes fausses, pondéré par l'inverse du nombre de paires ordonnées d'étiquettes fausses et réelles. La perte de classement la plus basse possible est zéro.\n",
    "\n",
    "Formellement, étant donné une matrice binaire d'indicateurs des étiquettes réelles $y \\in \\left\\{0, 1\\right\\}^{n_\\text{samples} \\times n_\\text{labels}}$ et le score associé à chaque étiquette $\\hat{f} \\in \\mathbb{R}^{n_\\text{samples} \\times n_\\text{labels}}$, la perte de classement est définie comme suit :\n",
    "\n",
    "$$ranking\\_loss(y, \\hat{f}) =  \\frac{1}{n_{\\text{samples}}}\n",
    "  \\sum_{i=0}^{n_{\\text{samples}} - 1} \\frac{1}{||y_i||_0(n_\\text{labels} - ||y_i||_0)}\n",
    "  \\left|\\left\\{(k, l): \\hat{f}_{ik} \\leq \\hat{f}_{il}, y_{ik} = 1, y_{il} = 0 \\right\\}\\right|$$\n",
    "\n",
    "où $|\\cdot|$ calcule le cardinal de l'ensemble (c'est-à-dire le nombre d'éléments dans l'ensemble) et $||\\cdot||_0$ est la \"norme\" $\\ell_0$ (qui calcule le nombre d'éléments non nuls dans un vecteur).\n",
    "\n",
    "Voici un petit exemple d'utilisation de cette fonction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import label_ranking_loss\n",
    "y_true = np.array([[1, 0, 0], [0, 0, 1]])\n",
    "y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n",
    "label_ranking_loss(y_true, y_score)\n",
    "# 0.75...\n",
    "# With the following prediction, we have perfect and minimal loss\n",
    "y_score = np.array([[1.0, 0.1, 0.2], [0.1, 0.2, 0.9]])\n",
    "label_ranking_loss(y_true, y_score)\n",
    "# 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Référence\n",
    "\n",
    "Z 🔬 Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010). [**“Mining multi-label data”**](http://lpis.csd.auth.gr/publications/tsoumakas09-dmkdh.pdf). In Data mining and knowledge discovery handbook (pp. 667-685). Springer US."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='normalized-discounted-cumulative-gain'></a> 3.3.3.4. Gain cumulé actualisé normalisé\n",
    "\n",
    "Le Gain cumulé actualisé (DCG) et le Gain cumulé actualisé normalisé (NDCG) sont des métriques de classement implémentées dans les fonctions `dcg_score` et `ndcg_score` ; elles comparent un ordre prédit aux scores réels, tels que la pertinence des réponses à une requête.\n",
    "\n",
    "D'après la page Wikipedia pour le [**Gain cumulé actualisé**](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) :\n",
    "\n",
    "_“Le Gain cumulé actualisé (DCG) est une mesure de la qualité de classement. En recherche d'informations, il est souvent utilisé pour mesurer l'efficacité des algorithmes de moteurs de recherche Web ou d'applications similaires. En utilisant une échelle de pertinence graduée des documents dans un ensemble de résultats de moteur de recherche, le DCG mesure l'utilité, ou le gain, d'un document en fonction de sa position dans la liste de résultats. Le gain est cumulé du haut de la liste de résultats vers le bas, le gain de chaque résultat étant réduit pour les rangs inférieurs”_\n",
    "\n",
    "Le DCG ordonne les cibles réelles (par exemple, la pertinence des réponses à une requête) dans l'ordre prédit, puis les multiplie par une décroissance logarithmique et en fait la somme. La somme peut être tronquée après les premiers $K$ résultats, auquel cas nous l'appelons DCG@K. Le NDCG, ou NDCG@K, est le DCG divisé par le DCG obtenu par une prédiction parfaite, de sorte qu'il soit toujours compris entre 0 et 1. En général, le NDCG est préféré au DCG.\n",
    "\n",
    "Comparé à la perte de classement, le NDCG peut prendre en compte les scores de pertinence, plutôt qu'un classement réel. Donc, si les données réelles consistent uniquement en un classement, la perte de classement devrait être préférée ; si les données réelles consistent en des scores d'utilité réels (par exemple, 0 pour non pertinent, 1 pour pertinent, 2 pour très pertinent), le NDCG peut être utilisé.\n",
    "\n",
    "Pour un échantillon, étant donné le vecteur des valeurs réelles continues pour chaque cible $y \\in \\mathbb{R}^{M}$, où $M$ est le nombre de sorties, et la prédiction $\\hat{y}$, qui induit la fonction de classement $f$, le score DCG est défini comme suit :\n",
    "\n",
    "$$\\sum_{r=1}^{\\min(K, M)}\\frac{y_{f(r)}}{\\log(1 + r)}$$\n",
    "\n",
    "et le score NDCG est le score DCG divisé par le score DCG obtenu pour $y$.\n",
    "\n",
    "#### Références\n",
    "\n",
    "Z 🔬 Jarvelin, K., & Kekalainen, J. (2002). [**“Cumulated Gain-Based Evaluation of IR Techniques”**](https://faculty.cc.gatech.edu/~zha/CS8803WST/dcg.pdf). ACM Transactions on Information Systems (TOIS), 20(4), 422-446.\n",
    "\n",
    "Z 🔬 Wang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May). [**“A Theoretical Analysis of NDCG Ranking Measures”**](http://proceedings.mlr.press/v30/Wang13.pdf). In Proceedings of the 26th Annual Conference on Learning Theory (COLT 2013)\n",
    "\n",
    "Z 🔬 McSherry, F., & Najork, M. (2008, March). [**“Computing Information Retrieval Performance Measures Efficiently in the Presence of Tied Scores”**](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ecir2008.pdf). In European conference on information retrieval (pp. 414-421). Springer, Berlin, Heidelberg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='regression-metrics'></a> 3.3.4. Métriques de régression\n",
    "\n",
    "Le module [**`sklearn.metrics`**](https://scikit-learn.org/stable/modules/classes.html) implémente plusieurs fonctions de perte, de score et d'utilité pour mesurer les performances en régression. Certaines d'entre elles ont été améliorées pour gérer le cas de sortie multiple : [**`mean_squared_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html), [**`mean_absolute_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html), [**`r2_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html), [**`explained_variance_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.explained_variance_score.html), [**`mean_pinball_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_pinball_loss.html), [**`d2_pinball_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.d2_pinball_score.html) et [**`d2_absolute_error_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.d2_absolute_error_score.html).\n",
    "\n",
    "Ces fonctions possèdent un argument facultatif `multioutput` qui spécifie la manière dont les scores ou les pertes pour chaque cible individuelle doivent être moyennés. La valeur par défaut est `'uniform_average'`, qui spécifie une moyenne pondérée uniformément sur les sorties. Si un `ndarray` de forme `(n_outputs,)` est passé, ses entrées sont interprétées comme des poids et une moyenne pondérée correspondante est renvoyée. Si `multioutput` est `'raw_values'`, alors tous les scores ou pertes individuels non modifiés seront renvoyés dans un tableau de forme `(n_outputs,)`.\n",
    "\n",
    "Les fonctions [**`r2_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html) et [**`explained_variance_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.explained_variance_score.html) acceptent une valeur supplémentaire `'variance_weighted'` pour le paramètre `multioutput`. Cette option conduit à une pondération de chaque score individuel par la variance de la variable cible correspondante. Ce réglage quantifie la variance globalement capturée et non mise à l'échelle. Si les variables cibles ont des échelles différentes, alors ce score accorde plus d'importance à l'explication des variables à variance plus élevée. `multioutput='variance_weighted'` est la valeur par défaut pour [**`r2_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html) pour assurer la compatibilité ascendante. Cela sera changé en `uniform_average` à l'avenir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='r2-score-the-coefficient-of-determination'></a> 3.3.4.1. Score R², le coefficient de détermination\n",
    "\n",
    "La fonction [**`r2_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html) calcule le [**coefficient de détermination**](https://en.wikipedia.org/wiki/Coefficient_of_determination), généralement noté $R^2$.\n",
    "\n",
    "Il représente la proportion de variance (de $y$) qui a été expliquée par les variables indépendantes dans le modèle. Il fournit une indication de l'ajustement du modèle et donc une mesure de la capacité du modèle à prédire correctement les échantillons non vus, à travers la proportion de variance expliquée.\n",
    "\n",
    "Comme la variance dépend du jeu de données, le score $R^2$ peut ne pas être comparé de manière significative entre différents ensembles de données. Le meilleur score possible est 1.0 et il peut être négatif (car le modèle peut être arbitrairement moins bon). Un modèle constant qui prédit toujours la valeur attendue (moyenne) de $y$, en ignorant les caractéristiques d'entrée, obtiendrait un score $R^2$ de 0.0.\n",
    "\n",
    "**Note :** lorsque les résidus de prédiction ont une moyenne nulle, le score $R^2$ et le [**score de variance expliquée** (3.3.4.8)](https://scikit-learn.org/stable/modules/model_evaluation.html) sont identiques.\n",
    "\n",
    "Si $\\hat{y}_i$ est la valeur prédite du $i$-ème échantillon et $y_i$ est la valeur réelle correspondante pour un total de $n$ échantillons, le $R^2$ estimé est défini comme suit :\n",
    "\n",
    "$$R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$$\n",
    "\n",
    "où $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i$ et $\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} \\epsilon_i^2$.\n",
    "\n",
    "Remarque : [**`r2_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html) calcule le $R^2$ non ajusté sans correction du biais dans la variance d'échantillon de $y$.\n",
    "\n",
    "Dans le cas particulier où la cible réelle est constante, le score $R^2$ n'est pas fini : il est soit `NaN` (prédictions parfaites), soit `-Inf` (prédictions imparfaites). De tels scores non finis peuvent empêcher une optimisation correcte du modèle, telle qu'une recherche en grille avec validation croisée. Pour cette raison, le comportement par défaut de [**`r2_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html) est de les remplacer par 1.0 (prédictions parfaites) ou 0.0 (prédictions imparfaites). Si `force_finite` est défini sur `False`, ce score revient à la définition originale du $R^2$.\n",
    "\n",
    "Voici un petit exemple d'utilisation de la fonction [**`r2_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9486081370449679"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "r2_score(y_true, y_pred)\n",
    "# 0.948..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9382566585956417"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    "r2_score(y_true, y_pred, multioutput='variance_weighted')\n",
    "# 0.938..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9368005266622779"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    "r2_score(y_true, y_pred, multioutput='uniform_average')\n",
    "# 0.936..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96543779, 0.90816327])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_true, y_pred, multioutput='raw_values')\n",
    "# array([0.965..., 0.908...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9253456221198156"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_true, y_pred, multioutput=[0.3, 0.7])\n",
    "# 0.925..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [-2, -2, -2]\n",
    "y_pred = [-2, -2, -2]\n",
    "r2_score(y_true, y_pred)\n",
    "# 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:675: RuntimeWarning: invalid value encountered in divide\n",
      "  output_scores = 1 - (numerator / denominator)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_true, y_pred, force_finite=False)\n",
    "# nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [-2, -2, -2]\n",
    "y_pred = [-2, -2, -2 + 1e-8]\n",
    "r2_score(y_true, y_pred)\n",
    "# 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:675: RuntimeWarning: divide by zero encountered in divide\n",
      "  output_scores = 1 - (numerator / denominator)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_true, y_pred, force_finite=False)\n",
    "# -inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemples\n",
    "\n",
    "##### [**Modèles basés sur L1 pour signaux creux**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_lasso_and_elasticnet.ipynb)<br/>([_L1-based models for Sparse Signals_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_and_elasticnet.html))\n",
    "\n",
    "Exemple d'utilisation du score R² pour évaluer Lasso et Elastic Net sur des signaux creux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='mean-absolute-error'></a> 3.3.4.2. Erreur absolue moyenne\n",
    "\n",
    "La fonction [**`mean_absolute_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html) (MAE) calcule l'[**erreur absolue moyenne**](https://en.wikipedia.org/wiki/Mean_absolute_error), une mesure de risque correspondant à la valeur attendue de la perte d'erreur absolue ou de la norme $\\ell_1$.\n",
    "\n",
    "Si $\\hat{y}_i$ représente la valeur prédite pour le $i$-ème échantillon, et $y_i$ la valeur réelle correspondante, alors l'erreur absolue moyenne (MAE) estimée sur $n_{\\text{samples}}$ est définie comme suit :\n",
    "\n",
    "$$\\text{MAE}(y, \\hat{y}) = \\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}}-1} \\left| y_i - \\hat{y}_i \\right|.$$\n",
    "\n",
    "Voici un petit exemple d'utilisation de la fonction [**`mean_absolute_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "mean_absolute_error(y_true, y_pred)\n",
    "# 0.5\n",
    "y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    "mean_absolute_error(y_true, y_pred)\n",
    "# 0.75\n",
    "mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
    "# array([0.5, 1. ])\n",
    "mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
    "# 0.85..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='mean-squared-error'></a> 3.3.4.3. Erreur quadratique moyenne\n",
    "\n",
    "La fonction [**`mean_squared_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) (MSE) calcule l'[**erreur quadratique moyenne**](https://en.wikipedia.org/wiki/Mean_squared_error), une métrique de risque correspondant à la valeur attendue de l'erreur au carré (quadratique) ou de la perte.\n",
    "\n",
    "Si $\\hat{y}_i$ représente la valeur prédite pour le $i$-ème échantillon, et $y_i$ la valeur réelle correspondante, alors l'erreur quadratique moyenne (MSE) estimée sur $n_{\\text{samples}}$ est définie comme suit :\n",
    "\n",
    "$$\\text{MSE}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples} - 1} (y_i - \\hat{y}_i)^2.$$\n",
    "\n",
    "Voici un petit exemple d'utilisation de la fonction [**`mean_squared_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7083333333333334"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "mean_squared_error(y_true, y_pred)\n",
    "# 0.375\n",
    "y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    "mean_squared_error(y_true, y_pred)\n",
    "# 0.7083..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemples\n",
    "\n",
    "##### [**Régression à amplification de gradient**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_11_ensembles/plot_gradient_boosting_regression.ipynb)<br/>([_Gradient Boosting regression_](https://scikit-learn.org/stable/auto_examples/ensembles/plot_gradient_boosting_regression.html))\n",
    "\n",
    "Exemple d'utilisation de l'erreur quadratique moyenne pour évaluer la régression par Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='mean-squared-logarithmic-error'></a> 3.3.4.4. Erreur quadratique logarithmique moyenne\n",
    "\n",
    "La fonction [**`mean_squared_log_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_log_error.html) (MSLE) calcule une métrique de risque correspondant à la valeur attendue de l'erreur logarithmique (quadratique) au carré ou de la perte.\n",
    "\n",
    "Si $\\hat{y}_i$ représente la valeur prédite pour le $i$-ème échantillon, et $y_i$ la valeur réelle correspondante, alors l'erreur quadratique logarithmique moyenne (MSLE) estimée sur $n_{\\text{samples}}$ est définie comme suit :\n",
    "\n",
    "$$\\text{MSLE}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples} - 1} (\\log_e (1 + y_i) - \\log_e (1 + \\hat{y}_i) )^2.$$\n",
    "\n",
    "Où $\\log_e (x)$ représente le logarithme népérien de $x$. Cette métrique est préférable à utiliser lorsque les cibles ont une croissance exponentielle, comme les nombres de population, les ventes moyennes d'une marchandise sur plusieurs années, etc. Notez que cette métrique pénalise une estimation sous-évaluée plus sévèrement qu'une estimation surévaluée.\n",
    "\n",
    "Voici un petit exemple d'utilisation de la fonction [**`mean_squared_log_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_log_error.html) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.044199361889160536"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "y_true = [3, 5, 2.5, 7]\n",
    "y_pred = [2.5, 5, 4, 8]\n",
    "mean_squared_log_error(y_true, y_pred)\n",
    "# 0.039...\n",
    "y_true = [[0.5, 1], [1, 2], [7, 6]]\n",
    "y_pred = [[0.5, 2], [1, 2.5], [8, 8]]\n",
    "mean_squared_log_error(y_true, y_pred)\n",
    "# 0.044..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='mean-absolute-percentage-error'></a> 3.3.4.5. Erreur absolue moyenne en pourcentage\n",
    "\n",
    "La fonction [**`mean_absolute_percentage_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_percentage_error.html) (MAPE), également connue sous le nom de [**déviation moyenne absolue en pourcentage**](https://en.wikipedia.org/wiki/Mean_absolute_percentage_error) (MAPD), est une métrique d'évaluation pour les problèmes de régression. L'idée de cette métrique est d'être sensible aux erreurs relatives. Par exemple, elle n'est pas affectée par une mise à l'échelle globale de la variable cible.\n",
    "\n",
    "Si $\\hat{y}_i$ représente la valeur prédite pour le $i$-ème échantillon et $y_i$ la valeur réelle correspondante, alors l'erreur absolue moyenne en pourcentage (MAPE) estimée sur $n_{\\text{samples}}$ est définie comme suit :\n",
    "\n",
    "$$\\text{MAPE}(y, \\hat{y}) = \\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}}-1} \\frac{{}\\left| y_i - \\hat{y}_i \\right|}{\\max(\\epsilon, \\left| y_i \\right|)}$$\n",
    "\n",
    "où $\\epsilon$ est un petit nombre arbitraire mais strictement positif pour éviter des résultats indéfinis lorsque $y$ est nul.\n",
    "\n",
    "La fonction [**`mean_absolute_percentage_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_percentage_error.html) prend en charge plusieurs sorties (multioutput).\n",
    "\n",
    "Voici un petit exemple d'utilisation de la fonction [**`mean_absolute_percentage_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_percentage_error.html) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26666666666666666"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "y_true = [1, 10, 1e6]\n",
    "y_pred = [0.9, 15, 1.2e6]\n",
    "mean_absolute_percentage_error(y_true, y_pred)\n",
    "# 0.2666..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans l'exemple ci-dessus, si nous avions utilisé la métrique `mean_absolute_error`, elle aurait ignoré les petites valeurs de magnitude et aurait seulement reflété l'erreur de prédiction de la valeur de plus grande magnitude. Mais ce problème est résolu dans le cas de la métrique MAPE car elle calcule l'erreur relative en pourcentage par rapport à la sortie réelle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='median-absolute-error'></a> 3.3.4.6. Erreur absolue médiane\n",
    "\n",
    "La fonction [**`median_absolute_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.median_absolute_error.html) (MedAE) est particulièrement intéressante car elle est robuste aux valeurs aberrantes (outliers). La perte est calculée en prenant la médiane de toutes les différences absolues entre la cible et la prédiction.\n",
    "\n",
    "Si $\\hat{y}_i$ représente la valeur prédite pour le $i$-ème échantillon et $y_i$ la valeur réelle correspondante, alors l'erreur absolue médiane (MedAE) estimée sur $n_{\\text{samples}}$ est définie comme suit :\n",
    "\n",
    "$$\\text{MedAE}(y, \\hat{y}) = \\text{médiane}(\\mid y_1 - \\hat{y}_1 \\mid, \\ldots, \\mid y_n - \\hat{y}_n \\mid).$$\n",
    "\n",
    "La fonction [**`median_absolute_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.median_absolute_error.html) ne prend **pas** en charge les sorties multiples (multioutput).\n",
    "\n",
    "Voici un petit exemple d'utilisation de la fonction [**`median_absolute_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.median_absolute_error.html) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import median_absolute_error\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "median_absolute_error(y_true, y_pred)\n",
    "# 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='max-error'></a> 3.3.4.7. Erreur maximale\n",
    "\n",
    "La fonction [**`max_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.max_error.html) calcule l'erreur maximale [**résiduelle**](https://en.wikipedia.org/wiki/Errors_and_residuals), une métrique qui capture l'erreur maximale entre la valeur prédite et la valeur réelle. Dans un modèle de régression avec une seule sortie parfaitement ajusté, `max_error` serait `0` sur l'ensemble d'entraînement, bien que cela soit très improbable dans le monde réel. Cette métrique montre l'étendue de l'erreur que le modèle a eue lorsqu'il a été ajusté.\n",
    "\n",
    "Si $\\hat{y}_i$ représente la valeur prédite pour le $i$-ème échantillon et $y_i$ la valeur réelle correspondante, alors l'erreur maximale est définie comme suit :\n",
    "\n",
    "$$\\text{Erreur maximale}(y, \\hat{y}) = \\max(| y_i - \\hat{y}_i |)$$\n",
    "\n",
    "Voici un petit exemple d'utilisation de la fonction [**`max_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.max_error.html) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import max_error\n",
    "y_true = [3, 2, 7, 1]\n",
    "y_pred = [9, 2, 7, 1]\n",
    "max_error(y_true, y_pred)\n",
    "# 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction [**`max_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.max_error.html) ne prend pas en charge les sorties multiples (multioutput)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='explained-variance-score'></a> 3.3.4.8. Score de variance expliquée\n",
    "\n",
    "La fonction [**`explained_variance_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.explained_variance_score.html) calcule le [**score de variance expliquée pour la régression**](https://en.wikipedia.org/wiki/Explained_variation).\n",
    "\n",
    "Si $\\hat{y}$ représente la valeur estimée de la sortie cible, $y$ la valeur cible correspondante (correcte), et $\\text{Var}$ est [**la variance**](https://en.wikipedia.org/wiki/Variance), le carré de l'écart type, alors la variance expliquée est estimée comme suit :\n",
    "\n",
    "$$\\text{Variance expliquée}(y, \\hat{y}) = 1 - \\frac{\\text{Var}(y - \\hat{y})}{\\text{Var}(y)}$$\n",
    "\n",
    "Le meilleur score possible est 1.0, les valeurs inférieures sont moins bonnes.\n",
    "\n",
    "#### Lien vers [**le score R², le coefficient de détermination** (3.3.4.1)](https://scikit-learn.org/stable/modules/model_evaluation.html#r2-score).\n",
    "\n",
    "La différence entre le score de variance expliquée et [**le score R², le coefficient de détermination** (3.3.4.1)](https://scikit-learn.org/stable/modules/model_evaluation.html#r2-score) est que le score de variance expliquée n'enregistre pas de décalage systématique dans la prédiction. Pour cette raison, [**le score R², le coefficient de détermination** (3.3.4.1)](https://scikit-learn.org/stable/modules/model_evaluation.html#r2-score) devrait être préféré en général.\n",
    "\n",
    "Dans le cas particulier où la vraie cible est constante, le score de variance expliquée n'est pas fini : il est soit `NaN` (prédictions parfaites), soit `-Inf` (prédictions imparfaites). De tels scores non finis peuvent empêcher une optimisation correcte du modèle, telle que la recherche en grille et la validation croisée, d'être effectuée correctement. Pour cette raison, le comportement par défaut de [**`explained_variance_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.explained_variance_score.html) est de les remplacer par 1.0 (prédictions parfaites) ou 0.0 (prédictions imparfaites). Vous pouvez définir le paramètre `force_finite` sur `False` pour empêcher cette correction de se produire et utiliser le score de variance expliquée original.\n",
    "\n",
    "Voici un petit exemple d'utilisation de la fonction [**`explained_variance_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.explained_variance_score.html) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:675: RuntimeWarning: invalid value encountered in divide\n",
      "  output_scores = 1 - (numerator / denominator)\n",
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:675: RuntimeWarning: divide by zero encountered in divide\n",
      "  output_scores = 1 - (numerator / denominator)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import explained_variance_score\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "explained_variance_score(y_true, y_pred)\n",
    "# 0.957...\n",
    "y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    "explained_variance_score(y_true, y_pred, multioutput='raw_values')\n",
    "# array([0.967..., 1.        ])\n",
    "explained_variance_score(y_true, y_pred, multioutput=[0.3, 0.7])\n",
    "# 0.990...\n",
    "y_true = [-2, -2, -2]\n",
    "y_pred = [-2, -2, -2]\n",
    "explained_variance_score(y_true, y_pred)\n",
    "# 1.0\n",
    "explained_variance_score(y_true, y_pred, force_finite=False)\n",
    "# nan\n",
    "y_true = [-2, -2, -2]\n",
    "y_pred = [-2, -2, -2 + 1e-8]\n",
    "explained_variance_score(y_true, y_pred)\n",
    "# 0.0\n",
    "explained_variance_score(y_true, y_pred, force_finite=False)\n",
    "# -inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='mean-poisson-gamma-and-tweedie-deviances'></a> 3.3.4.9. Deviances moyennes de Poisson, gamma et Tweedie\n",
    "\n",
    "La fonction [**`mean_tweedie_deviance`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_tweedie_deviance.html) calcule [**la déviance Tweedie moyenne**](https://en.wikipedia.org/wiki/Tweedie_distribution#The_Tweedie_deviance) avec un paramètre `power` ($p$). Il s'agit d'une mesure qui révèle les valeurs d'attente prédites des cibles de régression.\n",
    "\n",
    "Les cas spéciaux suivants existent :\n",
    "- lorsque `power=0`, cela équivaut à [**`mean_squared_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html).\n",
    "- lorsque `power=1`, cela équivaut à [**`mean_poisson_deviance`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_poisson_deviance.html).\n",
    "- lorsque `power=2`, cela équivaut à [**`mean_gamma_deviance`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_gamma_deviance.html).\n",
    "\n",
    "Si $\\hat{y}_i$ représente la valeur prédite du $i$-ème échantillon et $y_i$ la valeur cible correspondante (réelle), alors la déviance Tweedie moyenne ($\\text{D}$) pour une puissance $p$, estimée sur $n_{\\text{samples}}$, est définie comme suit :\n",
    "\n",
    "$$\\begin{split}\\text{D}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}}\n",
    "\\sum_{i=0}^{n_\\text{samples} - 1}\n",
    "\\begin{cases}\n",
    "(y_i-\\hat{y}_i)^2, & \\text{pour }p=0\\text{ (Normale)}\\\\\n",
    "2(y_i \\log(y_i/\\hat{y}_i) + \\hat{y}_i - y_i),  & \\text{pour }p=1\\text{ (Poisson)}\\\\\n",
    "2(\\log(\\hat{y}_i/y_i) + y_i/\\hat{y}_i - 1),  & \\text{pour }p=2\\text{ (Gamma)}\\\\\n",
    "2\\left(\\frac{\\max(y_i,0)^{2-p}}{(1-p)(2-p)}-\n",
    "\\frac{y_i\\,\\hat{y}_i^{1-p}}{1-p}+\\frac{\\hat{y}_i^{2-p}}{2-p}\\right),\n",
    "& \\text{autrement}\n",
    "\\end{cases}\\end{split}$$\n",
    "\n",
    "La déviance Tweedie est une fonction homogène de degré `2 - power`. Ainsi, la distribution gamma avec `power=2` signifie que mettre à l'échelle simultanément `y_true` et `y_pred` n'a aucun effet sur la déviance. Pour la distribution de Poisson avec `power=1`, la déviance s'échelonne linéairement, et pour la distribution normale (`power=0`), quadratiquement. En général, plus la valeur de `power` est élevée, moins on accorde de poids aux écarts extrêmes entre les cibles réelles et prédites.\n",
    "\n",
    "Par exemple, comparons les deux prédictions 1,5 et 150 qui sont toutes deux 50% plus grandes que leur valeur réelle correspondante.\n",
    "\n",
    "L'erreur quadratique moyenne (`power=0`) est très sensible à la différence de prédiction du deuxième point :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_tweedie_deviance\n",
    "mean_tweedie_deviance([1.0], [1.5], power=0)\n",
    "# 0.25\n",
    "mean_tweedie_deviance([100.], [150.], power=0)\n",
    "# 2500.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nous augmentons `power` à 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.906978378367114"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_tweedie_deviance([1.0], [1.5], power=1)\n",
    "# 0.18...\n",
    "mean_tweedie_deviance([100.], [150.], power=1)\n",
    "# 18.9..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La différence entre les erreurs diminue. Enfin, en fixant `power=2` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14426354954966225"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_tweedie_deviance([1.0], [1.5], power=2)\n",
    "# 0.14...\n",
    "mean_tweedie_deviance([100.], [150.], power=2)\n",
    "# 0.14..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nous obtiendrions des erreurs identiques. La déviance lorsque `power=2` est donc seulement sensible aux erreurs relatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='pinball-loss'></a> 3.3.4.10. Pinball Loss\n",
    "\n",
    "La fonction [**`mean_pinball_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_pinball_loss.html) est utilisée pour évaluer les performances prédictives des modèles de [**régression quantile**](https://en.wikipedia.org/wiki/Quantile_regression).\n",
    "\n",
    "$$\\text{pinball}(y, \\hat{y}) = \\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}}-1}  \\alpha \\max(y_i - \\hat{y}_i, 0) + (1 - \\alpha) \\max(\\hat{y}_i - y_i, 0)$$\n",
    "\n",
    "La valeur de la perte Pinball est équivalente à la moitié de [**`mean_absolute_error`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html) lorsque le paramètre de quantile `alpha` est fixé à 0.5.\n",
    "\n",
    "Voici un petit exemple d'utilisation de la fonction [**`mean_pinball_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_pinball_loss.html) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_pinball_loss\n",
    "y_true = [1, 2, 3]\n",
    "mean_pinball_loss(y_true, [0, 2, 3], alpha=0.1)\n",
    "# 0.03...\n",
    "mean_pinball_loss(y_true, [1, 2, 4], alpha=0.1)\n",
    "# 0.3...\n",
    "mean_pinball_loss(y_true, [0, 2, 3], alpha=0.9)\n",
    "# 0.3...\n",
    "mean_pinball_loss(y_true, [1, 2, 4], alpha=0.9)\n",
    "# 0.03...\n",
    "mean_pinball_loss(y_true, y_true, alpha=0.1)\n",
    "# 0.0\n",
    "mean_pinball_loss(y_true, y_true, alpha=0.9)\n",
    "# 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible de créer un objet mesureur (scorer) avec un choix spécifique de `alpha` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "mean_pinball_loss_95p = make_scorer(mean_pinball_loss, alpha=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un tel mesureur peut être utilisé pour évaluer les performances de généralisation d'un régresseur quantile via une validation croisée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.66913589,  9.78247589, 23.32807339,  9.518313  , 10.49785928])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = make_regression(n_samples=100, random_state=0)\n",
    "estimator = GradientBoostingRegressor(\n",
    "    loss=\"quantile\",\n",
    "    alpha=0.95,\n",
    "    random_state=0,\n",
    ")\n",
    "cross_val_score(estimator, X, y, cv=5, scoring=mean_pinball_loss_95p)\n",
    "# array([13.6..., 9.7..., 23.3..., 9.5..., 10.4...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est également possible de créer des objets mesureurs pour l'optimisation des hyperparamètres. Le signe de la perte doit être inversé pour s'assurer que plus grand signifie mieux, comme expliqué dans l'exemple lié ci-dessous.\n",
    "\n",
    "#### Exemple\n",
    "\n",
    "##### [**Intervalle de prédiction pour la régression par Gradient Boosting**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_11_ensembles/plot_gradient_boosting_quantile.ipynb)<br/>([_Prediction Intervals for Gradient Boosting Regression_](https://scikit-learn.org/stable/auto_examples/ensembles/plot_gradient_boosting_quantile.html))\n",
    "\n",
    "Exemple d'utilisation de la perte Pinball pour évaluer et ajuster les hyperparamètres des modèles de régression quantile sur des données avec un bruit asymétrique et des valeurs aberrantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='d2-score'></a> 3.3.4.11. Score D²\n",
    "\n",
    "Le score D² calcule la fraction de déviance expliquée. C'est une généralisation du R², où l'erreur quadratique est généralisée et remplacée par une déviance de choix $\\text{dev}(y, \\hat{y})$ (par exemple, déviance Tweedie, perte pinball ou erreur absolue moyenne). D² est une forme de \"skill score\". Il est calculé comme suit :\n",
    "\n",
    "$$D^2(y, \\hat{y}) = 1 - \\frac{\\text{dev}(y, \\hat{y})}{\\text{dev}(y, y_{\\text{null}})} \\,.$$\n",
    "\n",
    "Où $y_{\\text{null}}$ est la prédiction optimale d'un modèle contenant uniquement l'intercept (par exemple, la moyenne de `y_true` pour le cas de Tweedie, la médiane pour l'erreur absolue et le quantile alpha pour la perte pinball).\n",
    "\n",
    "Comme pour le R², le meilleur score possible est de 1,0 et il peut être négatif (parce que le modèle peut être arbitrairement pire). Un modèle constant qui prédit toujours $y_{\\text{null}}$, indépendamment des caractéristiques d'entrée, obtiendrait un score D² de 0,0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='d2-tweedie-score'></a> Score D² Tweedie\n",
    "\n",
    "La fonction [**`d2_tweedie_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.d2_tweedie_score.html) implémente le cas spécial du D² où $\\text{dev}(y, \\hat{y})$ est la déviance Tweedie, voir [**Mean Poisson, Gamma, and Tweedie deviances** (3.3.4.9)](https://scikit-learn.org/stable/modules/model_evaluation.html#mean-tweedie-deviance). Il est également connu sous le nom de D² Tweedie et est lié à l'indice du rapport de vraisemblance de McFadden.\n",
    "\n",
    "L'argument `power` définit la puissance de Tweedie, comme pour [**`d2_tweedie_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.d2_tweedie_score.html). Notez que pour `power=0`, [**`d2_tweedie_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.d2_tweedie_score.html) est égal à [**`r2_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html) (pour des cibles uniques).\n",
    "\n",
    "Un objet de score avec un choix spécifique de `power` peut être créé par :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import d2_tweedie_score, make_scorer\n",
    "d2_tweedie_score_15 = make_scorer(d2_tweedie_score, power=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='d2-pinball-score'></a> Score D² pinball\n",
    "\n",
    "La fonction [**`d2_pinball_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.d2_pinball_score.html) implémente le cas spécial du D² avec la perte pinball, voir [**Pinball loss** (3.3.4.10)](https://scikit-learn.org/stable/modules/model_evaluation.html#pinball-loss), c'est-à-dire :\n",
    "\n",
    "$$\\text{dev}(y, \\hat{y}) = \\text{pinball}(y, \\hat{y}).$$\n",
    "\n",
    "L'argument alpha définit la pente de la perte pinball, comme pour [**`mean_pinball_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_pinball_loss.html) ([**Pinball loss** (3.3.4.10)](https://scikit-learn.org/stable/modules/model_evaluation.html#pinball-loss)). Il détermine le niveau de quantile `alpha` pour lequel la perte pinball et donc D² sont optimaux. Notez que pour `alpha=0.5` (la valeur par défaut), [**`d2_pinball_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.d2_pinball_score.html) est égal à [**`d2_absolute_error_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.d2_absolute_error_score.html).\n",
    "\n",
    "Un objet de score avec un choix spécifique de `alpha` peut être créé par :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import d2_pinball_score, make_scorer\n",
    "d2_pinball_score_08 = make_scorer(d2_pinball_score, alpha=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='d2-absolute-error-score'></a> Score D² erreur absolue\n",
    "\n",
    "La fonction [**`d2_absolute_error_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.d2_absolute_error_score.html) implémente le cas spécial de l'erreur absolue moyenne [**Mean absolute error** (3.3.4.2)](https://scikit-learn.org/stable/modules/model_evaluation.html#mean-absolute-error) :\n",
    "\n",
    "$$\\text{dev}(y, \\hat{y}) = \\text{MAE}(y, \\hat{y}).$$\n",
    "\n",
    "Voici quelques exemples d'utilisation de la fonction [**`d2_absolute_error_score`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.d2_absolute_error_score.html) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import d2_absolute_error_score\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "d2_absolute_error_score(y_true, y_pred)\n",
    "# 0.764...\n",
    "y_true = [1, 2, 3]\n",
    "y_pred = [1, 2, 3]\n",
    "d2_absolute_error_score(y_true, y_pred)\n",
    "# 1.0\n",
    "y_true = [1, 2, 3]\n",
    "y_pred = [2, 2, 2]\n",
    "d2_absolute_error_score(y_true, y_pred)\n",
    "# 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='visual-evaluation-of-regression-models'></a> 3.3.4.12. Évaluation visuelle des modèles de régression\n",
    "\n",
    "Parmi les méthodes permettant d'évaluer la qualité des modèles de régression, scikit-learn fournit la classe [**`PredictionErrorDisplay`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.PredictionErrorDisplay.html). Elle permet d'inspecter visuellement les erreurs de prédiction d'un modèle de deux manières différentes.\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_predict_001.png)\n",
    "\n",
    "Le graphique de gauche montre les valeurs réelles par rapport aux valeurs prédites. Pour une tâche de régression sans bruit visant à prédire l'espérance (conditionnelle) de `y`, un modèle de régression parfait afficherait des points de données sur la diagonale définie par les valeurs prédites égales aux valeurs réelles. Plus on s'éloigne de cette ligne optimale, plus l'erreur du modèle est grande. Dans un contexte plus réaliste avec du bruit irréductible, c'est-à-dire lorsque toutes les variations de `y` ne peuvent pas être expliquées par les caractéristiques de `X`, le meilleur modèle conduirait à un nuage de points densement disposés autour de la diagonale.\n",
    "\n",
    "Notez que ce qui précède est valable uniquement lorsque les valeurs prédites sont la valeur attendue de `y` étant donné `X`. C'est généralement le cas pour les modèles de régression qui minimisent la fonction objectif de l'erreur quadratique moyenne ou plus généralement la [**déviance moyenne de Tweedie** (3.3.4.9)](https://scikit-learn.org/stable/modules/model_evaluation.html#mean-tweedie-deviance) pour n'importe quelle valeur de son paramètre `power`.\n",
    "\n",
    "Lorsque l'on trace les prédictions d'un estimateur qui prédit un quantile de `y` étant donné `X`, par exemple [**`QuantileRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.QuantileRegressor.html) ou tout autre modèle minimisant la [**perte pinball** (3.3.4.10)](https://scikit-learn.org/stable/modules/model_evaluation.html), une fraction des points est censée se situer au-dessus ou en dessous de la diagonale en fonction du niveau de quantile estimé.\n",
    "\n",
    "Dans l'ensemble, bien qu'intuitif à lire, ce graphique ne nous informe pas vraiment sur ce qu'il faut faire pour obtenir un meilleur modèle.\n",
    "\n",
    "Le graphique du côté droit montre les résidus (c'est-à-dire la différence entre les valeurs réelles et les valeurs prédites) par rapport aux valeurs prédites.\n",
    "\n",
    "Ce graphique permet de visualiser plus facilement si les résidus suivent une distribution homoscédastique ou hétéroschédistique.\n",
    "\n",
    "En particulier, si la vraie distribution de $y|X$ est distribuée selon la loi de Poisson ou de Gamma, on s'attend à ce que la variance des résidus du modèle optimal augmente avec la valeur prédite de $E[y|X]$ (linéairement pour la loi de Poisson ou quadratiquement pour la loi de Gamma).\n",
    "\n",
    "Lorsque l'on ajuste un modèle de régression linéaire des moindres carrés (voir [**`LinearRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) et [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)), on peut utiliser ce graphique pour vérifier si certaines des [**hypothèses du modèle**](https://en.wikipedia.org/wiki/Ordinary_least_squares#Assumptions) sont satisfaites, en particulier que les résidus doivent être non corrélés, que leur valeur attendue doit être nulle et que leur variance doit être constante (homoscédasticité).\n",
    "\n",
    "Si ce n'est pas le cas, et en particulier si le graphique des résidus montre une structure en forme de banane, cela indique que le modèle est probablement mal spécifié et qu'une ingénierie des caractéristiques non linéaire ou le passage à un modèle de régression non linéaire pourraient être utiles.\n",
    "\n",
    "Reportez-vous à l'exemple ci-dessous pour voir une évaluation de modèle qui utilise cette visualisation.\n",
    "\n",
    "#### Exemple\n",
    "\n",
    "##### [**Effet de la transformation des cibles dans le modèle de régression**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/6_1_compose/plot_transformed_target.ipynb)<br/>([_Effect of transforming the targets in regression model_](https://scikit-learn.org/stable/auto_examples/compose/plot_transformed_target.html))\n",
    "\n",
    "Exemple de l'utilisation de [**`PredictionErrorDisplay`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.PredictionErrorDisplay.html) pour visualiser l'amélioration de la qualité des prédictions d'un modèle de régression obtenu en transformant la cible avant l'apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='clustering-metrics'></a> 3.3.5. Mesures de clustering\n",
    "\n",
    "Le module [**`sklearn.metrics`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) implémente plusieurs fonctions de perte, de score et d'utilité pour l'évaluation des performances de clustering. Pour plus d'informations, consultez la section [**Évaluation des performances de clustering** (2.3.11)](https://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation) pour le clustering de groupes similaires et [**Évaluation biclustering** (2.4.3)](https://scikit-learn.org/stable/modules/biclustering.html#biclustering-evaluation) pour le biclustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='dummy-estimators'></a> 3.3.6. Estimateurs factices (Dummy estimators)\n",
    "\n",
    "Lorsque l'on effectue de l'apprentissage supervisé, une vérification simple consiste à comparer l'estimateur à des règles simples de base. [**`DummyClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html) implémente plusieurs stratégies simples pour la classification :\n",
    "\n",
    "- `stratified` génère des prédictions aléatoires en respectant la distribution des classes de l'ensemble d'entraînement.\n",
    "- `most_frequent` prédit toujours l'étiquette la plus fréquente dans l'ensemble d'entraînement.\n",
    "- `prior` prédit toujours la classe qui maximise la priorité de classe (comme `most_frequent`) et `predict_proba` renvoie la priorité de classe.\n",
    "- `uniform` génère des prédictions de manière uniforme et aléatoire.\n",
    "- **`constant` prédit toujours une étiquette constante fournie par l'utilisateur.**\n",
    "    - Une motivation majeure de cette méthode est l'évaluation F1, lorsque la classe positive est minoritaire.\n",
    "\n",
    "Notez qu'avec toutes ces stratégies, la méthode `predict` ignore complètement les données d'entrée !\n",
    "\n",
    "Pour illustrer [**`DummyClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html), commençons par créer un ensemble de données déséquilibré :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = load_iris(return_X_y=True)\n",
    "y[y != 1] = -1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, comparons la précision de [**`SVC`**](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) (Support Vector Classifier) avec celle de `most_frequent` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5789473684210527"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "# 0.63...\n",
    "clf = DummyClassifier(strategy='most_frequent', random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "# DummyClassifier(random_state=0, strategy='most_frequent')\n",
    "clf.score(X_test, y_test)\n",
    "# 0.57..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous constatons que [**`SVC`**](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) ne fait guère mieux qu'un classificateur fictif. Maintenant, changeons le noyau (kernel) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(kernel='rbf', C=1).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "# 0.94..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous constatons que la précision a augmenté jusqu'à près de 100%. Une stratégie de validation croisée est recommandée pour une meilleure estimation de la précision, si cela n'est pas trop coûteux en termes de calcul. Pour plus d'informations, consultez la section [**Validation croisée : évaluation des performances de l'estimateur** (3.1)](https://scikit-learn.org/stable/modules/cross_validation.html). De plus, si vous souhaitez optimiser l'espace des hyperparamètres, il est fortement recommandé d'utiliser une méthodologie appropriée ; consultez la section [**Ajustement des hyperparamètres d'un estimateur** (3.2)](https://scikit-learn.org/stable/modules/grid_search.html#grid-search) pour plus de détails.\n",
    "\n",
    "De manière générale, lorsque la précision d'un classificateur est trop proche du hasard, cela signifie probablement qu'il y a un problème : les caractéristiques ne sont pas utiles, un hyperparamètre n'est pas correctement réglé, le classificateur souffre d'un déséquilibre des classes, etc...\n",
    "\n",
    "[**`DummyRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html#sklearn.dummy.DummyRegressor) implémente également quatre règles de base simples pour la régression :\n",
    "\n",
    "- `mean` prédit toujours la moyenne des valeurs cibles de l'ensemble d'entraînement.\n",
    "- `median` prédit toujours la médiane des valeurs cibles de l'ensemble d'entraînement.\n",
    "- `quantile` prédit toujours un quantile fourni par l'utilisateur des valeurs cibles de l'ensemble d'entraînement.\n",
    "- `constant` prédit toujours une valeur constante fournie par l'utilisateur.\n",
    "\n",
    "Dans toutes ces stratégies, la méthode `predict` ignore complètement les données d'entrée."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
