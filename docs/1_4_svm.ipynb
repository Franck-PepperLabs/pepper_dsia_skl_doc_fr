{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='support-vector-machines'></a> 1.4. [**Machines à vecteurs de support (SVM)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_4_svm.ipynb)<br/>([*Support Vector Machines*](https://scikit-learn.org/stable/modules/svm.html))\n",
    "\n",
    "\n",
    "[SVC]: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "[NuSVC]: https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html\n",
    "\n",
    "[SVR]: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n",
    "\n",
    "[NuSVR]: https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVR.html\n",
    "\n",
    "[LinearSVC]: https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "\n",
    "[LinearSVR]: https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html\n",
    "\n",
    "[OneClassSVM]: https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n",
    "\n",
    "\n",
    "Les **machines à vecteurs de support** (SVM) sont un ensemble de méthodes d'apprentissage supervisé utilisées pour la [**classification** (1.4.1)](#svm-classification), la [**régression** (1.4.2)](#svm-regression) et la [**détection des valeurs aberrantes** (1.4.3)](#svm-outlier-detection).\n",
    "\n",
    "Les avantages des machines à vecteurs de support sont :\n",
    "* Efficace dans les espaces de grande dimension.\n",
    "* Toujours efficace dans les cas où le nombre de dimensions est supérieur au nombre d'échantillons.\n",
    "* Utilise un sous-ensemble de points d'apprentissage dans la fonction de décision (appelés vecteurs de support), il est donc également efficace en mémoire.\n",
    "* Polyvalent : différentes **fonctions du noyau** peuvent être spécifiées pour la fonction de décision. Des noyaux communs sont fournis, mais il est également possible de spécifier des noyaux *ad hoc*.\n",
    "\n",
    "Les inconvénients des machines à vecteurs de support incluent :\n",
    "* Si le nombre de caractéristiques est bien plus grand que le nombre d'échantillons, pour éviter le sur-apprentissage, le choix des fonctions du noyau et le terme de régularisation est crucial.\n",
    "* Les SVM ne fournissent pas directement des estimations de probabilité, celles-ci sont calculées à l'aide d'une validation croisée quintuple coûteuse (voir Scores et probabilités, ci-dessous).\n",
    "\n",
    "Les machines à vecteurs de support dans scikit-learn prennent en charge les vecteurs d'échantillon à la fois dense (`numpy.ndarray` et convertibles en celui-ci par `numpy.asarray`) et creux (tout `scipy.sparse`) en entrée. Cependant, pour utiliser une SVM pour faire des prédictions pour des données creuses, elle doit avoir été ajustée à ces données. Pour des performances optimales, utilisez `numpy.ndarray` ordonné en C (dense) ou `scipy.sparse.csr_matrix` (creuse) avec `dtype=float64`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='classification'></a> 1.4.1. [Classification](https://scikit-learn.org/stable/modules/svm.html#classification)\n",
    "\n",
    "[SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC), [NuSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC) et [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) sont des classes capables d'effectuer une classification binaire et multi-classes sur un jeu de données.\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_iris_svc_001.png)\n",
    "\n",
    "\n",
    "[SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) et [NuSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC) sont des méthodes similaires, mais elles acceptent des ensembles de paramètres légèrement différents et ont des formulations mathématiques différentes (voir la section [**Formulation mathématique** (1.4.7)](#svm-mathematical-formulation)). D'autre part, [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) est une autre implémentation (plus rapide) de Support Vector Classification pour le cas d'un noyau linéaire. Notez que [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) n'accepte pas le paramètre `kernel`, puisqu'il est supposé être linéaire. Il lui manque également certains des attributs de [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) et [NuSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC), comme `support_`.\n",
    "\n",
    "Comme d'autres classifieurs, [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC), [NuSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC) et [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) prennent en entrée deux tableaux : un tableau `X` de dimensions `(n_échantillons, n_caractéristiques)` contenant les échantillons d'entraînement, et un tableau `y` d'étiquettes de classe (chaînes ou entiers), de forme `(n_échantillons)` : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]\n",
    "clf = svm.SVC()\n",
    "clf.fit(X, y)\n",
    "# SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir été entraîné, le modèle peut être utilisé pour prédire de nouvelles valeurs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[2., 2.]])\n",
    "# array([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction de décision des SVM (détaillée dans [**Formulation mathématique** (1.4.7)](#svm-mathematical-formulation)) dépend d'un certain sous-ensemble des données d'entraînement, appelés vecteurs de support. Certaines propriétés de ces vecteurs de support peuvent être trouvées dans les attributs `support_vectors_`, `support_` and `n_support_` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get support vectors\n",
    "clf.support_vectors_\n",
    "# array([[0., 0.],\n",
    "#        [1., 1.]])\n",
    "# get indices of support vectors\n",
    "clf.support_\n",
    "# array([0, 1]...)\n",
    "# get number of support vectors for each class\n",
    "clf.n_support_\n",
    "# array([1, 1]...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**SVM : Marge maximale séparant l'hyperplan**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_4_svm/plot_separating_hyperplane.ipynb)<br/>([*SVM: Maximum margin separating hyperplane*](https://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane.html))\n",
    "\n",
    "#### [**SVM non linéaire**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_4_svm/plot_svm_nonlinear.ipynb)<br/>([*Non-linear SVM*](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_nonlinear.html))\n",
    "\n",
    "#### [**SVM-Anova : SVM avec sélection de caractéristique univariée**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_4_svm/plot_svm_anova.ipynb)<br/>([*SVM-Anova: SVM with univariate feature selection*](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_anova.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='multi-class-classification'></a> 1.4.1.1. Classification multi-classes\n",
    "\n",
    "[SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) et [NuSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC) implémentent l'approche \"un contre un\" pour la classification multi-classes. Au total, `n_classes * (n_classes - 1) / 2` classifieurs sont construits et chacun entraîne les données de deux classes. Pour fournir une interface cohérente avec d'autres classifieurs, l'option `decision_function_shape` permet de transformer de manière monotone les résultats des classificateurs \"un-vs-un\" en une fonction de décision \"un-vs-reste\" de forme `(n_échantillons, n_classes)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[0], [1], [2], [3]]\n",
    "Y = [0, 1, 2, 3]\n",
    "clf = svm.SVC(decision_function_shape='ovo')\n",
    "clf.fit(X, Y)\n",
    "svm.SVC(decision_function_shape='ovo')\n",
    "dec = clf.decision_function([[1]])\n",
    "dec.shape[1] # 4 classes: 4*3/2 = 6\n",
    "# 6\n",
    "clf.decision_function_shape = \"ovr\"\n",
    "dec = clf.decision_function([[1]])\n",
    "dec.shape[1] # 4 classes\n",
    "# 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'autre part, [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) implémente une stratégie multi-classes \"un-vs-reste\", formant ainsi des modèles `n_classes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_clf = svm.LinearSVC()\n",
    "lin_clf.fit(X, Y)\n",
    "# LinearSVC()\n",
    "dec = lin_clf.decision_function([[1]])\n",
    "dec.shape[1]\n",
    "# 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir [**Formulation mathématique** (1.4.7)](#svm-mathematical-formulation) pour une description complète de la fonction de décision.\n",
    "\n",
    "Notez que le [`LinearSVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) implémente également une stratégie multi-classes alternative, la SVM multi-classes formulée par Crammer et Singer [16], en utilisant l'option `multi_class='crammer_singer'`. En pratique, la classification un-vs-reste est généralement préférée, car les résultats sont pour la plupart similaires, mais le temps d'exécution est nettement inférieur.\n",
    "\n",
    "Pour le [`LinearSVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) \"un-contre-reste\", les attributs `coef_` et `intercept_` ont respectivement la forme `(n_classes, n_features)` et `(n_classes,)`. Chaque ligne des coefficients correspond à l'un des classifieurs `n_classes` \"un-vs-reste\", idem pour les intercepts, dans l'ordre de la classe \"one\".\n",
    "\n",
    "Dans le cas de [`SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) et [`NuSVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC) \"un-vs-un\", la disposition des attributs est un peu plus compliquée. Dans le cas d'un noyau linéaire, les attributs `coef_` et `intercept_` ont respectivement la forme `(n_classes * (n_classes - 1) / 2, n_features)` et `(n_classes * (n_classes - 1) / 2)`. Ceci est similaire à la disposition de [`LinearSVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) décrite ci-dessus, chaque ligne correspondant maintenant à un classifieur binaire. L'ordre des classes 0 à $n$ est \"0 vs 1\", \"0 vs 2\", … \"0 vs $n$\", \"1 vs 2\", \"1 vs 3\", \"1 vs $n$\", … \"$n-1$ vs $n$\".\n",
    "\n",
    "Les dimensions de `dual_coef_` sont `(n_classes-1, n_SV)` avec un cadre quelque peu difficile à saisir. Les colonnes correspondent aux vecteurs de support impliqués dans l'un des `n_classes * (n_classes - 1) / 2` classifieurs \"un vs un\". Chaque vecteur de support `v` a un double coefficient dans chacun des classificateurs `n_classes - 1` qui compare la classe de `v` à une autre classe. Notez que certains de ces coefficients duaux, mais pas tous, peuvent être nuls. Les `n_classes - 1` entrées dans chaque colonne sont ces doubles coefficients, ordonnés par la classe opposée.\n",
    "\n",
    "Cela pourrait être plus clair avec un exemple: considérons un problème à trois classes avec la classe 0 ayant trois vecteurs de support $v^0_0, v^1_0, v^2_0$ et les classes 1 et 2 ayant deux vecteurs supports $v^0_1, v^1_1$ et $v^0_2, v^1_2$ respectivement. Pour chaque vecteur support $v^j_i$, il y a deux coefficients duaux. Appelons le coefficient de vecteur de support $v^j_i$ dans le classifieur entre les classes $i$ et $k\\alpha^j_{i,k}$. Alors `dual_coef_` ressemble à ceci :\n",
    "\n",
    "|Coefficients pour<br/>les SVs de classe 0|Coefficients pour<br/>les SVs de classe 1|Coefficients pour<br/>les SVs de classe 2|\n",
    "|-|-|-|\n",
    "|$\\alpha^0_{0,1} \\alpha^1_{0,1} \\alpha^2_{0,1}$|$\\alpha^0_{1,0} \\alpha^1_{1,0}$|$\\alpha^0_{2,0} \\alpha^1_{2,0}$|\n",
    "|$\\alpha^0_{0,2} \\alpha^1_{0,2} \\alpha^2_{0,2}$|$\\alpha^0_{1,2} \\alpha^1_{1,2}$|$\\alpha^0_{2,1} \\alpha^1_{2,1}$|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemple : [**Tracé de différents classificateurs SVM dans l'ensemble de données iris**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_4_svm/plot_iris_svc.ipynb)<br/>([*Plot different SVM classifiers in the iris dataset*](https://scikit-learn.org/stable/auto_examples/svm/plot_iris_svc.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='scores-and-probabilities'></a> 1.4.1.2. Scores et probabilités\n",
    "\n",
    "La méthode `decision_function` de [**`SVC`**](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) et [**`NuSVC`**](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC) donne des scores par classe pour chaque échantillon (ou un seul score par échantillon dans le cas binaire). Lorsque l'option `probability` du constructeur est définie sur `True`, les estimations de probabilité d'appartenance à la classe (à partir des méthodes `predict_proba` et `predict_log_proba`) sont activées. Dans le cas binaire, les probabilités sont calibrées à l'aide de l'échelle de Platt [[9]](https://home.cs.colorado.edu/~mozer/Teaching/syllabi/6622/papers/Platt1999.pdf) : régression logistique sur les scores du SVM, ajustée par une validation croisée supplémentaire sur les données d'entraînement. Dans le cas multiclasse, ceci est étendu selon [[10]](https://www.csie.ntu.edu.tw/~cjlin/papers/svmprob/svmprob.pdf).\n",
    "\n",
    "**Note** La même procédure d'étalonnage de probabilité est disponible pour tous les estimateurs via [**`CalibratedClassifierCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html#sklearn.calibration.CalibratedClassifierCV) (voir [**Étalonnage de probabilité** (1.16)](https://scikit-learn.org/stable/modules/calibration.html#calibration)). Dans le cas de [**`SVC`**](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) et [**`NuSVC`**](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC), cette procédure est intégrée dans [`libsvm`](https://www.csie.ntu.edu.tw/~cjlin/libsvm/) qui est utilisée sous le capot, elle ne repose donc pas sur [**`CalibratedClassifierCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html#sklearn.calibration.CalibratedClassifierCV) de scikit-learn.\n",
    "\n",
    "La validation croisée impliquée dans la mise à l'échelle de Platt est une opération coûteuse pour les grands ensembles de données. De plus, les estimations de probabilité peuvent être incohérentes avec les scores :\n",
    "* l'\"argmax\" des scores peut ne pas être l'argmax des probabilités\n",
    "* dans la classification binaire, un échantillon peut être étiqueté par `predict` comme appartenant à la classe positive même si la sortie de `predict_proba` est inférieure à 0,5 ; et de même, il pourrait être étiqueté comme négatif même si la sortie de `predict_proba` est supérieure à 0,5.\n",
    "\n",
    "La méthode de Platt est également connue pour avoir des problèmes théoriques. Si des scores de confiance sont requis, mais que ceux-ci ne doivent pas nécessairement être des probabilités, il est conseillé de définir `probability=False` et d'utiliser `decision_function` au lieu de `predict_proba`.\n",
    "\n",
    "Veuillez noter que lorsque `decision_function_shape='ovr'` et `n_classes > 2`, contrairement à `decision_function`, la méthode `predict` n'essaie pas de rompre les liens par défaut. Vous pouvez définir `break_ties=True` pour que la sortie de `predict` soit identique à `np.argmax(clf.decision_function(...), axis=1)`, sinon la première classe parmi les classes liées sera toujours renvoyée ; mais gardez à l'esprit que cela a un coût de calcul. Voir [**Exemple de bris d'égalité SVM**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_4_svm/plot_svm_tie_breaking.ipynb) pour un exemple sur le bris d'égalité.\n",
    "\n",
    "###  [**Exemple de bris d'égalité SVM**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_4_svm/plot_svm_tie_breaking.ipynb)<br/>([*SVM Tie Breaking Example*](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_tie_breaking.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='unbalanced-problems'></a> 1.4.1.3. Problèmes déséquilibrés\n",
    "\n",
    "[SVC]: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "[NuSVC]: https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html\n",
    "\n",
    "[SVR]: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n",
    "\n",
    "[NuSVR]: https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVR.html\n",
    "\n",
    "[LinearSVC]: https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "\n",
    "[LinearSVR]: https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html\n",
    "\n",
    "[OneClassSVM]: https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n",
    "\n",
    "Dans les problèmes où l'on souhaite donner plus d'importance à certaines classes ou à certains échantillons individuels, les paramètres `class_weight` et `sample_weight` peuvent être utilisés.\n",
    "\n",
    "[**`SVC`**][SVC] (mais pas [**`NuSVC`**][NuSVC]) implémente le paramètre `class_weight` dans la méthode `fit`. C'est un dictionnaire de la forme `{class_label : value}`, où value est un nombre à virgule flottante > 0 qui définit le paramètre `C` de la classe `class_label` à `C * value`. La figure ci-dessous illustre la frontière de décision d'un problème déséquilibré, avec et sans correction de poids.\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_separating_hyperplane_unbalanced_001.png)\n",
    "\n",
    "[**`SVC`**][SVC], [**`NuSVC`**][NuSVC], [**`SVR`**][SVR], [**`NuSVR`**][NuSVR], [**`LinearSVC`**][], [**`LinearSVR`**][LinearSVR] et [**`OneClassSVM`**][OneClassSVM] implémentent également des pondérations pour des échantillons individuels dans la méthode `fit` via le paramètre `sample_weight`. Semblable à `class_weight`, cela définit le paramètre `C` pour le ième exemple sur `C * sample_weight[i]`, ce qui encouragera le classifieur à obtenir ces échantillons correctement. La figure ci-dessous illustre l'effet de la pondération de l'échantillon sur la limite de décision. La taille des cercles est proportionnelle aux poids de l'échantillon :\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_weighted_samples_001.png)\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**SVM : hyperplan séparateur pour les classes déséquilibrées**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_4_svm/plot_separating_hyperplane_unbalanced.ipynb)<br/>([*SVM: Separating hyperplane for unbalanced classes*](https://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html))\n",
    "\n",
    "#### [**SVM : échantillons pondérés**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_4_svm/plot_weighted_samples.ipynb)<br/>([*SVM: Weighted samples*](https://scikit-learn.org/stable/auto_examples/svm/plot_weighted_samples.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='regression'></a> 1.4.2. [Régression](https://scikit-learn.org/stable/modules/svm.html#regression)\n",
    "\n",
    "La méthode de classification à vecteurs de support peut être étendue pour résoudre les problèmes de régression. Cette méthode s'appelle régression à vecteurs de support (**SVR** - *Support Vector Regression*).\n",
    "\n",
    "Le modèle produit par la classification à vecteurs de support (comme décrit ci-dessus) ne dépend que d'un sous-ensemble des données d'entraînement, car la fonction de coût pour la construction du modèle ne se soucie pas des points d'entraînement qui se trouvent au-delà de la marge. De manière analogue, le modèle produit par régression à vecteurs de support ne dépend que d'un sous-ensemble des données d'apprentissage, car la fonction de coût ignore les échantillons dont la prédiction est proche de leur cible.\n",
    "\n",
    "Il existe trois implémentations différentes de la régression à vecteurs de support : [SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR), [NuSVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR) et [LinearSVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR). [LinearSVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR) fournit une implémentation plus rapide que [SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR) mais ne considère que le noyau linéaire, tandis que [NuSVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR) implémente une formulation légèrement différente de [SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR) et [LinearSVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR). Voir [**Détails d'implémentation** (1.4.8)](#svm-implementation-details) pour plus de détails.\n",
    "\n",
    "Comme pour les classes de classification, la méthode `fit` prendra comme arguments les vecteurs `X, y`, mais dans ce cas `y` devra avoir des valeurs à virgule flottante au lieu de valeurs entières :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "X = [[0, 0], [2, 2]]\n",
    "y = [0.5, 2.5]\n",
    "regr = svm.SVR()\n",
    "regr.fit(X, y)\n",
    "# SVR()\n",
    "regr.predict([[1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple : [**Prise en charge de la régression vectorielle (SVR) à l'aide de noyaux linéaires et non linéaires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_4_svm/plot_svm_regression.ipynb)<br/>([*Support Vector Regression (SVR) using linear and non-linear kernels*](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_regression.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='density-estimation-novelty-detection'></a> 1.4.3. [Estimation de densité, détection de nouveauté](https://scikit-learn.org/stable/modules/svm.html#density-estimation-novelty-detection)\n",
    "\n",
    "La classe [OneClassSVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM) implémente un SVM à classe unique qui est utilisé dans la détection des valeurs aberrantes.\n",
    "\n",
    "Voir [**Détection de nouveauté et de valeurs aberrantes** (2.7)](https://scikit-learn.org/stable/modules/outlier_detection.html#outlier-detection) pour la description et l'utilisation de OneClassSVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='complexity'></a> 1.4.4. [Complexité](https://scikit-learn.org/stable/modules/svm.html#complexity)\n",
    "\n",
    "Les machines à vecteurs de support sont des outils puissants, mais leurs besoins en calcul et en stockage augmentent rapidement avec le nombre de vecteurs d'entraînement. Le cœur d'une SVM est un problème de programmation quadratique (QP), séparant les vecteurs de support du reste des données d'apprentissage. Le solveur QP utilisé par l'implémentation basée sur [libsvm](https://www.csie.ntu.edu.tw/~cjlin/libsvm/) évolue entre $\\mathcal{O}(n_{features} \\times n^2_{samples})$\n",
    "et $\\mathcal{O}(n_{features} \\times n^3_{samples})$ en fonction de l'efficacité avec laquelle le cache [libsvm](https://www.csie.ntu.edu.tw/~cjlin/libsvm/) est utilisé dans la pratique (selon l'ensemble de données). Si les données sont très rares $n_{features}$ devrait être remplacé par le nombre moyen de caractéristiques non nulles dans un vecteur échantillon.\n",
    "\n",
    "Pour le cas linéaire, l'algorithme utilisé dans [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) par l'implémentation [liblinear](https://www.csie.ntu.edu.tw/~cjlin/liblinear/) est beaucoup plus efficace que son homologue [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) basé sur [libsvm](https://www.csie.ntu.edu.tw/~cjlin/libsvm/) et peut s'adapter de manière presque linéaire à des millions d'échantillons et/ou de caractéristiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='tips-on-practical-use'></a> 1.4.5. [Conseils d'utilisation pratique](https://scikit-learn.org/stable/modules/svm.html#tips-on-practical-use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Éviter la copie de données\n",
    "\n",
    "Pour [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC), [SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR), [NuSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC) et [NuSVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR), si les données transmises à certaines méthodes ne sont pas contiguës et double précision dans l'ordre C, elles seront copiées avant d'appeler l'implémentation C sous-jacente. Vous pouvez vérifier si un tableau numpy donné est C-contigu en inspectant son attribut `flags`.\n",
    "\n",
    "Pour [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) (et [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)), toute entrée transmise en tant que tableau numpy sera copiée et convertie en représentation interne [liblinear](https://www.csie.ntu.edu.tw/~cjlin/liblinear/) des données creuses (flottants à double précision et indices int32 de composants non nuls). Si vous souhaitez adapter un classifieur linéaire à grande échelle sans copier un tableau double précision numpy C-contigu dense en entrée, nous vous suggérons d'utiliser la classe [SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier) à la place. La fonction objectif peut être configurée pour être presque identique au modèle [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taille du cache du noyau\n",
    "\n",
    "Pour [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC), [SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR), [NuSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC) et [NuSVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR), la taille du cache du noyau a un fort impact sur les temps d'exécution pour les problèmes les plus larges. Si vous disposez de suffisamment de RAM, il est recommandé de définir `cache_size` sur une valeur supérieure à la valeur par défaut de 200 (Mo), telle que 500 (Mo) ou 1000 (Mo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurer C\n",
    "\n",
    "`C` est à `1` par défaut et c'est un choix par défaut raisonnable. Si vous avez beaucoup d'observations bruitées, vous devez le diminuer : diminuer C correspond à plus de régularisation.\n",
    "\n",
    "[LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) et [LinearSVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR) sont moins sensibles à C lorsqu'il devient grand, et les résultats de prédiction cessent de s'améliorer après un certain seuil. Pendant ce temps, des valeurs C plus grandes prendront plus de temps pour s'entraîner, parfois jusqu'à 10 fois plus longtemps, comme le montre [[11]](https://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensibilité à l'échelle des SVM\n",
    "\n",
    "Les algorithmes de machine à vecteurs de support sont sensibles à l'échelle, **il est donc fortement recommandé de mettre à l'échelle vos données**. Par exemple, mettez à l'échelle chaque attribut sur le vecteur d'entrée X à [0,1] ou [-1,+1], ou normalisez-le pour avoir une moyenne de 0 et une variance de 1. Notez que la même mise à l'échelle doit être appliquée au vecteur de test pour obtenir des résultats significatifs. Cela peut être fait facilement en utilisant un [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), SVC())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir la section [**Prétraitement des données** (6.3)](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing) pour plus de détails sur la mise à l'échelle et la normalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concernant le paramètre de rétrécissement `shrinking`\n",
    "\n",
    "Citant [[12]](https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf) : *Nous avons constaté que si le nombre d'itérations est important, la réduction peut raccourcir le temps d'entraînement. Cependant, si nous résolvons vaguement le problème d'optimisation (par exemple, en utilisant une grande tolérance d'arrêt), le code sans utilisation du rétrécissement peut être beaucoup plus rapide.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le paramètre `nu` dans [NuSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC)/[OneClassSVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM)/[NuSVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR)\n",
    "\n",
    "Le paramètre `nu` dans [NuSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC)/[OneClassSVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM)/[NuSVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR) approxime la fraction des erreurs d'entraînement et des vecteurs de support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equilibrage du SVC\n",
    "\n",
    "Dans [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC), si les données sont déséquilibrées (par exemple, beaucoup de positifs et peu de négatifs), définissez `class_weight='balanced'` et/ou essayez différents paramètres de pénalité `C`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caractère aléatoire des implémentations sous-jacentes\n",
    "\n",
    "Les implémentations sous-jacentes de [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) et [NuSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC) utilisent un générateur de nombres aléatoires uniquement pour mélanger les données pour l'estimation de la probabilité (lorsque `probability` est définie sur `True`). Ce caractère aléatoire peut être contrôlé avec le paramètre `random_state`. Si `probability` est définie sur `False`, ces estimateurs ne sont pas aléatoires et `random_state` n'a aucun effet sur les résultats. L'implémentation sous-jacente de [OneClassSVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM) est similaire à celles de [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) et [NuSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC). Comme aucune estimation de probabilité n'est fournie pour [OneClassSVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM), ce n'est pas aléatoire.\n",
    "\n",
    "L'implémentation sous-jacente de [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) utilise un générateur de nombres aléatoires pour sélectionner les caractéristiques lors de l'ajustement du modèle avec une descente à deux coordonnées (c'est-à-dire lorsque `dual` est défini sur `True`). Il n'est donc pas rare d'avoir des résultats légèrement différents pour les mêmes données d'entrée. Si cela se produit, essayez avec un paramètre `tol` plus petit. Ce caractère aléatoire peut également être contrôlé avec le paramètre `random_state`. Lorsque `dual` est défini sur `False`, l'implémentation sous-jacente de [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) n'est pas aléatoire et `random_state` n'a aucun effet sur les résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pénalisation $\\ell_1$\n",
    "\n",
    "L'utilisation de la pénalisation $\\ell_1$ telle que fournie par `LinearSVC(penalty='l1', dual=False)` donne une solution creuse, c'est-à-dire que seul un sous-ensemble de poids de caractéristiques est différent de zéro et contribue à la fonction de décision. L'augmentation de `C` donne un modèle plus complexe (plus de caractéristiques sont sélectionnées). La valeur `C` qui produit un modèle « nul » (tous les poids sont égaux à zéro) peut être calculée à l'aide de [l1_min_c](https://scikit-learn.org/stable/modules/generated/sklearn.svm.l1_min_c.html#sklearn.svm.l1_min_c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='kernel-functions'></a> 1.4.6. [Fonctions noyau](https://scikit-learn.org/stable/modules/svm.html#kernel-functions)\n",
    "\n",
    "La fonction noyau peut être l'une des suivantes :\n",
    "* linéaire : $\\left<x, x'\\right>$\n",
    "* polynôme : $\\left(\\gamma \\left<x, x'\\right> + r \\right)^d$, où $d$ est spécifié par le paramètre `degree`, $r$ par `coef0`.\n",
    "* RBF : $\\exp \\left(-\\gamma \\|x - x'\\| \\right)$, où $\\gamma$ est spécifié par le paramètre `gamma`, doit être supérieur à 0.\n",
    "* sigmoïde $\\tanh \\left( \\left<x, x'\\right> + r \\right)$, où $r$ est spécifié par `coef0`.\n",
    "\n",
    "Différents noyaux sont spécifiés par le paramètre `kernel` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rbf'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_svc = svm.SVC(kernel='linear')\n",
    "linear_svc.kernel\n",
    "# 'linear'\n",
    "rbf_svc = svm.SVC(kernel='rbf')\n",
    "rbf_svc.kernel\n",
    "# 'rbf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir aussi [**Kernel Approximation** (6.7)](https://scikit-learn.org/stable/modules/kernel_approximation.html#kernel-approximation) pour une solution d'utilisation des noyaux RBF qui est beaucoup plus rapide et plus évolutive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='parameters-of-the-rbf-kernel'></a> 1.4.6.1. Paramètres du noyau RBF\n",
    "\n",
    "Lors de l'entraînement d'un SVM avec le noyau *Radial Basis Function* (RBF), deux paramètres doivent être pris en compte : `C` et `gamma`. Le paramètre `C`, commun à tous les noyaux SVM, compense la mauvaise classification des exemples d'apprentissage par la simplicité de la surface de décision. Un `C` bas rend la surface de décision lisse, tandis qu'un `C` élevé vise à classer correctement tous les exemples d'entraînement. `gamma` définit l'influence d'un seul exemple d'entraînement. Plus le `gamma` est grand, plus les autres exemples doivent être proches pour être affectés.\n",
    "\n",
    "Le bon choix de `C` et de `gamma` est essentiel aux performances de la SVM. Il est conseillé d'utiliser [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) avec `C` et `gamma` espacés de manière exponentielle pour choisir de bonnes valeurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**Paramètres RBF SVM**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_4_svm/plot_rbf_parameters.ipynb)<br/>([*RBF SVM parameters*](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html))\n",
    "\n",
    "#### [**SVM non linéaire**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_4_svm/plot_svm_nonlinear.ipynb)<br/>([*Non-linear SVM*](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_nonlinear.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='custom-kernels'></a> 1.4.6.2. Noyaux personnalisés\n",
    "\n",
    "Vous pouvez définir vos propres noyaux en donnant le noyau comme une fonction python ou en précalculant la matrice de Gram.\n",
    "\n",
    "Les classificateurs avec des noyaux personnalisés se comportent de la même manière que tous les autres classificateurs, sauf que :\n",
    "* Le champ `support_vectors_` est maintenant vide, seuls les indices des vecteurs de support sont stockés dans `support_`\n",
    "* Une référence (et non une copie) du premier argument de la méthode `fit()` est stockée pour référence future. Si ce tableau change entre l'utilisation de `fit()` et de `predict()`, vous obtiendrez des résultats inattendus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='using-python-functions-as-kernels'></a> Utilisation des fonctions Python comme noyaux\n",
    "\n",
    "Vous pouvez utiliser vos propres noyaux définis en transmettant une fonction au paramètre `kernel`.\n",
    "\n",
    "Votre noyau doit prendre comme arguments deux matrices de forme `(n_samples_1, n_features)`, `(n_samples_2, n_features)` et retourner une matrice noyau de forme `(n_samples_1, n_samples_2)`.\n",
    "\n",
    "Le code suivant définit un noyau linéaire et crée une instance de classifieur qui utilisera ce noyau :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "def my_kernel(X, Y):\n",
    "    return np.dot(X, Y.T)\n",
    "    \n",
    "clf = svm.SVC(kernel=my_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemple : [**SVM avec noyau personnalisé**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_4_svm/plot_custom_kernel.ipynb)<br/>([*SVM with custom kernel*](https://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='using-the-gram-matrix'></a> Utilisation de la matrice de Gram\n",
    "\n",
    "Vous pouvez passer des noyaux pré-calculés en utilisant l'option `kernel='precomputed'`. Vous devez ensuite passer la matrice de Gram au lieu de X aux méthodes `fit` et `predict`. Les valeurs du noyau entre *tous* les vecteurs d'apprentissage et les vecteurs de test doivent être fournies :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "X, y = make_classification(n_samples=10, random_state=0)\n",
    "X_train , X_test , y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "clf = svm.SVC(kernel='precomputed')\n",
    "# linear kernel computation\n",
    "gram_train = np.dot(X_train, X_train.T)\n",
    "clf.fit(gram_train, y_train)\n",
    "SVC(kernel='precomputed')\n",
    "# predict on training examples\n",
    "gram_test = np.dot(X_test, X_train.T)\n",
    "clf.predict(gram_test)\n",
    "# array([0, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='mathematical-formulation'></a> 1.4.7. [Formulation mathématique](https://scikit-learn.org/stable/modules/svm.html#mathematical-formulation)\n",
    "\n",
    "Une machine à vecteurs de support construit un hyper-plan ou un ensemble d'hyper-plans dans un espace dimensionnel élevé ou infini, qui peut être utilisé pour la classification, la régression ou d'autres tâches. Intuitivement, une bonne séparation est obtenue par l'hyper-plan qui a la plus grande distance aux points de données d'apprentissage les plus proches de n'importe quelle classe (appelée **marge fonctionnelle**), car en général, plus la marge est grande, plus l'erreur de généralisation du classificateur est faible. La figure ci-dessous montre la fonction de décision pour un problème linéairement séparable, avec trois échantillons sur les limites des marges, appelés « vecteurs de support » :\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_separating_hyperplane_001.png)\n",
    "\n",
    "En général, lorsque le problème n'est pas linéairement séparable, les vecteurs de support sont les échantillons dans les limites de la marge.\n",
    "\n",
    "Nous recommandons [[13]](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf) et [[14]](https://link.springer.com/article/10.1023/B:STCO.0000035301.49549.88) comme de bonnes références pour la théorie et les aspects pratiques des SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='svc'></a> 1.4.7.1. SVC\n",
    "\n",
    "Étant donné les vecteurs d'entraînement $x_i \\in \\mathbb{R}^p$, $i=1,…, n$, en deux classes, et un vecteur $y \\in \\{1, -1\\}^n$, notre but est de trouver $w \\in \\mathbb{R}^p$ et $b \\in \\mathbb{R}$ tel que la prédiction donnée par $\\text{sign} (w^T\\phi(x) + b)$ soit correcte pour la plupart des échantillons.\n",
    "\n",
    "SVC résout le problème primal suivant :\n",
    "\n",
    "$\\begin{align}\\begin{aligned}\n",
    "    \\min_ {w, b, \\zeta} \\frac{1}{2} w^T w + C \\sum_{i=1}^{n} \\zeta_i\\\\\n",
    "    \\begin{split}\\textrm {sous la contrainte de } & y_i (w^T \\phi (x_i) + b) \\geq 1 - \\zeta_i,\\\\\n",
    "    & \\zeta_i \\geq 0, i=1, ..., n\\end{split}\n",
    "\\end{aligned}\\end{align}$\n",
    "\n",
    "\n",
    "Intuitivement, nous essayons de maximiser la marge (en minimisant $||w||^2 = w^\\top w$), tout en encourant une pénalité lorsqu'un échantillon est mal classé ou dans la limite de la marge. Idéalement, la valeur $y_i(w^\\top \\phi (x_i) + b)$ serait $\\ge 1$ pour tous les échantillons, ce qui indique une prédiction parfaite. Mais les problèmes ne sont généralement pas toujours parfaitement séparables avec un hyperplan, nous permettons donc à certains échantillons d'être à distance $\\zeta_i$ de leur limite de marge correcte. Le terme de pénalité `C` contrôle la force de cette pénalité et, par conséquent, agit comme un paramètre de régularisation inverse (voir note ci-dessous).\n",
    "\n",
    "Le problème dual du primal est :\n",
    "\n",
    "$\\begin{align}\\begin{aligned}\n",
    "    \\min_{\\alpha} \\frac{1}{2} \\alpha^\\top Q \\alpha - e^\\top \\alpha\\\\\\begin{split}\n",
    "    \\textrm {sous la contrainte de } & y^\\top \\alpha = 0\\\\\n",
    "    & 0 \\leq \\alpha_i \\leq C, i=1, ..., n\\end{split}\n",
    "\\end{aligned}\\end{align}$\n",
    "\n",
    "où $e$ est le vecteur de tous les uns, et $Q$ est une matrice $n$ par $n$ semi-définie positive, $Q_{ij} \\equiv y_i y_j K(x_i, x_j)$, où $K(x_i, x_j) = \\phi (x_i)^\\top \\phi (x_j)$ est le noyau. Les termes $\\alpha_i$ sont appelés les coefficients duaux, et ils sont majorés par $C$. Cette représentation duale met en évidence le fait que les vecteurs d'entraînement sont implicitement mappés dans un espace dimensionnel supérieur (peut-être infini) par la fonction $\\phi$ : voir [**wkpd** astuce du noyau](https://en.wikipedia.org/wiki/Kernel_method).\n",
    "\n",
    "Une fois le problème d'optimisation résolu, la sortie de [decision_function](https://scikit-learn.org/stable/glossary.html#term-decision_function) pour un échantillon donné $x$ devient:\n",
    "\n",
    "$$\\sum_{i\\in SV} y_i \\alpha_i K(x_i, x) + b,$$\n",
    "\n",
    "et la classe prédite correspond à son signe. Nous avons seulement besoin de faire la somme sur les vecteurs de support (c'est-à-dire les échantillons qui se trouvent dans la marge) parce que les coefficients duaux $alpha_i$ sont nuls pour les autres échantillons.\n",
    "\n",
    "Ces paramètres sont accessibles via les attributs `dual_coef_` qui contiennent le produit $y_i \\alpha_i$, `support_vectors_` qui contient les vecteurs de support et `intercept_` qui contient le terme indépendant $b$.\n",
    "\n",
    "**NB** > Alors que les modèles SVM dérivés de [libsvm](https://www.csie.ntu.edu.tw/~cjlin/libsvm/) et [liblinear](https://www.csie.ntu.edu.tw/~cjlin/liblinear/) utilisent `C` comme paramètre de régularisation, la plupart des autres estimateurs utilisent `alpha`. L'équivalence exacte entre le degré de régularisation de deux modèles dépend de la fonction objectif exacte optimisée par le modèle. Par exemple, lorsque l'estimateur utilisé est la régression [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge), la relation entre eux est donnée par $C=\\frac{1}{\\alpha}$.\n",
    ".\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.7.2. SVC linéaire\n",
    "\n",
    "Le problème primal peut être formulé de manière équivalente comme\n",
    "\n",
    "$$\\displaystyle\\min_ {w, b} \\displaystyle\\frac{1}{2} w^\\top w + C \\displaystyle\\sum_{i=1}^{n}\\max(0, 1 - y_i (w^\\top \\phi(x_i) + b)),$$\n",
    "\n",
    "où nous utilisons la [perte de charnière](https://en.wikipedia.org/wiki/Hinge_loss). C'est la forme qui est directement optimisée par [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC), mais contrairement à la forme duale, celle-ci n'implique pas de produits internes entre les échantillons, donc la fameuse astuce du noyau ne peut pas être appliquée. C'est pourquoi seul le noyau linéaire est supporté par [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) ($\\phi$ est la fonction d'identité)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.7.3. NuSVC\n",
    "\n",
    "La formulation $\\nu$-SVC [15](https://www.stat.purdue.edu/~yuzhu/stat598m3/Papers/NewSVM.pdf) est une reparamétrisation de la $C$-SVC et donc mathématiquement équivalent.\n",
    "\n",
    "Nous introduisons un nouveau paramètre $\\nu$ (au lieu de $C$) qui contrôle le nombre de vecteurs de support et d'*erreurs de marge* $\\nu \\in (0, 1]$ : est une borne supérieure sur la fraction d'erreurs de marge et une borne inférieure de la fraction de vecteurs de support. Une erreur de marge correspond à un échantillon qui se trouve du mauvais côté de sa limite de marge : soit il est mal classé, soit il est correctement classé mais ne se situe pas au-delà de la marge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.7.4. SVR\n",
    "\n",
    "Étant donné les vecteurs d'entraînement $x_i \\in \\mathbb{R}^p$, $i=1,…, n$, et un vecteur $y \\in \\mathbb{R}^n$ $\\varepsilon$-SVR résout le problème primal suivant :\n",
    "\n",
    "$\\begin{align}\\begin{aligned}\n",
    "    \\min_ {w, b, \\zeta, \\zeta^*} \\frac{1}{2} w^\\top w +\n",
    "    C \\sum_{i=1}^{n} (\\zeta_i + \\zeta_i^*)\\\\\n",
    "    \\begin{split}\\textrm {sous la contrainte de } & y_i - w^\\top \\phi (x_i) - b \\leq \\varepsilon + \\zeta_i,\\\\\n",
    "        & w^\\top \\phi (x_i) + b - y_i \\leq \\varepsilon + \\zeta_i^*,\\\\\n",
    "        & \\zeta_i, \\zeta_i^* \\geq 0, i=1, ..., n\\end{split}\n",
    "\\end{aligned}\\end{align}$\n",
    "\n",
    "Ici, nous pénalisons les échantillons dont la prédiction est au moins éloignée d'$\\varepsilon$ leur véritable cible. Ces prélèvements pénalisent l'objectif par $\\zeta_i$ ou $\\zeta_i^*$, selon que leurs prédictions se situent au-dessus ou au-dessous du $\\varepsilon$ tube.\n",
    "\n",
    "Le problème dual est\n",
    "\n",
    "$\\begin{align}\\begin{aligned}\n",
    "    \\min_{\\alpha, \\alpha^*} \\frac{1}{2} (\\alpha - \\alpha^*)^\\top Q (\\alpha - \\alpha^*) +\n",
    "    \\varepsilon e^\\top (\\alpha + \\alpha^*) - y^\\top (\\alpha - \\alpha^*)\\\\\n",
    "    \\begin{split}\n",
    "        \\textrm {sous la contrainte de } & e^\\top (\\alpha - \\alpha^*) = 0\\\\\n",
    "        & 0 \\leq \\alpha_i, \\alpha_i^* \\leq C, i=1, ..., n\n",
    "    \\end{split}\n",
    "\\end{aligned}\\end{align}$\n",
    "\n",
    "\n",
    "où $e$ est le vecteur de tous les uns, $Q$ est une matrice semi-définie par positive, $Q_{ij} \\equiv K(x_i, x_j) = \\phi (x_i)^\\top \\phi (x_j)$ est le noyau. Ici, les vecteurs d'entraînement sont implicitement mappés dans un espace dimensionnel supérieur (peut-être infini) par la fonction $\\phi$.\n",
    "\n",
    "La prédiction est :\n",
    "\n",
    "$$\\sum_{i \\in SV}(\\alpha_i - \\alpha_i^*) K(x_i, x) + b$$\n",
    "\n",
    "Ces paramètres sont accessibles via les attributs `dual_coef_` qui contient la différence $\\alpha_i - \\alpha_i^*$, `support_vectors_` qui contient les vecteurs de support et `intercept_` qui contient le terme indépendant $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.7.5. LinearSVR\n",
    "\n",
    "Le problème primal peut être formulé de manière équivalente comme\n",
    "\n",
    "$$\\displaystyle\\min_ {w, b} \\displaystyle\\frac{1}{2} w^\\top w + C \\displaystyle\\sum_{i=1}\\max(0, |y_i - (w^\\top \\phi(x_i) + b)| - \\varepsilon),$$\n",
    "\n",
    "où nous utilisons la perte insensible à epsilon, c'est-à-dire que les erreurs inférieures à $\\varepsilon$ sont ignorées. C'est la forme qui est directement optimisée par [LinearSVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4.8. [Détails d'implémentation](https://scikit-learn.org/stable/modules/svm.html#implementation-details)\n",
    "\n",
    "En interne, nous utilisons [libsvm](https://www.csie.ntu.edu.tw/~cjlin/libsvm/) [12] et [liblinear](https://www.csie.ntu.edu.tw/~cjlin/liblinear/) [11] pour gérer tous les calculs. Ces bibliothèques sont encapsulées à l'aide de C et de Cython. Pour une description de l'implémentation et des détails sur les algorithmes utilisés, veuillez vous référer à leurs articles respectifs.\n",
    "\n",
    "## Références\n",
    "\n",
    "[9] Platt [“**Probabilistic outputs for SVMs and comparisons to regularized likelihood methods**](https://home.cs.colorado.edu/~mozer/Teaching/syllabi/6622/papers/Platt1999.pdf)[”](https://drive.google.com/file/d/1cBfroxgInFyZmZ52e-iYi444d5w9AI2m/view?usp=share_link).\n",
    "\n",
    "[10] Wu, Lin and Weng, [“**Probability estimates for multi-class classification by pairwise coupling**](https://www.csie.ntu.edu.tw/~cjlin/papers/svmprob/svmprob.pdf)[”](https://drive.google.com/file/d/1DH6u13pnd9xxTAO1hs3OMY7MBFyQV13X/view?usp=share_link), JMLR 5:975-1005, 2004.\n",
    "\n",
    "[11] (1,2) Fan, Rong-En, et al., [“**LIBLINEAR: A library for large linear classification**](https://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf)[”](https://drive.google.com/file/d/1OgxjdAxo4_OnE0e-FGBBxF59YibgI1ns/view?usp=share_link), Journal of machine learning research 9.Aug (2008): 1871-1874.\n",
    "\n",
    "[12] (1,2) Chang and Lin, [“**LIBSVM: A Library for Support Vector Machines**](https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf)[”](https://drive.google.com/file/d/18YORFpFwaXxh4-Ls4rApH5HfT0r8DKgO/view?usp=share_link).\n",
    "\n",
    "[13] Bishop, [“**Pattern recognition and machine learning**](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)[”](https://drive.google.com/file/d/1TPZWto6Aj4qeaOmmD5xTwZ1TcI0_WVSr/view?usp=share_link), chapter 7 Sparse Kernel Machines\n",
    "\n",
    "[14] [**“A Tutorial on Support Vector Regression**](https://web.cs.dal.ca/~tt/CSCI650806/projects/papers/SmolaSchoelkopf.pdf)[”](https://drive.google.com/file/d/1eEwkHfY3OWXAc22_rvEHZrw4BQ8gk7ou/view?usp=share_link) Alex J. Smola, Bernhard Schölkopf - Statistics and Computing archive Volume 14 Issue 3, August 2004, p. 199-222.\n",
    "\n",
    "[15] Schölkopf et. al [**“New Support Vector Algorithms**](https://www.stat.purdue.edu/~yuzhu/stat598m3/Papers/NewSVM.pdf)[”](https://drive.google.com/file/d/1auxg6rehTRbBGzxmrxl_XLVphVxSIyAS/view?usp=share_link)\n",
    "\n",
    "[16] Crammer and Singer [**“On the Algorithmic Implementation ofMulticlass Kernel-based Vector Machines**](https://jmlr.csail.mit.edu/papers/volume2/crammer01a/crammer01a.pdf)[”](https://drive.google.com/file/d/1BOEbH6RpscH2dNi-RTgRZyG0-jCZgiua/view?usp=share_link), JMLR 2001.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e03b612d84ba21ce95ed447e81b3062e1eb99b56c6d885cdab4aaa12f1b8e240"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
