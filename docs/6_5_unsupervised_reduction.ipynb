{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.5. [**Réduction de dimensionnalité non supervisée**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_5_unsupervised_reduction.ipynb)</br>([*Unsupervised dimensionality reduction*](https://scikit-learn.org/stable/modules/unsupervised_reduction.html))\n",
    "\n",
    "Si le nombre de vos caractéristiques est élevé, il peut être utile de le réduire avec une étape non supervisée avant les étapes supervisées. De nombreuses méthodes d'[**apprentissage non supervisé** (2)](https://scikit-learn.org/stable/unsupervised_learning.html#unsupervised-learning) implémentent une méthode de transformation qui peut être utilisée pour réduire la dimensionnalité. Ci-dessous, nous discutons de deux exemples spécifiques de cette méthode qui sont largement utilisés.\n",
    "\n",
    "### Pipelining\n",
    "\n",
    "La réduction de données non supervisée et l'estimateur supervisé peuvent être enchaînés en une seule étape. Voir [**Pipeline: chaînage d'estimateurs** (6.1.1)](https://scikit-learn.org/stable/modules/compose.html#pipeline-chaining-estimators)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='pca-principal-component-analysis'></a> 6.5.1. PCA : analyse en composantes principales\n",
    "\n",
    "[**`decomposition.PCA`**](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) cherche une combinaison de caractéristiques qui capture bien la variance des caractéristiques d'origine. Voir [**Décomposition de signaux en composants (problèmes de factorisation de matrices)** (2.5)](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_5_decomposition.ipynb).\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Exemple de reconnaissance faciale utilisant des eigenfaces et des SVMs**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/applications/plot_face_recognition.ipynb)<br/>([*Faces recognition example using eigenfaces and SVMs*](https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='random-projections'></a> 6.5.2. Projections aléatoires\n",
    "\n",
    "Le module `random_projection` fournit plusieurs outils pour la réduction de données par des projections aléatoires. Voir la section pertinente de la documentation : [**Random Projection** (6.6)](https://scikit-learn.org/stable/modules/random_projection.html#random-projection).\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**La borne de Johnson-Lindenstrauss pour l'incorporation avec des projections aléatoires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/misc/plot_johnson_lindenstrauss_bound.ipynb)<br/>([*The Johnson-Lindenstrauss bound for embedding with random projections*](https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_johnson_lindenstrauss_bound.html))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='feature-agglomeration'></a> 6.5.3. Agglomération de caractéristiques\n",
    "\n",
    "[**`cluster.FeatureAgglomeration`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.FeatureAgglomeration.html) applique une [**classification hiérarchique** (2.3.6)](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering) pour regrouper des caractéristiques qui se comportent de manière similaire.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Agglomération de caractéristiques vs sélection univariée**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_3_cluster/plot_feature_agglomeration_vs_univariate_selection.ipynb)<br/>([*Feature agglomeration vs. univariate selection*](https://scikit-learn.org/stable/auto_examples/cluster/plot_feature_agglomeration_vs_univariate_selection.html))\n",
    "\n",
    "#### [**Agglomération de caractéristiques**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_3_cluster/plot_digits_agglomeration.ipynb)<br/>([*Feature agglomeration*](https://scikit-learn.org/stable/auto_examples/cluster/plot_digits_agglomeration.html))\n",
    "\n",
    "### Mise à l'échelle des caractéristiques\n",
    "\n",
    "Notez que si les caractéristiques ont des échelles ou des propriétés statistiques très différentes, [**`cluster.FeatureAgglomeration`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.FeatureAgglomeration.html) peut ne pas être en mesure de capturer les liens entre les caractéristiques apparentées. L'utilisation d'un prétraitement [**`preprocessing.StandardScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) peut être utile dans ces paramètres."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
