{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='supervised-learning'></a> 1. [**Apprentissage supervisé**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#supervised-learning)</br>([*Supervised learning*](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning))\n",
    "\n",
    "# 1.2. [**Analyse discriminante linéaire et quadratique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_2_lda_qda.ipynb)<br/>([_Linear and Quadratic Discriminant Analysis_](https://scikit-learn.org/stable/modules/lda_qda.html#linear-and-quadratic-discriminant-analysis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 6 pages, 3 exemples, 3 papiers\n",
    "- 1.2.1. [**Réduction de la dimensionnalité à l'aide de l'analyse discriminante linéaire**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_2_lda_qda.ipynb#dimensionality-reduction-using-linear-discriminant-analysis)<br/>([_Dimensionality reduction using Linear Discriminant Analysis_](https://scikit-learn.org/stable/modules/lda_qda.html#dimensionality-reduction-using-linear-discriminant-analysis))\n",
    "- 1.2.2. [**Formulation mathématique des classificateurs LDA et QDA**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_2_lda_qda.ipynb#mathematical-formulation-of-the-lda-and-qda-classifiers)<br/>([_Mathematical formulation of the LDA and QDA classifiers_](https://scikit-learn.org/stable/modules/lda_qda.html#mathematical-formulation-of-the-lda-and-qda-classifiers))\n",
    "    - 1.2.2.1. [**QDA**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_2_lda_qda.ipynb#qda)<br/>([_QDA_](https://scikit-learn.org/stable/modules/lda_qda.html#qda))\n",
    "    - 1.2.2.2. [**LDA**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_2_lda_qda.ipynb#lda)<br/>([_LDA_](https://scikit-learn.org/stable/modules/lda_qda.html#lda))\n",
    "- 1.2.3. [**Formulation mathématique de la réduction de la dimensionnalité LDA**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_2_lda_qda.ipynb#mathematical-formulation-of-lda-dimensionality-reduction)<br/>([_Mathematical formulation of LDA dimensionality reduction_](https://scikit-learn.org/stable/modules/lda_qda.html#mathematical-formulation-of-lda-dimensionality-reduction))\n",
    "- 1.2.4. [**Rétrécissement et estimateur de covariance**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_2_lda_qda.ipynb#shrinkage-and-covariance-estimator)<br/>([_Shrinkage and Covariance Estimator_](https://scikit-learn.org/stable/modules/lda_qda.html#shrinkage-and-covariance-estimator))\n",
    "- 1.2.5. [**Algorithmes d'estimation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_2_lda_qda.ipynb#estimation-algorithms)<br/>([_Estimation algorithms_](https://scikit-learn.org/stable/modules/lda_qda.html#estimation-algorithms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='linear-and-quadratic-discriminant-analysis'></a> 1.2. **Analyse discriminante linéaire et quadratique**<br/>([_Linear and Quadratic Discriminant Analysis_](https://scikit-learn.org/stable/modules/lda_qda.html))\n",
    "\n",
    "L'analyse discriminante linéaire ([**`LinearDiscriminantAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis)) et l'analyse discriminante quadratique ([**`QuadraticDiscriminantAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis)) sont deux classifieurs classiques, avec, comme leur nom l'indiquent, une surface de décision linéaire et quadratique, respectivement.\n",
    "\n",
    "Ces classifieurs sont populaires car ils ont des solutions analytiques qui peuvent être facilement calculées, sont intrinsèquement multiclasse, ont fait leurs preuves en pratique, et n'ont pas d'hyperparamètres à régler.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lda_qda_001.png\"\n",
    "    alt=\"LDA vs QDA\"\n",
    "    style=\"max-width: 75%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Le graphique montre les frontières de décision pour l'analyse discriminante linéaire et l'analyse discriminante quadratique. La rangée du bas démontre que l'analyse discriminante linéaire peut seulement apprendre des frontières linéaires, tandis que l'analyse discriminante quadratique, plus flexible, peut apprendre des frontières quadratiques.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Analyse Discriminante Linéaire et Quadratique avec des ellipsoïdes de covariance**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_2_lda_qda/plot_lda_qda.ipynb)<br/>([_Linear and Quadratic Discriminant Analysis with covariance ellipsoid_](https://scikit-learn.org/stable/auto_examples/classification/plot_lda_qda.html))\n",
    "\n",
    "Comparaison de la LDA et de la QDA sur des données synthétiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='dimensionality-reduction-using-linear-discriminant-analysis'></a> 1.2.1. **Réduction de la dimensionnalité à l'aide de l'analyse discriminante linéaire**<br/>([_Dimensionality reduction using Linear Discriminant Analysis_](https://scikit-learn.org/stable/modules/linear_model.html#dimensionality-reduction-using-linear-discriminant-analysis))\n",
    "\n",
    "[**`LinearDiscriminantAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis) peut être utilisée pour effectuer une réduction de dimension supervisée, en projetant les données d'entrée dans un sous-espace linéaire composé des directions qui maximisent la séparation entre les classes (dans un sens précis discuté dans la section mathématique ci-dessous). La dimension de la sortie est nécessairement inférieure au nombre de classes, il s'agit donc en général d'une réduction de dimension plutôt forte, et cela n'a de sens que dans un contexte multiclasse.\n",
    "\n",
    "Cela est implémenté dans la méthode `transform`. La dimensionnalité souhaitée peut être définie en utilisant le paramètre `n_components`. Ce paramètre n'a aucune influence sur les méthodes `fit` et `predict`.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Comparaison des projections 2D LDA et PCA de l'ensemble de données Iris**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_5_decomposition/plot_pca_vs_lda.ipynb)<br/>([*Comparison of LDA and PCA 2D projection of Iris dataset*](https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_lda.html))\n",
    "\n",
    "Comparaison de la réduction de dimension LDA et PCA de l'ensemble de données Iris."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='mathematical-formulation-of-the-lda-and-qda-classifiers'></a> 1.2.2. **Formulation mathématique des classificateurs LDA et QDA**<br/>([_Mathematical formulation of the LDA and QDA classifiers_](https://scikit-learn.org/stable/modules/lda_qda.html#mathematical-formulation-of-the-lda-and-qda-classifiers))\n",
    "\n",
    "L'ADL et l'ADQ peuvent tous deux être dérivés à partir de modèles probabilistes simples qui modélisent la distribution conditionnelle de classe des données $P(X|y=k)$ pour chaque classe $k$. Les prédictions peuvent ensuite être obtenues en utilisant la règle de Bayes, pour chaque échantillon d'apprentissage $x \\in \\mathcal{R}^d$ :\n",
    "\n",
    "$$\n",
    "P(y=k | x) = \\frac{P(x | y=k) P(y=k)}{P(x)} = \\frac{P(x | y=k) P(y = k)}{ \\sum_{l} P(x | y=l) \\cdot P(y=l)}\n",
    "$$\n",
    "\n",
    "et nous sélectionnons la classe $k$ qui maximise cette probabilité a posteriori.\n",
    "\n",
    "Plus précisément, pour l'analyse discriminante linéaire et quadratique, $P(x|y)$ est modélisé comme une distribution gaussienne multivariée avec une densité :\n",
    "\n",
    "$$\n",
    "P(x | y=k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}}\\exp\\left(-\\frac{1}{2} (x-\\mu_k)^t \\Sigma_k^{-1} (x-\\mu_k)\\right)\n",
    "$$\n",
    "\n",
    "où $d$ est le nombre de caractéristiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='qda'></a> 1.2.2.1. **QDA**<br/>([_QDA_](https://scikit-learn.org/stable/modules/lda_qda.html#qda))\n",
    "\n",
    "Selon le modèle ci-dessus, le logarithme de la probabilité a posteriori est :\n",
    "\n",
    "$$\n",
    "\\begin{split}\\log P(y=k | x) &= \\log P(x | y=k) + \\log P(y = k) + Cst \\\\\n",
    "&= -\\frac{1}{2} \\log |\\Sigma_k| -\\frac{1}{2} (x-\\mu_k)^t \\Sigma_k^{-1} (x-\\mu_k) + \\log P(y = k) + Cst,\\end{split}\n",
    "$$\n",
    "\n",
    "où le terme constant $Cst$ correspond au dénominateur $P(x)$, en plus d'autres termes constants provenant de la distribution gaussienne. La classe prédite est celle qui maximise ce logarithme a posteriori.\n",
    "\n",
    "> **Note :** Relation avec le classifieur bayésien naïf gaussien\n",
    ">\n",
    "> Si dans le modèle QDA on suppose que les matrices de covariance sont diagonales, alors les entrées sont supposées être conditionnellement indépendantes dans chaque classe, et le classifieur résultant est équivalent au classifieur bayésien naïf gaussien [**`naive_bayes.GaussianNB`**](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='lda'></a> 1.2.2.2. **LDA**<br/>([_LDA_](https://scikit-learn.org/stable/modules/lda_qda.html#lda))\n",
    "\n",
    "L'analyse discriminante linéaire (LDA) est un cas particulier de l'analyse discriminante quadratique (QDA), où l'on suppose que les gaussiennes de chaque classe partagent la même matrice de covariance : $\\Sigma_k = \\Sigma$ pour tous les $k$. Cela réduit le logarithme de la probabilité a posteriori à :\n",
    "\n",
    "$$\n",
    "\\log P(y=k | x) = -\\frac{1}{2} (x-\\mu_k)^t \\Sigma^{-1} (x-\\mu_k) + \\log P(y = k) + Cst.\n",
    "$$\n",
    "\n",
    "Le terme $(x-\\mu_k)^t \\Sigma^{-1} (x-\\mu_k)$ correspond à la [**distance de Mahalanobis**](https://en.wikipedia.org/wiki/Mahalanobis_distance) entre l'échantillon $x$ et la moyenne $\\mu_k$. La distance de Mahalanobis indique à quelle distance $x$ se trouve de $\\mu_k$, tout en tenant compte de la variance de chaque caractéristique. Ainsi, on peut interpréter LDA comme attribuant $x$ à la classe dont la moyenne est la plus proche en termes de distance de Mahalanobis, tout en tenant compte des probabilités a priori de classe.\n",
    "\n",
    "Le logarithme a posteriori de LDA peut également être écrit [3] comme suit :\n",
    "\n",
    "$$\n",
    "\\log P(y=k | x) = \\omega_k^t x + \\omega_{k0} + Cst.\n",
    "$$\n",
    "\n",
    "où $\\omega_k = \\Sigma^{-1} \\mu_k$ et $\\omega_{k0} = -\\frac{1}{2} \\mu_k^t\\Sigma^{-1}\\mu_k + \\log P (y = k)$. Ces quantités correspondent respectivement aux attributs `coef_` et `intercept_`.\n",
    "\n",
    "À partir de la formule ci-dessus, il est clair que LDA a une surface de décision linéaire. Dans le cas de QDA, il n'y a aucune hypothèse sur les matrices de covariance $\\Sigma_k$ des gaussiennes, ce qui conduit à des surfaces de décision quadratiques. Pour plus de détails, voir [1].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='mathematical-formulation-of-lda-dimensionality-reduction'></a> 1.2.3. **Formulation mathématique de la réduction de la dimensionnalité LDA**<br/>([_Mathematical formulation of LDA dimensionality reduction_](https://scikit-learn.org/stable/modules/lda_qda.html#mathematical-formulation-of-lda-dimensionality-reduction))\n",
    "\n",
    "Tout d'abord, notez que les K-moyennes $\\mu_k$ sont des vecteurs dans $\\mathcal{R}^d$, et ils résident dans un sous-espace affine $H$ de dimension au plus $K - 1$ (2 points forment une ligne, 3 points forment un plan, etc.).\n",
    "\n",
    "Comme mentionné précédemment, nous pouvons interpréter LDA comme attribuant $x$ à la classe dont la moyenne $\\mu_k$ est la plus proche en termes de distance de Mahalanobis, tout en tenant compte des probabilités a priori de classe. Alternativement, LDA est équivalent à d'abord _centrer_ les données de sorte que la matrice de covariance soit l'identité, puis à attribuer $x$ à la moyenne la plus proche en termes de distance euclidienne (tout en tenant toujours compte des probabilités de classe).\n",
    "\n",
    "Calculer les distances euclidiennes dans cet espace $d$-dimensionnel équivaut à d'abord projeter les points de données dans $H$, puis à y calculer les distances (car les autres dimensions contribueront également également à chaque classe en termes de distance). En d'autres termes, si $x$ est plus proche de $\\mu_k$ dans l'espace d'origine, il en sera de même dans $H$. Cela montre qu'implicitement dans le classifieur LDA, il y a une réduction de dimension par projection linéaire sur un espace $K - 1$ dimensionnel.\n",
    "\n",
    "Nous pouvons réduire davantage la dimension, à une dimension choisie $L$, en projetant sur le sous-espace linéaire $H_L$ qui maximise la variance des $\\mu^*_k$ après la projection (en effet, nous effectuons une forme de PCA pour les moyennes de classe transformées $\\mu^*_k$). Ce $L$ correspond au paramètre `n_components` utilisé dans la méthode [**`transform`**](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform). Voir [1] pour plus de détails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='shrinkage-and-covariance-estimator'></a> 1.2.4. **Rétrécissement et estimateur de covariance**<br/>([_Shrinkage and Covariance Estimator_](https://scikit-learn.org/stable/modules/lda_qda.html#shrinkage-and-covariance-estimator))\n",
    "\n",
    "Le rétrécissement est une forme de régularisation utilisée pour améliorer l'estimation des matrices de covariance dans des situations où le nombre d'échantillons d'entraînement est petit par rapport au nombre de caractéristiques. Dans ce scénario, la covariance d'échantillon empirique est un mauvais estimateur, et le rétrécissement aide à améliorer les performances de généralisation du classifieur. L'analyse discriminante linéaire avec rétrécissement peut être utilisée en définissant le paramètre `shrinkage` de la classe [**`LinearDiscriminantAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis) à `\"auto\"`. Cela détermine automatiquement le paramètre de rétrécissement optimal de manière analytique en suivant le lemme introduit par Ledoit et Wolf [2]. Notez que le rétrécissement ne fonctionne actuellement que lorsque le paramètre `solver` est défini sur `\"lsqr\"` ou `\"eigen\"`.\n",
    "\n",
    "Le paramètre `shrinkage` peut également être défini manuellement entre 0 et 1. En particulier, une valeur de 0 correspond à l'absence de rétrécissement (ce qui signifie que la matrice de covariance empirique sera utilisée) et une valeur de 1 correspond à un rétrécissement complet (ce qui signifie que la matrice diagonale des variances sera utilisée comme estimation de la matrice de covariance). Le paramètre défini entre ces deux extrêmes estimera une version rétrécie de la matrice de covariance.\n",
    "\n",
    "L'estimateur rétréci de la covariance de Ledoit et Wolf peut ne pas toujours être le meilleur choix. Par exemple, si la distribution des données suit une distribution normale, l'estimateur d'approximation de rétrécissement Oracle [**`sklearn.covariance.OAS`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.OAS.html#sklearn.covariance.OAS) produit une erreur quadratique moyenne plus faible que celle donnée par la formule de Ledoit et Wolf utilisée avec `shrinkage=\"auto\"`. Dans l'analyse discriminante linéaire, on suppose que les données sont gaussiennes conditionnellement à la classe. Si ces hypothèses sont respectées, l'utilisation de l'analyse discriminante linéaire avec l'estimateur OAS de la covariance permettra d'obtenir une meilleure précision de classification que si l'on utilise Ledoit et Wolf ou l'estimateur de covariance empirique.\n",
    "\n",
    "L'estimateur de covariance peut être choisi en utilisant le paramètre `covariance_estimator` de la classe [**`discriminant_analysis.LinearDiscriminantAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis). Un estimateur de covariance doit avoir une méthode `fit` et un attribut `covariance_`, comme tous les estimateurs de covariance dans le module [**`sklearn.covariance`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.covariance).\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lda_001.png\"\n",
    "    alt=\"Analyse discriminante linéaire (LDA) vs LDA avec Ledoit Wolf vs LDA avec OAS (1 caractéristique discriminative)\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Analyse discriminante linéaire normale, avec Ledoit-Wolf et OAS pour la classification**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_2_lda_qda/plot_lda.ipynb)<br/>([_Normal, Ledoit-Wolf and OAS Linear Discriminant Analysis for classification_](https://scikit-learn.org/stable/auto_examples/classification/plot_lda.html))\n",
    "\n",
    "Comparaison des classifieurs LDA avec l'estimateur de covariance empirique, Ledoit Wolf et OAS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='estimation-algorithms'></a> 1.2.5. **Algorithmes d'estimation**<br/>([_Estimation algorithms_](https://scikit-learn.org/stable/modules/lda_qda.html#estimation-algorithms))\n",
    "\n",
    "L'utilisation de LDA et QDA nécessite le calcul du log-postérieur, qui dépend des probabilités a priori de classe $P(y=k)$, des moyennes de classe $\\mu_k$, et des matrices de covariance.\n",
    "\n",
    "Le solveur par défaut utilisé pour [**`LinearDiscriminantAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis) est `\"svd\"`, et c'est le seul solveur disponible pour [**`QuadraticDiscriminantAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis). Il peut effectuer à la fois la classification et la transformation (pour LDA). Comme il ne dépend pas du calcul de la matrice de covariance, le solveur `\"svd\"` peut être préférable dans les situations où le nombre de caractéristiques est élevé. Le solveur `\"svd\"` ne peut pas être utilisé avec le rétrécissement. Pour QDA, l'utilisation du solveur SVD repose sur le fait que la matrice de covariance $\\Sigma_k$ est, par définition, égale à $\\frac{1}{n - 1} X_k^tX_k = \\frac{1}{n - 1} V S^2 V^t$ où $V$ provient de la SVD de la matrice (centrée) : $X_k = U S V^t$. Il s'avère que nous pouvons calculer le log-postérieur ci-dessus sans avoir à calculer explicitement $\\Sigma$ : le calcul de $S$ et $V$ via la SVD de $X$ suffit. Pour LDA, deux SVD sont calculées : la SVD de la matrice d'entrée centrée $X$ et la SVD des vecteurs de moyenne par classe.\n",
    "\n",
    "Le solveur `\"lsqr\"` est un algorithme efficace qui fonctionne uniquement pour la classification. Il doit calculer explicitement la matrice de covariance $\\Sigma$ et prend en charge le rétrécissement et les estimateurs de covariance personnalisés. Ce solveur calcule les coefficients $\\omega_k = \\Sigma^{-1}\\mu_k$ en résolvant $\\Sigma \\omega = \\mu_k$, évitant ainsi le calcul explicite de l'inverse $\\Sigma^{-1}$.\n",
    "\n",
    "Le solveur `\"eigen\"` est basé sur l'optimisation du ratio de dispersion entre les classes et à l'intérieur des classes. Il peut être utilisé à la fois pour la classification et la transformation, et prend en charge le rétrécissement. Cependant, le solveur `\"eigen\"` doit calculer la matrice de covariance, il peut donc ne pas être adapté aux situations avec un grand nombre de caractéristiques.\n",
    "\n",
    "### Références\n",
    "\n",
    "📚 [1] (1,2) [**“The Elements of Statistical Learning”**](https://hastie.su.domains/ElemStatLearn/), Hastie T., Tibshirani R., Friedman J., Section 4.3, p.106-119, 2008.\n",
    "\n",
    "🔬 [2] Ledoit O, Wolf M. Honey, [**“I Shrunk the Sample Covariance Matrix”**](https://repositori.upf.edu/bitstream/handle/10230/560/691.pdf). The Journal of Portfolio Management 30(4), 110-119, 2004.\n",
    "\n",
    "📚 [3] R. O. Duda, P. E. Hart, D. G. Stork. [**“Pattern Classification (Second Edition)”**](https://link.springer.com/article/10.1007/s00357-007-0015-9), section 2.6.2."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
