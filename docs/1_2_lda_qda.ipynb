{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='supervised-learning'></a> 1. [**Apprentissage supervis√©**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#supervised-learning)</br>([*Supervised learning*](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning))\n",
    "\n",
    "# 1.2. [**Analyse discriminante lin√©aire et quadratique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_2_lda_qda.ipynb)<br/>([_Linear and Quadratic Discriminant Analysis_](https://scikit-learn.org/stable/modules/lda_qda.html#linear-and-quadratic-discriminant-analysis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 6 pages, 3 exemples, 3 papiers\n",
    "- 1.2.1. [**R√©duction de la dimensionnalit√© √† l'aide de l'analyse discriminante lin√©aire**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_2_lda_qda.ipynb#dimensionality-reduction-using-linear-discriminant-analysis)<br/>([_Dimensionality reduction using Linear Discriminant Analysis_](https://scikit-learn.org/stable/modules/lda_qda.html#dimensionality-reduction-using-linear-discriminant-analysis))\n",
    "- 1.2.2. [**Formulation math√©matique des classificateurs LDA et QDA**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_2_lda_qda.ipynb#mathematical-formulation-of-the-lda-and-qda-classifiers)<br/>([_Mathematical formulation of the LDA and QDA classifiers_](https://scikit-learn.org/stable/modules/lda_qda.html#mathematical-formulation-of-the-lda-and-qda-classifiers))\n",
    "    - 1.2.2.1. [**QDA**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_2_lda_qda.ipynb#qda)<br/>([_QDA_](https://scikit-learn.org/stable/modules/lda_qda.html#qda))\n",
    "    - 1.2.2.2. [**LDA**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_2_lda_qda.ipynb#lda)<br/>([_LDA_](https://scikit-learn.org/stable/modules/lda_qda.html#lda))\n",
    "- 1.2.3. [**Formulation math√©matique de la r√©duction de la dimensionnalit√© LDA**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_2_lda_qda.ipynb#mathematical-formulation-of-lda-dimensionality-reduction)<br/>([_Mathematical formulation of LDA dimensionality reduction_](https://scikit-learn.org/stable/modules/lda_qda.html#mathematical-formulation-of-lda-dimensionality-reduction))\n",
    "- 1.2.4. [**R√©tr√©cissement et estimateur de covariance**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_2_lda_qda.ipynb#shrinkage-and-covariance-estimator)<br/>([_Shrinkage and Covariance Estimator_](https://scikit-learn.org/stable/modules/lda_qda.html#shrinkage-and-covariance-estimator))\n",
    "- 1.2.5. [**Algorithmes d'estimation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_2_lda_qda.ipynb#estimation-algorithms)<br/>([_Estimation algorithms_](https://scikit-learn.org/stable/modules/lda_qda.html#estimation-algorithms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='linear-and-quadratic-discriminant-analysis'></a> 1.2. **Analyse discriminante lin√©aire et quadratique**<br/>([_Linear and Quadratic Discriminant Analysis_](https://scikit-learn.org/stable/modules/lda_qda.html))\n",
    "\n",
    "L'analyse discriminante lin√©aire ([**`LinearDiscriminantAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis)) et l'analyse discriminante quadratique ([**`QuadraticDiscriminantAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis)) sont deux classifieurs classiques, avec, comme leur nom l'indiquent, une surface de d√©cision lin√©aire et quadratique, respectivement.\n",
    "\n",
    "Ces classifieurs sont populaires car ils ont des solutions analytiques qui peuvent √™tre facilement calcul√©es, sont intrins√®quement multiclasse, ont fait leurs preuves en pratique, et n'ont pas d'hyperparam√®tres √† r√©gler.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lda_qda_001.png\"\n",
    "    alt=\"LDA vs QDA\"\n",
    "    style=\"max-width: 75%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Le graphique montre les fronti√®res de d√©cision pour l'analyse discriminante lin√©aire et l'analyse discriminante quadratique. La rang√©e du bas d√©montre que l'analyse discriminante lin√©aire peut seulement apprendre des fronti√®res lin√©aires, tandis que l'analyse discriminante quadratique, plus flexible, peut apprendre des fronti√®res quadratiques.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Analyse Discriminante Lin√©aire et Quadratique avec des ellipso√Ødes de covariance**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_2_lda_qda/plot_lda_qda.ipynb)<br/>([_Linear and Quadratic Discriminant Analysis with covariance ellipsoid_](https://scikit-learn.org/stable/auto_examples/classification/plot_lda_qda.html))\n",
    "\n",
    "Comparaison de la LDA et de la QDA sur des donn√©es synth√©tiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='dimensionality-reduction-using-linear-discriminant-analysis'></a> 1.2.1. **R√©duction de la dimensionnalit√© √† l'aide de l'analyse discriminante lin√©aire**<br/>([_Dimensionality reduction using Linear Discriminant Analysis_](https://scikit-learn.org/stable/modules/linear_model.html#dimensionality-reduction-using-linear-discriminant-analysis))\n",
    "\n",
    "[**`LinearDiscriminantAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis) peut √™tre utilis√©e pour effectuer une r√©duction de dimension supervis√©e, en projetant les donn√©es d'entr√©e dans un sous-espace lin√©aire compos√© des directions qui maximisent la s√©paration entre les classes (dans un sens pr√©cis discut√© dans la section math√©matique ci-dessous). La dimension de la sortie est n√©cessairement inf√©rieure au nombre de classes, il s'agit donc en g√©n√©ral d'une r√©duction de dimension plut√¥t forte, et cela n'a de sens que dans un contexte multiclasse.\n",
    "\n",
    "Cela est impl√©ment√© dans la m√©thode `transform`. La dimensionnalit√© souhait√©e peut √™tre d√©finie en utilisant le param√®tre `n_components`. Ce param√®tre n'a aucune influence sur les m√©thodes `fit` et `predict`.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Comparaison des projections 2D LDA et PCA de l'ensemble de donn√©es Iris**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_5_decomposition/plot_pca_vs_lda.ipynb)<br/>([*Comparison of LDA and PCA 2D projection of Iris dataset*](https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_lda.html))\n",
    "\n",
    "Comparaison de la r√©duction de dimension LDA et PCA de l'ensemble de donn√©es Iris."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='mathematical-formulation-of-the-lda-and-qda-classifiers'></a> 1.2.2. **Formulation math√©matique des classificateurs LDA et QDA**<br/>([_Mathematical formulation of the LDA and QDA classifiers_](https://scikit-learn.org/stable/modules/lda_qda.html#mathematical-formulation-of-the-lda-and-qda-classifiers))\n",
    "\n",
    "L'ADL et l'ADQ peuvent tous deux √™tre d√©riv√©s √† partir de mod√®les probabilistes simples qui mod√©lisent la distribution conditionnelle de classe des donn√©es $P(X|y=k)$ pour chaque classe $k$. Les pr√©dictions peuvent ensuite √™tre obtenues en utilisant la r√®gle de Bayes, pour chaque √©chantillon d'apprentissage $x \\in \\mathcal{R}^d$ :\n",
    "\n",
    "$$\n",
    "P(y=k | x) = \\frac{P(x | y=k) P(y=k)}{P(x)} = \\frac{P(x | y=k) P(y = k)}{ \\sum_{l} P(x | y=l) \\cdot P(y=l)}\n",
    "$$\n",
    "\n",
    "et nous s√©lectionnons la classe $k$ qui maximise cette probabilit√© a posteriori.\n",
    "\n",
    "Plus pr√©cis√©ment, pour l'analyse discriminante lin√©aire et quadratique, $P(x|y)$ est mod√©lis√© comme une distribution gaussienne multivari√©e avec une densit√© :\n",
    "\n",
    "$$\n",
    "P(x | y=k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}}\\exp\\left(-\\frac{1}{2} (x-\\mu_k)^t \\Sigma_k^{-1} (x-\\mu_k)\\right)\n",
    "$$\n",
    "\n",
    "o√π $d$ est le nombre de caract√©ristiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='qda'></a> 1.2.2.1. **QDA**<br/>([_QDA_](https://scikit-learn.org/stable/modules/lda_qda.html#qda))\n",
    "\n",
    "Selon le mod√®le ci-dessus, le logarithme de la probabilit√© a posteriori est :\n",
    "\n",
    "$$\n",
    "\\begin{split}\\log P(y=k | x) &= \\log P(x | y=k) + \\log P(y = k) + Cst \\\\\n",
    "&= -\\frac{1}{2} \\log |\\Sigma_k| -\\frac{1}{2} (x-\\mu_k)^t \\Sigma_k^{-1} (x-\\mu_k) + \\log P(y = k) + Cst,\\end{split}\n",
    "$$\n",
    "\n",
    "o√π le terme constant $Cst$ correspond au d√©nominateur $P(x)$, en plus d'autres termes constants provenant de la distribution gaussienne. La classe pr√©dite est celle qui maximise ce logarithme a posteriori.\n",
    "\n",
    "> **Note :** Relation avec le classifieur bay√©sien na√Øf gaussien\n",
    ">\n",
    "> Si dans le mod√®le QDA on suppose que les matrices de covariance sont diagonales, alors les entr√©es sont suppos√©es √™tre conditionnellement ind√©pendantes dans chaque classe, et le classifieur r√©sultant est √©quivalent au classifieur bay√©sien na√Øf gaussien [**`naive_bayes.GaussianNB`**](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='lda'></a> 1.2.2.2. **LDA**<br/>([_LDA_](https://scikit-learn.org/stable/modules/lda_qda.html#lda))\n",
    "\n",
    "L'analyse discriminante lin√©aire (LDA) est un cas particulier de l'analyse discriminante quadratique (QDA), o√π l'on suppose que les gaussiennes de chaque classe partagent la m√™me matrice de covariance : $\\Sigma_k = \\Sigma$ pour tous les $k$. Cela r√©duit le logarithme de la probabilit√© a posteriori √† :\n",
    "\n",
    "$$\n",
    "\\log P(y=k | x) = -\\frac{1}{2} (x-\\mu_k)^t \\Sigma^{-1} (x-\\mu_k) + \\log P(y = k) + Cst.\n",
    "$$\n",
    "\n",
    "Le terme $(x-\\mu_k)^t \\Sigma^{-1} (x-\\mu_k)$ correspond √† la [**distance de Mahalanobis**](https://en.wikipedia.org/wiki/Mahalanobis_distance) entre l'√©chantillon $x$ et la moyenne $\\mu_k$. La distance de Mahalanobis indique √† quelle distance $x$ se trouve de $\\mu_k$, tout en tenant compte de la variance de chaque caract√©ristique. Ainsi, on peut interpr√©ter LDA comme attribuant $x$ √† la classe dont la moyenne est la plus proche en termes de distance de Mahalanobis, tout en tenant compte des probabilit√©s a priori de classe.\n",
    "\n",
    "Le logarithme a posteriori de LDA peut √©galement √™tre √©crit [3] comme suit :\n",
    "\n",
    "$$\n",
    "\\log P(y=k | x) = \\omega_k^t x + \\omega_{k0} + Cst.\n",
    "$$\n",
    "\n",
    "o√π $\\omega_k = \\Sigma^{-1} \\mu_k$ et $\\omega_{k0} = -\\frac{1}{2} \\mu_k^t\\Sigma^{-1}\\mu_k + \\log P (y = k)$. Ces quantit√©s correspondent respectivement aux attributs `coef_` et `intercept_`.\n",
    "\n",
    "√Ä partir de la formule ci-dessus, il est clair que LDA a une surface de d√©cision lin√©aire. Dans le cas de QDA, il n'y a aucune hypoth√®se sur les matrices de covariance $\\Sigma_k$ des gaussiennes, ce qui conduit √† des surfaces de d√©cision quadratiques. Pour plus de d√©tails, voir [1].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='mathematical-formulation-of-lda-dimensionality-reduction'></a> 1.2.3. **Formulation math√©matique de la r√©duction de la dimensionnalit√© LDA**<br/>([_Mathematical formulation of LDA dimensionality reduction_](https://scikit-learn.org/stable/modules/lda_qda.html#mathematical-formulation-of-lda-dimensionality-reduction))\n",
    "\n",
    "Tout d'abord, notez que les K-moyennes $\\mu_k$ sont des vecteurs dans $\\mathcal{R}^d$, et ils r√©sident dans un sous-espace affine $H$ de dimension au plus $K - 1$ (2 points forment une ligne, 3 points forment un plan, etc.).\n",
    "\n",
    "Comme mentionn√© pr√©c√©demment, nous pouvons interpr√©ter LDA comme attribuant $x$ √† la classe dont la moyenne $\\mu_k$ est la plus proche en termes de distance de Mahalanobis, tout en tenant compte des probabilit√©s a priori de classe. Alternativement, LDA est √©quivalent √† d'abord _centrer_ les donn√©es de sorte que la matrice de covariance soit l'identit√©, puis √† attribuer $x$ √† la moyenne la plus proche en termes de distance euclidienne (tout en tenant toujours compte des probabilit√©s de classe).\n",
    "\n",
    "Calculer les distances euclidiennes dans cet espace $d$-dimensionnel √©quivaut √† d'abord projeter les points de donn√©es dans $H$, puis √† y calculer les distances (car les autres dimensions contribueront √©galement √©galement √† chaque classe en termes de distance). En d'autres termes, si $x$ est plus proche de $\\mu_k$ dans l'espace d'origine, il en sera de m√™me dans $H$. Cela montre qu'implicitement dans le classifieur LDA, il y a une r√©duction de dimension par projection lin√©aire sur un espace $K - 1$ dimensionnel.\n",
    "\n",
    "Nous pouvons r√©duire davantage la dimension, √† une dimension choisie $L$, en projetant sur le sous-espace lin√©aire $H_L$ qui maximise la variance des $\\mu^*_k$ apr√®s la projection (en effet, nous effectuons une forme de PCA pour les moyennes de classe transform√©es $\\mu^*_k$). Ce $L$ correspond au param√®tre `n_components` utilis√© dans la m√©thode [**`transform`**](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform). Voir [1] pour plus de d√©tails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='shrinkage-and-covariance-estimator'></a> 1.2.4. **R√©tr√©cissement et estimateur de covariance**<br/>([_Shrinkage and Covariance Estimator_](https://scikit-learn.org/stable/modules/lda_qda.html#shrinkage-and-covariance-estimator))\n",
    "\n",
    "Le r√©tr√©cissement est une forme de r√©gularisation utilis√©e pour am√©liorer l'estimation des matrices de covariance dans des situations o√π le nombre d'√©chantillons d'entra√Ænement est petit par rapport au nombre de caract√©ristiques. Dans ce sc√©nario, la covariance d'√©chantillon empirique est un mauvais estimateur, et le r√©tr√©cissement aide √† am√©liorer les performances de g√©n√©ralisation du classifieur. L'analyse discriminante lin√©aire avec r√©tr√©cissement peut √™tre utilis√©e en d√©finissant le param√®tre `shrinkage` de la classe [**`LinearDiscriminantAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis) √† `\"auto\"`. Cela d√©termine automatiquement le param√®tre de r√©tr√©cissement optimal de mani√®re analytique en suivant le lemme introduit par Ledoit et Wolf [2]. Notez que le r√©tr√©cissement ne fonctionne actuellement que lorsque le param√®tre `solver` est d√©fini sur `\"lsqr\"` ou `\"eigen\"`.\n",
    "\n",
    "Le param√®tre `shrinkage` peut √©galement √™tre d√©fini manuellement entre 0 et 1. En particulier, une valeur de 0 correspond √† l'absence de r√©tr√©cissement (ce qui signifie que la matrice de covariance empirique sera utilis√©e) et une valeur de 1 correspond √† un r√©tr√©cissement complet (ce qui signifie que la matrice diagonale des variances sera utilis√©e comme estimation de la matrice de covariance). Le param√®tre d√©fini entre ces deux extr√™mes estimera une version r√©tr√©cie de la matrice de covariance.\n",
    "\n",
    "L'estimateur r√©tr√©ci de la covariance de Ledoit et Wolf peut ne pas toujours √™tre le meilleur choix. Par exemple, si la distribution des donn√©es suit une distribution normale, l'estimateur d'approximation de r√©tr√©cissement Oracle [**`sklearn.covariance.OAS`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.OAS.html#sklearn.covariance.OAS) produit une erreur quadratique moyenne plus faible que celle donn√©e par la formule de Ledoit et Wolf utilis√©e avec `shrinkage=\"auto\"`. Dans l'analyse discriminante lin√©aire, on suppose que les donn√©es sont gaussiennes conditionnellement √† la classe. Si ces hypoth√®ses sont respect√©es, l'utilisation de l'analyse discriminante lin√©aire avec l'estimateur OAS de la covariance permettra d'obtenir une meilleure pr√©cision de classification que si l'on utilise Ledoit et Wolf ou l'estimateur de covariance empirique.\n",
    "\n",
    "L'estimateur de covariance peut √™tre choisi en utilisant le param√®tre `covariance_estimator` de la classe [**`discriminant_analysis.LinearDiscriminantAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis). Un estimateur de covariance doit avoir une m√©thode `fit` et un attribut `covariance_`, comme tous les estimateurs de covariance dans le module [**`sklearn.covariance`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.covariance).\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lda_001.png\"\n",
    "    alt=\"Analyse discriminante lin√©aire (LDA) vs LDA avec Ledoit Wolf vs LDA avec OAS (1 caract√©ristique discriminative)\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Analyse discriminante lin√©aire normale, avec Ledoit-Wolf et OAS pour la classification**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_2_lda_qda/plot_lda.ipynb)<br/>([_Normal, Ledoit-Wolf and OAS Linear Discriminant Analysis for classification_](https://scikit-learn.org/stable/auto_examples/classification/plot_lda.html))\n",
    "\n",
    "Comparaison des classifieurs LDA avec l'estimateur de covariance empirique, Ledoit Wolf et OAS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='estimation-algorithms'></a> 1.2.5. **Algorithmes d'estimation**<br/>([_Estimation algorithms_](https://scikit-learn.org/stable/modules/lda_qda.html#estimation-algorithms))\n",
    "\n",
    "L'utilisation de LDA et QDA n√©cessite le calcul du log-post√©rieur, qui d√©pend des probabilit√©s a priori de classe $P(y=k)$, des moyennes de classe $\\mu_k$, et des matrices de covariance.\n",
    "\n",
    "Le solveur par d√©faut utilis√© pour [**`LinearDiscriminantAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis) est `\"svd\"`, et c'est le seul solveur disponible pour [**`QuadraticDiscriminantAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis). Il peut effectuer √† la fois la classification et la transformation (pour LDA). Comme il ne d√©pend pas du calcul de la matrice de covariance, le solveur `\"svd\"` peut √™tre pr√©f√©rable dans les situations o√π le nombre de caract√©ristiques est √©lev√©. Le solveur `\"svd\"` ne peut pas √™tre utilis√© avec le r√©tr√©cissement. Pour QDA, l'utilisation du solveur SVD repose sur le fait que la matrice de covariance $\\Sigma_k$ est, par d√©finition, √©gale √† $\\frac{1}{n - 1} X_k^tX_k = \\frac{1}{n - 1} V S^2 V^t$ o√π $V$ provient de la SVD de la matrice (centr√©e) : $X_k = U S V^t$. Il s'av√®re que nous pouvons calculer le log-post√©rieur ci-dessus sans avoir √† calculer explicitement $\\Sigma$ : le calcul de $S$ et $V$ via la SVD de $X$ suffit. Pour LDA, deux SVD sont calcul√©es : la SVD de la matrice d'entr√©e centr√©e $X$ et la SVD des vecteurs de moyenne par classe.\n",
    "\n",
    "Le solveur `\"lsqr\"` est un algorithme efficace qui fonctionne uniquement pour la classification. Il doit calculer explicitement la matrice de covariance $\\Sigma$ et prend en charge le r√©tr√©cissement et les estimateurs de covariance personnalis√©s. Ce solveur calcule les coefficients $\\omega_k = \\Sigma^{-1}\\mu_k$ en r√©solvant $\\Sigma \\omega = \\mu_k$, √©vitant ainsi le calcul explicite de l'inverse $\\Sigma^{-1}$.\n",
    "\n",
    "Le solveur `\"eigen\"` est bas√© sur l'optimisation du ratio de dispersion entre les classes et √† l'int√©rieur des classes. Il peut √™tre utilis√© √† la fois pour la classification et la transformation, et prend en charge le r√©tr√©cissement. Cependant, le solveur `\"eigen\"` doit calculer la matrice de covariance, il peut donc ne pas √™tre adapt√© aux situations avec un grand nombre de caract√©ristiques.\n",
    "\n",
    "### R√©f√©rences\n",
    "\n",
    "üìö [1] (1,2) [**‚ÄúThe Elements of Statistical Learning‚Äù**](https://hastie.su.domains/ElemStatLearn/), Hastie T., Tibshirani R., Friedman J., Section 4.3, p.106-119, 2008.\n",
    "\n",
    "üî¨ [2] Ledoit O, Wolf M. Honey, [**‚ÄúI Shrunk the Sample Covariance Matrix‚Äù**](https://repositori.upf.edu/bitstream/handle/10230/560/691.pdf). The Journal of Portfolio Management 30(4), 110-119, 2004.\n",
    "\n",
    "üìö [3] R. O. Duda, P. E. Hart, D. G. Stork. [**‚ÄúPattern Classification (Second Edition)‚Äù**](https://link.springer.com/article/10.1007/s00357-007-0015-9), section 2.6.2."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
