{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='supervised-learning'></a> 1. [**Apprentissage supervisé**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#supervised-learning)</br>([*Supervised learning*](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning))\n",
    "\n",
    "# 1.2. [**Analyse discriminante linéaire et quadratique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_2_lda_qda.ipynb)<br/>([_Linear and Quadratic Discriminant Analysis_](https://scikit-learn.org/stable/modules/lda_qda.html#linear-and-quadratic-discriminant-analysis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 6 pages, 3 exemples, 3 papiers\n",
    "- 1.2.1. [**Réduction de la dimensionnalité à l'aide de l'analyse discriminante linéaire**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_2_lda_qda.ipynb#dimensionality-reduction-using-linear-discriminant-analysis)<br/>([_Dimensionality reduction using Linear Discriminant Analysis_](https://scikit-learn.org/stable/modules/lda_qda.html#dimensionality-reduction-using-linear-discriminant-analysis))\n",
    "- 1.2.2. [**Formulation mathématique des classificateurs LDA et QDA**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_2_lda_qda.ipynb#mathematical-formulation-of-the-lda-and-qda-classifiers)<br/>([_Mathematical formulation of the LDA and QDA classifiers_](https://scikit-learn.org/stable/modules/lda_qda.html#mathematical-formulation-of-the-lda-and-qda-classifiers))\n",
    "    - 1.2.2.1. [**QDA**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_2_lda_qda.ipynb#qda)<br/>([_QDA_](https://scikit-learn.org/stable/modules/lda_qda.html#qda))\n",
    "    - 1.2.2.2. [**LDA**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_2_lda_qda.ipynb#lda)<br/>([_LDA_](https://scikit-learn.org/stable/modules/lda_qda.html#lda))\n",
    "- 1.2.3. [**Formulation mathématique de la réduction de la dimensionnalité LDA**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_2_lda_qda.ipynb#mathematical-formulation-of-lda-dimensionality-reduction)<br/>([_Mathematical formulation of LDA dimensionality reduction_](https://scikit-learn.org/stable/modules/lda_qda.html#mathematical-formulation-of-lda-dimensionality-reduction))\n",
    "- 1.2.4. [**Rétrécissement et estimateur de covariance**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_2_lda_qda.ipynb#shrinkage-and-covariance-estimator)<br/>([_Shrinkage and Covariance Estimator_](https://scikit-learn.org/stable/modules/lda_qda.html#shrinkage-and-covariance-estimator))\n",
    "- 1.2.5. [**Algorithmes d'estimation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_2_lda_qda.ipynb#estimation-algorithms)<br/>([_Estimation algorithms_](https://scikit-learn.org/stable/modules/lda_qda.html#estimation-algorithms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='linear-and-quadratic-discriminant-analysis'></a> 1.2. **Analyse discriminante linéaire et quadratique**<br/>([_Linear and Quadratic Discriminant Analysis_](https://scikit-learn.org/stable/modules/lda_qda.html))\n",
    "\n",
    "L'analyse discriminante linéaire ([**`LinearDiscriminantAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis)) et l'analyse discriminante quadratique ([**`QuadraticDiscriminantAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis)) sont deux classifieurs classiques, avec, comme leur nom l'indiquent, une surface de décision linéaire et quadratique, respectivement.\n",
    "\n",
    "Ces classifieurs sont populaires car ils ont des solutions analytiques qui peuvent être facilement calculées, sont intrinsèquement multiclasse, ont fait leurs preuves en pratique, et n'ont pas d'hyperparamètres à régler.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lda_qda_001.png\"\n",
    "    alt=\"LDA vs QDA\"\n",
    "    style=\"max-width: 75%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Le graphique montre les frontières de décision pour l'analyse discriminante linéaire et l'analyse discriminante quadratique. La rangée du bas démontre que l'analyse discriminante linéaire peut seulement apprendre des frontières linéaires, tandis que l'analyse discriminante quadratique, plus flexible, peut apprendre des frontières quadratiques.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Analyse Discriminante Linéaire et Quadratique avec des ellipsoïdes de covariance**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_ols.ipynb)<br/>([_Linear and Quadratic Discriminant Analysis with covariance ellipsoid_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html))\n",
    "\n",
    "Comparaison de la LDA et de la QDA sur des données synthétiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='dimensionality-reduction-using-linear-discriminant-analysis'></a> 1.2.1. **Réduction de la dimensionnalité à l'aide de l'analyse discriminante linéaire**<br/>([_Dimensionality reduction using Linear Discriminant Analysis_](https://scikit-learn.org/stable/modules/linear_model.html#dimensionality-reduction-using-linear-discriminant-analysis))\n",
    "\n",
    "L'ADL et l'ADQ peuvent tous deux être dérivés à partir de modèles probabilistes simples qui modélisent la distribution conditionnelle de classe des données $P(X|y=k)$ pour chaque classe $k$. Les prédictions peuvent ensuite être obtenues en utilisant la règle de Bayes, pour chaque échantillon d'apprentissage $x \\in \\mathcal{R}^d$ :\n",
    "\n",
    "$$\n",
    "P(y=k | x) = \\frac{P(x | y=k) P(y=k)}{P(x)} = \\frac{P(x | y=k) P(y = k)}{ \\sum_{l} P(x | y=l) \\cdot P(y=l)}\n",
    "$$\n",
    "\n",
    "et nous sélectionnons la classe $k$ qui maximise cette probabilité a posteriori.\n",
    "\n",
    "Plus précisément, pour l'analyse discriminante linéaire et quadratique, $P(x|y)$ est modélisé comme une distribution gaussienne multivariée avec une densité :\n",
    "\n",
    "$$\n",
    "P(x | y=k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}}\\exp\\left(-\\frac{1}{2} (x-\\mu_k)^t \\Sigma_k^{-1} (x-\\mu_k)\\right)\n",
    "$$\n",
    "\n",
    "où $d$ est le nombre de caractéristiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='qda'></a> 1.2.2.1. **QDA**<br/>([_QDA_](https://scikit-learn.org/stable/modules/lda_qda.html#qda))\n",
    "\n",
    "Selon le modèle ci-dessus, le logarithme de la probabilité a posteriori est :\n",
    "\n",
    "$$\n",
    "\\begin{split}\\log P(y=k | x) &= \\log P(x | y=k) + \\log P(y = k) + Cst \\\\\n",
    "&= -\\frac{1}{2} \\log |\\Sigma_k| -\\frac{1}{2} (x-\\mu_k)^t \\Sigma_k^{-1} (x-\\mu_k) + \\log P(y = k) + Cst,\\end{split}\n",
    "$$\n",
    "\n",
    "où le terme constant $Cst$ correspond au dénominateur $P(x)$, en plus d'autres termes constants provenant de la distribution gaussienne. La classe prédite est celle qui maximise ce logarithme a posteriori.\n",
    "\n",
    "> **Note :** Relation avec le classifieur bayésien naïf gaussien\n",
    ">\n",
    "> Si dans le modèle QDA on suppose que les matrices de covariance sont diagonales, alors les entrées sont supposées être conditionnellement indépendantes dans chaque classe, et le classifieur résultant est équivalent au classifieur bayésien naïf gaussien [**`naive_bayes.GaussianNB`**](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='lda'></a> 1.2.2.2. **LDA**<br/>([_LDA_](https://scikit-learn.org/stable/modules/lda_qda.html#lda))\n",
    "\n",
    "L'analyse discriminante linéaire (LDA) est un cas particulier de l'analyse discriminante quadratique (QDA), où l'on suppose que les gaussiennes de chaque classe partagent la même matrice de covariance : $\\Sigma_k = \\Sigma$ pour tous les $k$. Cela réduit le logarithme de la probabilité a posteriori à :\n",
    "\n",
    "$$\n",
    "\\log P(y=k | x) = -\\frac{1}{2} (x-\\mu_k)^t \\Sigma^{-1} (x-\\mu_k) + \\log P(y = k) + Cst.\n",
    "$$\n",
    "\n",
    "Le terme $(x-\\mu_k)^t \\Sigma^{-1} (x-\\mu_k)$ correspond à la [**distance de Mahalanobis**](https://en.wikipedia.org/wiki/Mahalanobis_distance) entre l'échantillon $x$ et la moyenne $\\mu_k$. La distance de Mahalanobis indique à quelle distance $x$ se trouve de $\\mu_k$, tout en tenant compte de la variance de chaque caractéristique. Ainsi, on peut interpréter LDA comme attribuant $x$ à la classe dont la moyenne est la plus proche en termes de distance de Mahalanobis, tout en tenant compte des probabilités a priori de classe.\n",
    "\n",
    "Le logarithme a posteriori de LDA peut également être écrit [3] comme suit :\n",
    "\n",
    "$$\n",
    "\\log P(y=k | x) = \\omega_k^t x + \\omega_{k0} + Cst.\n",
    "$$\n",
    "\n",
    "où $\\omega_k = \\Sigma^{-1} \\mu_k$ et $\\omega_{k0} = -\\frac{1}{2} \\mu_k^t\\Sigma^{-1}\\mu_k + \\log P (y = k)$. Ces quantités correspondent respectivement aux attributs `coef_` et `intercept_`.\n",
    "\n",
    "À partir de la formule ci-dessus, il est clair que LDA a une surface de décision linéaire. Dans le cas de QDA, il n'y a aucune hypothèse sur les matrices de covariance $\\Sigma_k$ des gaussiennes, ce qui conduit à des surfaces de décision quadratiques. Pour plus de détails, voir [1].\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
