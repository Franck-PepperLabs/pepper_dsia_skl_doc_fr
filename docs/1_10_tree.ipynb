{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='supervised-learning'></a> 1. [**Apprentissage supervisé**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_supervised_learning.ipynb#supervised-learning)</br>([*Supervised learning*](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning))\n",
    "\n",
    "# <a id='decision-trees'></a> 1.10. [**Arbres de décision**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_10_tree.ipynb)<br/>([_Decision Trees_](https://scikit-learn.org/stable/modules/tree.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 15 pages, 6 exemples, 6 papiers\n",
    "- 1.10.1. [**Classification**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_10_tree.ipynb#classification)<br/>([_Classification_](https://scikit-learn.org/stable/modules/tree.html#classification))\n",
    "- 1.10.2. [**Régression**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_10_tree.ipynb#regression)<br/>([_Regression_](https://scikit-learn.org/stable/modules/tree.html#regression))\n",
    "- 1.10.3. [**Problèmes à sorties multiples**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_10_tree.ipynb#multi-output-problems)<br/>([_Multi-output problems_](https://scikit-learn.org/stable/modules/tree.html#multi-output-problems))\n",
    "- 1.10.4. [**Complexité**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_10_tree.ipynb#complexity)<br/>([_Complexity_](https://scikit-learn.org/stable/modules/tree.html#complexity))\n",
    "- 1.10.5. [**Conseils d'utilisation pratique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_10_tree.ipynb#tips-on-practical-use)<br/>([_Tips on practical use_](https://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use))\n",
    "- 1.10.6. [**Algorithmes d'arbres : ID3, C4.5, C5.0 et CART**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_10_tree.ipynb#tree-algorithms-id3-c4-5-c5-0-and-cart)<br/>([_Tree algorithms: ID3, C4.5, C5.0 and CART_](https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart))\n",
    "- 1.10.7. [**Formulation mathématique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_10_tree.ipynb#mathematical-formulation)<br/>([_Mathematical formulation_](https://scikit-learn.org/stable/modules/tree.html#mathematical-formulation))\n",
    "    - 1.10.7.1. [**Critères de classification**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_10_tree.ipynb#classification-criteria)<br/>([_Classification criteria_](https://scikit-learn.org/stable/modules/tree.html#classification-criteria))\n",
    "    - 1.10.7.2. [**Critères de régression**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_10_tree.ipynb#regression-criteria)<br/>([_Regression criteria_](https://scikit-learn.org/stable/modules/tree.html#regression-criteria))\n",
    "- 1.10.8. [**Prise en charge des valeurs manquantes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_10_tree.ipynb#missing-values-support)<br/>([_Missing Values Support_](https://scikit-learn.org/stable/modules/tree.html#missing-values-support))\n",
    "- 1.10.9. [**Élagage de complexité minimale-coût**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_10_tree.ipynb#minimal-cost-complexity-pruning)<br/>([_Minimal Cost-Complexity Pruning_](https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='decision-trees'></a> 1.10. **Arbres de décision**<br/>([_Decision Trees_](https://scikit-learn.org/stable/modules/tree.html))\n",
    "\n",
    "**Les arbres de décision (DTs)** sont une méthode d'apprentissage supervisé non paramétrique utilisée pour la [**classification** (1.10.1)](#tree-classification) et la [**régression** (1.10.2)](#regression). L'objectif est de créer un modèle qui prédit la valeur d'une variable cible en apprenant des règles de décision simples déduites des caractéristiques des données. Un arbre peut être vu comme une approximation constante par morceaux.\n",
    "\n",
    "Par exemple, dans l'exemple ci-dessous, les arbres de décision apprennent à partir des données pour approximer une courbe sinusoïdale avec un ensemble de règles de décision si-alors-sinon. Plus l'arbre est profond, plus les règles de décision sont complexes et plus le modèle est ajusté.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_tree_regression_001.png\"\n",
    "    alt=\"Régression par arbre de décision\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Quelques avantages des arbres de décision sont les suivants :\n",
    "- Faciles à comprendre et à interpréter. Les arbres peuvent être visualisés.\n",
    "- Nécessitent peu de préparation des données. D'autres techniques nécessitent souvent une normalisation des données, la création de variables factices et la suppression des valeurs manquantes. Certaines combinaisons d'arbres et d'algorithmes prennent en charge les [**valeurs manquantes** (1.10.8)](#tree-missing-value-support).\n",
    "- Le coût de l'utilisation de l'arbre (c'est-à-dire la prédiction des données) est logarithmique par rapport au nombre de points de données utilisés pour entraîner l'arbre.\n",
    "- Capables de traiter à la fois des données numériques et catégorielles. Cependant, l'implémentation de scikit-learn ne prend pas en charge les variables catégorielles pour le moment. D'autres techniques sont généralement spécialisées dans l'analyse de jeux de données qui n'ont qu'un seul type de variable. Consultez [**algorithmes** (1.10.6)](#tree-algorithms) pour plus d'informations.\n",
    "- Capables de gérer des problèmes à sorties multiples.\n",
    "- Utilisent un modèle boîte blanche. Si une situation donnée est observable dans un modèle, l'explication de la condition est facilement expliquée par la logique booléenne. En revanche, dans un modèle boîte noire (par exemple, dans un réseau de neurones artificiels), les résultats peuvent être plus difficiles à interpréter.\n",
    "- Possibilité de valider un modèle à l'aide de tests statistiques. Cela permet de tenir compte de la fiabilité du modèle.\n",
    "- Bonnes performances même si ses hypothèses sont quelque peu violées par le modèle réel à partir duquel les données ont été générées.\n",
    "\n",
    "Les inconvénients des arbres de décision comprennent :\n",
    "- Les apprenants d'arbres de décision peuvent créer des arbres trop complexes qui ne généralisent pas bien les données. C'est ce que l'on appelle le surajustement. Des mécanismes tels que l'élagage, le paramétrage du nombre minimal d'échantillons requis dans un nœud feuille ou le paramétrage de la profondeur maximale de l'arbre sont nécessaires pour éviter ce problème.\n",
    "- Les arbres de décision peuvent être instables car de petites variations dans les données peuvent entraîner la création d'un arbre complètement différent. Ce problème est atténué en utilisant des arbres de décision dans un ensemble.\n",
    "- Les prédictions des arbres de décision ne sont ni lisses ni continues, mais des approximations constantes par morceaux comme on peut le voir dans la figure ci-dessus. Par conséquent, ils ne sont pas adaptés à l'extrapolation.\n",
    "- Le problème d'apprentissage d'un arbre de décision optimal est connu pour être NP-complet sous plusieurs aspects d'optimalité et même pour des concepts simples. Par conséquent, les algorithmes pratiques d'apprentissage d'arbres de décision sont basés sur des algorithmes heuristiques tels que l'algorithme glouton, où des décisions localement optimales sont prises à chaque nœud. De tels algorithmes ne peuvent pas garantir de renvoyer l'arbre de décision globalement optimal. Cela peut être atténué en entraînant de multiples arbres dans un apprenant de type ensemble, où les caractéristiques et les échantillons sont échantillonnés de manière aléatoire avec remplacement.\n",
    "- Il existe des concepts difficiles à apprendre car les arbres de décision ne les expriment pas facilement, tels que les problèmes XOR, parité ou multiplexeur.\n",
    "- Les apprenants d'arbres de décision créent des arbres biaisés si certaines classes dominent. Il est donc recommandé d'équilibrer le jeu de données avant l'ajustement de l'arbre de décision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='decision-trees'></a> 1.10.1. **Classification**<br/>([_Classification_](https://scikit-learn.org/stable/modules/tree.html#classification))\n",
    "\n",
    "[**`DecisionTreeClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) est une classe capable d'effectuer une classification multi-classes sur un ensemble de données.\n",
    "\n",
    "Comme avec d'autres classificateurs, [**`DecisionTreeClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) prend en entrée deux tableaux : un tableau `X`, creux ou dense, de forme `(n_samples, n_features)` contenant les échantillons d'entraînement, et un tableau `Y` de valeurs entières, de forme `(n_samples,)`, contenant les étiquettes de classe pour les échantillons d'entraînement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "X = [[0, 0], [1, 1]]\n",
    "Y = [0, 1]\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois ajusté, le modèle peut ensuite être utilisé pour prédire la classe des échantillons :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[2., 2.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas où il existe plusieurs classes ayant la même probabilité la plus élevée, le classifieur prédira la classe avec l'indice le plus bas parmi ces classes.\n",
    "\n",
    "En alternative à la sortie d'une classe spécifique, il est possible de prédire la probabilité de chaque classe, qui est la fraction d'échantillons d'entraînement de la classe dans une feuille :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba([[2., 2.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`DecisionTreeClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) est capable à la fois de la classification binaire (où les étiquettes sont $[-1, 1]$) et de la classification multi-classes (où les étiquettes sont $[0, \\ldots, K-1]$).\n",
    "\n",
    "En utilisant l'ensemble de données Iris, nous pouvons construire un arbre comme suit :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois entraîné, vous pouvez afficher l'arbre avec la fonction [**`plot_tree`**](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html#sklearn.tree.plot_tree) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0.9166666666666666, 'x[2] <= 2.45\\ngini = 0.667\\nsamples = 150\\nvalue = [50, 50, 50]'),\n",
       " Text(0.4230769230769231, 0.75, 'gini = 0.0\\nsamples = 50\\nvalue = [50, 0, 0]'),\n",
       " Text(0.5769230769230769, 0.75, 'x[3] <= 1.75\\ngini = 0.5\\nsamples = 100\\nvalue = [0, 50, 50]'),\n",
       " Text(0.3076923076923077, 0.5833333333333334, 'x[2] <= 4.95\\ngini = 0.168\\nsamples = 54\\nvalue = [0, 49, 5]'),\n",
       " Text(0.15384615384615385, 0.4166666666666667, 'x[3] <= 1.65\\ngini = 0.041\\nsamples = 48\\nvalue = [0, 47, 1]'),\n",
       " Text(0.07692307692307693, 0.25, 'gini = 0.0\\nsamples = 47\\nvalue = [0, 47, 0]'),\n",
       " Text(0.23076923076923078, 0.25, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 1]'),\n",
       " Text(0.46153846153846156, 0.4166666666666667, 'x[3] <= 1.55\\ngini = 0.444\\nsamples = 6\\nvalue = [0, 2, 4]'),\n",
       " Text(0.38461538461538464, 0.25, 'gini = 0.0\\nsamples = 3\\nvalue = [0, 0, 3]'),\n",
       " Text(0.5384615384615384, 0.25, 'x[0] <= 6.95\\ngini = 0.444\\nsamples = 3\\nvalue = [0, 2, 1]'),\n",
       " Text(0.46153846153846156, 0.08333333333333333, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 2, 0]'),\n",
       " Text(0.6153846153846154, 0.08333333333333333, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 1]'),\n",
       " Text(0.8461538461538461, 0.5833333333333334, 'x[2] <= 4.85\\ngini = 0.043\\nsamples = 46\\nvalue = [0, 1, 45]'),\n",
       " Text(0.7692307692307693, 0.4166666666666667, 'x[0] <= 5.95\\ngini = 0.444\\nsamples = 3\\nvalue = [0, 1, 2]'),\n",
       " Text(0.6923076923076923, 0.25, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1, 0]'),\n",
       " Text(0.8461538461538461, 0.25, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 0, 2]'),\n",
       " Text(0.9230769230769231, 0.4166666666666667, 'gini = 0.0\\nsamples = 43\\nvalue = [0, 0, 43]')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tree.plot_tree(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative ways to export trees\n",
    "\n",
    "Il existe également des moyens alternatifs d'exporter les arbres dans le format [Graphviz](https://www.graphviz.org/) en utilisant l'exportateur [**`export_graphviz`**](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html#sklearn.tree.export_graphviz). Si vous utilisez le gestionnaire de paquets [conda](https://conda.io/), les binaires de graphviz et le package Python peuvent être installés avec `conda install python-graphviz`.\n",
    "\n",
    "Alternativement, les binaires pour graphviz peuvent être téléchargés depuis la page d'accueil du projet graphviz, et le wrapper Python installé depuis pypi avec `pip install graphviz`.\n",
    "\n",
    "Ci-dessous se trouve un exemple d'exportation Graphviz de l'arbre ci-dessus entraîné sur l'ensemble de données complet de l'iris ; les résultats sont enregistrés dans un fichier de sortie `iris.pdf` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iris.pdf'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz \n",
    "dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"iris\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'exportateur [**`export_graphviz`**](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html#sklearn.tree.export_graphviz) prend également en charge diverses options esthétiques, notamment la coloration des nœuds en fonction de leur classe (ou de leur valeur pour la régression) et l'utilisation de noms de variables et de classes explicites si nécessaire. Les cahiers Jupyter rendent également ces graphiques automatiquement en ligne :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 7.1.0 (20230121.1956)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"896pt\" height=\"671pt\"\n",
       " viewBox=\"0.00 0.00 896.00 671.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 667)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-667 892,-667 892,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M539,-663C539,-663 398,-663 398,-663 392,-663 386,-657 386,-651 386,-651 386,-592 386,-592 386,-586 392,-580 398,-580 398,-580 539,-580 539,-580 545,-580 551,-586 551,-592 551,-592 551,-651 551,-651 551,-657 545,-663 539,-663\"/>\n",
       "<text text-anchor=\"start\" x=\"394\" y=\"-647.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 2.45</text>\n",
       "<text text-anchor=\"start\" x=\"431\" y=\"-632.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.667</text>\n",
       "<text text-anchor=\"start\" x=\"421\" y=\"-617.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 150</text>\n",
       "<text text-anchor=\"start\" x=\"408\" y=\"-602.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [50, 50, 50]</text>\n",
       "<text text-anchor=\"start\" x=\"422.5\" y=\"-587.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M438,-536.5C438,-536.5 341,-536.5 341,-536.5 335,-536.5 329,-530.5 329,-524.5 329,-524.5 329,-480.5 329,-480.5 329,-474.5 335,-468.5 341,-468.5 341,-468.5 438,-468.5 438,-468.5 444,-468.5 450,-474.5 450,-480.5 450,-480.5 450,-524.5 450,-524.5 450,-530.5 444,-536.5 438,-536.5\"/>\n",
       "<text text-anchor=\"start\" x=\"360.5\" y=\"-521.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"346\" y=\"-506.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 50</text>\n",
       "<text text-anchor=\"start\" x=\"337\" y=\"-491.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [50, 0, 0]</text>\n",
       "<text text-anchor=\"start\" x=\"343.5\" y=\"-476.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M440.87,-579.58C433.57,-568.77 425.68,-557.09 418.33,-546.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"421.37,-544.44 412.87,-538.12 415.57,-548.36 421.37,-544.44\"/>\n",
       "<text text-anchor=\"middle\" x=\"407.27\" y=\"-557.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M615,-544C615,-544 480,-544 480,-544 474,-544 468,-538 468,-532 468,-532 468,-473 468,-473 468,-467 474,-461 480,-461 480,-461 615,-461 615,-461 621,-461 627,-467 627,-473 627,-473 627,-532 627,-532 627,-538 621,-544 615,-544\"/>\n",
       "<text text-anchor=\"start\" x=\"476\" y=\"-528.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.75</text>\n",
       "<text text-anchor=\"start\" x=\"518.5\" y=\"-513.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"500\" y=\"-498.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 100</text>\n",
       "<text text-anchor=\"start\" x=\"491\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 50, 50]</text>\n",
       "<text text-anchor=\"start\" x=\"492\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M496.13,-579.58C501.82,-571.16 507.86,-562.2 513.74,-553.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"516.48,-555.7 519.17,-545.45 510.67,-551.78 516.48,-555.7\"/>\n",
       "<text text-anchor=\"middle\" x=\"524.78\" y=\"-565.04\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<path fill=\"#4de88e\" stroke=\"black\" d=\"M501,-425C501,-425 360,-425 360,-425 354,-425 348,-419 348,-413 348,-413 348,-354 348,-354 348,-348 354,-342 360,-342 360,-342 501,-342 501,-342 507,-342 513,-348 513,-354 513,-354 513,-413 513,-413 513,-419 507,-425 501,-425\"/>\n",
       "<text text-anchor=\"start\" x=\"356\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.95</text>\n",
       "<text text-anchor=\"start\" x=\"393\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.168</text>\n",
       "<text text-anchor=\"start\" x=\"387\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 54</text>\n",
       "<text text-anchor=\"start\" x=\"378\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 49, 5]</text>\n",
       "<text text-anchor=\"start\" x=\"375\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M506.58,-460.58C497.71,-451.71 488.25,-442.25 479.12,-433.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"481.82,-430.87 472.27,-426.27 476.87,-435.82 481.82,-430.87\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<path fill=\"#843de6\" stroke=\"black\" d=\"M735,-425C735,-425 594,-425 594,-425 588,-425 582,-419 582,-413 582,-413 582,-354 582,-354 582,-348 588,-342 594,-342 594,-342 735,-342 735,-342 741,-342 747,-348 747,-354 747,-354 747,-413 747,-413 747,-419 741,-425 735,-425\"/>\n",
       "<text text-anchor=\"start\" x=\"590\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.85</text>\n",
       "<text text-anchor=\"start\" x=\"627\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.043</text>\n",
       "<text text-anchor=\"start\" x=\"621\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 46</text>\n",
       "<text text-anchor=\"start\" x=\"612\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 45]</text>\n",
       "<text text-anchor=\"start\" x=\"614.5\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>2&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M588.42,-460.58C597.29,-451.71 606.75,-442.25 615.88,-433.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"618.13,-435.82 622.73,-426.27 613.18,-430.87 618.13,-435.82\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<path fill=\"#3de684\" stroke=\"black\" d=\"M271,-306C271,-306 136,-306 136,-306 130,-306 124,-300 124,-294 124,-294 124,-235 124,-235 124,-229 130,-223 136,-223 136,-223 271,-223 271,-223 277,-223 283,-229 283,-235 283,-235 283,-294 283,-294 283,-300 277,-306 271,-306\"/>\n",
       "<text text-anchor=\"start\" x=\"132\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.65</text>\n",
       "<text text-anchor=\"start\" x=\"166\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.041</text>\n",
       "<text text-anchor=\"start\" x=\"160\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 48</text>\n",
       "<text text-anchor=\"start\" x=\"151\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 47, 1]</text>\n",
       "<text text-anchor=\"start\" x=\"148\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M351.11,-341.58C332.36,-331.92 312.24,-321.55 293.08,-311.67\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"294.69,-308.56 284.19,-307.09 291.48,-314.78 294.69,-308.56\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<path fill=\"#c09cf2\" stroke=\"black\" d=\"M498,-306C498,-306 363,-306 363,-306 357,-306 351,-300 351,-294 351,-294 351,-235 351,-235 351,-229 357,-223 363,-223 363,-223 498,-223 498,-223 504,-223 510,-229 510,-235 510,-235 510,-294 510,-294 510,-300 504,-306 498,-306\"/>\n",
       "<text text-anchor=\"start\" x=\"359\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.55</text>\n",
       "<text text-anchor=\"start\" x=\"393\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n",
       "<text text-anchor=\"start\" x=\"391\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\n",
       "<text text-anchor=\"start\" x=\"382\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 4]</text>\n",
       "<text text-anchor=\"start\" x=\"380.5\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>3&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M430.5,-341.58C430.5,-333.79 430.5,-325.53 430.5,-317.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"434,-317.71 430.5,-307.71 427,-317.71 434,-317.71\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<path fill=\"#39e581\" stroke=\"black\" d=\"M115,-179.5C115,-179.5 12,-179.5 12,-179.5 6,-179.5 0,-173.5 0,-167.5 0,-167.5 0,-123.5 0,-123.5 0,-117.5 6,-111.5 12,-111.5 12,-111.5 115,-111.5 115,-111.5 121,-111.5 127,-117.5 127,-123.5 127,-123.5 127,-167.5 127,-167.5 127,-173.5 121,-179.5 115,-179.5\"/>\n",
       "<text text-anchor=\"start\" x=\"34.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"20\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 47</text>\n",
       "<text text-anchor=\"start\" x=\"11\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 47, 0]</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M154.53,-222.58C140.82,-211.12 125.92,-198.67 112.22,-187.22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"114.5,-184.56 104.58,-180.83 110.01,-189.93 114.5,-184.56\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<path fill=\"#8139e5\" stroke=\"black\" d=\"M249.5,-179.5C249.5,-179.5 157.5,-179.5 157.5,-179.5 151.5,-179.5 145.5,-173.5 145.5,-167.5 145.5,-167.5 145.5,-123.5 145.5,-123.5 145.5,-117.5 151.5,-111.5 157.5,-111.5 157.5,-111.5 249.5,-111.5 249.5,-111.5 255.5,-111.5 261.5,-117.5 261.5,-123.5 261.5,-123.5 261.5,-167.5 261.5,-167.5 261.5,-173.5 255.5,-179.5 249.5,-179.5\"/>\n",
       "<text text-anchor=\"start\" x=\"174.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"164\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n",
       "<text text-anchor=\"start\" x=\"155\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n",
       "<text text-anchor=\"start\" x=\"153.5\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>4&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M203.5,-222.58C203.5,-212.43 203.5,-201.5 203.5,-191.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"207,-191.37 203.5,-181.37 200,-191.37 207,-191.37\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<path fill=\"#8139e5\" stroke=\"black\" d=\"M383.5,-179.5C383.5,-179.5 291.5,-179.5 291.5,-179.5 285.5,-179.5 279.5,-173.5 279.5,-167.5 279.5,-167.5 279.5,-123.5 279.5,-123.5 279.5,-117.5 285.5,-111.5 291.5,-111.5 291.5,-111.5 383.5,-111.5 383.5,-111.5 389.5,-111.5 395.5,-117.5 395.5,-123.5 395.5,-123.5 395.5,-167.5 395.5,-167.5 395.5,-173.5 389.5,-179.5 383.5,-179.5\"/>\n",
       "<text text-anchor=\"start\" x=\"308.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"298\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n",
       "<text text-anchor=\"start\" x=\"289\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 3]</text>\n",
       "<text text-anchor=\"start\" x=\"287.5\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M397.97,-222.58C389.3,-211.66 379.91,-199.85 371.17,-188.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"373.92,-186.7 364.96,-181.05 368.44,-191.05 373.92,-186.7\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<path fill=\"#9cf2c0\" stroke=\"black\" d=\"M571,-187C571,-187 426,-187 426,-187 420,-187 414,-181 414,-175 414,-175 414,-116 414,-116 414,-110 420,-104 426,-104 426,-104 571,-104 571,-104 577,-104 583,-110 583,-116 583,-116 583,-175 583,-175 583,-181 577,-187 571,-187\"/>\n",
       "<text text-anchor=\"start\" x=\"422\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">sepal length (cm) ≤ 6.95</text>\n",
       "<text text-anchor=\"start\" x=\"461\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n",
       "<text text-anchor=\"start\" x=\"459\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n",
       "<text text-anchor=\"start\" x=\"450\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 1]</text>\n",
       "<text text-anchor=\"start\" x=\"443\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>7&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M454.28,-222.58C459.07,-214.34 464.16,-205.58 469.12,-197.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"472.09,-198.91 474.08,-188.51 466.03,-195.4 472.09,-198.91\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<path fill=\"#39e581\" stroke=\"black\" d=\"M480,-68C480,-68 377,-68 377,-68 371,-68 365,-62 365,-56 365,-56 365,-12 365,-12 365,-6 371,0 377,0 377,0 480,0 480,0 486,0 492,-6 492,-12 492,-12 492,-56 492,-56 492,-62 486,-68 480,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"399.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"389\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n",
       "<text text-anchor=\"start\" x=\"380\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 0]</text>\n",
       "<text text-anchor=\"start\" x=\"373\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M472.43,-103.73C467.01,-95.24 461.28,-86.28 455.82,-77.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"458.94,-76.12 450.6,-69.57 453.04,-79.89 458.94,-76.12\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<path fill=\"#8139e5\" stroke=\"black\" d=\"M614.5,-68C614.5,-68 522.5,-68 522.5,-68 516.5,-68 510.5,-62 510.5,-56 510.5,-56 510.5,-12 510.5,-12 510.5,-6 516.5,0 522.5,0 522.5,0 614.5,0 614.5,0 620.5,0 626.5,-6 626.5,-12 626.5,-12 626.5,-56 626.5,-56 626.5,-62 620.5,-68 614.5,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"539.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"529\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n",
       "<text text-anchor=\"start\" x=\"520\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n",
       "<text text-anchor=\"start\" x=\"518.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>9&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M524.57,-103.73C529.99,-95.24 535.72,-86.28 541.18,-77.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"543.96,-79.89 546.4,-69.57 538.06,-76.12 543.96,-79.89\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>13</title>\n",
       "<path fill=\"#c09cf2\" stroke=\"black\" d=\"M737,-306C737,-306 592,-306 592,-306 586,-306 580,-300 580,-294 580,-294 580,-235 580,-235 580,-229 586,-223 592,-223 592,-223 737,-223 737,-223 743,-223 749,-229 749,-235 749,-235 749,-294 749,-294 749,-300 743,-306 737,-306\"/>\n",
       "<text text-anchor=\"start\" x=\"588\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">sepal length (cm) ≤ 5.95</text>\n",
       "<text text-anchor=\"start\" x=\"627\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n",
       "<text text-anchor=\"start\" x=\"625\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n",
       "<text text-anchor=\"start\" x=\"616\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 2]</text>\n",
       "<text text-anchor=\"start\" x=\"614.5\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>12&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M664.5,-341.58C664.5,-333.79 664.5,-325.53 664.5,-317.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"668,-317.71 664.5,-307.71 661,-317.71 668,-317.71\"/>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>16</title>\n",
       "<path fill=\"#8139e5\" stroke=\"black\" d=\"M876,-298.5C876,-298.5 779,-298.5 779,-298.5 773,-298.5 767,-292.5 767,-286.5 767,-286.5 767,-242.5 767,-242.5 767,-236.5 773,-230.5 779,-230.5 779,-230.5 876,-230.5 876,-230.5 882,-230.5 888,-236.5 888,-242.5 888,-242.5 888,-286.5 888,-286.5 888,-292.5 882,-298.5 876,-298.5\"/>\n",
       "<text text-anchor=\"start\" x=\"798.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"784\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 43</text>\n",
       "<text text-anchor=\"start\" x=\"775\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 43]</text>\n",
       "<text text-anchor=\"start\" x=\"777.5\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;16 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>12&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M721.51,-341.58C737.78,-329.9 755.48,-317.19 771.68,-305.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"773.71,-308.42 779.79,-299.74 769.63,-302.73 773.71,-308.42\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>14</title>\n",
       "<path fill=\"#39e581\" stroke=\"black\" d=\"M716,-179.5C716,-179.5 613,-179.5 613,-179.5 607,-179.5 601,-173.5 601,-167.5 601,-167.5 601,-123.5 601,-123.5 601,-117.5 607,-111.5 613,-111.5 613,-111.5 716,-111.5 716,-111.5 722,-111.5 728,-117.5 728,-123.5 728,-123.5 728,-167.5 728,-167.5 728,-173.5 722,-179.5 716,-179.5\"/>\n",
       "<text text-anchor=\"start\" x=\"635.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"625\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n",
       "<text text-anchor=\"start\" x=\"616\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 0]</text>\n",
       "<text text-anchor=\"start\" x=\"609\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>13&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M664.5,-222.58C664.5,-212.43 664.5,-201.5 664.5,-191.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"668,-191.37 664.5,-181.37 661,-191.37 668,-191.37\"/>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>15</title>\n",
       "<path fill=\"#8139e5\" stroke=\"black\" d=\"M850.5,-179.5C850.5,-179.5 758.5,-179.5 758.5,-179.5 752.5,-179.5 746.5,-173.5 746.5,-167.5 746.5,-167.5 746.5,-123.5 746.5,-123.5 746.5,-117.5 752.5,-111.5 758.5,-111.5 758.5,-111.5 850.5,-111.5 850.5,-111.5 856.5,-111.5 862.5,-117.5 862.5,-123.5 862.5,-123.5 862.5,-167.5 862.5,-167.5 862.5,-173.5 856.5,-179.5 850.5,-179.5\"/>\n",
       "<text text-anchor=\"start\" x=\"775.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"765\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n",
       "<text text-anchor=\"start\" x=\"756\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 2]</text>\n",
       "<text text-anchor=\"start\" x=\"754.5\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;15 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>13&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M713.47,-222.58C727.18,-211.12 742.08,-198.67 755.78,-187.22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"757.99,-189.93 763.42,-180.83 753.5,-184.56 757.99,-189.93\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x22007c87b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "    feature_names=iris.feature_names,  \n",
    "    class_names=iris.target_names,  \n",
    "    filled=True, rounded=True,  \n",
    "    special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "display(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_iris_dtc_001.png\"\n",
    "    alt=\"Decision surface of decision trees trained on pairs of features\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "De plus, l'arbre peut également être exporté au format textuel à l'aide de la fonction [**`export_text`**](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_text.html#sklearn.tree.export_text). Cette méthode ne nécessite pas l'installation de bibliothèques externes et est plus compacte :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- petal width (cm) <= 0.80\n",
      "|   |--- class: 0\n",
      "|--- petal width (cm) >  0.80\n",
      "|   |--- petal width (cm) <= 1.75\n",
      "|   |   |--- class: 1\n",
      "|   |--- petal width (cm) >  1.75\n",
      "|   |   |--- class: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_text\n",
    "iris = load_iris()\n",
    "decision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)\n",
    "decision_tree = decision_tree.fit(iris.data, iris.target)\n",
    "r = export_text(decision_tree, feature_names=iris['feature_names'])\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**Tracer la surface de décision des arbres de décision entraînés sur le jeu de données Iris**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_10_tree/plot_iris_dtc.ipynb)<br/>([_Plot the decision surface of decision trees trained on the iris dataset_](https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html))\n",
    "\n",
    "#### [**Comprendre la structure de l'arbre de décision**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_10_tree/plot_unveil_tree_structure.ipynb)<br/>([_Understanding the decision tree structure_](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='regression'></a> 1.10.2. **Régression**<br/>([_Regression_](https://scikit-learn.org/stable/modules/tree.html#regression))\n",
    "\n",
    "Les arbres de décision peuvent également être appliqués aux problèmes de régression en utilisant la classe [**`DecisionTreeRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor).\n",
    "\n",
    "Comme dans le cadre de la classification, la méthode d'ajustement prendra en argument les tableaux `X` et `y`, sauf que dans ce cas, `y` est censé contenir des valeurs en virgule flottante au lieu de valeurs entières :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "X = [[0, 0], [2, 2]]\n",
    "y = [0.5, 2.5]\n",
    "clf = tree.DecisionTreeRegressor()\n",
    "clf = clf.fit(X, y)\n",
    "clf.predict([[1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**Régression à l'aide de l'arbre de décision**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_10_tree/plot_tree_regression.ipynb)<br/>([_Decision Tree Regression_](https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='regression'></a> 1.10.3. [**Problèmes à sorties multiples**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_10_tree.ipynb#multi-output-problems)<br/>([_Multi-output problems_](https://scikit-learn.org/stable/modules/tree.html#multi-output-problems))\n",
    "\n",
    "Un problème à sorties multiples est un problème d'apprentissage supervisé avec plusieurs sorties à prédire, c'est-à-dire lorsque `Y` est un tableau 2D de forme `(n_samples, n_outputs)`.\n",
    "\n",
    "Lorsqu'il n'y a aucune corrélation entre les sorties, une manière très simple de résoudre ce genre de problème consiste à construire $n$ modèles indépendants, c'est-à-dire un pour chaque sortie, puis à utiliser ces modèles pour prédire indépendamment chacune des $n$ sorties. Cependant, parce qu'il est probable que les valeurs de sortie liées à la même entrée soient elles-mêmes corrélées, une meilleure manière est souvent de construire un seul modèle capable de prédire simultanément ces $n$ sorties. Tout d'abord, cela nécessite moins de temps d'entraînement car un seul estimateur est construit. Deuxièmement, la précision de généralisation de l'estimateur résultant est souvent améliorée.\n",
    "\n",
    "En ce qui concerne les arbres de décision, cette stratégie peut être facilement utilisée pour prendre en charge les problèmes à sorties multiples. Cela nécessite les changements suivants :\n",
    "- Stocker $n$ valeurs de sortie dans les feuilles, au lieu d'une seule ;\n",
    "- Utiliser des critères de division qui calculent la réduction moyenne sur les $n$ sorties.\n",
    "\n",
    "Ce module offre une prise en charge pour les problèmes à sorties multiples en mettant en œuvre cette stratégie tant avec [**`DecisionTreeClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) qu'avec [**`DecisionTreeRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor). Si un arbre de décision est ajusté sur un tableau de sortie `Y` de forme `(n_samples, n_outputs)`, l'estimateur résultant :\n",
    "- Produira `n_output` valeurs lors de l'appel à `predict`;\n",
    "- Produira une liste de `n_output` tableaux de probabilités de classe lors de l'appel à `predict_proba`.\n",
    "\n",
    "L'utilisation d'arbres de décision à sorties multiples pour la régression est illustrée dans [**Régression multi-sortie avec les arbres de décision**](https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression_multioutput.html#sphx-glr-auto-examples-tree-plot-tree-regression-multioutput-py). Dans cet exemple, l'entrée `X` est une seule valeur réelle et les sorties `Y` sont le sinus et le cosinus de `X`.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_tree_regression_multioutput_001.png\"\n",
    "    alt=\"Régression multi-sortie avec les arbres de décision\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "L'utilisation d'arbres de décision à sorties multiples pour la classification est illustrée dans [**Complétion de visage avec des estimateurs multi-sorties**](https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_multioutput_face_completion.html#sphx-glr-auto-examples-miscellaneous-plot-multioutput-face-completion-py). Dans cet exemple, les entrées `X` sont les pixels de la moitié supérieure des visages et les sorties `Y` sont les pixels de la moitié inférieure de ces visages.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_multioutput_face_completion_001.png\"\n",
    "    alt=\"Complétion de visage avec des estimateurs multi-sorties\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**Régression multi-sorties avec les arbres de décision**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_10_tree/plot_tree_regression_multioutput.ipynb)<br/>([_Multi-output Decision Tree Regression_](https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression_multioutput.html))\n",
    "\n",
    "#### [**Complétion de visages avec des estimateurs multi-sorties**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/misc/plot_multioutput_face_completion.ipynb)<br/>([*Face completion with multi-output estimators*](https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_multioutput_face_completion.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Références\n",
    "\n",
    "🔬 M. Dumont et al, [**“Fast multi-class image annotation with random subwindows and multiple output randomized trees”**](https://services.montefiore.uliege.be//stochastic/pubs/2009/DMWG09/dumont-visapp09-shortpaper.pdf), International Conference on Computer Vision Theory and Applications 2009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='regression'></a> 1.10.4. [**Complexité**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_10_tree.ipynb#complexity)<br/>([_Complexity_](https://scikit-learn.org/stable/modules/tree.html#complexity))\n",
    "\n",
    "Posons $N$ = `n_samples` et $D$ = `n_features`.\n",
    "\n",
    "En général, le coût en temps d'exécution pour construire un arbre binaire équilibré est $\\mathcal{O}(D N\\log(N))$ et le temps de requête est $\\mathcal{O}(\\log(N))$. Bien que l'algorithme de construction de l'arbre tente de générer des arbres équilibrés, ils ne seront pas toujours équilibrés. En supposant que les sous-arbres restent approximativement équilibrés, le coût à chaque nœud consiste à rechercher dans $\\mathcal{O}(D)$ pour trouver la caractéristique qui offre la plus grande réduction dans le critère d'impureté, par exemple, la perte logarithmique (qui est équivalente à un gain d'information). Cela a un coût de $\\mathcal{O}(D N\\log(N))$ à chaque nœud, conduisant à un coût total sur l'ensemble de l'arbre (en additionnant le coût à chaque nœud) de $\\mathcal{O}(D N^{2}\\log(N))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='regression'></a> 1.10.5. **Conseils d'utilisation pratique**<br/>([_Tips on practical use_](https://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use))\n",
    "\n",
    "- Les arbres de décision ont tendance à surajuster les données comportant un grand nombre de caractéristiques. Obtenir le bon rapport entre le nombre d'échantillons et le nombre de caractéristiques est important, car un arbre avec peu d'échantillons dans un espace de grande dimension a de fortes chances de surajuster.\n",
    "\n",
    "- Envisagez de réaliser une réduction de la dimensionnalité ([**PCA** (2.5.1)](https://scikit-learn.org/stable/modules/decomposition.html#pca), [**ICA** (2.5.6)](https://scikit-learn.org/stable/modules/decomposition.html#ica), ou [**Sélection de caractéristiques** (1.13)](https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection)) au préalable pour donner à votre arbre une meilleure chance de trouver des caractéristiques discriminantes.\n",
    "\n",
    "- [**Comprendre la structure de l'arbre de décision**](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py) vous aidera à mieux comprendre comment l'arbre de décision effectue des prédictions, ce qui est important pour comprendre les caractéristiques importantes des données.\n",
    "\n",
    "- Visualisez votre arbre pendant l'entraînement en utilisant la fonction d'exportation. Utilisez `max_depth=3` comme profondeur initiale de l'arbre pour avoir une idée de l'ajustement de l'arbre à vos données, puis augmentez la profondeur.\n",
    "\n",
    "- Rappelez-vous que le nombre d'échantillons nécessaires pour peupler l'arbre double à chaque niveau supplémentaire de croissance de l'arbre. Utilisez `max_depth` pour contrôler la taille de l'arbre et éviter le surajustement.\n",
    "\n",
    "- Utilisez `min_samples_split` ou `min_samples_leaf` pour vous assurer que plusieurs échantillons influencent chaque décision de l'arbre en contrôlant quelles divisions seront prises en compte. Un nombre très faible signifie généralement que l'arbre surajustera, tandis qu'un nombre élevé empêchera l'arbre d'apprendre les données. Essayez `min_samples_leaf=5` comme valeur initiale. Si la taille de l'échantillon varie beaucoup, un nombre flottant peut être utilisé comme pourcentage dans ces deux paramètres. Alors que `min_samples_split` peut créer des feuilles arbitrairement petites, `min_samples_leaf` garantit qu'une taille minimale est attribuée à chaque feuille, évitant ainsi les nœuds de feuille à faible variance et surajustés dans les problèmes de régression. Pour la classification avec peu de classes, `min_samples_leaf=1` est souvent le meilleur choix.<br/><br/>\n",
    "Notez que `min_samples_split` tient compte des échantillons directement et indépendamment de `sample_weight`, s'ils sont fournis (par exemple, un nœud avec $m$ échantillons pondérés est toujours traité comme ayant exactement $m$ échantillons). Considérez `min_weight_fraction_leaf` ou `min_impurity_decrease` si la prise en compte des poids des échantillons est nécessaire lors des divisions.\n",
    "\n",
    "- Équilibrez votre ensemble de données avant l'entraînement pour éviter que l'arbre ne soit biaisé en faveur des classes dominantes. L'équilibrage des classes peut se faire en échantillonnant un nombre égal d'échantillons de chaque classe, ou de préférence en normalisant la somme des poids des échantillons (`sample_weight`) pour chaque classe à la même valeur. Notez également que les critères de pré-élagage basés sur les poids, tels que `min_weight_fraction_leaf`, seront alors moins biaisés en faveur des classes dominantes que les critères qui ne tiennent pas compte des poids des échantillons, tels que `min_samples_leaf`.\n",
    "\n",
    "- Si les échantillons sont pondérés, il sera plus facile d'optimiser la structure de l'arbre en utilisant un critère de pré-élagage basé sur les poids, tel que `min_weight_fraction_leaf`, qui garantit que les nœuds de feuille contiennent au moins une fraction de la somme totale des poids des échantillons.\n",
    "\n",
    "- Tous les arbres de décision utilisent des tableaux `np.float32` en interne. Si les données d'entraînement ne sont pas dans ce format, une copie de l'ensemble de données sera effectuée.\n",
    "\n",
    "- Si la matrice d'entrée `X` est très creuse, il est recommandé de la convertir en matrice creuse de type `csc_matrix` avant d'appeler `fit` et en matrice creuse de type `csr_matrix` avant d'appeler `predict`. Le temps d'entraînement peut être de l'ordre de grandeur plus rapide pour une matrice d'entrée clairsemée par rapport à une matrice dense lorsque les caractéristiques ont des valeurs nulles dans la plupart des échantillons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='tree-algorithms-id3-c4-5-c5-0-and-cart'></a> 1.10.6. **Algorithmes d'arbres : ID3, C4.5, C5.0 et CART**<br/>([_Tree algorithms: ID3, C4.5, C5.0 and CART_](https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart))\n",
    "\n",
    "Quels sont les différents algorithmes d'arbres de décision et comment diffèrent-ils les uns des autres ? Lequel est implémenté dans scikit-learn ?\n",
    "\n",
    "[**ID3**](https://en.wikipedia.org/wiki/ID3_algorithm) (Iterative Dichotomiser 3) a été développé en 1986 par Ross Quinlan. L'algorithme crée un arbre à ramifications multiples, en trouvant pour chaque nœud (c'est-à-dire de manière exhaustive) la caractéristique catégorielle qui produira le gain d'information le plus élevé pour les cibles catégorielles. Les arbres sont développés jusqu'à leur taille maximale, puis une étape de taille est généralement appliquée pour améliorer la capacité de l'arbre à généraliser sur des données non observées.\n",
    "\n",
    "C4.5 est le successeur de l'ID3 et a supprimé la restriction selon laquelle les caractéristiques doivent être catégorielles en définissant dynamiquement une caractéristique discrète (basée sur des variables numériques) qui partitionne la valeur de la caractéristique continue en un ensemble discret d'intervalles. C4.5 convertit les arbres entraînés (c'est-à-dire la sortie de l'algorithme ID3) en ensembles de règles du type \"si-alors\". L'exactitude de chaque règle est ensuite évaluée pour déterminer l'ordre dans lequel elles doivent être appliquées. L'élagage est effectué en supprimant la prémisse d'une règle si l'exactitude de la règle s'améliore sans elle.\n",
    "\n",
    "C5.0 est la dernière version de Quinlan, publiée sous licence propriétaire. Il utilise moins de mémoire et construit des ensembles de règles plus petits que C4.5 tout en étant plus précis.\n",
    "\n",
    "CART (Classification and Regression Trees) est très similaire à C4.5, mais diffère en ce qu'il prend en charge les variables cibles numériques (régression) et ne calcule pas d'ensembles de règles. CART construit des arbres binaires en utilisant la caractéristique et le seuil qui produisent le plus grand gain d'information à chaque nœud.\n",
    "\n",
    "scikit-learn utilise une version optimisée de l'algorithme CART ; cependant, l'implémentation de scikit-learn ne prend pas en charge les variables catégorielles pour le moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='mathematical-formulation'></a> 1.10.7. [**Formulation mathématique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_10_tree.ipynb#mathematical-formulation)<br/>([_Mathematical formulation_](https://scikit-learn.org/stable/modules/tree.html#mathematical-formulation))\n",
    "\n",
    "Étant donné des vecteurs d'apprentissage $x_i \\in R^n$, $i=1, \\ldots, l$ et un vecteur d'étiquettes $y \\in R^l$, un arbre de décision partitionne récursivement l'espace des caractéristiques de telle sorte que les échantillons ayant les mêmes étiquettes ou des valeurs cibles similaires soient regroupés.\n",
    "\n",
    "Laissez les données au nœud $m$ être représentées par $Q_m$ avec $n_m$ échantillons. Pour chaque division candidate $\\theta = (j, t_m)$ composée d'une caractéristique $j$ et d'une valeur seuil $t_m$, les données sont partitionnées en sous-ensembles $Q_m^{left}(\\theta)$ et $Q_m^{right}(\\theta)$ :\n",
    "\n",
    "$$\n",
    "\\begin{align*}\\begin{aligned}\n",
    "Q_m^{left}(\\theta) = \\{(x, y) | x_j \\leq t_m\\}\\\\\n",
    "Q_m^{right}(\\theta) = Q_m \\setminus Q_m^{left}(\\theta)\n",
    "\\end{aligned}\\end{align*}\n",
    "$$\n",
    "\n",
    "La qualité d'une division candidate du nœud $m$ est ensuite calculée à l'aide d'une fonction d'impureté ou d'une fonction de perte $H()$, le choix dépendant de la tâche à résoudre (classification ou régression) :\n",
    "\n",
    "$$\n",
    "G(Q_m, \\theta) = \\frac{n_m^{left}}{n_m} H(Q_m^{left}(\\theta))\n",
    "+ \\frac{n_m^{right}}{n_m} H(Q_m^{right}(\\theta))\n",
    "$$\n",
    "\n",
    "Sélectionnez les paramètres qui minimisent l'impureté :\n",
    "\n",
    "$$\n",
    "\\theta^* = \\operatorname{argmin}_\\theta  G(Q_m, \\theta)\n",
    "$$\n",
    "\n",
    "Récursivement, pour les sous-ensembles $Q_m^{left}(\\theta^*)$ et $Q_m^{right}(\\theta^*)$, jusqu'à ce que la profondeur maximale autorisée soit atteinte, $n_m < \\min_{samples}$ ou $n_m = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='classification-criteria'></a> 1.10.7.1. [**Critères de classification**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_10_tree.ipynb#classification-criteria)<br/>([_Classification criteria_](https://scikit-learn.org/stable/modules/tree.html#classification-criteria))\n",
    "\n",
    "Si une cible est une issue de classification prenant des valeurs $0, 1, \\ldots, K-1$, pour le nœud $m$, la proportion d'observations de la classe $k$ dans le nœud $m$ est définie comme suit :\n",
    "\n",
    "$$\n",
    "p_{mk} = \\frac{1}{n_m} \\sum_{y \\in Q_m} I(y = k)\n",
    "$$\n",
    "\n",
    "où $I(y = k)$ est une fonction indicatrice évaluée à 1 si $y = k$ et à 0 sinon. Si c'est un nœud terminal, `predict_proba` pour cette région est défini comme $p_{mk}$. Les mesures courantes d'impureté sont les suivantes.\n",
    "\n",
    "Indice de Gini :\n",
    "\n",
    "$$\n",
    "H(Q_m) = \\sum_k p_{mk} (1 - p_{mk})\n",
    "$$\n",
    "\n",
    "Entropie ou Perte de Log :\n",
    "\n",
    "$$\n",
    "H(Q_m) = - \\sum_k p_{mk} \\log(p_{mk})\n",
    "$$\n",
    "\n",
    "#### Entropie de Shannon\n",
    "\n",
    "Le critère d'entropie calcule l'entropie de Shannon des classes possibles. Il prend les fréquences de classe des points de données d'entraînement qui ont atteint un nœud terminal donné $m$ comme probabilité. Utiliser l'**entropie de Shannon comme critère de division du nœud de l'arbre équivaut à minimiser la perte de log** (également appelée entropie croisée et déviance multinomiale) entre les vraies étiquettes $y_i$ et les prédictions probabilistes $T_k(x_i)$ du modèle d'arbre $T$ pour la classe $k$.\n",
    "\n",
    "Pour voir cela, rappelons d'abord que la perte de log d'un modèle d'arbre $T$ calculée sur un ensemble de données $D$ est définie comme suit :\n",
    "\n",
    "$$\n",
    "\\mathrm{LL}(D, T) = -\\frac{1}{n} \\sum_{(x_i, y_i) \\in D} \\sum_k I(y_i = k) \\log(T_k(x_i))\n",
    "$$\n",
    "\n",
    "où $D$ est un ensemble de données d'entraînement de $n$ paires $(x_i, y_i)$.\n",
    "\n",
    "Dans un arbre de classification, les probabilités de classe prédites au sein des nœuds terminaux sont constantes, c'est-à-dire que pour tous $(x_i, y_i) \\in Q_m$, on a : $T_k(x_i) = p_{mk}$ pour chaque classe $k$.\n",
    "\n",
    "Cette propriété permet de réécrire $\\mathrm{LL}(D, T)$ comme la somme des entropies de Shannon calculées pour chaque feuille de $T$ pondérées par le nombre de points de données d'entraînement qui ont atteint chaque feuille :\n",
    "\n",
    "$$\n",
    "\\mathrm{LL}(D, T) = \\sum_{m \\in T} \\frac{n_m}{n} H(Q_m)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='regression-criteria'></a> 1.10.7.2. [**Critères de régression**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_10_tree.ipynb#regression-criteria)<br/>([_Regression criteria_](https://scikit-learn.org/stable/modules/tree.html#regression-criteria))\n",
    "\n",
    "Si la cible est une valeur continue, alors pour le nœud $m$, les critères courants à minimiser pour déterminer les emplacements des futures divisions sont l'Erreur Quadratique Moyenne (MSE ou erreur $\\ell_2$), la déviance de Poisson ainsi que l'Erreur Absolue Moyenne (MAE ou erreur $\\ell_1$). La MSE et la déviance de Poisson définissent toutes deux la valeur prédite des nœuds terminaux comme étant la valeur moyenne apprise $\\bar{y}_m$ du nœud, tandis que la MAE définie la valeur prédite des nœuds terminaux comme la médiane $\\text{median}(y)_m$.\n",
    "\n",
    "Erreur Quadratique Moyenne :\n",
    "\n",
    "$$\n",
    "\\begin{align*}\\begin{aligned}\n",
    "\\bar{y}_m = \\frac{1}{n_m} \\sum_{y \\in Q_m} y\\\\\n",
    "H(Q_m) = \\frac{1}{n_m} \\sum_{y \\in Q_m} (y - \\bar{y}_m)^2\n",
    "\\end{aligned}\\end{align*}\n",
    "$$\n",
    "\n",
    "Demi-déviance de Poisson :\n",
    "\n",
    "$$\n",
    "H(Q_m) = \\frac{1}{n_m} \\sum_{y \\in Q_m} (y \\log\\frac{y}{\\bar{y}_m}\n",
    "- y + \\bar{y}_m)\n",
    "$$\n",
    "\n",
    "Définir `criterion=\"poisson\"` peut être un bon choix si votre cible est un nombre ou une fréquence (nombre par unité). En tout cas, $y \\ge 0$ est une condition nécessaire pour utiliser ce critère. Notez que son ajustement est beaucoup plus lent que le critère MSE.\n",
    "\n",
    "Erreur Absolue Moyenne :\n",
    "\n",
    "$$\n",
    "\\begin{align*}\\begin{aligned}\n",
    "\\text{median}(y)_m = \\underset{y \\in Q_m}{\\text{median}}(y)\\\\\n",
    "H(Q_m) = \\frac{1}{n_m} \\sum_{y \\in Q_m} |y - \\text{median}(y)_m|\n",
    "\\end{aligned}\\end{align*}\n",
    "$$\n",
    "\n",
    "Notez que son ajustement est beaucoup plus lent que le critère MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='missing-values-support'></a> 1.10.8. **Prise en charge des valeurs manquantes**<br/>([_Missing Values Support_](https://scikit-learn.org/stable/modules/tree.html#missing-values-support))\n",
    "\n",
    "[**`DecisionTreeClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) et [**`DecisionTreeRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor) prennent en charge les valeurs manquantes intégrées lorsque `splitter='best'` et que le critère est `'gini'`, `'entropy'`, ou `'log_loss'`, pour la classification, ou `'squared_error'`, `'friedman_mse'`, ou 'poisson' pour la régression.\n",
    "\n",
    "Pour chaque seuil potentiel sur les données non manquantes, le séparateur évalue la division avec toutes les valeurs manquantes allant vers le nœud gauche ou le nœud droit.\n",
    "\n",
    "Les décisions sont prises comme suit :\n",
    "\n",
    "- Par défaut lors de la prédiction, les échantillons avec des valeurs manquantes sont classés avec la classe utilisée dans la division trouvée lors de l'apprentissage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([0, 1, 6, np.nan]).reshape(-1, 1)\n",
    "y = [0, 0, 1, 1]\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=0).fit(X, y)\n",
    "tree.predict(X)\n",
    "# array([0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Si l'évaluation du critère est la même pour les deux nœuds, alors l'égalité pour les valeurs manquantes au moment de la prédiction est rompue en allant vers le nœud de droite. Le séparateur vérifie également la division où toutes les valeurs manquantes vont vers un enfant et les valeurs non manquantes vont vers l'autre :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([np.nan, -1, np.nan, 1]).reshape(-1, 1)\n",
    "y = [0, 0, 1, 1]\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=0).fit(X, y)\n",
    "\n",
    "X_test = np.array([np.nan]).reshape(-1, 1)\n",
    "tree.predict(X_test)\n",
    "# array([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Si aucune valeur manquante n'est observée pendant l'apprentissage pour une caractéristique donnée, alors lors de la prédiction, les valeurs manquantes sont associées à l'enfant ayant le plus d'échantillons :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([0, 1, 2, 3]).reshape(-1, 1)\n",
    "y = [0, 1, 1, 1]\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=0).fit(X, y)\n",
    "\n",
    "X_test = np.array([np.nan]).reshape(-1, 1)\n",
    "tree.predict(X_test)\n",
    "# array([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='minimal-cost-complexity-pruning'></a> 1.10.9. **Élagage de complexité minimale-coût**<br/>([_Minimal Cost-Complexity Pruning_](https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning))\n",
    "\n",
    "L'élagage de complexité minimale-coût est un algorithme utilisé pour élaguer un arbre afin d'éviter la surajustement, comme décrit dans le chapitre 3 de [BRE]. Cet algorithme est paramétré par $\\alpha \\ge 0$, connu sous le nom de paramètre de complexité. Le paramètre de complexité est utilisé pour définir la mesure de coût-complexité, $R_\\alpha(T)$, d'un arbre donné $T$ :\n",
    "\n",
    "$$\n",
    "R_\\alpha(T) = R(T) + \\alpha|\\widetilde{T}|\n",
    "$$\n",
    "\n",
    "où $|\\widetilde{T}|$ est le nombre de nœuds terminaux dans $T$ et $R(T)$ est traditionnellement défini comme le taux total de classification erronée des nœuds terminaux. En revanche, scikit-learn utilise l'impureté totale pondérée par les échantillons des nœuds terminaux pour $R(T)$. Comme indiqué ci-dessus, l'impureté d'un nœud dépend du critère. L'élagage de complexité minimale-coût trouve le sous-arbre de $T$ qui minimise $R_\\alpha(T)$.\n",
    "\n",
    "La mesure de complexité de coût d'un seul nœud est $R_\\alpha(t)=R(t)+\\alpha$. La branche, $T_t$, est définie comme un arbre où le nœud $t$ en est la racine. En général, l'impureté d'un nœud est supérieure à la somme des impuretés de ses nœuds terminaux, $R(T_t)<R(t)$. Cependant, la mesure de complexité de coût d'un nœud, $t$, et de sa branche, $T_t$, peut être égale en fonction de $\\alpha$. Nous définissons l'efficacité $\\alpha$ d'un nœud comme la valeur où ils sont égaux, $R_\\alpha(T_t)=R_\\alpha(t)$ ou $\\displaystyle \\alpha_{eff}(t)=\\frac{R(t)-R(T_t)}{|T|-1}$. Un nœud non terminal avec la plus petite valeur d'$\\alpha_{eff}$ est le maillon le plus faible et sera élagué. Ce processus s'arrête lorsque l'$\\alpha_{eff}$ minimal de l'arbre élagué est supérieur au paramètre `ccp_alpha`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemples\n",
    "\n",
    "#### [**Élagage postérieur des arbres de décision avec élagage de complexité minimale-coût**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_10_tree/plot_cost_complexity_pruning.ipynb)<br/>([_Post pruning decision trees with cost complexity pruning_](https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Références\n",
    "\n",
    "📚 [BRE] L. Breiman, J. Friedman, R. Olshen, and C. Stone. **“Classification and Regression Trees”**. Wadsworth, Belmont, CA, 1984.\n",
    "\n",
    "🌐 [**Wikipedia entry on _Decision tree learning_**](https://en.wikipedia.org/wiki/Decision_tree_learning)\n",
    "\n",
    "🌐 [**Wikipedia entry on _Predictive analytics_**](https://en.wikipedia.org/wiki/Predictive_analytics)\n",
    "\n",
    "📚J.R. Quinlan. **“C4.5: programs for machine learning”**. Morgan Kaufmann, 1993.\n",
    "\n",
    "📚 T. Hastie, R. Tibshirani et J. Friedman, [**“Elements of Statistical Learning Ed. 2”**](https://hastie.su.domains/Papers/ESLII.pdf), Springer, 2009."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
