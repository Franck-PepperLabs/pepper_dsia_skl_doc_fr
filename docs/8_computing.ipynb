{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. [**Calculer avec scikit-learn**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/8_computing.ipynb)</br>([*Computing with scikit-learn*](https://scikit-learn.org/stable/computing.html))\n",
    "\n",
    "✔ 8.1. [**Stratégies de mise à l'échelle informatique : données plus volumineuses**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/8_computing.ipynb#strategies-to-scale-computationally-bigger-data)<br/>([*Strategies to scale computationally: bigger data*](https://scikit-learn.org/stable/computing.html#strategies-to-scale-computationally-bigger-data))\n",
    "\n",
    "* ✔ 8.1.1. [**Mise à l'échelle avec des instances utilisant l'apprentissage hors cœur**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/8_computing.ipynb#scaling-with-instances-using-out-of-core-learning)<br/>([*Scaling with instances using out-of-core learning*](https://scikit-learn.org/stable/computing.html#scaling-with-instances-using-out-of-core-learning))\n",
    "\n",
    "✔ 8.2. [**Performances informatiques**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/8_computing.ipynb#)<br/>([*Computational Performance*](https://scikit-learn.org/stable/computing.html#))\n",
    "\n",
    "* ✔ 8.2.1. [**Latence de prédiction**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/8_computing.ipynb#)<br/>([*Prediction Latency*](https://scikit-learn.org/stable/computing.html#))\n",
    "* ✔ 8.2.2. [**Débit de prédiction**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/8_computing.ipynb#)<br/>([*Prediction Throughput*](https://scikit-learn.org/stable/computing.html#))\n",
    "* ✔ 8.2.3. [**Trucs et astuces**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/8_computing.ipynb#)<br/>([*Tips and Tricks*](https://scikit-learn.org/stable/computing.html#))\n",
    "\n",
    "✔ 8.3. [**Parallélisme, gestion des ressources et configuration**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/8_computing.ipynb#)<br/>([*Parallelism, resource management, and configuration*]())\n",
    "\n",
    "* ✔ 8.3.1. [**Parallélisme**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/8_computing.ipynb#)<br/>([*Parallelism*]())\n",
    "* ✔ 8.3.2. [**Commutateurs de configuration**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/8_computing.ipynb#)<br/>([*Configuration switches*]())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='strategies-to-scale-computationally-bigger-data'></a> 8.1. Stratégies de mise à l'échelle informatique : données plus volumineuses\n",
    "\n",
    "Pour certaines applications, la quantité d'exemples, de caractéristiques (ou les deux) et/ou la vitesse à laquelle ils doivent être traités sont difficiles pour les approches traditionnelles. Dans ces cas, scikit-learn propose un certain nombre d'options que vous pouvez envisager pour faire évoluer votre système.\n",
    "\n",
    "## <a id='scaling-with-instances-using-out-of-core-learning'></a> 8.1.1. Mise à l'échelle avec des instances utilisant l'apprentissage hors cœur\n",
    "\n",
    "L'apprentissage hors cœur (ou \"mémoire externe\") est une technique utilisée pour apprendre à partir de données qui ne peuvent pas tenir dans la mémoire principale (RAM) d'un ordinateur.\n",
    "\n",
    "Voici une esquisse d'un système conçu pour atteindre cet objectif :\n",
    "1. un moyen de diffuser des instances\n",
    "2. un moyen d'extraire des caractéristiques à partir d'instances\n",
    "3. un algorithme incrémental\n",
    "\n",
    "### <a id='streaming-instances'></a> 8.1.1.1. Instances de diffusion en continu\n",
    "\n",
    "Fondamentalement, 1. peut être un lecteur qui produit des instances à partir de fichiers sur un disque dur, une base de données, à partir d'un flux réseau, etc. Cependant, les détails sur la façon d'y parvenir dépassent le cadre de cette documentation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='extracting-features'></a> 8.1.1.2. Extraction de caractéristiques\n",
    "\n",
    "Le 2. pourrait être n'importe quel moyen pertinent d'extraire des caractéristiques parmi les différentes méthodes d'[extraction de caractéristiques (6.2.)](https://scikit-learn.org/stable/modules/feature_extraction.html) prises en charge par scikit-learn. Cependant, lorsque vous travaillez avec des données nécessitant une vectorisation et lorsque l'ensemble de caractéristiques ou de valeurs n'est pas connu à l'avance, vous devez faire particulièrement attention. Un bon exemple est la classification de texte où des termes inconnus sont susceptibles d'être trouvés pendant l'entraînement. Il est possible d'utiliser un vectoriseur avec état si effectuer plusieurs passages sur les données est raisonnable d'un point de vue applicatif. Sinon, on peut augmenter la difficulté en utilisant un extracteur de caractéristiques sans état. Actuellement, la méthode préférée consiste à utiliser la prétendue [astuce de hachage (6.2.2.)](https://scikit-learn.org/stable/modules/feature_extraction.html#feature-hashing) telle qu'implémentée par [`sklearn.feature_extraction.FeatureHasher`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher) pour les ensembles de données avec des variables catégorielles représentées sous forme de liste de dicts Python ou [`sklearn.feature_extraction.text.HashingVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer) pour les documents texte."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='incremental-learning'></a> 8.1.1.3. Apprentissage incrémental\n",
    "\n",
    "Enfin, pour 3. nous avons un certain nombre d'options dans scikit-learn. Bien que tous les algorithmes ne puissent pas apprendre de manière incrémentielle (c'est-à-dire sans voir toutes les instances à la fois), tous les estimateurs implémentant l'API `partial_fit` sont candidats. En fait, la capacité d'apprendre progressivement à partir d'un mini-lot d'instances (parfois appelé \"apprentissage en ligne\") est la clé de l'apprentissage hors cœur car il garantit qu'à un moment donné, il n'y aura qu'un petit nombre d'instances dans le mémoire principale. Le choix d'une bonne taille pour le mini-lot qui équilibre la pertinence et l'empreinte mémoire peut impliquer quelques ajustements [1].\n",
    "\n",
    "Voici une liste d'estimateurs incrémentaux pour différentes tâches :\n",
    "\n",
    "#### Classification\n",
    "* [`sklearn.naive_bayes.MultinomialNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB)\n",
    "* [`sklearn.naive_bayes.BernoulliNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB)\n",
    "* [`sklearn.linear_model.Perceptron`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron)\n",
    "* [`sklearn.linear_model.SGDClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier)\n",
    "* [`sklearn.linear_model.PassiveAggressiveClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PassiveAggressiveClassifier.html#sklearn.linear_model.PassiveAggressiveClassifier)\n",
    "* [`sklearn.neural_network.MLPClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier)\n",
    "\n",
    "#### Régression\n",
    "* [`sklearn.linear_model.SGDRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor)\n",
    "* [`sklearn.linear_model.PassiveAggressiveRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PassiveAggressiveRegressor.html#sklearn.linear_model.PassiveAggressiveRegressor)\n",
    "* [`sklearn.neural_network.MLPRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor)\n",
    "\n",
    "#### Regroupement\n",
    "* [`sklearn.cluster.MiniBatchKMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans)\n",
    "* [`sklearn.cluster.Birch`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.Birch.html#sklearn.cluster.Birch)\n",
    "\n",
    "#### Décomposition / Extraction de caractéristiques\n",
    "* [`sklearn.decomposition.MiniBatchDictionaryLearning`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.MiniBatchDictionaryLearning.html#sklearn.decomposition.MiniBatchDictionaryLearning)\n",
    "* [`sklearn.decomposition.IncrementalPCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA)\n",
    "* [`sklearn.decomposition.LatentDirichletAllocation`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation)\n",
    "* [`sklearn.decomposition.MiniBatchNMF`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.MiniBatchNMF.html#sklearn.decomposition.MiniBatchNMF)\n",
    "\n",
    "#### Prétraitement\n",
    "* [`sklearn.preprocessing.StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler)\n",
    "* [`sklearn.preprocessing.MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler)\n",
    "* [`sklearn.preprocessing.MaxAbsScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html#sklearn.preprocessing.MaxAbsScaler)\n",
    "\n",
    "Pour la classification, une chose quelque peu importante à noter est que bien qu'une routine d'extraction de caractéristiques sans état puisse être capable de faire face à des attributs nouveaux/indédits, l'apprenant incrémental lui-même peut être incapable de faire face à des classes cibles nouvelles/indédites. Dans ce cas, vous devez passer toutes les classes possibles au premier appel de `partial_fit` en utilisant le paramètre `classes=`.\n",
    "\n",
    "Un autre aspect à considérer lors du choix d'un algorithme approprié est que tous n'accordent pas la même importance à chaque exemple au fil du temps. A savoir, le `Perceptron` est toujours sensible aux exemples mal étiquetés même après de nombreux exemples alors que les familles `SGD*` et `PassiveAggressive*` sont plus robustes à ce genre d'artefacts. À l'inverse, ces derniers ont également tendance à accorder moins d'importance à des exemples remarquablement différents, mais correctement étiquetés, lorsqu'ils arrivent tard dans le flux, car leur taux d'apprentissage diminue avec le temps."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='examples'></a> 8.1.1.4. Exemples\n",
    "\n",
    "Enfin, nous avons un exemple complet de [classification hors cœur de documents texte](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/plot_out_of_core_classification.ipynb). Il vise à fournir un point de départ aux personnes souhaitant créer des systèmes d'apprentissage hors cœur et illustre la plupart des notions abordées ci-dessus.\n",
    "\n",
    "De plus, il montre également l'évolution des performances des différents algorithmes avec le nombre d'exemples traités.\n",
    "\n",
    "<img alt=\"accuracy_over_time\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_out_of_core_classification_001.png\" style=\"width: 512.0px; height: 384.0px;\" />\n",
    "\n",
    "En regardant maintenant le temps de calcul des différentes parties, on s'aperçoit que la vectorisation est bien plus coûteuse que l'apprentissage lui-même. Parmi les différents algorithmes, `MultinomialNB` est le plus cher, mais sa surcharge peut être atténuée en augmentant la taille des mini-lots (exercice : changez `minibatch_size` en 100 et 10000 dans le programme et comparez).\n",
    "\n",
    "<img alt=\"computation_time\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_out_of_core_classification_003.png\" style=\"width: 512.0px; height: 384.0px;\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='notes'></a> 8.1.1.5. Remarques\n",
    "\n",
    "[1] Selon l'algorithme, la taille du mini-lot peut influencer ou non les résultats. `SGD*`, `PassiveAggressive*` et les NaiveBayes discrets sont véritablement en ligne et ne sont pas affectés par la taille du lot. Inversement, le taux de convergence de MiniBatchKMeans est affecté par la taille du lot. En outre, son empreinte mémoire peut varier considérablement en fonction de la taille du lot."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='computational-performance'></a> 8.2. Performances de calcul\n",
    "\n",
    "Pour certaines applications, les performances (notamment la latence et le débit au moment de la prédiction) des estimateurs sont cruciales. Il peut également être intéressant de prendre en compte le débit d'apprentissage, mais cela est souvent moins important dans un environnement de production (où il se déroule souvent hors ligne).\n",
    "\n",
    "Nous passerons en revue ici les ordres de grandeur que vous pouvez attendre de plusieurs estimateurs de scikit-learn dans différents contextes, et nous vous fournirons quelques conseils et astuces pour surmonter les goulots d'étranglement de performance.\n",
    "\n",
    "La latence de prédiction est mesurée comme le temps écoulé nécessaire pour effectuer une prédiction (par exemple, en microsecondes). La latence est souvent considérée comme une distribution, et les ingénieurs des opérations se concentrent souvent sur la latence à un certain percentile de cette distribution (par exemple, le 90e percentile).\n",
    "\n",
    "Le débit de prédiction est défini comme le nombre de prédictions que le logiciel peut fournir en une certaine période de temps (par exemple, en prédictions par seconde).\n",
    "\n",
    "Un aspect important de l'optimisation des performances est également qu'elle peut nuire à la précision des prédictions. En effet, les modèles plus simples (par exemple, linéaires plutôt que non linéaires, ou avec moins de paramètres) s'exécutent souvent plus rapidement, mais ne sont pas toujours capables de prendre en compte exactement les mêmes propriétés des données que les modèles plus complexes.\n",
    "\n",
    "## <a id='prediction-latency'></a> 8.2.1. Latence de prédiction\n",
    "\n",
    "L'une des préoccupations les plus évidentes lors de l'utilisation ou du choix d'une boîte à outils d'apprentissage automatique est la latence à laquelle les prédictions peuvent être effectuées dans un environnement de production.\n",
    "\n",
    "Les principaux facteurs qui influencent la latence de prédiction sont les suivants :\n",
    "* Nombre de caractéristiques\n",
    "* Représentation et densité des données d'entrée\n",
    "* Complexité du modèle\n",
    "* Extraction des caractéristiques\n",
    "\n",
    "Un dernier paramètre majeur est également la possibilité de faire des prédictions en mode \"bulk\" (en vrac) ou en mode \"one-at-a-time\" (une par une).\n",
    "\n",
    "### <a id='bulk-versus-atomic-mode'></a> 8.2.1.1. Mode en vrac par rapport au mode atomique\n",
    "\n",
    "En général, effectuer des prédictions en vrac (plusieurs instances en même temps) est plus efficace pour plusieurs raisons (prévisibilité des branches, mémoire cache du processeur, optimisations des bibliothèques d'algèbre linéaire, etc.). Ici, nous constatons dans un contexte avec peu de caractéristiques que, quel que soit le choix de l'estimateur, le mode en vrac est toujours plus rapide, et pour certains d'entre eux, de 1 à 2 ordres de grandeur de différence :\n",
    "\n",
    "<img alt=\"atomic_prediction_latency\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_001.png\" style=\"width: 800.0px; height: 480.0px;\" />\n",
    "\n",
    "<img alt=\"bulk_prediction_latency\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_002.png\" style=\"width: 800.0px; height: 480.0px;\" />\n",
    "\n",
    "Pour comparer différents estimateurs pour votre cas, vous pouvez simplement modifier le paramètre `n_features` dans cet exemple : [**Latence de prédiction**](https://scikit-learn.org/stable/auto_examples/applications/plot_prediction_latency.html#sphx-glr-auto-examples-applications-plot-prediction-latency-py). Cela devrait vous donner une estimation de l'ordre de grandeur de la latence de prédiction.\n",
    "\n",
    "### <a id='configuring-scikit-learn-for-reduced-validation-overhead'></a> 8.2.1.2. Configuration de Scikit-learn pour réduire la surcharge de validation\n",
    "\n",
    "Scikit-learn effectue une certaine validation des données qui augmente la surcharge par appel à `predict` et à des fonctions similaires. En particulier, la vérification que les caractéristiques sont finies (non NaN ou infinies) nécessite un parcours complet des données. Si vous vous assurez que vos données sont acceptables, vous pouvez supprimer la vérification de la finitude en définissant la variable d'environnement `SKLEARN_ASSUME_FINITE` sur une chaîne non vide avant d'importer scikit-learn, ou en la configurant en Python avec [**`set_config`**](https://scikit-learn.org/stable/modules/generated/sklearn.set_config.html#sklearn.set_config). Pour un contrôle plus précis que ces paramètres globaux, un [**`config_context`**](https://scikit-learn.org/stable/modules/generated/sklearn.config_context.html#sklearn.config_context) vous permet de définir cette configuration dans un contexte spécifié :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "with sklearn.config_context(assume_finite=True):\n",
    "    pass  # do learning/prediction here with reduced validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez que cela affectera toutes les utilisations de [**`assert_all_finite`**](https://scikit-learn.org/stable/modules/generated/sklearn.utils.assert_all_finite.html#sklearn.utils.assert_all_finite) dans le contexte.\n",
    "\n",
    "### <a id='influence-of-the-number-of-features'></a> 8.2.1.3. Influence du nombre de caractéristiques\n",
    "\n",
    "Évidemment, lorsque le nombre de caractéristiques augmente, la consommation de mémoire de chaque exemple augmente également. En effet, pour une matrice de $m$ exemples avec $n$ caractéristiques, la complexité de l'espace est en $\\mathcal{O}(mn)$. D'un point de vue informatique, cela signifie également que le nombre d'opérations de base (par exemple, les multiplications pour les produits vecteur-matrice dans les modèles linéaires) augmente également. Voici un graphique montrant l'évolution de la latence de prédiction en fonction du nombre de caractéristiques :\n",
    "\n",
    "<img alt=\"influence_of_n_features_on_latency\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_003.png\" style=\"width: 800.0px; height: 480.0px;\" />\n",
    "\n",
    "Dans l'ensemble, vous pouvez vous attendre à ce que le temps de prédiction augmente au moins de manière linéaire avec le nombre de caractéristiques (des cas non linéaires peuvent se produire en fonction de l'emprunte mémoire globale et de l'estimateur).\n",
    "\n",
    "### <a id='influence-of-the-input-data-representation'></a> 8.2.1.4. Influence de la représentation des données d'entrée\n",
    "\n",
    "Scipy fournit des structures de données de matrices creuses optimisées pour le stockage de données creuses. La principale caractéristique des formats creux est que vous ne stockez pas les zéros, donc si vos données sont creuses, vous utilisez beaucoup moins de mémoire. Une valeur non nulle dans une représentation creuse ([CSR ou CSC](https://docs.scipy.org/doc/scipy/reference/sparse.html)) ne prendra en moyenne qu'une position d'entier 32 bits + la valeur en virgule flottante de 64 bits + 32 bits supplémentaires par ligne ou colonne dans la matrice. L'utilisation d'une entrée creuse sur un modèle linéaire dense (ou creux) peut accélérer la prédiction de manière significative, car seules les caractéristiques de valeur non nulle ont un impact sur le produit scalaire et donc les prédictions du modèle. Ainsi, si vous avez 100 valeurs non nulles dans un espace de 1 million de dimensions, vous n'avez besoin que de 100 opérations de multiplication et d'addition au lieu de 1 million.\n",
    "\n",
    "Cependant, les calculs sur une représentation dense peuvent exploiter des opérations vectorielles et un multithreading hautement optimisés dans BLAS, et tendent à entraîner moins de pertes de cache du CPU. Par conséquent, la sparsité devrait généralement être assez élevée (10% de non-nuls maximum, à vérifier en fonction du matériel) pour que la représentation des données d'entrée creuse soit plus rapide que la représentation dense sur une machine avec de nombreux processeurs et une implémentation BLAS optimisée.\n",
    "\n",
    "Voici un code d'exemple pour tester la sparsité de vos données d'entrée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sparsity_ratio(X):\n",
    "    return 1.0 - np.count_nonzero(X) / float(X.shape[0] * X.shape[1])\n",
    "print(\"input sparsity ratio:\", sparsity_ratio(X))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme règle générale, vous pouvez considérer que si le taux de sparsité est supérieur à 90%, vous pouvez probablement bénéficier des formats creux. Consultez la [documentation](https://docs.scipy.org/doc/scipy/reference/sparse.html) sur les formats de matrices creuses de Scipy pour plus d'informations sur la façon de construire (ou convertir vos données en) des formats de matrices creuses. La plupart du temps, les formats `CSR` et `CSC` fonctionnent le mieux."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='influence-of-the-model-complexity'></a> 8.2.1.5. Influence de la complexité du modèle\n",
    "\n",
    "En général, lorsque la complexité du modèle augmente, la puissance prédictive et la latence sont censées augmenter. Augmenter la puissance prédictive est généralement intéressant, mais pour de nombreuses applications, il est préférable de ne pas augmenter trop la latence de prédiction. Nous allons maintenant examiner cette idée pour différentes familles de modèles supervisés.\n",
    "\n",
    "Pour [**`sklearn.linear_model`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model) (par exemple, Lasso, ElasticNet, SGDClassifier/Regressor, Ridge & RidgeClassifier, PassiveAggressiveClassifier/Regressor, LinearSVC, LogisticRegression...), la fonction de décision appliquée au moment de la prédiction est la même (un produit scalaire), donc la latence devrait être équivalente.\n",
    "\n",
    "Voici un exemple utilisant [SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier) avec la pénalité `elasticnet`. La force de régularisation est contrôlée globalement par le paramètre `alpha`. Avec un `alpha` suffisamment élevé, on peut ensuite augmenter le paramètre `l1_ratio` d'elasticnet pour imposer différents niveaux de parcimonie dans les coefficients du modèle. Une plus grande parcimonie ici est interprétée comme une complexité de modèle moindre, car nous avons besoin de moins de coefficients pour le décrire complètement. Bien sûr, la parcimonie influence à son tour le temps de prédiction, car le produit scalaire creux prend du temps proportionnel au nombre de coefficients non nuls.\n",
    "\n",
    "![en_model_complexity](https://scikit-learn.org/stable/_images/sphx_glr_plot_model_complexity_influence_001.png)\n",
    "\n",
    "Pour la famille d'algorithmes [**`sklearn.svm`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm) avec un noyau non linéaire, la latence est liée au nombre de vecteurs de support (moins il y en a, plus c'est rapide). La latence et le débit devraient (asymptotiquement) croître linéairement avec le nombre de vecteurs de support dans un modèle SVC ou SVR. Le noyau influence également la latence car il est utilisé pour calculer la projection du vecteur d'entrée une fois par vecteur de support. Dans le graphique suivant, le paramètre `nu` de [**`NuSVR`**](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR) a été utilisé pour influencer le nombre de vecteurs de support.\n",
    "\n",
    "![nusvr_model_complexity](https://scikit-learn.org/stable/_images/sphx_glr_plot_model_complexity_influence_002.png)\n",
    "\n",
    "Pour l'ensemble d'arbres [sklearn.ensemble](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble) (par exemple, RandomForest, GBT, ExtraTrees, etc.), le nombre d'arbres et leur profondeur jouent le rôle le plus important. La latence et le débit devraient augmenter linéairement avec le nombre d'arbres. Dans ce cas, nous avons directement utilisé le paramètre `n_estimators` de [GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor).\n",
    "\n",
    "![gbt_model_complexity](https://scikit-learn.org/stable/_images/sphx_glr_plot_model_complexity_influence_003.png)\n",
    "\n",
    "En tout cas, soyez avertis que diminuer la complexité du modèle peut nuire à l'exactitude, comme mentionné ci-dessus. Par exemple, un problème non linéairement séparable peut être traité avec un modèle linéaire rapide, mais la puissance prédictive en souffrira très probablement dans le processus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='feature-extraction-latency'></a> 8.2.1.6. Latence de l'extraction des caractéristiques\n",
    "\n",
    "La plupart des modèles de scikit-learn sont généralement assez rapides car ils sont implémentés soit avec des extensions Cython compilées, soit avec des bibliothèques de calcul optimisées. D'autre part, dans de nombreuses applications du monde réel, le processus d'extraction des caractéristiques (c'est-à-dire la transformation de données brutes telles que des lignes de base de données ou des paquets réseau en tableaux numpy) régit le temps de prédiction global. Par exemple, lors de la classification de texte Reuters, toute la préparation (lecture et analyse des fichiers SGML, tokenisation du texte et hachage dans un espace vectoriel commun) prend de 100 à 500 fois plus de temps que le code de prédiction réel, selon le modèle choisi.\n",
    "\n",
    "![prediction_time](https://scikit-learn.org/stable/_images/sphx_glr_plot_out_of_core_classification_004.png)\n",
    "\n",
    "Dans de nombreux cas, il est donc recommandé de chronométrer et de profiler attentivement votre code d'extraction de caractéristiques, car il peut être un bon point de départ pour l'optimisation lorsque votre latence globale est trop lente pour votre application."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='prediction-throughput'></a> 8.2.2. Débit de prédiction\n",
    "\n",
    "Une autre métrique importante à prendre en compte lors du dimensionnement des systèmes de production est le débit, c'est-à-dire le nombre de prédictions que vous pouvez effectuer dans un laps de temps donné. Voici un benchmark de l'exemple de [**latence de prédiction**](https://scikit-learn.org/stable/auto_examples/applications/plot_prediction_latency.html#sphx-glr-auto-examples-applications-plot-prediction-latency-py) qui mesure cette quantité pour plusieurs estimateurs sur des données synthétiques :\n",
    "\n",
    "![throughput_benchmark](https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_004.png)\n",
    "\n",
    "Ces débits sont obtenus sur un seul processus. Une façon évidente d'augmenter le débit de votre application est de créer des instances supplémentaires (généralement des processus en Python en raison du GIL) qui partagent le même modèle. On peut également ajouter des machines pour répartir la charge. Cependant, une explication détaillée sur la façon d'y parvenir dépasse le cadre de cette documentation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='tips-and-tricks'></a> 8.2.3. Conseils et astuces\n",
    "\n",
    "### <a id='linear-algebra-libraries'></a> 8.2.3.1. Bibliothèques d'algèbre linéaire\n",
    "\n",
    "Étant donné que scikit-learn s'appuie fortement sur Numpy/Scipy et sur l'algèbre linéaire en général, il est important de prendre explicitement en compte les versions de ces bibliothèques. Fondamentalement, vous devez vous assurer que Numpy est construit en utilisant une bibliothèque [BLAS](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms) / [LAPACK](https://en.wikipedia.org/wiki/LAPACK) optimisée.\n",
    "\n",
    "Tous les modèles ne bénéficient pas d'une implémentation BLAS et Lapack optimisée. Par exemple, les modèles basés sur les arbres de décision (aléatoires) ne font généralement pas appel à BLAS dans leurs boucles internes, de même que les SVM à noyau (`SVC`, `SVR`, `NuSVC`, `NuSVR`). En revanche, un modèle linéaire implémenté avec un appel BLAS DGEMM (via `numpy.dot`) bénéficiera généralement énormément d'une implémentation BLAS optimisée et permettra d'obtenir des gains de vitesse de l'ordre de plusieurs ordres de grandeur par rapport à une implémentation BLAS non optimisée.\n",
    "\n",
    "Vous pouvez afficher l'implémentation BLAS/LAPACK utilisée par votre installation NumPy/SciPy/scikit-learn avec la commande suivante :\n",
    "\n",
    "```sh\n",
    "python -c \"import sklearn; sklearn.show_versions()\"\n",
    "```\n",
    "\n",
    "**Les implémentations BLAS/LAPACK optimisées incluent :**\n",
    "* Atlas (nécessite un réglage spécifique au matériel en le reconstruisant sur la machine cible)\n",
    "* OpenBLAS\n",
    "* MKL\n",
    "* Les frameworks Apple Accelerate et vecLib (uniquement sur OSX)\n",
    "\n",
    "Vous trouverez plus d'informations sur la [page d'installation de NumPy](https://numpy.org/install/) et dans ce [billet de blog](https://danielnouri.org/notes/2012/12/19/libblas-and-liblapack-issues-and-speed,-with-scipy-and-ubuntu/) de Daniel Nouri, qui propose des instructions d'installation pas à pas pour Debian/Ubuntu."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='limiting-working-memory'></a> 8.2.3.2. Limiter la mémoire de travail\n",
    "\n",
    "Certaines calculs, lorsqu'ils sont implémentés à l'aide d'opérations vectorisées standard de Numpy, nécessitent une grande quantité de mémoire temporaire. Cela peut potentiellement épuiser la mémoire du système. Lorsque les calculs peuvent être effectués par morceaux avec une mémoire fixe, nous essayons de le faire, et permettons à l'utilisateur d'indiquer la taille maximale de cette mémoire de travail (par défaut, 1 Go) en utilisant [**`set_config`**](https://scikit-learn.org/stable/modules/generated/sklearn.set_config.html#sklearn.set_config) ou [**`config_context`**](https://scikit-learn.org/stable/modules/generated/sklearn.config_context.html#sklearn.config_context). L'exemple suivant suggère de limiter la mémoire de travail temporaire à 128 MiB :\n",
    "\n",
    "```python\n",
    "import sklearn\n",
    "with sklearn.config_context(working_memory=128):\n",
    "    pass # effectuez ici les travaux par morceaux\n",
    "```\n",
    "\n",
    "Un exemple d'opération par morceaux respectant ce paramètre est [**`pairwise_distances_chunked`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances_chunked.html), qui facilite le calcul de réductions par ligne d'une matrice de distances par paires."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='model-compression'></a> 8.2.3.3. Compression de modèle\n",
    "\n",
    "La compression de modèle dans scikit-learn concerne uniquement les modèles linéaires pour le moment. Dans ce contexte, cela signifie que nous voulons contrôler la sparsité du modèle (c'est-à-dire le nombre de coordonnées non nulles dans les vecteurs du modèle). Il est généralement recommandé de combiner la sparsité du modèle avec une représentation des données d'entrée sparse.\n",
    "\n",
    "Voici un exemple de code qui illustre l'utilisation de la méthode `sparsify()` :\n",
    "\n",
    "```python\n",
    "clf = SGDRegressor(penalty='elasticnet', l1_ratio=0.25)\n",
    "clf.fit(X_train, y_train).sparsify()\n",
    "clf.predict(X_test)\n",
    "```\n",
    "\n",
    "Dans cet exemple, nous préférons la pénalité `elasticnet` car c'est souvent un bon compromis entre compacité du modèle et puissance de prédiction. On peut également ajuster davantage le paramètre 'l1_ratio' (en combinaison avec la force de régularisation 'alpha') pour contrôler ce compromis.\n",
    "\n",
    "Un [benchmark](https://github.com/scikit-learn/scikit-learn/blob/main/benchmarks/bench_sparsify.py)  typique sur des données synthétiques montre une diminution de plus de 30 % de la latence lorsque à la fois le modèle et les données d'entrée sont parcimonieux (avec un taux de parcimonie de 0,000024 et 0,027400 respectivement). Vos résultats peuvent varier en fonction de la parcimonie et de la taille de vos données et de votre modèle. De plus, la réduction de la sparsité peut être très utile pour réduire l'utilisation de mémoire des modèles prédictifs déployés sur des serveurs de production."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='model-reshaping'></a> 8.2.3.4. Remodelage du modèle\n",
    "\n",
    "Le remodelage du modèle consiste à sélectionner uniquement une partie des caractéristiques disponibles pour ajuster un modèle. En d'autres termes, si un modèle élimine des caractéristiques pendant la phase d'apprentissage, nous pouvons ensuite les supprimer de l'entrée. Cela présente plusieurs avantages. Tout d'abord, cela réduit la surcharge de mémoire (et donc de temps) du modèle lui-même. Cela permet également d'éliminer les composants de sélection explicite des caractéristiques dans un pipeline une fois que nous savons quelles caractéristiques conserver à partir d'une exécution précédente. Enfin, cela peut aider à réduire le temps de traitement et l'utilisation d'E/S en amont dans les couches d'accès aux données et d'extraction des caractéristiques en ne collectant pas et en ne construisant pas les caractéristiques qui sont éliminées par le modèle. Par exemple, si les données brutes proviennent d'une base de données, cela peut permettre d'écrire des requêtes plus simples et plus rapides ou de réduire l'utilisation d'E/S en faisant en sorte que les requêtes renvoient des enregistrements plus légers. Pour le moment, le remodelage doit être effectué manuellement dans scikit-learn. Dans le cas d'une entrée creuse (en particulier au format `CSR`), il est généralement suffisant de ne pas générer les caractéristiques pertinentes, en laissant leurs colonnes vides."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='model-reshaping'></a> 8.2.3.5. Liens\n",
    "\n",
    "* [Documentation des performances pour les développeurs scikit-learn](https://scikit-learn.org/stable/developers/performance.html#performance-howto)\n",
    "* [Documentation des formats de matrices creuses de Scipy](https://docs.scipy.org/doc/scipy/reference/sparse.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9aff9e50adfaa9e30c910fb3872ffdc72747acb5f50803ca0504f00e980f7c25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
