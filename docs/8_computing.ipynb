{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. [**Calculer avec scikit-learn**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/8_computing.ipynb)</br>([*Computing with scikit-learn*](https://scikit-learn.org/stable/computing.html))\n",
    "\n",
    "\n",
    "- **Volume** : 21 pages, 2 exemples, 0 papiers\n",
    "- ✔ 8.1. [**Stratégies de mise à l'échelle informatique : données plus volumineuses**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/8_computing.ipynb#strategies-to-scale-computationally-bigger-data)<br/>([*Strategies to scale computationally: bigger data*](https://scikit-learn.org/stable/computing.html#strategies-to-scale-computationally-bigger-data))\n",
    "    - ✔ 8.1.1. [**Mise à l'échelle avec des instances utilisant l'apprentissage hors cœur**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/8_computing.ipynb#scaling-with-instances-using-out-of-core-learning)<br/>([*Scaling with instances using out-of-core learning*](https://scikit-learn.org/stable/computing.html#scaling-with-instances-using-out-of-core-learning))\n",
    "- ✔ 8.2. [**Performances calculatoires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/8_computing.ipynb#computational-performance)<br/>([*Computational Performance*](https://scikit-learn.org/stable/computing/computational_performance.html#computational-performance))\n",
    "    - ✔ 8.2.1. [**Latence de prédiction**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/8_computing.ipynb#prediction-latency)<br/>([*Prediction Latency*](https://scikit-learn.org/stable/computing.html#prediction-latency))\n",
    "    - ✔ 8.2.2. [**Débit de prédiction**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/8_computing.ipynb#prediction-throughput)<br/>([*Prediction Throughput*](https://scikit-learn.org/stable/computing.html#prediction-throughput))\n",
    "    - ✔ 8.2.3. [**Trucs et astuces**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/8_computing.ipynb#tips-and-tricks)<br/>([*Tips and Tricks*](https://scikit-learn.org/stable/computing.html#tips-and-tricks))\n",
    "- ✔ 8.3. [**Parallélisme, gestion des ressources et configuration**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/8_computing.ipynb#parallelism-resource-management-and-configuration)<br/>([*Parallelism, resource management, and configuration*](https://scikit-learn.org/stable/computing/parallelism.html#parallelism-resource-management-and-configuration))\n",
    "    - ✔ 8.3.1. [**Parallélisme**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/8_computing.ipynb#parallelism)<br/>([*Parallelism*](https://scikit-learn.org/stable/computing/parallelism.html#parallelism))\n",
    "    - ✔ 8.3.2. [**Commutateurs de configuration**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/8_computing.ipynb#configuration-switches)<br/>([*Configuration switches*](https://scikit-learn.org/stable/computing/parallelism.html#configuration-switches))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='strategies-to-scale-computationally-bigger-data'></a> 8.1. Stratégies de mise à l'échelle informatique : données plus volumineuses\n",
    "\n",
    "Pour certaines applications, la quantité d'exemples, de caractéristiques (ou les deux) et/ou la vitesse à laquelle ils doivent être traités sont difficiles pour les approches traditionnelles. Dans ces cas, scikit-learn propose un certain nombre d'options que vous pouvez envisager pour faire évoluer votre système.\n",
    "\n",
    "## <a id='scaling-with-instances-using-out-of-core-learning'></a> 8.1.1. Mise à l'échelle avec des instances utilisant l'apprentissage hors cœur\n",
    "\n",
    "L'apprentissage hors cœur (ou \"mémoire externe\") est une technique utilisée pour apprendre à partir de données qui ne peuvent pas tenir dans la mémoire principale (RAM) d'un ordinateur.\n",
    "\n",
    "Voici une esquisse d'un système conçu pour atteindre cet objectif :\n",
    "1. un moyen de diffuser des instances\n",
    "2. un moyen d'extraire des caractéristiques à partir d'instances\n",
    "3. un algorithme incrémental\n",
    "\n",
    "### <a id='streaming-instances'></a> 8.1.1.1. Instances de diffusion en continu\n",
    "\n",
    "Fondamentalement, 1. peut être un lecteur qui produit des instances à partir de fichiers sur un disque dur, une base de données, à partir d'un flux réseau, etc. Cependant, les détails sur la façon d'y parvenir dépassent le cadre de cette documentation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='extracting-features'></a> 8.1.1.2. Extraction de caractéristiques\n",
    "\n",
    "Le 2. pourrait être n'importe quel moyen pertinent d'extraire des caractéristiques parmi les différentes méthodes d'[**extraction de caractéristiques** (6.2)](https://scikit-learn.org/stable/modules/feature_extraction.html) prises en charge par scikit-learn. Cependant, lorsque vous travaillez avec des données nécessitant une vectorisation et lorsque l'ensemble de caractéristiques ou de valeurs n'est pas connu à l'avance, vous devez faire particulièrement attention. Un bon exemple est la classification de texte où des termes inconnus sont susceptibles d'être trouvés pendant l'entraînement. Il est possible d'utiliser un vectoriseur avec état si effectuer plusieurs passages sur les données est raisonnable d'un point de vue applicatif. Sinon, on peut augmenter la difficulté en utilisant un extracteur de caractéristiques sans état. Actuellement, la méthode préférée consiste à utiliser la prétendue [**astuce de hachage** (6.2.2)](https://scikit-learn.org/stable/modules/feature_extraction.html#feature-hashing) telle qu'implémentée par [**`sklearn.feature_extraction.FeatureHasher`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html) pour les ensembles de données avec des variables catégorielles représentées sous forme de liste de dicts Python ou [**`sklearn.feature_extraction.text.HashingVectorizer`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html) pour les documents texte."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='incremental-learning'></a> 8.1.1.3. Apprentissage incrémental\n",
    "\n",
    "Enfin, pour 3. nous avons un certain nombre d'options dans scikit-learn. Bien que tous les algorithmes ne puissent pas apprendre de manière incrémentielle (c'est-à-dire sans voir toutes les instances à la fois), tous les estimateurs implémentant l'API `partial_fit` sont candidats. En fait, la capacité d'apprendre progressivement à partir d'un mini-lot d'instances (parfois appelé \"apprentissage en ligne\") est la clé de l'apprentissage hors cœur car il garantit qu'à un moment donné, il n'y aura qu'un petit nombre d'instances dans le mémoire principale. Le choix d'une bonne taille pour le mini-lot qui équilibre la pertinence et l'empreinte mémoire peut impliquer quelques ajustements [1].\n",
    "\n",
    "Voici une liste d'estimateurs incrémentaux pour différentes tâches :\n",
    "\n",
    "#### Classification\n",
    "* [**`sklearn.naive_bayes.MultinomialNB`**](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB)\n",
    "* [**`sklearn.naive_bayes.BernoulliNB`**](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB)\n",
    "* [**`sklearn.linear_model.Perceptron`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron)\n",
    "* [**`sklearn.linear_model.SGDClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier)\n",
    "* [**`sklearn.linear_model.PassiveAggressiveClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PassiveAggressiveClassifier.html#sklearn.linear_model.PassiveAggressiveClassifier)\n",
    "* [**`sklearn.neural_network.MLPClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier)\n",
    "\n",
    "#### Régression\n",
    "* [**`sklearn.linear_model.SGDRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor)\n",
    "* [**`sklearn.linear_model.PassiveAggressiveRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PassiveAggressiveRegressor.html#sklearn.linear_model.PassiveAggressiveRegressor)\n",
    "* [**`sklearn.neural_network.MLPRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor)\n",
    "\n",
    "#### Regroupement\n",
    "* [**`sklearn.cluster.MiniBatchKMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans)\n",
    "* [**`sklearn.cluster.Birch`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.Birch.html#sklearn.cluster.Birch)\n",
    "\n",
    "#### Décomposition / Extraction de caractéristiques\n",
    "* [**`sklearn.decomposition.MiniBatchDictionaryLearning`**](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.MiniBatchDictionaryLearning.html#sklearn.decomposition.MiniBatchDictionaryLearning)\n",
    "* [**`sklearn.decomposition.IncrementalPCA`**](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA)\n",
    "* [**`sklearn.decomposition.LatentDirichletAllocation`**](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation)\n",
    "* [**`sklearn.decomposition.MiniBatchNMF`**](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.MiniBatchNMF.html#sklearn.decomposition.MiniBatchNMF)\n",
    "\n",
    "#### Prétraitement\n",
    "* [**`sklearn.preprocessing.StandardScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler)\n",
    "* [**`sklearn.preprocessing.MinMaxScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler)\n",
    "* [**`sklearn.preprocessing.MaxAbsScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html#sklearn.preprocessing.MaxAbsScaler)\n",
    "\n",
    "Pour la classification, une chose quelque peu importante à noter est que bien qu'une routine d'extraction de caractéristiques sans état puisse être capable de faire face à des attributs nouveaux/indédits, l'apprenant incrémental lui-même peut être incapable de faire face à des classes cibles nouvelles/indédites. Dans ce cas, vous devez passer toutes les classes possibles au premier appel de `partial_fit` en utilisant le paramètre `classes=`.\n",
    "\n",
    "Un autre aspect à considérer lors du choix d'un algorithme approprié est que tous n'accordent pas la même importance à chaque exemple au fil du temps. A savoir, le `Perceptron` est toujours sensible aux exemples mal étiquetés même après de nombreux exemples alors que les familles `SGD*` et `PassiveAggressive*` sont plus robustes à ce genre d'artefacts. À l'inverse, ces derniers ont également tendance à accorder moins d'importance à des exemples remarquablement différents, mais correctement étiquetés, lorsqu'ils arrivent tard dans le flux, car leur taux d'apprentissage diminue avec le temps."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='examples'></a> 8.1.1.4. Exemples\n",
    "\n",
    "Enfin, nous avons un exemple complet de [**Classification hors-mémoire de documents textuels**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/applications/plot_out_of_core_classification.ipynb). Il vise à fournir un point de départ aux personnes souhaitant créer des systèmes d'apprentissage hors cœur et illustre la plupart des notions abordées ci-dessus.\n",
    "\n",
    "De plus, il montre également l'évolution des performances des différents algorithmes avec le nombre d'exemples traités.\n",
    "\n",
    "<img alt=\"accuracy_over_time\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_out_of_core_classification_001.png\" style=\"width: 512.0px; height: 384.0px;\" />\n",
    "\n",
    "En regardant maintenant le temps de calcul des différentes parties, on s'aperçoit que la vectorisation est bien plus coûteuse que l'apprentissage lui-même. Parmi les différents algorithmes, `MultinomialNB` est le plus cher, mais sa surcharge peut être atténuée en augmentant la taille des mini-lots (exercice : changez `minibatch_size` en 100 et 10000 dans le programme et comparez).\n",
    "\n",
    "<img alt=\"computation_time\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_out_of_core_classification_003.png\" style=\"width: 512.0px; height: 384.0px;\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='notes'></a> 8.1.1.5. Remarques\n",
    "\n",
    "[1] Selon l'algorithme, la taille du mini-lot peut influencer ou non les résultats. `SGD*`, `PassiveAggressive*` et les NaiveBayes discrets sont véritablement en ligne et ne sont pas affectés par la taille du lot. Inversement, le taux de convergence de MiniBatchKMeans est affecté par la taille du lot. En outre, son empreinte mémoire peut varier considérablement en fonction de la taille du lot."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='computational-performance'></a> 8.2. Performances calculatoires\n",
    "\n",
    "Pour certaines applications, les performances (notamment la latence et le débit au moment de la prédiction) des estimateurs sont cruciales. Il peut également être intéressant de prendre en compte le débit d'entraînement, mais cela est souvent moins important dans un environnement de production (où il se déroule souvent hors ligne).\n",
    "\n",
    "Nous passerons en revue ici les ordres de grandeur que vous pouvez attendre de plusieurs estimateurs de scikit-learn dans différents contextes, et nous vous fournirons quelques conseils et astuces pour surmonter les goulots d'étranglement de performance.\n",
    "\n",
    "La latence de prédiction est mesurée comme le temps écoulé nécessaire pour effectuer une prédiction (par exemple, en microsecondes). La latence est souvent considérée comme une distribution, et les ingénieurs des opérations se concentrent souvent sur la latence à un certain percentile de cette distribution (par exemple, le 90e percentile).\n",
    "\n",
    "Le débit de prédiction est défini comme le nombre de prédictions que le logiciel peut fournir en une certaine période de temps (par exemple, en prédictions par seconde).\n",
    "\n",
    "Un aspect important de l'optimisation des performances est également qu'elle peut nuire à la précision des prédictions. En effet, les modèles plus simples (par exemple, linéaires plutôt que non linéaires, ou avec moins de paramètres) s'exécutent souvent plus rapidement, mais ne sont pas toujours capables de prendre en compte exactement les mêmes propriétés des données que les modèles plus complexes.\n",
    "\n",
    "## <a id='prediction-latency'></a> 8.2.1. Latence de prédiction\n",
    "\n",
    "L'une des préoccupations les plus évidentes lors de l'utilisation ou du choix d'une boîte à outils d'apprentissage automatique est la latence à laquelle les prédictions peuvent être effectuées dans un environnement de production.\n",
    "\n",
    "Les principaux facteurs qui influencent la latence de prédiction sont les suivants :\n",
    "* Nombre de caractéristiques\n",
    "* Représentation et densité des données d'entrée\n",
    "* Complexité du modèle\n",
    "* Extraction des caractéristiques\n",
    "\n",
    "Un dernier paramètre majeur est également la possibilité de faire des prédictions en mode \"bulk\" (en vrac) ou en mode \"one-at-a-time\" (une par une).\n",
    "\n",
    "### <a id='bulk-versus-atomic-mode'></a> 8.2.1.1. Mode en vrac par rapport au mode atomique\n",
    "\n",
    "En général, effectuer des prédictions en vrac (plusieurs instances en même temps) est plus efficace pour plusieurs raisons (prévisibilité des branches, mémoire cache du processeur, optimisations des bibliothèques d'algèbre linéaire, etc.). Ici, nous constatons dans un contexte avec peu de caractéristiques que, quel que soit le choix de l'estimateur, le mode en vrac est toujours plus rapide, et pour certains d'entre eux, de 1 à 2 ordres de grandeur de différence :\n",
    "\n",
    "<img alt=\"atomic_prediction_latency\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_001.png\" style=\"width: 800.0px; height: 480.0px;\" />\n",
    "\n",
    "<img alt=\"bulk_prediction_latency\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_002.png\" style=\"width: 800.0px; height: 480.0px;\" />\n",
    "\n",
    "Pour comparer différents estimateurs pour votre cas, vous pouvez simplement modifier le paramètre `n_features` dans cet exemple : [**Latence de prédiction**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/applications/plot_prediction_latency.ipynb). Cela devrait vous donner une estimation de l'ordre de grandeur de la latence de prédiction.\n",
    "\n",
    "### <a id='configuring-scikit-learn-for-reduced-validation-overhead'></a> 8.2.1.2. Configuration de Scikit-learn pour réduire la surcharge de validation\n",
    "\n",
    "Scikit-learn effectue une certaine validation des données qui augmente la surcharge par appel à `predict` et à des fonctions similaires. En particulier, la vérification que les caractéristiques sont finies (non NaN ou infinies) nécessite un parcours complet des données. Si vous vous assurez que vos données sont acceptables, vous pouvez supprimer la vérification de la finitude en définissant la variable d'environnement `SKLEARN_ASSUME_FINITE` sur une chaîne non vide avant d'importer scikit-learn, ou en la configurant en Python avec [**`set_config`**](https://scikit-learn.org/stable/modules/generated/sklearn.set_config.html#sklearn.set_config). Pour un contrôle plus précis que ces paramètres globaux, un [**`config_context`**](https://scikit-learn.org/stable/modules/generated/sklearn.config_context.html#sklearn.config_context) vous permet de définir cette configuration dans un contexte spécifié :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "with sklearn.config_context(assume_finite=True):\n",
    "    pass  # do learning/prediction here with reduced validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez que cela affectera toutes les utilisations de [**`assert_all_finite`**](https://scikit-learn.org/stable/modules/generated/sklearn.utils.assert_all_finite.html#sklearn.utils.assert_all_finite) dans le contexte.\n",
    "\n",
    "### <a id='influence-of-the-number-of-features'></a> 8.2.1.3. Influence du nombre de caractéristiques\n",
    "\n",
    "Évidemment, lorsque le nombre de caractéristiques augmente, la consommation de mémoire de chaque exemple augmente également. En effet, pour une matrice de $m$ exemples avec $n$ caractéristiques, la complexité de l'espace est en $\\mathcal{O}(mn)$. D'un point de vue informatique, cela signifie également que le nombre d'opérations de base (par exemple, les multiplications pour les produits vecteur-matrice dans les modèles linéaires) augmente également. Voici un graphique montrant l'évolution de la latence de prédiction en fonction du nombre de caractéristiques :\n",
    "\n",
    "<img alt=\"influence_of_n_features_on_latency\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_003.png\" style=\"width: 800.0px; height: 480.0px;\" />\n",
    "\n",
    "Dans l'ensemble, vous pouvez vous attendre à ce que le temps de prédiction augmente au moins de manière linéaire avec le nombre de caractéristiques (des cas non linéaires peuvent se produire en fonction de l'emprunte mémoire globale et de l'estimateur).\n",
    "\n",
    "### <a id='influence-of-the-input-data-representation'></a> 8.2.1.4. Influence de la représentation des données d'entrée\n",
    "\n",
    "Scipy fournit des structures de données de matrices creuses optimisées pour le stockage de données creuses. La principale caractéristique des formats creux est que vous ne stockez pas les zéros, donc si vos données sont creuses, vous utilisez beaucoup moins de mémoire. Une valeur non nulle dans une représentation creuse ([CSR ou CSC](https://docs.scipy.org/doc/scipy/reference/sparse.html)) ne prendra en moyenne qu'une position d'entier 32 bits + la valeur en virgule flottante de 64 bits + 32 bits supplémentaires par ligne ou colonne dans la matrice. L'utilisation d'une entrée creuse sur un modèle linéaire dense (ou creux) peut accélérer la prédiction de manière significative, car seules les caractéristiques de valeur non nulle ont un impact sur le produit scalaire et donc les prédictions du modèle. Ainsi, si vous avez 100 valeurs non nulles dans un espace de 1 million de dimensions, vous n'avez besoin que de 100 opérations de multiplication et d'addition au lieu de 1 million.\n",
    "\n",
    "Cependant, les calculs sur une représentation dense peuvent exploiter des opérations vectorielles et un multithreading hautement optimisés dans BLAS, et tendent à entraîner moins de pertes de cache du CPU. Par conséquent, la sparsité devrait généralement être assez élevée (10% de non-nuls maximum, à vérifier en fonction du matériel) pour que la représentation des données d'entrée creuse soit plus rapide que la représentation dense sur une machine avec de nombreux processeurs et une implémentation BLAS optimisée.\n",
    "\n",
    "Voici un code d'exemple pour tester la sparsité de vos données d'entrée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sparsity_ratio(X):\n",
    "    return 1.0 - np.count_nonzero(X) / float(X.shape[0] * X.shape[1])\n",
    "print(\"input sparsity ratio:\", sparsity_ratio(X))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme règle générale, vous pouvez considérer que si le taux de sparsité est supérieur à 90%, vous pouvez probablement bénéficier des formats creux. Consultez la [documentation](https://docs.scipy.org/doc/scipy/reference/sparse.html) sur les formats de matrices creuses de Scipy pour plus d'informations sur la façon de construire (ou convertir vos données en) des formats de matrices creuses. La plupart du temps, les formats `CSR` et `CSC` fonctionnent le mieux."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='influence-of-the-model-complexity'></a> 8.2.1.5. Influence de la complexité du modèle\n",
    "\n",
    "En général, lorsque la complexité du modèle augmente, la puissance prédictive et la latence sont censées augmenter. Augmenter la puissance prédictive est généralement intéressant, mais pour de nombreuses applications, il est préférable de ne pas augmenter trop la latence de prédiction. Nous allons maintenant examiner cette idée pour différentes familles de modèles supervisés.\n",
    "\n",
    "Pour [**`sklearn.linear_model`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model) (par exemple, Lasso, ElasticNet, SGDClassifier/Regressor, Ridge & RidgeClassifier, PassiveAggressiveClassifier/Regressor, LinearSVC, LogisticRegression...), la fonction de décision appliquée au moment de la prédiction est la même (un produit scalaire), donc la latence devrait être équivalente.\n",
    "\n",
    "Voici un exemple utilisant [**`SGDClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier) avec la pénalité `elasticnet`. La force de régularisation est contrôlée globalement par le paramètre `alpha`. Avec un `alpha` suffisamment élevé, on peut ensuite augmenter le paramètre `l1_ratio` d'elasticnet pour imposer différents niveaux de parcimonie dans les coefficients du modèle. Une plus grande parcimonie ici est interprétée comme une complexité de modèle moindre, car nous avons besoin de moins de coefficients pour le décrire complètement. Bien sûr, la parcimonie influence à son tour le temps de prédiction, car le produit scalaire creux prend du temps proportionnel au nombre de coefficients non nuls.\n",
    "\n",
    "![en_model_complexity](https://scikit-learn.org/stable/_images/sphx_glr_plot_model_complexity_influence_001.png)\n",
    "\n",
    "Pour la famille d'algorithmes [**`sklearn.svm`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm) avec un noyau non linéaire, la latence est liée au nombre de vecteurs de support (moins il y en a, plus c'est rapide). La latence et le débit devraient (asymptotiquement) croître linéairement avec le nombre de vecteurs de support dans un modèle SVC ou SVR. Le noyau influence également la latence car il est utilisé pour calculer la projection du vecteur d'entrée une fois par vecteur de support. Dans le graphique suivant, le paramètre `nu` de [**`NuSVR`**](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR) a été utilisé pour influencer le nombre de vecteurs de support.\n",
    "\n",
    "![nusvr_model_complexity](https://scikit-learn.org/stable/_images/sphx_glr_plot_model_complexity_influence_002.png)\n",
    "\n",
    "Pour l'ensemble d'arbres [**`sklearn.ensemble`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble) (par exemple, RandomForest, GBT, ExtraTrees, etc.), le nombre d'arbres et leur profondeur jouent le rôle le plus important. La latence et le débit devraient augmenter linéairement avec le nombre d'arbres. Dans ce cas, nous avons directement utilisé le paramètre `n_estimators` de [**`GradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor).\n",
    "\n",
    "![gbt_model_complexity](https://scikit-learn.org/stable/_images/sphx_glr_plot_model_complexity_influence_003.png)\n",
    "\n",
    "En tout cas, soyez avertis que diminuer la complexité du modèle peut nuire à l'exactitude, comme mentionné ci-dessus. Par exemple, un problème non linéairement séparable peut être traité avec un modèle linéaire rapide, mais la puissance prédictive en souffrira très probablement dans le processus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='feature-extraction-latency'></a> 8.2.1.6. Latence de l'extraction des caractéristiques\n",
    "\n",
    "La plupart des modèles de scikit-learn sont généralement assez rapides car ils sont implémentés soit avec des extensions Cython compilées, soit avec des bibliothèques de calcul optimisées. D'autre part, dans de nombreuses applications du monde réel, le processus d'extraction des caractéristiques (c'est-à-dire la transformation de données brutes telles que des lignes de base de données ou des paquets réseau en tableaux numpy) régit le temps de prédiction global. Par exemple, lors de la classification de texte Reuters, toute la préparation (lecture et analyse des fichiers SGML, tokenisation du texte et hachage dans un espace vectoriel commun) prend de 100 à 500 fois plus de temps que le code de prédiction réel, selon le modèle choisi.\n",
    "\n",
    "![prediction_time](https://scikit-learn.org/stable/_images/sphx_glr_plot_out_of_core_classification_004.png)\n",
    "\n",
    "Dans de nombreux cas, il est donc recommandé de chronométrer et de profiler attentivement votre code d'extraction de caractéristiques, car il peut être un bon point de départ pour l'optimisation lorsque votre latence globale est trop lente pour votre application."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='prediction-throughput'></a> 8.2.2. Débit de prédiction\n",
    "\n",
    "Une autre métrique importante à prendre en compte lors du dimensionnement des systèmes de production est le débit, c'est-à-dire le nombre de prédictions que vous pouvez effectuer dans un laps de temps donné. Voici un benchmark de l'exemple de [**latence de prédiction**](https://scikit-learn.org/stable/auto_examples/applications/plot_prediction_latency.html#sphx-glr-auto-examples-applications-plot-prediction-latency-py) qui mesure cette quantité pour plusieurs estimateurs sur des données synthétiques :\n",
    "\n",
    "![throughput_benchmark](https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_004.png)\n",
    "\n",
    "Ces débits sont obtenus sur un seul processus. Une façon évidente d'augmenter le débit de votre application est de créer des instances supplémentaires (généralement des processus en Python en raison du GIL) qui partagent le même modèle. On peut également ajouter des machines pour répartir la charge. Cependant, une explication détaillée sur la façon d'y parvenir dépasse le cadre de cette documentation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='tips-and-tricks'></a> 8.2.3. Conseils et astuces\n",
    "\n",
    "### <a id='linear-algebra-libraries'></a> 8.2.3.1. Bibliothèques d'algèbre linéaire\n",
    "\n",
    "Étant donné que scikit-learn s'appuie fortement sur Numpy/Scipy et sur l'algèbre linéaire en général, il est important de prendre explicitement en compte les versions de ces bibliothèques. Fondamentalement, vous devez vous assurer que Numpy est construit en utilisant une bibliothèque [BLAS](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms) / [LAPACK](https://en.wikipedia.org/wiki/LAPACK) optimisée.\n",
    "\n",
    "Tous les modèles ne bénéficient pas d'une implémentation BLAS et Lapack optimisée. Par exemple, les modèles basés sur les arbres de décision (aléatoires) ne font généralement pas appel à BLAS dans leurs boucles internes, de même que les SVM à noyau (`SVC`, `SVR`, `NuSVC`, `NuSVR`). En revanche, un modèle linéaire implémenté avec un appel BLAS DGEMM (via `numpy.dot`) bénéficiera généralement énormément d'une implémentation BLAS optimisée et permettra d'obtenir des gains de vitesse de l'ordre de plusieurs ordres de grandeur par rapport à une implémentation BLAS non optimisée.\n",
    "\n",
    "Vous pouvez afficher l'implémentation BLAS/LAPACK utilisée par votre installation NumPy/SciPy/scikit-learn avec la commande suivante :\n",
    "\n",
    "```sh\n",
    "python -c \"import sklearn; sklearn.show_versions()\"\n",
    "```\n",
    "\n",
    "**Les implémentations BLAS/LAPACK optimisées incluent :**\n",
    "* Atlas (nécessite un réglage spécifique au matériel en le reconstruisant sur la machine cible)\n",
    "* OpenBLAS\n",
    "* MKL\n",
    "* Les frameworks Apple Accelerate et vecLib (uniquement sur OSX)\n",
    "\n",
    "Vous trouverez plus d'informations sur la [page d'installation de NumPy](https://numpy.org/install/) et dans ce [billet de blog](https://danielnouri.org/notes/2012/12/19/libblas-and-liblapack-issues-and-speed,-with-scipy-and-ubuntu/) de Daniel Nouri, qui propose des instructions d'installation pas à pas pour Debian/Ubuntu."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='limiting-working-memory'></a> 8.2.3.2. Limiter la mémoire de travail\n",
    "\n",
    "Certaines calculs, lorsqu'ils sont implémentés à l'aide d'opérations vectorisées standard de Numpy, nécessitent une grande quantité de mémoire temporaire. Cela peut potentiellement épuiser la mémoire du système. Lorsque les calculs peuvent être effectués par morceaux avec une mémoire fixe, nous essayons de le faire, et permettons à l'utilisateur d'indiquer la taille maximale de cette mémoire de travail (par défaut, 1 Go) en utilisant [**`set_config`**](https://scikit-learn.org/stable/modules/generated/sklearn.set_config.html#sklearn.set_config) ou [**`config_context`**](https://scikit-learn.org/stable/modules/generated/sklearn.config_context.html#sklearn.config_context). L'exemple suivant suggère de limiter la mémoire de travail temporaire à 128 MiB :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "with sklearn.config_context(working_memory=128):\n",
    "    pass # do chunked work here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un exemple d'opération par morceaux respectant ce paramètre est [**`pairwise_distances_chunked`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances_chunked.html), qui facilite le calcul de réductions par ligne d'une matrice de distances par paires."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='model-compression'></a> 8.2.3.3. Compression de modèle\n",
    "\n",
    "La compression de modèle dans scikit-learn concerne uniquement les modèles linéaires pour le moment. Dans ce contexte, cela signifie que nous voulons contrôler la sparsité du modèle (c'est-à-dire le nombre de coordonnées non nulles dans les vecteurs du modèle). Il est généralement recommandé de combiner la sparsité du modèle avec une représentation des données d'entrée sparse.\n",
    "\n",
    "Voici un exemple de code qui illustre l'utilisation de la méthode `sparsify()` :\n",
    "\n",
    "```python\n",
    "clf = SGDRegressor(penalty='elasticnet', l1_ratio=0.25)\n",
    "clf.fit(X_train, y_train).sparsify()\n",
    "clf.predict(X_test)\n",
    "```\n",
    "\n",
    "Dans cet exemple, nous préférons la pénalité `elasticnet` car c'est souvent un bon compromis entre compacité du modèle et puissance de prédiction. On peut également ajuster davantage le paramètre 'l1_ratio' (en combinaison avec la force de régularisation 'alpha') pour contrôler ce compromis.\n",
    "\n",
    "Un [benchmark](https://github.com/scikit-learn/scikit-learn/blob/main/benchmarks/bench_sparsify.py)  typique sur des données synthétiques montre une diminution de plus de 30 % de la latence lorsque à la fois le modèle et les données d'entrée sont parcimonieux (avec un taux de parcimonie de 0,000024 et 0,027400 respectivement). Vos résultats peuvent varier en fonction de la parcimonie et de la taille de vos données et de votre modèle. De plus, la réduction de la sparsité peut être très utile pour réduire l'utilisation de mémoire des modèles prédictifs déployés sur des serveurs de production."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='model-reshaping'></a> 8.2.3.4. Remodelage du modèle\n",
    "\n",
    "Le remodelage du modèle consiste à sélectionner uniquement une partie des caractéristiques disponibles pour ajuster un modèle. En d'autres termes, si un modèle élimine des caractéristiques pendant la phase d'apprentissage, nous pouvons ensuite les supprimer de l'entrée. Cela présente plusieurs avantages. Tout d'abord, cela réduit la surcharge de mémoire (et donc de temps) du modèle lui-même. Cela permet également d'éliminer les composants de sélection explicite des caractéristiques dans un pipeline une fois que nous savons quelles caractéristiques conserver à partir d'une exécution précédente. Enfin, cela peut aider à réduire le temps de traitement et l'utilisation d'E/S en amont dans les couches d'accès aux données et d'extraction des caractéristiques en ne collectant pas et en ne construisant pas les caractéristiques qui sont éliminées par le modèle. Par exemple, si les données brutes proviennent d'une base de données, cela peut permettre d'écrire des requêtes plus simples et plus rapides ou de réduire l'utilisation d'E/S en faisant en sorte que les requêtes renvoient des enregistrements plus légers. Pour le moment, le remodelage doit être effectué manuellement dans scikit-learn. Dans le cas d'une entrée creuse (en particulier au format `CSR`), il est généralement suffisant de ne pas générer les caractéristiques pertinentes, en laissant leurs colonnes vides."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='model-reshaping'></a> 8.2.3.5. Liens\n",
    "\n",
    "* [Documentation des performances pour les développeurs scikit-learn](https://scikit-learn.org/stable/developers/performance.html#performance-howto)\n",
    "* [Documentation des formats de matrices creuses de Scipy](https://docs.scipy.org/doc/scipy/reference/sparse.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='parallelism-resource-management-and-configuration'></a> 8.3. Parallélisme, gestion des ressources, et configuration\n",
    "\n",
    "## <a id='parallelism'></a> 8.3.1. Parallélisme\n",
    "\n",
    "Certains estimateurs et utilitaires de scikit-learn parallélisent les opérations coûteuses en utilisant plusieurs cœurs de CPU.\n",
    "\n",
    "En fonction du type d'estimateur et parfois des valeurs des paramètres du constructeur, cela se fait soit :\n",
    "- avec un parallélisme de plus haut niveau via [joblib](https://joblib.readthedocs.io/en/latest/).\n",
    "- avec un parallélisme de plus bas niveau via OpenMP, utilisé dans du code C ou Cython.\n",
    "- avec un parallélisme de plus bas niveau via BLAS, utilisé par NumPy et SciPy pour les opérations génériques sur les tableaux.\n",
    "\n",
    "Le paramètre `n_jobs` des estimateurs contrôle toujours la quantité de parallélisme gérée par joblib (processus ou threads en fonction du backend de joblib). Le parallélisme au niveau des threads géré par OpenMP dans le code Cython propre à scikit-learn ou par les bibliothèques BLAS & LAPACK utilisées par les opérations NumPy et SciPy utilisées dans scikit-learn est toujours contrôlé par les variables d'environnement ou par `threadpoolctl`, comme expliqué ci-dessous. Notez que certains estimateurs peuvent exploiter les trois types de parallélisme à différents moments de leurs méthodes d'apprentissage et de prédiction.\n",
    "\n",
    "Nous décrivons ces 3 types de parallélisme dans les sous-sections suivantes en détail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='higher-level-parallelism-with-joblib'></a> 8.3.1.1. Parallélisme de plus haut niveau avec joblib\n",
    "\n",
    "Lorsque l'implémentation sous-jacente utilise joblib, le nombre de travailleurs (threads ou processus) qui sont lancés en parallèle peut être contrôlé via le paramètre `n_jobs`.\n",
    "\n",
    "> **Note** : L'endroit (et la manière) où la parallélisation se produit dans les estimateurs en utilisant joblib en spécifiant `n_jobs` est actuellement mal documenté. Aidez-nous à améliorer notre documentation en abordant le [problème 14228](https://github.com/scikit-learn/scikit-learn/issues/14228) !\n",
    "\n",
    "Joblib est capable de prendre en charge à la fois le multi-processing et le multi-threading. Que joblib choisisse de lancer un thread ou un processus dépend du **backend** qu'il utilise.\n",
    "\n",
    "scikit-learn s'appuie généralement sur le backend `loky`, qui est le backend par défaut de joblib. Loky est un backend de multi-processing. Lorsqu'il effectue du multi-processing, afin d'éviter de dupliquer la mémoire dans chaque processus (ce qui n'est pas raisonnable avec de grands ensembles de données), joblib va créer une [**`numpy.memmap`**](https://numpy.org/doc/stable/reference/generated/numpy.memmap.html) que tous les processus peuvent partager lorsque les données dépassent 1 Mo.\n",
    "\n",
    "Dans certains cas spécifiques (lorsque le code exécuté en parallèle libère le GIL (Global Interpreter Lock)), scikit-learn indiquera à `joblib` qu'un backend de multi-threading est préférable.\n",
    "\n",
    "En tant qu'utilisateur, vous pouvez contrôler le backend que joblib utilisera (indépendamment de ce que scikit-learn recommande) en utilisant un gestionnaire de contexte :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import parallel_backend\n",
    "\n",
    "with parallel_backend('threading', n_jobs=2):\n",
    "    # Your scikit-learn code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veuillez vous référer à la [documentation de joblib](https://joblib.readthedocs.io/en/latest/parallel.html#thread-based-parallelism-vs-process-based-parallelism) pour plus de détails.\n",
    "\n",
    "En pratique, l'utilité du parallélisme pour améliorer le temps d'exécution dépend de nombreux facteurs. Il est généralement préférable de faire des expérimentations plutôt que de supposer que l'augmentation du nombre de travailleurs est toujours bénéfique. Dans certains cas, il peut être très préjudiciable aux performances d'exécuter plusieurs copies d'estimateurs ou de fonctions en parallèle (voir la section sur la sursouscription ci-dessous)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='lower-level-parallelism-with-openmp'></a> 8.3.1.2. Parallélisme de plus bas niveau avec OpenMP\n",
    "\n",
    "OpenMP est utilisé pour paralléliser du code écrit en Cython ou en C, en s'appuyant exclusivement sur le multi-threading. Par défaut, les implémentations utilisant OpenMP utiliseront autant de threads que possible, c'est-à-dire autant de threads que de cœurs logiques.\n",
    "\n",
    "Vous pouvez contrôler le nombre exact de threads utilisés de deux manières :\n",
    "\n",
    "- via la variable d'environnement `OMP_NUM_THREADS`, par exemple lorsque vous exécutez un script Python :\n",
    "```sh\n",
    "$ OMP_NUM_THREADS=4 python my_script.py\n",
    "```\n",
    "- ou via `threadpoolctl`, comme expliqué dans [cette documentation](https://github.com/joblib/threadpoolctl/#setting-the-maximum-size-of-thread-pools).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='parallel-numpy-and-scipy-routines-from-numerical-libraries'></a> 8.3.1.3. Routines parallèles de NumPy et SciPy à partir de bibliothèques numériques\n",
    "\n",
    "scikit-learn s'appuie fortement sur NumPy et SciPy, qui font appel en interne à des routines d'algèbre linéaire multi-threadées (BLAS & LAPACK) implémentées dans des bibliothèques telles que MKL, OpenBLAS ou BLIS.\n",
    "\n",
    "Vous pouvez contrôler le nombre exact de threads utilisés par BLAS pour chaque bibliothèque en utilisant des variables d'environnement, à savoir :\n",
    "\n",
    "- `MKL_NUM_THREADS` définit le nombre de threads utilisés par MKL.\n",
    "- `OPENBLAS_NUM_THREADS` définit le nombre de threads utilisés par OpenBLAS.\n",
    "- `BLIS_NUM_THREADS` définit le nombre de threads utilisés par BLIS.\n",
    "\n",
    "Notez que les implémentations de BLAS & LAPACK peuvent également être impactées par `OMP_NUM_THREADS`. Pour vérifier si c'est le cas dans votre environnement, vous pouvez inspecter comment le nombre de threads effectivement utilisés par ces bibliothèques est affecté en exécutant la commande suivante dans un terminal bash ou zsh pour différentes valeurs de `OMP_NUM_THREADS` :\n",
    "\n",
    "```sh\n",
    "$ OMP_NUM_THREADS=2 python -m threadpoolctl -i numpy scipy\n",
    "```\n",
    "\n",
    "> **Remarque** : Au moment de la rédaction (2022), les packages NumPy et SciPy distribués sur pypi.org (c'est-à-dire ceux installés via `pip install`) et sur le canal conda-forge (c'est-à-dire ceux installés via `conda install --channel conda-forge`) sont liés à OpenBLAS, tandis que les packages NumPy et SciPy fournis sur le canal par défaut de conda depuis Anaconda.org (c'est-à-dire ceux installés via `conda install`) sont liés par défaut à MKL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='oversubscription-spawning-too-many-threads'></a> 8.3.1.4. Sursouscription : engendrer trop de threads\n",
    "\n",
    "Il est généralement recommandé d'éviter d'utiliser significativement plus de processus ou de threads que le nombre de CPU sur une machine. La sursouscription se produit lorsque l'on exécute simultanément trop de threads dans un programme.\n",
    "\n",
    "Supposons que vous disposiez d'une machine avec 8 CPU. Considérons un cas où vous exécutez une [**`GridSearchCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) (parallélisée avec joblib) avec `n_jobs=8` sur un [**`HistGradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html) (parallélisé avec OpenMP). Chaque instance de [**`HistGradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html) va engendrer 8 threads (puisque vous avez 8 CPU). Cela fait un total de `8 * 8 = 64` threads, ce qui entraîne une sursouscription des ressources CPU physiques et donc une surcharge de planification.\n",
    "\n",
    "La sursouscription peut se produire de la même manière avec des routines parallèles de MKL, OpenBLAS ou BLIS qui sont imbriquées dans des appels joblib.\n",
    "\n",
    "À partir de `joblib >= 0.14`, lorsque le backend `loky` est utilisé (qui est le backend par défaut), joblib indiquera à ses **processus** enfants de limiter le nombre de threads qu'ils peuvent utiliser afin d'éviter la sursouscription. En pratique, l'heuristique utilisée par joblib est d'indiquer aux processus d'utiliser `max_threads = n_cpus // n_jobs`, via leur variable d'environnement correspondante. Revenons à notre exemple précédent, puisque le backend joblib de [**`GridSearchCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) est `loky`, chaque processus ne pourra utiliser qu'un seul thread au lieu de 8, atténuant ainsi le problème de sursouscription.\n",
    "\n",
    "Notez que :\n",
    "- La définition manuelle de l'une des variables d'environnement (`OMP_NUM_THREADS`, `MKL_NUM_THREADS`, `OPENBLAS_NUM_THREADS` ou `BLIS_NUM_THREADS`) aura priorité sur ce que joblib essaie de faire. Le nombre total de threads sera `n_jobs * <LIB>_NUM_THREADS`. Notez que la définition de cette limite affectera également vos calculs dans le processus principal, qui n'utilisera que `<LIB>_NUM_THREADS`. Joblib expose un gestionnaire de contexte pour un contrôle plus précis du nombre de threads dans ses travailleurs (voir la documentation de joblib liée ci-dessous).\n",
    "- Lorsque joblib est configuré pour utiliser le backend `threading`, il n'y a aucun mécanisme pour éviter la sursouscription lors de l'appel aux bibliothèques natives parallèles dans les threads gérés par joblib.\n",
    "- Tous les estimateurs de scikit-learn qui dépendent explicitement d'OpenMP dans leur code Cython utilisent toujours `threadpoolctl` en interne pour adapter automatiquement le nombre de threads utilisés par OpenMP et les appels BLAS éventuellement imbriqués afin d'éviter la sursouscription.\n",
    "\n",
    "Vous trouverez des détails supplémentaires sur la mitigation de la sursouscription par joblib dans la [documentation de joblib](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources).\n",
    "\n",
    "Vous trouverez des détails supplémentaires sur le parallélisme dans les bibliothèques numériques Python dans [ce document de Thomas J. Fan](https://thomasjpfan.github.io/parallelism-python-libraries-design/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='configuration-switches'></a> 8.3.2. Commutateurs de configuration\n",
    "\n",
    "### <a id='python-api'></a> 8.3.2.1. API Python\n",
    "\n",
    "[**`sklearn.set_config`**](https://scikit-learn.org/stable/modules/generated/sklearn.set_config.html#sklearn.set_config) et [**`sklearn.config_context`**](https://scikit-learn.org/stable/modules/generated/sklearn.config_context.html) peuvent être utilisés pour modifier les paramètres de configuration qui contrôlent les aspects du parallélisme.\n",
    "\n",
    "### <a id='environment-variables'></a> 8.3.2.2. Variables d'environnement\n",
    "\n",
    "Ces variables d'environnement doivent être définies avant d'importer scikit-learn.\n",
    "\n",
    "#### `SKLEARN_ASSUME_FINITE`\n",
    "\n",
    "Définit la valeur par défaut de l'argument `assume_finite` de [**`sklearn.set_config`**](https://scikit-learn.org/stable/modules/generated/sklearn.set_config.html#sklearn.set_config).\n",
    "\n",
    "#### `SKLEARN_WORKING_MEMORY`\n",
    "\n",
    "Définit la valeur par défaut de l'argument `working_memory` de [**`sklearn.set_config`**](https://scikit-learn.org/stable/modules/generated/sklearn.set_config.html#sklearn.set_config).\n",
    "\n",
    "#### `SKLEARN_SEED`\n",
    "\n",
    "Définit la graine du générateur aléatoire global lors de l'exécution des tests, pour la reproductibilité.\n",
    "\n",
    "Notez que les tests de scikit-learn sont censés s'exécuter de manière déterministe avec un ensemencement explicite de leurs propres instances de RNG indépendantes, plutôt que de s'appuyer sur les singletons RNG de numpy ou de la bibliothèque standard Python, afin de garantir que les résultats des tests sont indépendants de l'ordre d'exécution des tests. Cependant, certains tests peuvent oublier d'utiliser un ensemencement explicite et cette variable est un moyen de contrôler l'état initial des singletons mentionnés.\n",
    "\n",
    "#### `SKLEARN_TESTS_GLOBAL_RANDOM_SEED`\n",
    "\n",
    "Contrôle l'ensemencement du générateur de nombres aléatoires utilisé dans les tests qui dépendent de la fixture `global_random_seed`.\n",
    "\n",
    "Tous les tests utilisant cette fixture acceptent le contrat selon lequel ils doivent passer de manière déterministe pour toute valeur de graine de 0 à 99 inclus.\n",
    "\n",
    "Si la variable d'environnement `SKLEARN_TESTS_GLOBAL_RANDOM_SEED` est définie sur `\"any\"` (ce qui devrait être le cas lors des builds nocturnes sur le CI), la fixture choisira une graine arbitraire dans la plage mentionnée ci-dessus (en fonction du numéro de build ou du jour actuel) et tous les tests fixés s'exécuteront pour cette graine spécifique. L'objectif est de garantir que, avec le temps, notre CI exécutera tous les tests avec des graines différentes tout en limitant la durée des tests pour une exécution unique de l'ensemble complet des tests. Cela permettra de vérifier que les assertions des tests écrits pour utiliser cette fixture ne dépendent pas d'une valeur de graine spécifique.\n",
    "\n",
    "La plage de valeurs de graine admissibles est limitée à [0, 99] car il est souvent impossible d'écrire un test qui peut fonctionner pour n'importe quelle graine possible et nous souhaitons éviter d'avoir des tests qui échouent de manière aléatoire sur le CI.\n",
    "\n",
    "Valeurs valides pour `SKLEARN_TESTS_GLOBAL_RANDOM_SEED` :\n",
    "- `SKLEARN_TESTS_GLOBAL_RANDOM_SEED=\"42\"` : exécute les tests avec une graine fixe de 42\n",
    "- `SKLEARN_TESTS_GLOBAL_RANDOM_SEED=\"40-42\"` : exécute les tests avec toutes les graines entre 40 et 42 incluses\n",
    "- `SKLEARN_TESTS_GLOBAL_RANDOM_SEED=\"any\"` : exécute les tests avec une graine arbitraire sélectionnée entre 0 et 99 inclus\n",
    "- `SKLEARN_TESTS_GLOBAL_RANDOM_SEED=\"all\"` : exécute les tests avec toutes les graines entre 0 et 99 incluses. Cela peut prendre beaucoup de temps : à utiliser uniquement pour des tests individuels, pas pour l'ensemble complet des tests !\n",
    "\n",
    "Si la variable n'est pas définie, alors 42 est utilisé comme graine globale de manière déterministe. Cela garantit que, par défaut, l'ensemble des tests de scikit-learn est aussi déterministe que possible afin d'éviter de perturber nos aimables mainteneurs de packages tiers. De même, cette variable ne doit pas être définie dans la configuration CI des pull-requests afin de s'assurer que nos aimables contributeurs ne sont pas les premières personnes à rencontrer une régression de sensibilité aux graines dans un test sans rapport avec les modifications de leur propre PR. Seuls les\n",
    "\n",
    " mainteneurs de scikit-learn qui surveillent les résultats des builds nocturnes sont censés être ennuyés par cela.\n",
    "\n",
    "Lors de l'écriture d'une nouvelle fonction de test utilisant cette fixture, veuillez utiliser la commande suivante pour vous assurer qu'elle passe de manière déterministe pour toutes les graines admissibles sur votre machine locale :\n",
    "\n",
    "```sh\n",
    "$ SKLEARN_TESTS_GLOBAL_RANDOM_SEED=\"all\" pytest -v -k test_your_test_name`\n",
    "```\n",
    "\n",
    "#### `SKLEARN_SKIP_NETWORK_TESTS`\n",
    "\n",
    "Lorsque cette variable d'environnement est définie sur une valeur différente de zéro, les tests nécessitant un accès réseau sont ignorés. Lorsque cette variable d'environnement n'est pas définie, les tests réseau sont ignorés.\n",
    "\n",
    "#### `SKLEARN_RUN_FLOAT32_TESTS`\n",
    "\n",
    "Lorsque cette variable d'environnement est définie sur '1', les tests utilisant la fixture `global_dtype` sont également exécutés sur des données de type float32. Lorsque cette variable d'environnement n'est pas définie, les tests sont uniquement exécutés sur des données de type float64.\n",
    "\n",
    "#### `SKLEARN_ENABLE_DEBUG_CYTHON_DIRECTIVES`\n",
    "\n",
    "Lorsque cette variable d'environnement est définie sur une valeur différente de zéro, la directive `Cython` `boundscheck` est définie sur `True`. Cela est utile pour trouver des erreurs de segmentation.\n",
    "\n",
    "#### `SKLEARN_BUILD_ENABLE_DEBUG_SYMBOLS`\n",
    "\n",
    "Lorsque cette variable d'environnement est définie sur une valeur différente de zéro, les symboles de débogage seront inclus dans les extensions C compilées. Seuls les symboles de débogage pour les systèmes POSIX sont configurés.\n",
    "\n",
    "#### `SKLEARN_PAIRWISE_DIST_CHUNK_SIZE`\n",
    "\n",
    "Cela définit la taille du morceau à utiliser par les implémentations sous-jacentes de `PairwiseDistancesReductions`. La valeur par défaut est de 256, ce qui s'est révélé adéquat sur la plupart des machines.\n",
    "\n",
    "Les utilisateurs recherchant les meilleures performances peuvent ajuster cette variable en utilisant des puissances de 2 afin d'obtenir le meilleur comportement de parallélisme pour leur matériel, en particulier en ce qui concerne les tailles de cache."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9aff9e50adfaa9e30c910fb3872ffdc72747acb5f50803ca0504f00e980f7c25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
