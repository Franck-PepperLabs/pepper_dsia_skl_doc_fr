{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='supervised-learning'></a> 1. [**Apprentissage supervis√©**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_supervised_learning.ipynb#supervised-learning)</br>([*Supervised learning*](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning))\n",
    "\n",
    "# <a id='semi-supervised-learning'></a> 1.14. [**Apprentissage semi-supervis√©**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_14_semi_supervised.ipynb)<br/>([_Semi-supervised learning_](https://scikit-learn.org/stable/modules/semi_supervised.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 3 pages, 6 exemples, 3 papiers\n",
    "- 1.14.1. [**Auto-Entra√Ænement**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_14_semi_supervised.ipynb#self-training)<br/>([_Self Training_](https://scikit-learn.org/stable/modules/semi_supervised.html#self-training))\n",
    "- 1.14.2. [**Propagation d'√©tiquettes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_14_semi_supervised.ipynb#label-propagation)<br/>([_Label Propagation_](https://scikit-learn.org/stable/modules/semi_supervised.html#label-propagation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='semi-supervised-learning'></a> 1.14. **Apprentissage semi-supervis√©**<br/>([_Semi-supervised learning_](https://scikit-learn.org/stable/modules/semi_supervised.html))\n",
    "\n",
    "L'[**apprentissage semi-supervis√©**](https://en.wikipedia.org/wiki/Weak_supervision) est une situation dans laquelle, dans vos donn√©es d'entra√Ænement, certains des √©chantillons ne sont pas √©tiquet√©s. Les estimateurs semi-supervis√©s dans [**`sklearn.semi_supervised`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.semi_supervised) sont capables d'utiliser ces donn√©es non √©tiquet√©es suppl√©mentaires pour mieux capturer la forme de la distribution sous-jacente des donn√©es et g√©n√©raliser plus efficacement vers de nouveaux √©chantillons. Ces algorithmes peuvent fonctionner de mani√®re optimale lorsque vous disposez d'un tr√®s petit nombre de points √©tiquet√©s et d'un grand nombre de points non √©tiquet√©s.\n",
    "\n",
    "**Entr√©es non √©tiquet√©es dans `y`**\n",
    "\n",
    "Il est important d'attribuer un identifiant aux points non √©tiquet√©s en plus des donn√©es √©tiquet√©es lors de l'entra√Ænement du mod√®le avec la m√©thode `fit`. L'identifiant utilis√© par cette impl√©mentation est la valeur enti√®re $-1$. Notez que pour les √©tiquettes de type cha√Æne, le type de donn√©es de `y` doit √™tre `object` pour qu'il puisse contenir √† la fois des cha√Ænes et des entiers.\n",
    "\n",
    "> **Remarque :** Les algorithmes semi-supervis√©s doivent faire des hypoth√®ses sur la distribution de l'ensemble de donn√©es afin d'am√©liorer leurs performances. Consultez [**ici**](https://en.wikipedia.org/wiki/Semi-supervised_learning#Assumptions) pour plus de d√©tails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='self-training'></a> 1.14.1. [**Auto-Entra√Ænement**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_14_semi_supervised.ipynb#self-training)<br/>([_Self Training_](https://scikit-learn.org/stable/modules/semi_supervised.html#self-training))\n",
    "\n",
    "Cette impl√©mentation d'auto-entra√Ænement est bas√©e sur l'algorithme de Yarowsky [1]. En utilisant cet algorithme, un classifieur supervis√© donn√© peut fonctionner comme un classifieur semi-supervis√©, lui permettant d'apprendre √† partir de donn√©es non √©tiquet√©es.\n",
    "\n",
    "[**`SelfTrainingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.SelfTrainingClassifier.html#sklearn.semi_supervised.SelfTrainingClassifier) peut √™tre appel√© avec n'importe quel classificateur qui impl√©mente `predict_proba`, pass√© en tant que param√®tre `base_classifier`. √Ä chaque it√©ration, le `base_classifier` pr√©dit les √©tiquettes des √©chantillons non √©tiquet√©s et ajoute un sous-ensemble de ces √©tiquettes √† l'ensemble de donn√©es √©tiquet√©es.\n",
    "\n",
    "Le choix de ce sous-ensemble est d√©termin√© par le crit√®re de s√©lection. Cette s√©lection peut √™tre effectu√©e √† l'aide d'un `seuil` sur les probabilit√©s de pr√©diction, ou en choisissant les `k_best` √©chantillons en fonction des probabilit√©s de pr√©diction.\n",
    "\n",
    "Les √©tiquettes utilis√©es pour l'ajustement final ainsi que l'it√©ration √† laquelle chaque √©chantillon a √©t√© √©tiquet√© sont disponibles en tant qu'attributs. Le param√®tre `max_iter` facultatif sp√©cifie combien de fois la boucle est ex√©cut√©e au maximum.\n",
    "\n",
    "Le param√®tre `max_iter` peut √™tre d√©fini sur `None`, ce qui fait it√©rer l'algorithme jusqu'√† ce que tous les √©chantillons aient des √©tiquettes ou qu'aucun nouvel √©chantillon ne soit s√©lectionn√© lors de cette it√©ration.\n",
    "\n",
    "**Remarque :** Lors de l'utilisation du classifieur d'auto-entra√Ænement, l'[**√©talonnage** (1.16)](https://scikit-learn.org/stable/modules/calibration.html#calibration) du classifieur est important.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Effet de la variation du seuil pour l'auto-entra√Ænement**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_14_semi_supervised/plot_self_training_varying_threshold.ipynb)<br/>([_Effect of varying threshold for self-training_](https://scikit-learn.org/stable/auto_examples/semi_supervised/plot_self_training_varying_threshold.html))\n",
    "\n",
    "##### [**Fronti√®re de d√©cision des classifieurs semi-supervis√©s par rapport aux SVM sur l'ensemble de donn√©es Iris**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_14_semi_supervised/plot_self_training_varying_threshold.ipynb)<br/>([_Decision boundary of semi-supervised classifiers versus SVM on the Iris dataset_](https://scikit-learn.org/stable/auto_examples/semi_supervised/plot_self_training_varying_threshold.html))\n",
    "\n",
    "#### R√©f√©rences\n",
    "\n",
    "üî¨ [1] [**‚ÄúUnsupervised Word Sense Disambiguation Rivaling Supervised Methods‚Äù**](http://dl.acm.org/ft_gateway.cfm?id=981684&type=pdf) David Yarowsky, Proceedings of the 33rd annual meeting on Association for Computational Linguistics (ACL ‚Äò95). Association for Computational Linguistics, Stroudsburg, PA, USA, 189-196."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='label-propagation'></a> 1.14.2. [**Propagation d'√©tiquettes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_14_semi_supervised.ipynb#label-propagation)<br/>([_Label Propagation_](https://scikit-learn.org/stable/modules/semi_supervised.html#label-propagation))\n",
    "\n",
    "La propagation d'√©tiquettes d√©signe quelques variations d'algorithmes d'inf√©rence de graphe semi-supervis√©s.\n",
    "\n",
    "**Quelques caract√©ristiques disponibles dans ce mod√®le :**\n",
    "- Utilis√© pour les t√¢ches de classification.\n",
    "- M√©thodes de noyau pour projeter les donn√©es dans des espaces dimensionnels alternatifs.\n",
    "\n",
    "`scikit-learn` propose deux mod√®les de propagation d'√©tiquettes : [**`LabelPropagation`**](https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelPropagation.html#sklearn.semi_supervised.LabelPropagation) et [**`LabelSpreading`**](https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelSpreading.html#sklearn.semi_supervised.LabelSpreading). Les deux mod√®les fonctionnent en construisant un graphe de similarit√© sur l'ensemble des √©l√©ments de l'ensemble de donn√©es en entr√©e.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_label_propagation_structure_001.png\"\n",
    "    alt=\"Apprentissage de la propagation d'√©tiquettes d'une structure complexe\"\n",
    "    style=\"max-width: 30%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "**Une illustration de la propagation d'√©tiquettes :** _la structure des observations non √©tiquet√©es est coh√©rente avec la structure des classes, et donc l'√©tiquette de classe peut √™tre propag√©e aux observations non √©tiquet√©es de l'ensemble d'entra√Ænement._\n",
    "\n",
    "[**`LabelPropagation`**](https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelPropagation.html#sklearn.semi_supervised.LabelPropagation) et [**`LabelSpreading`**](https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelSpreading.html#sklearn.semi_supervised.LabelSpreading) diff√®rent par les modifications apport√©es √† la matrice de similarit√© du graphe et par l'effet de resserrement sur les distributions d'√©tiquettes. Le resserrement permet √† l'algorithme de changer le poids des donn√©es √©tiquet√©es r√©elles dans une certaine mesure. L'algorithme [**`LabelPropagation`**](https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelPropagation.html#sklearn.semi_supervised.LabelPropagation) effectue un resserrement fort des √©tiquettes d'entr√©e, ce qui signifie $\\alpha=0$. Ce facteur de resserrement peut √™tre assoupli, par exemple $\\alpha=0.2$, ce qui signifie que nous conserverons toujours 80 % de notre distribution d'√©tiquettes d'origine, mais que l'algorithme pourra changer sa confiance dans la distribution dans les 20 % restants.\n",
    "\n",
    "[**`LabelPropagation`**](https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelPropagation.html#sklearn.semi_supervised.LabelPropagation) utilise la matrice de similarit√© brute construite √† partir des donn√©es sans aucune modification. En revanche, [**`LabelSpreading`**](https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelSpreading.html#sklearn.semi_supervised.LabelSpreading) minimise une fonction de perte avec des propri√©t√©s de r√©gularisation, ce qui le rend souvent plus robuste au bruit. L'algorithme it√®re sur une version modifi√©e du graphe d'origine et normalise les poids des ar√™tes en calculant la matrice laplacienne du graphe normalis√©. Cette proc√©dure est √©galement utilis√©e dans le [**regroupement spectral** (2.3.5)](https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering).\n",
    "\n",
    "Les mod√®les de propagation d'√©tiquettes disposent de deux m√©thodes de noyau int√©gr√©es. Le choix du noyau affecte √† la fois la capacit√© de mont√©e en charge et les performances des algorithmes. Les suivantes sont disponibles :\n",
    "- rbf ($\\exp(-\\gamma |x-y|^2), \\gamma > 0$). $\\gamma$ est sp√©cifi√© par le mot-cl√© `gamma`.\n",
    "- knn ($1[x' \\in kNN(x)]$). $k$ est sp√©cifi√© par le mot-cl√© `n_neighbors`.\n",
    "\n",
    "Le noyau RBF produira un graphe enti√®rement connect√©, repr√©sent√© en m√©moire par une matrice dense. Cette matrice peut √™tre tr√®s volumineuse et combin√©e avec le co√ªt de la r√©alisation d'un calcul de multiplication de matrices complet √† chaque it√©ration de l'algorithme, cela peut entra√Æner des temps d'ex√©cution prohibitivement longs. En revanche, le noyau kNN produira une matrice creuse beaucoup plus raisonnable en termes de m√©moire, ce qui peut r√©duire consid√©rablement les temps d'ex√©cution.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Fronti√®re de d√©cision des classifieurs semi-supervis√©s par rapport aux SVM sur l'ensemble de donn√©es Iris**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_14_semi_supervised/plot_self_training_varying_threshold.ipynb)<br/>([_Decision boundary of semi-supervised classifiers versus SVM on the Iris dataset_](https://scikit-learn.org/stable/auto_examples/semi_supervised/plot_self_training_varying_threshold.html))\n",
    "\n",
    "##### [**Apprentissage de la propagation d'√©tiquettes d'une structure complexe**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data_science_practising/blob/main/Sklearn/examples/1_14_semi_supervised/plot_label_propagation_structure.ipynb)<br/>([_Label Propagation learning a complex structure_](https://scikit-learn.org/stable/auto_examples/semi_supervised/plot_label_propagation_structure.html))\n",
    "\n",
    "##### [**Propagation d'√©tiquettes de chiffres : d√©monstration des performances**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data_science_practising/blob/main/Sklearn/examples/1_14_semi_supervised/plot_label_propagation_digits.ipynb)<br/>([_Label Propagation digits: Demonstrating performance_](https://scikit-learn.org/stable/auto_examples/semi_supervised/plot_label_propagation_digits.html))\n",
    "\n",
    "##### [**Propagation d'√©tiquettes de chiffres : apprentissage actif**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data_science_practising/blob/main/Sklearn/examples/1_14_semi_supervised/plot_label_propagation_digits_active_learning.ipynb)<br/>([_Label Propagation digits active learning_](https://scikit-learn.org/stable/auto_examples/semi_supervised/plot_label_propagation_digits_active_learning.html))\n",
    "\n",
    "#### R√©f√©rences\n",
    "\n",
    "üî¨ [2] Yoshua Bengio, Olivier Delalleau, Nicolas Le Roux. [**‚ÄúLabel Propagation and Quadratic Criterion‚Äù**](http://www.iro.umontreal.ca/~lisa/bib/pub_subject/finance/pointeurs/bengio_ssl.pdf), In Semi-Supervised Learning (2006), pp. 193-216\n",
    "\n",
    "üî¨ [3] Olivier Delalleau, Yoshua Bengio, Nicolas Le Roux. [**‚ÄúEfficient Non-Parametric Function Induction in Semi-Supervised Learning‚Äù**](https://www.gatsby.ucl.ac.uk/aistats/fullpapers/204.pdf). AISTAT 2005"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
