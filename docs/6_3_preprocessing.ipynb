{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='preprocessing-data'></a> 6.3. [**Prétraitement des données**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_3_preprocessing.ipynb#preprocessing-data)</br>([*Preprocessing data*](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-data))\n",
    "\n",
    "Le package `sklearn.preprocessing` fournit plusieurs fonctions utilitaires courantes et des classes de transformeurs pour changer les vecteurs de caractéristiques brutes en une représentation plus adaptée aux estimateurs aval.\n",
    "\n",
    "En général, les algorithmes d'apprentissage bénéficient d'une standardisation de l'ensemble de données. Si des outliers sont présents dans l'ensemble, des redimensionneurs ou transformateurs robustes sont plus appropriés. Le comportement des différents redimensionneurs, transformeurs et normaliseurs sur un jeu de données contenant des outliers marginaux est mis en évidence dans [**Comparaison de l'effet de différents redimensionneurs sur des données avec des outliers**](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='standardization-or-mean-removal-and-variance-scaling'></a> 6.3.1. Standardisation, ou suppression de la moyenne et normalisation de la variance\n",
    "\n",
    "La **standardisation** des ensembles de données est **une exigence courante pour de nombreux estimateurs d'apprentissage automatique** mis en œuvre dans scikit-learn; ils pourraient mal se comporter si les caractéristiques individuelles ne ressemblent pas plus ou moins à des données distribuées suivant la distribution normale standard : Gaussienne avec une **moyenne nulle et une variance unitaire**.\n",
    "\n",
    "En pratique, nous ignorons souvent la forme de la distribution et transformons simplement les données pour les centrer en supprimant la valeur moyenne de chaque caractéristique, puis en les dimensionnant en divisant les caractéristiques non constantes par leur écart-type.\n",
    "\n",
    "Par exemple, de nombreux éléments utilisés dans la fonction objectif d'un algorithme d'apprentissage (tels que le noyau RBF des Machines à Vecteurs de Support ou les régulariseurs $\\ell_1$ et $\\ell_2$ des modèles linéaires) peuvent supposer que toutes les caractéristiques sont centrées autour de zéro ou ont une variance de même ordre. Si une caractéristique a une variance qui est des ordres de grandeur plus grande que les autres, elle pourrait dominer la fonction objectif et empêcher l'estimateur d'apprendre correctement des autres caractéristiques comme prévu.\n",
    "\n",
    "Le module [**`preprocessing`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) fournit la classe utilitaire [**`StandardScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html), qui est un moyen rapide et facile pour effectuer les opérations suivantes sur un jeu de données de type tableau :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "scaler\n",
    "# StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.        , 0.33333333])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.mean_\n",
    "# array([1. ..., 0. ..., 0.33...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81649658, 0.81649658, 1.24721913])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.scale_\n",
    "#array([0.81..., 0.81..., 1.24...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled = scaler.transform(X_train)\n",
    "X_scaled\n",
    "# array([[ 0.  ..., -1.22...,  1.33...],\n",
    "#        [ 1.22...,  0.  ..., -0.26...],\n",
    "#        [-1.22...,  1.22..., -1.06...]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données mises à l'échelle ont une moyenne nulle et une variance unitaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled.mean(axis=0)\n",
    "# array([0., 0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled.std(axis=0)\n",
    "# array([1., 1., 1.])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette classe implémente l'API `Transformer` pour calculer la moyenne et l'écart-type sur un ensemble d'entraînement, afin de pouvoir réappliquer la même transformation sur l'ensemble de test. Cette classe est donc adaptée aux premières étapes d'un [**`Pipeline`**](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;logisticregression&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;logisticregression&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('logisticregression', LogisticRegression())])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y = make_classification(random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "pipe.fit(X_train, y_train)  # apply scaling on training data\n",
    "# Pipeline(steps=[('standardscaler', StandardScaler()),\n",
    "#                 ('logisticregression', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(X_test, y_test)  # apply scaling on testing data, without leaking training data.\n",
    "# 0.96"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible de désactiver le centrage ou la mise à l'échelle en passant respectivement les paramètres `with_mean=False` ou `with_std=False` au constructeur de [**`StandardScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='scaling-features-to-a-range'></a> 6.3.1.1. Mise à l'échelle des caractéristiques dans une plage donnée\n",
    "\n",
    "Une autre méthode de standardisation consiste à mettre à l'échelle les caractéristiques pour ramener entre des valeurs minimale et maximale données, souvent entre zéro et un, ou de manière à ce que la valeur absolue maximale de chaque caractéristique soit mise à l'échelle à une taille unitaire. Cela peut être réalisé en utilisant [**`MinMaxScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) ou [**`MaxAbsScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html), respectivement.\n",
    "\n",
    "La motivation pour utiliser cette mise à l'échelle comprend la robustesse par rapport aux écarts types très faibles des caractéristiques et la préservation des entrées nulles dans les données éparses.\n",
    "\n",
    "Voici un exemple pour mettre à l'échelle une matrice de données factices dans la plage `[0, 1]` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.        , 1.        ],\n",
       "       [1.        , 0.5       , 0.33333333],\n",
       "       [0.        , 1.        , 0.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "X_train_minmax\n",
    "# array([[0.5       , 0.        , 1.        ],\n",
    "#        [1.        , 0.5       , 0.33333333],\n",
    "#        [0.        , 1.        , 0.        ]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La même instance du transformeur peut ensuite être appliquée à de nouvelles données de test qui n'ont pas été vues lors de l'appel à la méthode `fit` : les mêmes opérations de mise à l'échelle et de translation seront appliquées pour assurer la cohérence avec la transformation effectuée sur les données d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5       ,  0.        ,  1.66666667]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.array([[-3., -1.,  4.]])\n",
    "X_test_minmax = min_max_scaler.transform(X_test)\n",
    "X_test_minmax\n",
    "# array([[-1.5       ,  0.        ,  1.66666667]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible d'introspecter les attributs du redimensionneur pour en savoir plus sur la nature exacte de la transformation apprise sur les données d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5       , 0.5       , 0.33333333])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_scaler.scale_\n",
    "# array([0.5       , 0.5       , 0.33...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.5       , 0.33333333])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_scaler.min_\n",
    "# array([0.        , 0.5       , 0.33...])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si [**`MinMaxScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) reçoit une plage explicite `feature_range=(min, max)`, la formule complète est :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "X_scaled = X_std * (max - min) + min"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`MaxAbsScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html) fonctionne de manière très similaire, mais met à l'échelle de manière à ce que les données d'entraînement se situent dans la plage `[-1, 1]` en divisant par la plus grande valeur maximale de chaque caractéristique. Il est conçu pour les données qui sont déjà centrées sur zéro ou des données creuses.\n",
    "\n",
    "Voici comment utiliser les données factices de l'exemple précédent avec ce redimensionneur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5, -1. ,  1. ],\n",
       "       [ 1. ,  0. ,  0. ],\n",
       "       [ 0. ,  1. , -0.5]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "X_train_maxabs = max_abs_scaler.fit_transform(X_train)\n",
    "X_train_maxabs\n",
    "# array([[ 0.5, -1. ,  1. ],\n",
    "#        [ 1. ,  0. ,  0. ],\n",
    "#        [ 0. ,  1. , -0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5, -1. ,  2. ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.array([[ -3., -1.,  4.]])\n",
    "X_test_maxabs = max_abs_scaler.transform(X_test)\n",
    "X_test_maxabs\n",
    "# array([[-1.5, -1. ,  2. ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 2.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_abs_scaler.scale_\n",
    "# array([2.,  1.,  2.])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='encoding-categorical-features'></a> 6.3.4. Encodage des caractéristiques catégorielles\n",
    "\n",
    "Souvent, les caractéristiques ne sont pas données sous forme de valeurs continues mais catégorielles. Par exemple, une personne peut avoir des caractéristiques `[\"homme\", \"femme\"]`, `[\"from Europe\", \"from États-Unis\", \"from Asia\"]`, `[\"uses Firefox\", \"uses Chrome\", \"uses Safari\", \"uses Internet Explorer\"]`. De telles caractéristiques peuvent être efficacement codées sous forme d'entiers, par exemple `[\"male\", \"from États-Unis\", \"uses Internet Explorer\"]` pourrait être exprimé par `[0, 1, 3]` tandis que `[\"female\", \"from Asia\", \"uses Chrome\"]` serait `[1, 2, 1]`.\n",
    "\n",
    "Pour convertir des caractéristiques catégorielles en de tels codes entiers, nous pouvons utiliser [**`OrdinalEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html). Cet estimateur transforme chaque caractéristique catégorielle en une nouvelle caractéristique d'entiers ($0$ à $n_{\\text{categories}} - 1$) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "enc = preprocessing.OrdinalEncoder()\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)\n",
    "# OrdinalEncoder()\n",
    "enc.transform([['female', 'from US', 'uses Safari']])\n",
    "# array([[0., 1., 1.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une telle représentation entière ne peut cependant pas être utilisée directement avec tous les estimateurs scikit-learn, car ceux-ci attendent une entrée continue et interpréteraient les catégories en tant qu'ordinaux, ce qui n'est souvent pas souhaité (c'est-à-dire que l'ensemble des navigateurs a été ordonné arbitrairement).\n",
    "\n",
    "Par défaut, [**`OrdinalEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html) transmettra également les valeurs manquantes représentées par `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "enc = preprocessing.OrdinalEncoder()\n",
    "X = [['male'], ['female'], [np.nan], ['female']]\n",
    "enc.fit_transform(X)\n",
    "# array([[ 1.],\n",
    "#       [ 0.],\n",
    "#       [nan],\n",
    "#       [ 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`OrdinalEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html) fournit un paramètre `encoded_missing_value` pour encoder les valeurs manquantes sans avoir besoin de créer un pipeline et en utilisant [**`SimpleImputer`**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 0.],\n",
       "       [-1.],\n",
       "       [ 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc = preprocessing.OrdinalEncoder(encoded_missing_value=-1)\n",
    "X = [['male'], ['female'], [np.nan], ['female']]\n",
    "enc.fit_transform(X)\n",
    "# array([[ 1.],\n",
    "#        [ 0.],\n",
    "#        [-1.],\n",
    "#        [ 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le traitement ci-dessus est équivalent au pipeline suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "enc = Pipeline(steps=[\n",
    "        (\"encoder\", preprocessing.OrdinalEncoder()),\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=-1)),\n",
    "    ])\n",
    "enc.fit_transform(X)\n",
    "# array([[ 1.],\n",
    "#        [ 0.],\n",
    "#        [-1.],\n",
    "#        [ 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une autre possibilité de convertir des caractéristiques catégorielles en caractéristiques pouvant être utilisées avec les estimateurs scikit-learn consiste à utiliser un one-of-K, également connu sous le nom de codage one-hot ou factice. Ce type d'encodage peut être obtenu avec le [**`OneHotEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html), qui transforme chaque caractéristique catégorique avec `n_categories` valeurs possibles en `n_categories` caractéristiques binaires, avec l'une d'elles à 1, et toutes les autres à 0.\n",
    "\n",
    "Reprenons l'exemple ci-dessus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 1., 0., 0., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc = preprocessing.OneHotEncoder()\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)\n",
    "# OneHotEncoder()\n",
    "enc.transform([['female', 'from US', 'uses Safari'],\n",
    "               ['male', 'from Europe', 'uses Safari']]).toarray()\n",
    "# array([[1., 0., 0., 1., 0., 1.],\n",
    "#        [0., 1., 1., 0., 0., 1.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par défaut, les valeurs que chaque caractéristique peut prendre sont automatiquement déduites du jeu de données et peuvent être trouvées dans l'attribut `categories_` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['female', 'male'], dtype=object),\n",
       " array(['from Europe', 'from US'], dtype=object),\n",
       " array(['uses Firefox', 'uses Safari'], dtype=object)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc.categories_\n",
    "# [array(['female', 'male'], dtype=object),\n",
    "#  array(['from Europe', 'from US'], dtype=object),\n",
    "#  array(['uses Firefox', 'uses Safari'], dtype=object)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible de le spécifier explicitement à l'aide du paramètre `categories`. Il existe deux genres, quatre continents possibles et quatre navigateurs Web dans notre ensemble de données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 0., 1., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "genders = ['female', 'male']\n",
    "locations = ['from Africa', 'from Asia', 'from Europe', 'from US']\n",
    "browsers = ['uses Chrome', 'uses Firefox', 'uses IE', 'uses Safari']\n",
    "enc = preprocessing.OneHotEncoder(categories=[genders, locations, browsers])\n",
    "# Note that for there are missing categorical values for the 2nd and 3rd\n",
    "# feature\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)\n",
    "# OneHotEncoder(categories=[['female', 'male'],\n",
    "#                           ['from Africa', 'from Asia', 'from Europe',\n",
    "#                            'from US'],\n",
    "#                           ['uses Chrome', 'uses Firefox', 'uses IE',\n",
    "#                            'uses Safari']])\n",
    "enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray()\n",
    "# array([[1., 0., 0., 1., 0., 0., 1., 0., 0., 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S'il est possible que les données d'entraînement aient des caractéristiques catégorielles manquantes, il peut souvent être préférable de spécifier `handle_unknown='infrequent_if_exist'` au lieu de définir les catégories manuellement comme ci-dessus. Lorsque `handle_unknown='infrequent_if_exist'` est spécifié et que des catégories inconnues sont rencontrées lors de la transformation, aucune erreur n'est générée, mais les colonnes encodées à chaud résultantes pour cette caractéristique sont toutes nulles ou considérées comme une catégorie peu fréquente si elle cette option est activée. (`handle_unknown='infrequent_if_exist'` n'est pris en charge que pour l'encodage one-hot) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc = preprocessing.OneHotEncoder(handle_unknown='infrequent_if_exist')\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)\n",
    "# OneHotEncoder(handle_unknown='infrequent_if_exist')\n",
    "enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray()\n",
    "# array([[1., 0., 0., 0., 0., 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est également possible d'encoder chaque colonne en `n_categories - 1` colonnes au lieu de `n_categories` colonnes en utilisant le paramètre `drop`. Ce paramètre permet à l'utilisateur de spécifier une catégorie pour chaque caractéristique à supprimer. Ceci est utile pour éviter la colinéarité dans la matrice d'entrée dans certains classificateurs. Une telle fonctionnalité est utile, par exemple, lors de l'utilisation d'une régression non régularisée ([**`LinearRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)), car la colinéarité rendrait la matrice de covariance non inversible :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [['male', 'from US', 'uses Safari'],\n",
    "     ['female', 'from Europe', 'uses Firefox']]\n",
    "drop_enc = preprocessing.OneHotEncoder(drop='first').fit(X)\n",
    "drop_enc.categories_\n",
    "# [array(['female', 'male'], dtype=object), array(['from Europe', 'from US'], dtype=object),\n",
    "#  array(['uses Firefox', 'uses Safari'], dtype=object)]\n",
    "drop_enc.transform(X).toarray()\n",
    "# array([[1., 1., 1.],\n",
    "#        [0., 0., 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut vouloir supprimer l'une des deux colonnes uniquement pour les caractéristiques à 2 catégories. Dans ce cas, vous pouvez définir le paramètre `drop='if_binary'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 1., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [['male', 'US', 'Safari'],\n",
    "     ['female', 'Europe', 'Firefox'],\n",
    "     ['female', 'Asia', 'Chrome']]\n",
    "drop_enc = preprocessing.OneHotEncoder(drop='if_binary').fit(X)\n",
    "drop_enc.categories_\n",
    "# [array(['female', 'male'], dtype=object), array(['Asia', 'Europe', 'US'], dtype=object),\n",
    "#  array(['Chrome', 'Firefox', 'Safari'], dtype=object)]\n",
    "drop_enc.transform(X).toarray()\n",
    "# array([[1., 0., 0., 1., 0., 0., 1.],\n",
    "#        [0., 0., 1., 0., 0., 1., 0.],\n",
    "#        [0., 1., 0., 0., 1., 0., 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le `X` transformé, la première colonne est encode la caractéristique avec les catégories \"masculin\"/\"féminin\", tandis que les 6 colonnes restantes encodent les 2 caractéristiques avec respectivement 3 catégories chacune.\n",
    "\n",
    "Lorsque `handle_unknown='ignore'` et que `drop` n'est pas `None`, les catégories inconnues seront encodées en zéros :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_enc = preprocessing.OneHotEncoder(drop='first',\n",
    "                                       handle_unknown='ignore').fit(X)\n",
    "X_test = [['unknown', 'America', 'IE']]\n",
    "drop_enc.transform(X_test).toarray()\n",
    "# array([[0., 0., 0., 0., 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toutes les catégories dans `X_test` sont inconnues lors de la transformation et seront mappées sur tous les zéros. Cela signifie que les catégories inconnues auront la même correspondance que la catégorie supprimée. : `OneHotEncoder.inverse_transform` mappera tous les zéros sur la catégorie supprimée si une catégorie est supprimée et `None` si une catégorie n'est pas supprimée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:188: UserWarning: Found unknown categories in columns [0, 1, 2] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['female', None, None]], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drop_enc = preprocessing.OneHotEncoder(drop='if_binary', sparse=False,\n",
    "                                       handle_unknown='ignore').fit(X)\n",
    "X_test = [['unknown', 'America', 'IE']]\n",
    "X_trans = drop_enc.transform(X_test)\n",
    "X_trans\n",
    "# array([[0., 0., 0., 0., 0., 0., 0.]])\n",
    "drop_enc.inverse_transform(X_trans)\n",
    "# array([['female', None, None]], dtype=object)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`OneHotEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder) prend en charge les caractéristiques catégorielles avec des valeurs manquantes en considérant les valeurs manquantes comme une catégorie supplémentaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 1., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [['male', 'Safari'],\n",
    "     ['female', None],\n",
    "     [np.nan, 'Firefox']]\n",
    "enc = preprocessing.OneHotEncoder(handle_unknown='error').fit(X)\n",
    "enc.categories_\n",
    "# [array(['female', 'male', nan], dtype=object),\n",
    "#  array(['Firefox', 'Safari', None], dtype=object)]\n",
    "enc.transform(X).toarray()\n",
    "# array([[0., 1., 0., 0., 1., 0.],\n",
    "#        [1., 0., 0., 0., 0., 1.],\n",
    "#        [0., 0., 1., 1., 0., 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si une caractéristique contient à la fois `np.nan` et `None`, elles seront considérées comme des catégories distinctes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [['Safari'], [None], [np.nan], ['Firefox']]\n",
    "enc = preprocessing.OneHotEncoder(handle_unknown='error').fit(X)\n",
    "enc.categories_\n",
    "# [array(['Firefox', 'Safari', None, nan], dtype=object)]\n",
    "enc.transform(X).toarray()\n",
    "# array([[0., 1., 0., 0.],\n",
    "#        [0., 0., 1., 0.],\n",
    "#        [0., 0., 0., 1.],\n",
    "#        [1., 0., 0., 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir [**Chargement de caractéristiques** (6.2.1)](https://scikit-learn.org/stable/modules/feature_extraction.html#dict-feature-extraction) à partir de dicts pour des caractéristiques catégorielles représentées sous forme de dictionnaires, et non sous forme de scalaires."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='infrequent-categories'></a> 6.3.4.1. Catégories peu fréquentes\n",
    "\n",
    "[**`OneHotEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) prend en charge l'agrégation de catégories peu fréquentes en une seule sortie pour chaque caractéristique. Les paramètres permettant de rassembler les catégories peu fréquentes sont `min_frequency` et `max_categories`.\n",
    "\n",
    "1. `min_frequency` est soit un entier supérieur ou égal à 1, soit un flottant dans l'intervalle `(0.0, 1.0)`. Si `min_frequency` est un entier, les catégories avec une cardinalité inférieure à `min_frequency` seront considérées comme peu fréquentes. Si `min_frequency` est un flottant, les catégories avec une cardinalité inférieure à cette fraction du nombre total d'échantillons seront considérées comme peu fréquentes. La valeur par défaut est 1, ce qui signifie que chaque catégorie est encodée séparément.\n",
    "\n",
    "2. `max_categories` vaut `None` ou tout entier supérieur à 1. Ce paramètre définit une limite supérieure au nombre de caractéristiques en sortie pour chaque caractéristique en entrée. `max_categories` inclut la caractéristique qui combine des catégories peu fréquentes.\n",
    "\n",
    "Dans l'exemple suivant, les catégories `dog`, `snake` sont considérées comme peu fréquentes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.array([['dog'] * 5 + ['cat'] * 20 + ['rabbit'] * 10 +\n",
    "              ['snake'] * 3], dtype=object).T\n",
    "enc = preprocessing.OneHotEncoder(min_frequency=6, sparse=False).fit(X)\n",
    "enc.infrequent_categories_\n",
    "# [array(['dog', 'snake'], dtype=object)]\n",
    "enc.transform(np.array([['dog'], ['cat'], ['rabbit'], ['snake']]))\n",
    "# array([[0., 0., 1.],\n",
    "#        [1., 0., 0.],\n",
    "#        [0., 1., 0.],\n",
    "#        [0., 0., 1.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En définissant `handle_unknown` sur `'infrequent_if_exist'`, les catégories inconnues seront considérées comme peu fréquentes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc = preprocessing.OneHotEncoder(\n",
    "    handle_unknown='infrequent_if_exist', sparse=False, min_frequency=6)\n",
    "enc = enc.fit(X)\n",
    "enc.transform(np.array([['dragon']]))\n",
    "# array([[0., 0., 1.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`OneHotEncoder.get_feature_names_out`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder.get_feature_names_out) utilise `'infrequent'` pour désigner les caractéristiques peu fréquentes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x0_cat', 'x0_rabbit', 'x0_infrequent_sklearn'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc.get_feature_names_out()\n",
    "# array(['x0_cat', 'x0_rabbit', 'x0_infrequent_sklearn'], dtype=object)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque `'handle_unknown'` est défini sur `'infrequent_if_exist'` et qu'une catégorie inconnue est rencontrée dans la transformation :\n",
    "\n",
    "1. Si la prise en charge de la catégorie peu fréquente n'a pas été configurée ou s'il n'y avait pas de catégorie peu fréquente pendant l'entraînement, les colonnes encodées à chaud résultantes pour cette caractéristique seront toutes des zéros. Dans la transformation inverse, une catégorie inconnue sera notée `None`.\n",
    "\n",
    "2. S'il y a une catégorie peu fréquente pendant l'entraînement, la catégorie inconnue sera considérée comme peu fréquente. Dans la transformation inverse, \"infrequent_sklearn\" sera utilisé pour représenter la catégorie peu fréquente.\n",
    "\n",
    "Les catégories peu fréquentes peuvent également être configurées à l'aide de `max_categories`. Dans l'exemple suivant, nous définissons `max_categories=2` pour limiter le nombre de caractéristiques dans la sortie. Il en résultera que toutes les catégories sauf la catégorie `'cat'` seront considérées comme peu fréquentes, ce qui conduira à deux caractéristiques, une pour les catégories `'cat'` et une pour les catégories peu fréquentes - qui sont toutes les autres :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc = preprocessing.OneHotEncoder(max_categories=2, sparse=False)\n",
    "enc = enc.fit(X)\n",
    "enc.transform([['dog'], ['cat'], ['rabbit'], ['snake']])\n",
    "# array([[0., 1.],\n",
    "#        [1., 0.],\n",
    "#        [0., 1.],\n",
    "#        [0., 1.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si `max_categories` et `min_frequency` ne sont pas des valeurs par défaut, les catégories sont sélectionnées en fonction de `min_frequency` en premier et les catégories `max_categories` sont conservées. Dans l'exemple suivant, `min_frequency=4` considère que seul `snake` est peu fréquent, mais `max_categories=3`, force `dog` à être également peu fréquent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = preprocessing.OneHotEncoder(min_frequency=4, max_categories=3, sparse=False)\n",
    "enc = enc.fit(X)\n",
    "enc.transform([['dog'], ['cat'], ['rabbit'], ['snake']])\n",
    "# array([[0., 0., 1.],\n",
    "#        [1., 0., 0.],\n",
    "#        [0., 1., 0.],\n",
    "#        [0., 0., 1.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S'il existe des catégories peu fréquentes avec la même cardinalité au seuil de `max_categories`, alors les premières `max_categories` sont prises en fonction de l'ordre du lexique. Dans l'exemple suivant, « b », « c » et « d » ont la même cardinalité et avec `max_categories=2`, « b » et « c » sont peu fréquents car ils ont un ordre de lexique plus élevé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['b', 'c'], dtype=object)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.asarray([[\"a\"] * 20 + [\"b\"] * 10 + [\"c\"] * 10 + [\"d\"] * 10], dtype=object).T\n",
    "enc = preprocessing.OneHotEncoder(max_categories=3).fit(X)\n",
    "enc.infrequent_categories_\n",
    "# [array(['b', 'c'], dtype=object)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.5. Discrétisation\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9aff9e50adfaa9e30c910fb3872ffdc72747acb5f50803ca0504f00e980f7c25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
