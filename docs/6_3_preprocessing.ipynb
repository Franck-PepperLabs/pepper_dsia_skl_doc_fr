{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='preprocessing-data'></a> 6.3. [**Prétraitement des données**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_3_preprocessing.ipynb#preprocessing-data)</br>([*Preprocessing data*](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-data))\n",
    "\n",
    "Le package `sklearn.preprocessing` fournit plusieurs fonctions utilitaires courantes et des classes de transformeurs pour changer les vecteurs de caractéristiques brutes en une représentation plus adaptée aux estimateurs aval.\n",
    "\n",
    "En général, les algorithmes d'apprentissage bénéficient d'une standardisation de l'ensemble de données. Si des outliers sont présents dans l'ensemble, des redimensionneurs ou transformateurs robustes sont plus appropriés. Le comportement des différents redimensionneurs, transformeurs et normaliseurs sur un jeu de données contenant des outliers marginaux est mis en évidence dans [**Comparaison de l'effet de différents redimensionneurs sur des données avec des outliers**](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='standardization-or-mean-removal-and-variance-scaling'></a> 6.3.1. Standardisation, ou suppression de la moyenne et normalisation de la variance\n",
    "\n",
    "La **standardisation** des ensembles de données est **une exigence courante pour de nombreux estimateurs d'apprentissage automatique** mis en œuvre dans scikit-learn; ils pourraient mal se comporter si les caractéristiques individuelles ne ressemblent pas plus ou moins à des données distribuées suivant la distribution normale standard : Gaussienne avec une **moyenne nulle et une variance unitaire**.\n",
    "\n",
    "En pratique, nous ignorons souvent la forme de la distribution et transformons simplement les données pour les centrer en supprimant la valeur moyenne de chaque caractéristique, puis en les dimensionnant en divisant les caractéristiques non constantes par leur écart-type.\n",
    "\n",
    "Par exemple, de nombreux éléments utilisés dans la fonction objectif d'un algorithme d'apprentissage (tels que le noyau RBF des Machines à Vecteurs de Support ou les régulariseurs $\\ell_1$ et $\\ell_2$ des modèles linéaires) peuvent supposer que toutes les caractéristiques sont centrées autour de zéro ou ont une variance de même ordre. Si une caractéristique a une variance qui est des ordres de grandeur plus grande que les autres, elle pourrait dominer la fonction objectif et empêcher l'estimateur d'apprendre correctement des autres caractéristiques comme prévu.\n",
    "\n",
    "Le module [**`preprocessing`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) fournit la classe utilitaire [**`StandardScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html), qui est un moyen rapide et facile pour effectuer les opérations suivantes sur un jeu de données de type tableau :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "scaler\n",
    "# StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.        , 0.33333333])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.mean_\n",
    "# array([1. ..., 0. ..., 0.33...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81649658, 0.81649658, 1.24721913])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.scale_\n",
    "#array([0.81..., 0.81..., 1.24...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled = scaler.transform(X_train)\n",
    "X_scaled\n",
    "# array([[ 0.  ..., -1.22...,  1.33...],\n",
    "#        [ 1.22...,  0.  ..., -0.26...],\n",
    "#        [-1.22...,  1.22..., -1.06...]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données mises à l'échelle ont une moyenne nulle et une variance unitaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled.mean(axis=0)\n",
    "# array([0., 0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled.std(axis=0)\n",
    "# array([1., 1., 1.])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette classe implémente l'API `Transformer` pour calculer la moyenne et l'écart-type sur un ensemble d'entraînement, afin de pouvoir réappliquer la même transformation sur l'ensemble de test. Cette classe est donc adaptée aux premières étapes d'un [**`Pipeline`**](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;logisticregression&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;logisticregression&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('logisticregression', LogisticRegression())])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y = make_classification(random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "pipe.fit(X_train, y_train)  # apply scaling on training data\n",
    "# Pipeline(steps=[('standardscaler', StandardScaler()),\n",
    "#                 ('logisticregression', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(X_test, y_test)  # apply scaling on testing data, without leaking training data.\n",
    "# 0.96"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible de désactiver le centrage ou la mise à l'échelle en passant respectivement les paramètres `with_mean=False` ou `with_std=False` au constructeur de [**`StandardScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='scaling-features-to-a-range'></a> 6.3.1.1. Mise à l'échelle des caractéristiques dans une plage donnée\n",
    "\n",
    "Une autre méthode de standardisation consiste à mettre à l'échelle les caractéristiques pour ramener entre des valeurs minimale et maximale données, souvent entre zéro et un, ou de manière à ce que la valeur absolue maximale de chaque caractéristique soit mise à l'échelle à une taille unitaire. Cela peut être réalisé en utilisant [**`MinMaxScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) ou [**`MaxAbsScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html), respectivement.\n",
    "\n",
    "La motivation pour utiliser cette mise à l'échelle comprend la robustesse par rapport aux écarts types très faibles des caractéristiques et la préservation des entrées nulles dans les données éparses.\n",
    "\n",
    "Voici un exemple pour mettre à l'échelle une matrice de données factices dans la plage `[0, 1]` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.        , 1.        ],\n",
       "       [1.        , 0.5       , 0.33333333],\n",
       "       [0.        , 1.        , 0.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "X_train_minmax\n",
    "# array([[0.5       , 0.        , 1.        ],\n",
    "#        [1.        , 0.5       , 0.33333333],\n",
    "#        [0.        , 1.        , 0.        ]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La même instance du transformeur peut ensuite être appliquée à de nouvelles données de test qui n'ont pas été vues lors de l'appel à la méthode `fit` : les mêmes opérations de mise à l'échelle et de translation seront appliquées pour assurer la cohérence avec la transformation effectuée sur les données d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5       ,  0.        ,  1.66666667]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.array([[-3., -1.,  4.]])\n",
    "X_test_minmax = min_max_scaler.transform(X_test)\n",
    "X_test_minmax\n",
    "# array([[-1.5       ,  0.        ,  1.66666667]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible d'introspecter les attributs du redimensionneur pour en savoir plus sur la nature exacte de la transformation apprise sur les données d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5       , 0.5       , 0.33333333])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_scaler.scale_\n",
    "# array([0.5       , 0.5       , 0.33...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.5       , 0.33333333])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_scaler.min_\n",
    "# array([0.        , 0.5       , 0.33...])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si [**`MinMaxScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) reçoit une plage explicite `feature_range=(min, max)`, la formule complète est :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "X_scaled = X_std * (max - min) + min"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`MaxAbsScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html) fonctionne de manière très similaire, mais met à l'échelle de manière à ce que les données d'entraînement se situent dans la plage `[-1, 1]` en divisant par la plus grande valeur maximale de chaque caractéristique. Il est conçu pour les données qui sont déjà centrées sur zéro ou des données creuses.\n",
    "\n",
    "Voici comment utiliser les données factices de l'exemple précédent avec ce redimensionneur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5, -1. ,  1. ],\n",
       "       [ 1. ,  0. ,  0. ],\n",
       "       [ 0. ,  1. , -0.5]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "X_train_maxabs = max_abs_scaler.fit_transform(X_train)\n",
    "X_train_maxabs\n",
    "# array([[ 0.5, -1. ,  1. ],\n",
    "#        [ 1. ,  0. ,  0. ],\n",
    "#        [ 0. ,  1. , -0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5, -1. ,  2. ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.array([[ -3., -1.,  4.]])\n",
    "X_test_maxabs = max_abs_scaler.transform(X_test)\n",
    "X_test_maxabs\n",
    "# array([[-1.5, -1. ,  2. ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 2.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_abs_scaler.scale_\n",
    "# array([2.,  1.,  2.])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='scaling-sparse-data'></a> 6.3.1.2. Mise à l'échelle des données clairsemées\n",
    "\n",
    "Recentrer des données creuses détruirait leur structure creuse, et donc cela n'est généralement pas pertinent. Cependant, il peut être judicieux de mettre à l'échelle des entrées creuses, en particulier si les caractéristiques sont sur des échelles différentes.\n",
    "\n",
    "[**`MaxAbsScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html) a été spécifiquement conçu pour la mise à l'échelle de données clairsemées et est la méthode recommandée pour cela. Cependant, [**`StandardScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) peut accepter des matrices creuses de `scipy.sparse` en entrée, à condition que `with_mean=False` soit explicitement passé au constructeur. Sinon, une `ValueError` sera levée car la recentralisation silencieuse romprait la cavité et pourrait souvent entraîner un crash de l'exécution en allouant accidentellement des quantités excessives de mémoire. [**`RobustScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) ne peut pas être adapté à des entrées creuses, mais vous pouvez utiliser la méthode `transform` sur des entrées creuses.\n",
    "\n",
    "Notez que les scalers acceptent à la fois le format CSR (Compressed Sparse Rows) et le format CSC (Compressed Sparse Columns) (voir `scipy.sparse.csr_matrix` et `scipy.sparse.csc_matrix`). Toute autre entrée creuse sera **convertie en représentation CSR**. Pour éviter des copies de mémoire inutiles, il est recommandé de choisir la représentation CSR ou CSC en amont.\n",
    "\n",
    "Enfin, si l'on s'attend à ce que les données recentrées soient suffisamment petites, une autre option consiste à convertir explicitement l'entrée en un tableau en utilisant la méthode `toarray` des matrices creuses.\n",
    "\n",
    "## <a id='scaling-data-with-outliers'></a> 6.3.1.3. Mise à l'échelle des données avec des valeurs aberrantes\n",
    "\n",
    "Si vos données contiennent de nombreuses valeurs aberrantes, une mise à l'échelle utilisant la moyenne et la variance des données risque de ne pas fonctionner très bien. Dans ces cas, vous pouvez utiliser [**`RobustScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) comme remplacement direct. Il utilise des estimations plus robustes pour le centre et l'étendue de vos données.\n",
    "\n",
    "### Références\n",
    "\n",
    "Une discussion plus approfondie sur l'importance du recentrage et de la mise à l'échelle des données est disponible dans cette FAQ : [Dois-je normaliser/standardiser/mettre à l'échelle les données ?](http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html)\n",
    "\n",
    "### Mise à l'échelle vs Blanchiment\n",
    "\n",
    "Il n'est parfois pas suffisant de centrer et mettre à l'échelle les caractéristiques indépendamment, car un modèle ultérieur peut faire des hypothèses supplémentaires sur l'indépendance linéaire des caractéristiques.\n",
    "\n",
    "Pour résoudre ce problème, vous pouvez utiliser [**`PCA`** (Analyse en Composantes Principales)](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA) avec `whiten=True` pour éliminer davantage la corrélation linéaire entre les caractéristiques."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='centering-kernel-matrices'></a> 6.3.1.4. Recentrage des matrices de noyau\n",
    "\n",
    "Si vous disposez d'une matrice de noyau de noyau $K$ qui calcule un produit scalaire dans un espace de caractéristiques (éventuellement de manière implicite) défini par une fonction $\\phi(\\cdot)$, un [**`KernelCenterer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KernelCenterer.html) peut transformer la matrice de noyau de manière à contenir des produits scalaires dans l'espace de caractéristiques défini par suivi de la suppression de la moyenne dans cet espace. En d'autres termes, le [**`KernelCenterer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KernelCenterer.html) calcule la matrice de Gram centrée associée à un noyau semi-défini positif $K$.\n",
    "\n",
    "### Formulation mathématique\n",
    "\n",
    "Nous pouvons examiner maintenant la formulation mathématique maintenant que nous avons l'intuition. Soit $K$\n",
    "une matrice de noyau de forme `(n_samples, n_samples)` calculée à partir de $X$, une matrice de données de forme `(n_samples, n_features)`, lors de l'étape de l'ajustement (`fit`). $K$ est défini par\n",
    "\n",
    "$$K(X, X) = \\phi(X) . \\phi(X)^{T}$$\n",
    "\n",
    "$\\phi(X)$ est une fonction qui mappe $X$ dans un espace de Hilbert. Un noyau centré $\\tilde{K}$ est défini comme :\n",
    "\n",
    "$$K(X, X) = \\phi(X) . \\phi(X)^{T}$$\n",
    "\n",
    "où $\\tilde{\\phi}(X)$ résulte du centrage de $\\phi(X)$ dans l'espace de Hilbert.\n",
    "\n",
    "Ainsi, on pourrait calculer $\\tilde{K}$ en effectuant le mappage en utilisant la fonction $\\phi(\\cdot)$ et en recentrant les données dans ce nouvel espace. Cependant, les noyaux sont souvent utilisés car ils permettent des calculs algébriques qui évitent de calculer explicitement ce mappage en utilisant $\\phi(\\cdot)$. En effet, on peut implicitement centrer comme indiqué dans l'annexe B de [Scholkopf1998] :\n",
    "\n",
    "$$\\tilde{K} = K - 1_{\\text{n}_{samples}} K - K 1_{\\text{n}_{samples}} + 1_{\\text{n}_{samples}} K 1_{\\text{n}_{samples}}$$\n",
    "\n",
    "$1_{\\text{n}_{samples}}$ est une matrice de `(n_samples, n_samples)` où toutes les entrées sont égales à $\\frac{1}{\\text{n}_{samples}}$. Lors de l'étape `transform`, le noyau devient $K_{test}(X, Y)$ défini comme :\n",
    "\n",
    "$$K_{test}(X, Y) = \\phi(Y) . \\phi(X)^{T}$$\n",
    "\n",
    "$Y$ est l'ensemble de données de test de forme `(n_samples_test, n_features)` et donc $K_{test}$ de forme `(n_samples_test, n_samples)`. Dans ce cas, le centrage de $K_{test}$ est effectué comme suit :\n",
    "\n",
    "$$\\tilde{K}_{test}(X, Y) = K_{test} - 1'_{\\text{n}_{samples}} K - K_{test} 1_{\\text{n}_{samples}} + 1'_{\\text{n}_{samples}} K 1_{\\text{n}_{samples}}$$\n",
    "\n",
    "$1'_{\\text{n}_{samples}}$ est une matrice de forme `(n_samples_test, n_samples)` où toutes les entrées sont égales à $\\frac{1}{\\text{n}_{samples}}$.\n",
    "\n",
    "### Références\n",
    "\n",
    "[Scholkopf1998] B. Schölkopf, A. Smola, and K.R. Müller, “Nonlinear component analysis as a kernel eigenvalue problem.” Neural computation 10.5 (1998): 1299-1319."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='non-linear-transformation'></a> 6.3.2. Transformation non linéaire\n",
    "\n",
    "Deux types de transformations sont disponibles : les transformations quantiles et les transformations de puissance. Les transformations quantiles et les transformations de puissance sont basées sur des transformations monotones des caractéristiques, ce qui préserve le rang des valeurs le long de chaque caractéristique.\n",
    "\n",
    "Les transformations quantiles placent toutes les caractéristiques dans la même distribution souhaitée en utilisant la formule $G^{-1}(F(X))$ où $F$ est la fonction de distribution cumulative de la caractéristique et $G^{-1}$ la [fonction quantile](https://en.wikipedia.org/wiki/Quantile_function) de la distribution de sortie souhaitée $G$. Cette formule utilise les deux faits suivants : (i) si $X$ est une variable aléatoire avec une fonction de distribution cumulative continue $F$, alors $F(X)$ est uniformément distribué sur $[0,1]$; (ii) si $U$ est une variable aléatoire avec une distribution uniforme sur $[0,1]$, alors $G^{-1}(U)$ a une distribution $G$. En effectuant une transformation de rang, une transformation quantile lisse les distributions inhabituelles et est moins influencée par les valeurs aberrantes que les méthodes de mise à l'échelle. Cependant, elle déforme les corrélations et les distances à l'intérieur et entre les caractéristiques.\n",
    "\n",
    "Les transformations de puissance sont une famille de transformations paramétriques qui visent à mapper les données de n'importe quelle distribution aussi proche que possible d'une distribution gaussienne."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='mapping-to-a-uniform-distribution'></a> 6.3.2.1. Mappage vers une distribution uniforme\n",
    "\n",
    "[**`QuantileTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html) fournit une transformation non paramétrique pour mapper les données vers une distribution uniforme avec des valeurs entre 0 et 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "quantile_transformer = preprocessing.QuantileTransformer(random_state=0)\n",
    "X_train_trans = quantile_transformer.fit_transform(X_train)\n",
    "X_test_trans = quantile_transformer.transform(X_test)\n",
    "np.percentile(X_train[:, 0], [0, 25, 50, 75, 100])\n",
    "# array([ 4.3,  5.1,  5.8,  6.5,  7.9])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette caractéristique correspond à la longueur des sépales en cm. Une fois la transformation quantile appliquée, ces points de repère se rapprochent étroitement des percentiles précédemment définis :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(X_train_trans[:, 0], [0, 25, 50, 75, 100])\n",
    "# array([ 0.00... ,  0.24...,  0.49...,  0.73...,  0.99... ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela peut être confirmé sur un ensemble de test indépendant avec des remarques similaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(X_test[:, 0], [0, 25, 50, 75, 100])\n",
    "# array([ 4.4  ,  5.125,  5.75 ,  6.175,  7.3  ])\n",
    "np.percentile(X_test_trans[:, 0], [0, 25, 50, 75, 100])\n",
    "# array([ 0.01...,  0.25...,  0.46...,  0.60... ,  0.94...])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='mapping-to-a-gaussian-distribution'></a> 6.3.2.2. Mappage vers une distribution gaussienne\n",
    "\n",
    "Dans de nombreux scénarios de modélisation, la normalité des caractéristiques d'un ensemble de données est souhaitable. Les transformations de puissance sont une famille de transformations paramétriques et monotones qui visent à mapper les données de n'importe quelle distribution aussi proche que possible d'une distribution gaussienne afin de stabiliser la variance et de minimiser l'asymétrie.\n",
    "\n",
    "[**`PowerTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html) fournit actuellement deux transformations de puissance, la transformation Yeo-Johnson et la transformation Box-Cox.\n",
    "\n",
    "La transformation Yeo-Johnson est donnée par :\n",
    "\n",
    "$$\\begin{split}x_i^{(\\lambda)} =\n",
    "\\begin{cases}\n",
    " [(x_i + 1)^\\lambda - 1] / \\lambda & \\text{if } \\lambda \\neq 0, x_i \\geq 0, \\\\[8pt]\n",
    "\\ln{(x_i + 1)} & \\text{if } \\lambda = 0, x_i \\geq 0 \\\\[8pt]\n",
    "-[(-x_i + 1)^{2 - \\lambda} - 1] / (2 - \\lambda) & \\text{if } \\lambda \\neq 2, x_i < 0, \\\\[8pt]\n",
    " - \\ln (- x_i + 1) & \\text{if } \\lambda = 2, x_i < 0\n",
    "\\end{cases}\\end{split}$$\n",
    "\n",
    "tandis que la transformation Box-Cox est donnée par :\n",
    "\n",
    "$$\\begin{split}x_i^{(\\lambda)} =\n",
    "\\begin{cases}\n",
    "\\dfrac{x_i^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0, \\\\[8pt]\n",
    "\\ln{(x_i)} & \\text{if } \\lambda = 0,\n",
    "\\end{cases}\\end{split}$$\n",
    "\n",
    "Box-Cox ne peut être appliqué qu'à des données strictement positives. Dans les deux méthodes, la transformation est paramétrée par , qui est déterminé par une estimation du maximum de vraisemblance. Voici un exemple d'utilisation de Box-Cox pour mapper des échantillons tirés d'une distribution lognormale vers une distribution normale :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = preprocessing.PowerTransformer(method='box-cox', standardize=False)\n",
    "X_lognormal = np.random.RandomState(616).lognormal(size=(3, 3))\n",
    "X_lognormal\n",
    "# array([[1.28..., 1.18..., 0.84...],\n",
    "#        [0.94..., 1.60..., 0.38...],\n",
    "#        [1.35..., 0.21..., 1.09...]])\n",
    "pt.fit_transform(X_lognormal)\n",
    "# array([[ 0.49...,  0.17..., -0.15...],\n",
    "#        [-0.05...,  0.58..., -0.57...],\n",
    " #       [ 0.69..., -0.84...,  0.10...]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien que l'exemple ci-dessus définisse l'option standardize sur False, PowerTransformer appliquera par défaut une normalisation à moyenne nulle et variance unitaire à la sortie transformée.\n",
    "\n",
    "Ci-dessous, des exemples de Box-Cox et de Yeo-Johnson appliqués à diverses distributions de probabilité. Notez que lorsqu'ils sont appliqués à certaines distributions, les transformations de puissance donnent des résultats très similaires à une distribution gaussienne, mais avec d'autres, elles sont inefficaces. Cela souligne l'importance de visualiser les données avant et après la transformation.\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_map_data_to_normal_001.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est également possible de mapper les données vers une distribution normale en utilisant QuantileTransformer avec output_distribution='normal'. En utilisant l'exemple précédent avec l'ensemble de données Iris :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_transformer = preprocessing.QuantileTransformer(\n",
    "      output_distribution='normal', random_state=0)\n",
    "X_trans = quantile_transformer.fit_transform(X)\n",
    "quantile_transformer.quantiles_\n",
    "# array([[4.3, 2. , 1. , 0.1],\n",
    "#        [4.4, 2.2, 1.1, 0.1],\n",
    "#        [4.4, 2.2, 1.2, 0.1],\n",
    "#        ...,\n",
    "#        [7.7, 4.1, 6.7, 2.5],\n",
    "#        [7.7, 4.2, 6.7, 2.5],\n",
    "#        [7.9, 4.4, 6.9, 2.5]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi, la médiane de l'entrée devient la moyenne de la sortie, centrée sur 0. La sortie normale est tronquée de sorte que le minimum et le maximum de l'entrée - correspondant respectivement aux quantiles 1e-7 et 1 - 1e-7 - ne deviennent pas infinis sous la transformation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='normalization'></a> 6.3.3. Normalisation\n",
    "\n",
    "La **normalisation** est le processus de **mise à l'échelle des échantillons individuels pour avoir une norme unitaire**. Ce processus peut être utile si vous prévoyez d'utiliser une forme quadratique telle que le produit scalaire ou tout autre noyau pour quantifier la similarité de chaque paire d'échantillons.\n",
    "\n",
    "Cette hypothèse est à la base du [**modèle d'espace vectoriel** (VSM)](https://en.wikipedia.org/wiki/Vector_Space_Model) souvent utilisé dans les contextes de classification et de regroupement de texte.\n",
    "\n",
    "La fonction [**`normalize`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html) fournit un moyen rapide et facile d'effectuer cette opération sur un seul ensemble de données de type tableau, en utilisant les normes `l1`, `l2` ou `max` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[ 1., -1.,  2.],\n",
    "    [ 2.,  0.,  0.],\n",
    "    [ 0.,  1., -1.]]\n",
    "X_normalized = preprocessing.normalize(X, norm='l2')\n",
    "\n",
    "X_normalized\n",
    "# array([[ 0.40..., -0.40...,  0.81...],\n",
    "#        [ 1.  ...,  0.  ...,  0.  ...],\n",
    "#        [ 0.  ...,  0.70..., -0.70...]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le module `preprocessing` fournit également une classe utilitaire [**`Normalizer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer) qui implémente la même opération en utilisant l'API du `Transformer` (même si la méthode `fit` est inutile dans ce cas : la classe est sans état car cette opération traite les échantillons indépendamment).\n",
    "\n",
    "Cette classe convient donc à une utilisation dans les premières étapes d'un [**`Pipeline`**](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = preprocessing.Normalizer().fit(X)  # fit does nothing\n",
    "normalizer\n",
    "# Normalizer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'instance du normaliseur peut ensuite être utilisée sur des vecteurs d'échantillons comme n'importe quel transformeur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer.transform(X)\n",
    "# array([[ 0.40..., -0.40...,  0.81...],\n",
    "#        [ 1.  ...,  0.  ...,  0.  ...],\n",
    "#        [ 0.  ...,  0.70..., -0.70...]])\n",
    "\n",
    "normalizer.transform([[-1.,  1., 0.]])\n",
    "# array([[-0.70...,  0.70...,  0.  ...]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : La normalisation $\\ell_2$ est également connue sous le nom de prétraitement par signe spatial.\n",
    "\n",
    "### Entrée creuse\n",
    "\n",
    "[**`normalize`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html) et [**`Normalizer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html) acceptent **à la fois des tableaux denses et des matrices creuses de type array-like provenant de scipy.sparse en entrée.**\n",
    "\n",
    "Pour les entrées creuses, les données sont **converties en représentation Compressed Sparse Rows** (voir `scipy.sparse.csr_matrix`) avant d'être traitées par des routines Cython efficaces. Pour éviter des copies inutiles de mémoire, il est recommandé de choisir la représentation CSR en amont."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='encoding-categorical-features'></a> 6.3.4. Encodage des caractéristiques catégorielles\n",
    "\n",
    "Souvent, les caractéristiques ne sont pas données sous forme de valeurs continues mais catégorielles. Par exemple, une personne peut avoir des caractéristiques `[\"homme\", \"femme\"]`, `[\"from Europe\", \"from États-Unis\", \"from Asia\"]`, `[\"uses Firefox\", \"uses Chrome\", \"uses Safari\", \"uses Internet Explorer\"]`. De telles caractéristiques peuvent être efficacement codées sous forme d'entiers, par exemple `[\"male\", \"from États-Unis\", \"uses Internet Explorer\"]` pourrait être exprimé par `[0, 1, 3]` tandis que `[\"female\", \"from Asia\", \"uses Chrome\"]` serait `[1, 2, 1]`.\n",
    "\n",
    "Pour convertir des caractéristiques catégorielles en de tels codes entiers, nous pouvons utiliser [**`OrdinalEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html). Cet estimateur transforme chaque caractéristique catégorielle en une nouvelle caractéristique d'entiers ($0$ à $n_{\\text{categories}} - 1$) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "enc = preprocessing.OrdinalEncoder()\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)\n",
    "# OrdinalEncoder()\n",
    "enc.transform([['female', 'from US', 'uses Safari']])\n",
    "# array([[0., 1., 1.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une telle représentation entière ne peut cependant pas être utilisée directement avec tous les estimateurs scikit-learn, car ceux-ci attendent une entrée continue et interpréteraient les catégories en tant qu'ordinaux, ce qui n'est souvent pas souhaité (c'est-à-dire que l'ensemble des navigateurs a été ordonné arbitrairement).\n",
    "\n",
    "Par défaut, [**`OrdinalEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html) transmettra également les valeurs manquantes représentées par `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "enc = preprocessing.OrdinalEncoder()\n",
    "X = [['male'], ['female'], [np.nan], ['female']]\n",
    "enc.fit_transform(X)\n",
    "# array([[ 1.],\n",
    "#       [ 0.],\n",
    "#       [nan],\n",
    "#       [ 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`OrdinalEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html) fournit un paramètre `encoded_missing_value` pour encoder les valeurs manquantes sans avoir besoin de créer un pipeline et en utilisant [**`SimpleImputer`**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 0.],\n",
       "       [-1.],\n",
       "       [ 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc = preprocessing.OrdinalEncoder(encoded_missing_value=-1)\n",
    "X = [['male'], ['female'], [np.nan], ['female']]\n",
    "enc.fit_transform(X)\n",
    "# array([[ 1.],\n",
    "#        [ 0.],\n",
    "#        [-1.],\n",
    "#        [ 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le traitement ci-dessus est équivalent au pipeline suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "enc = Pipeline(steps=[\n",
    "        (\"encoder\", preprocessing.OrdinalEncoder()),\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=-1)),\n",
    "    ])\n",
    "enc.fit_transform(X)\n",
    "# array([[ 1.],\n",
    "#        [ 0.],\n",
    "#        [-1.],\n",
    "#        [ 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une autre possibilité de convertir des caractéristiques catégorielles en caractéristiques pouvant être utilisées avec les estimateurs scikit-learn consiste à utiliser un one-of-K, également connu sous le nom de codage one-hot ou factice. Ce type d'encodage peut être obtenu avec le [**`OneHotEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html), qui transforme chaque caractéristique catégorique avec `n_categories` valeurs possibles en `n_categories` caractéristiques binaires, avec l'une d'elles à 1, et toutes les autres à 0.\n",
    "\n",
    "Reprenons l'exemple ci-dessus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 1., 0., 0., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc = preprocessing.OneHotEncoder()\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)\n",
    "# OneHotEncoder()\n",
    "enc.transform([['female', 'from US', 'uses Safari'],\n",
    "               ['male', 'from Europe', 'uses Safari']]).toarray()\n",
    "# array([[1., 0., 0., 1., 0., 1.],\n",
    "#        [0., 1., 1., 0., 0., 1.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par défaut, les valeurs que chaque caractéristique peut prendre sont automatiquement déduites du jeu de données et peuvent être trouvées dans l'attribut `categories_` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['female', 'male'], dtype=object),\n",
       " array(['from Europe', 'from US'], dtype=object),\n",
       " array(['uses Firefox', 'uses Safari'], dtype=object)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc.categories_\n",
    "# [array(['female', 'male'], dtype=object),\n",
    "#  array(['from Europe', 'from US'], dtype=object),\n",
    "#  array(['uses Firefox', 'uses Safari'], dtype=object)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible de le spécifier explicitement à l'aide du paramètre `categories`. Il existe deux genres, quatre continents possibles et quatre navigateurs Web dans notre ensemble de données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 0., 1., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "genders = ['female', 'male']\n",
    "locations = ['from Africa', 'from Asia', 'from Europe', 'from US']\n",
    "browsers = ['uses Chrome', 'uses Firefox', 'uses IE', 'uses Safari']\n",
    "enc = preprocessing.OneHotEncoder(categories=[genders, locations, browsers])\n",
    "# Note that for there are missing categorical values for the 2nd and 3rd\n",
    "# feature\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)\n",
    "# OneHotEncoder(categories=[['female', 'male'],\n",
    "#                           ['from Africa', 'from Asia', 'from Europe',\n",
    "#                            'from US'],\n",
    "#                           ['uses Chrome', 'uses Firefox', 'uses IE',\n",
    "#                            'uses Safari']])\n",
    "enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray()\n",
    "# array([[1., 0., 0., 1., 0., 0., 1., 0., 0., 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S'il est possible que les données d'entraînement aient des caractéristiques catégorielles manquantes, il peut souvent être préférable de spécifier `handle_unknown='infrequent_if_exist'` au lieu de définir les catégories manuellement comme ci-dessus. Lorsque `handle_unknown='infrequent_if_exist'` est spécifié et que des catégories inconnues sont rencontrées lors de la transformation, aucune erreur n'est générée, mais les colonnes encodées à chaud résultantes pour cette caractéristique sont toutes nulles ou considérées comme une catégorie peu fréquente si elle cette option est activée. (`handle_unknown='infrequent_if_exist'` n'est pris en charge que pour l'encodage one-hot) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc = preprocessing.OneHotEncoder(handle_unknown='infrequent_if_exist')\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)\n",
    "# OneHotEncoder(handle_unknown='infrequent_if_exist')\n",
    "enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray()\n",
    "# array([[1., 0., 0., 0., 0., 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est également possible d'encoder chaque colonne en `n_categories - 1` colonnes au lieu de `n_categories` colonnes en utilisant le paramètre `drop`. Ce paramètre permet à l'utilisateur de spécifier une catégorie pour chaque caractéristique à supprimer. Ceci est utile pour éviter la colinéarité dans la matrice d'entrée dans certains classificateurs. Une telle fonctionnalité est utile, par exemple, lors de l'utilisation d'une régression non régularisée ([**`LinearRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)), car la colinéarité rendrait la matrice de covariance non inversible :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [['male', 'from US', 'uses Safari'],\n",
    "     ['female', 'from Europe', 'uses Firefox']]\n",
    "drop_enc = preprocessing.OneHotEncoder(drop='first').fit(X)\n",
    "drop_enc.categories_\n",
    "# [array(['female', 'male'], dtype=object), array(['from Europe', 'from US'], dtype=object),\n",
    "#  array(['uses Firefox', 'uses Safari'], dtype=object)]\n",
    "drop_enc.transform(X).toarray()\n",
    "# array([[1., 1., 1.],\n",
    "#        [0., 0., 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut vouloir supprimer l'une des deux colonnes uniquement pour les caractéristiques à 2 catégories. Dans ce cas, vous pouvez définir le paramètre `drop='if_binary'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 1., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [['male', 'US', 'Safari'],\n",
    "     ['female', 'Europe', 'Firefox'],\n",
    "     ['female', 'Asia', 'Chrome']]\n",
    "drop_enc = preprocessing.OneHotEncoder(drop='if_binary').fit(X)\n",
    "drop_enc.categories_\n",
    "# [array(['female', 'male'], dtype=object), array(['Asia', 'Europe', 'US'], dtype=object),\n",
    "#  array(['Chrome', 'Firefox', 'Safari'], dtype=object)]\n",
    "drop_enc.transform(X).toarray()\n",
    "# array([[1., 0., 0., 1., 0., 0., 1.],\n",
    "#        [0., 0., 1., 0., 0., 1., 0.],\n",
    "#        [0., 1., 0., 0., 1., 0., 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le `X` transformé, la première colonne est encode la caractéristique avec les catégories \"masculin\"/\"féminin\", tandis que les 6 colonnes restantes encodent les 2 caractéristiques avec respectivement 3 catégories chacune.\n",
    "\n",
    "Lorsque `handle_unknown='ignore'` et que `drop` n'est pas `None`, les catégories inconnues seront encodées en zéros :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_enc = preprocessing.OneHotEncoder(drop='first',\n",
    "                                       handle_unknown='ignore').fit(X)\n",
    "X_test = [['unknown', 'America', 'IE']]\n",
    "drop_enc.transform(X_test).toarray()\n",
    "# array([[0., 0., 0., 0., 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toutes les catégories dans `X_test` sont inconnues lors de la transformation et seront mappées sur tous les zéros. Cela signifie que les catégories inconnues auront la même correspondance que la catégorie supprimée. : `OneHotEncoder.inverse_transform` mappera tous les zéros sur la catégorie supprimée si une catégorie est supprimée et `None` si une catégorie n'est pas supprimée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:188: UserWarning: Found unknown categories in columns [0, 1, 2] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['female', None, None]], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drop_enc = preprocessing.OneHotEncoder(drop='if_binary', sparse=False,\n",
    "                                       handle_unknown='ignore').fit(X)\n",
    "X_test = [['unknown', 'America', 'IE']]\n",
    "X_trans = drop_enc.transform(X_test)\n",
    "X_trans\n",
    "# array([[0., 0., 0., 0., 0., 0., 0.]])\n",
    "drop_enc.inverse_transform(X_trans)\n",
    "# array([['female', None, None]], dtype=object)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`OneHotEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder) prend en charge les caractéristiques catégorielles avec des valeurs manquantes en considérant les valeurs manquantes comme une catégorie supplémentaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 1., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [['male', 'Safari'],\n",
    "     ['female', None],\n",
    "     [np.nan, 'Firefox']]\n",
    "enc = preprocessing.OneHotEncoder(handle_unknown='error').fit(X)\n",
    "enc.categories_\n",
    "# [array(['female', 'male', nan], dtype=object),\n",
    "#  array(['Firefox', 'Safari', None], dtype=object)]\n",
    "enc.transform(X).toarray()\n",
    "# array([[0., 1., 0., 0., 1., 0.],\n",
    "#        [1., 0., 0., 0., 0., 1.],\n",
    "#        [0., 0., 1., 1., 0., 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si une caractéristique contient à la fois `np.nan` et `None`, elles seront considérées comme des catégories distinctes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [['Safari'], [None], [np.nan], ['Firefox']]\n",
    "enc = preprocessing.OneHotEncoder(handle_unknown='error').fit(X)\n",
    "enc.categories_\n",
    "# [array(['Firefox', 'Safari', None, nan], dtype=object)]\n",
    "enc.transform(X).toarray()\n",
    "# array([[0., 1., 0., 0.],\n",
    "#        [0., 0., 1., 0.],\n",
    "#        [0., 0., 0., 1.],\n",
    "#        [1., 0., 0., 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir [**Chargement de caractéristiques** (6.2.1)](https://scikit-learn.org/stable/modules/feature_extraction.html#dict-feature-extraction) à partir de dicts pour des caractéristiques catégorielles représentées sous forme de dictionnaires, et non sous forme de scalaires."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='infrequent-categories'></a> 6.3.4.1. Catégories peu fréquentes\n",
    "\n",
    "[**`OneHotEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) prend en charge l'agrégation de catégories peu fréquentes en une seule sortie pour chaque caractéristique. Les paramètres permettant de rassembler les catégories peu fréquentes sont `min_frequency` et `max_categories`.\n",
    "\n",
    "1. `min_frequency` est soit un entier supérieur ou égal à 1, soit un flottant dans l'intervalle `(0.0, 1.0)`. Si `min_frequency` est un entier, les catégories avec une cardinalité inférieure à `min_frequency` seront considérées comme peu fréquentes. Si `min_frequency` est un flottant, les catégories avec une cardinalité inférieure à cette fraction du nombre total d'échantillons seront considérées comme peu fréquentes. La valeur par défaut est 1, ce qui signifie que chaque catégorie est encodée séparément.\n",
    "\n",
    "2. `max_categories` vaut `None` ou tout entier supérieur à 1. Ce paramètre définit une limite supérieure au nombre de caractéristiques en sortie pour chaque caractéristique en entrée. `max_categories` inclut la caractéristique qui combine des catégories peu fréquentes.\n",
    "\n",
    "Dans l'exemple suivant, les catégories `dog`, `snake` sont considérées comme peu fréquentes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.array([['dog'] * 5 + ['cat'] * 20 + ['rabbit'] * 10 +\n",
    "              ['snake'] * 3], dtype=object).T\n",
    "enc = preprocessing.OneHotEncoder(min_frequency=6, sparse=False).fit(X)\n",
    "enc.infrequent_categories_\n",
    "# [array(['dog', 'snake'], dtype=object)]\n",
    "enc.transform(np.array([['dog'], ['cat'], ['rabbit'], ['snake']]))\n",
    "# array([[0., 0., 1.],\n",
    "#        [1., 0., 0.],\n",
    "#        [0., 1., 0.],\n",
    "#        [0., 0., 1.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En définissant `handle_unknown` sur `'infrequent_if_exist'`, les catégories inconnues seront considérées comme peu fréquentes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc = preprocessing.OneHotEncoder(\n",
    "    handle_unknown='infrequent_if_exist', sparse=False, min_frequency=6)\n",
    "enc = enc.fit(X)\n",
    "enc.transform(np.array([['dragon']]))\n",
    "# array([[0., 0., 1.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`OneHotEncoder.get_feature_names_out`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder.get_feature_names_out) utilise `'infrequent'` pour désigner les caractéristiques peu fréquentes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x0_cat', 'x0_rabbit', 'x0_infrequent_sklearn'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc.get_feature_names_out()\n",
    "# array(['x0_cat', 'x0_rabbit', 'x0_infrequent_sklearn'], dtype=object)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque `'handle_unknown'` est défini sur `'infrequent_if_exist'` et qu'une catégorie inconnue est rencontrée dans la transformation :\n",
    "\n",
    "1. Si la prise en charge de la catégorie peu fréquente n'a pas été configurée ou s'il n'y avait pas de catégorie peu fréquente pendant l'entraînement, les colonnes encodées à chaud résultantes pour cette caractéristique seront toutes des zéros. Dans la transformation inverse, une catégorie inconnue sera notée `None`.\n",
    "\n",
    "2. S'il y a une catégorie peu fréquente pendant l'entraînement, la catégorie inconnue sera considérée comme peu fréquente. Dans la transformation inverse, \"infrequent_sklearn\" sera utilisé pour représenter la catégorie peu fréquente.\n",
    "\n",
    "Les catégories peu fréquentes peuvent également être configurées à l'aide de `max_categories`. Dans l'exemple suivant, nous définissons `max_categories=2` pour limiter le nombre de caractéristiques dans la sortie. Il en résultera que toutes les catégories sauf la catégorie `'cat'` seront considérées comme peu fréquentes, ce qui conduira à deux caractéristiques, une pour les catégories `'cat'` et une pour les catégories peu fréquentes - qui sont toutes les autres :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc = preprocessing.OneHotEncoder(max_categories=2, sparse=False)\n",
    "enc = enc.fit(X)\n",
    "enc.transform([['dog'], ['cat'], ['rabbit'], ['snake']])\n",
    "# array([[0., 1.],\n",
    "#        [1., 0.],\n",
    "#        [0., 1.],\n",
    "#        [0., 1.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si `max_categories` et `min_frequency` ne sont pas des valeurs par défaut, les catégories sont sélectionnées en fonction de `min_frequency` en premier et les catégories `max_categories` sont conservées. Dans l'exemple suivant, `min_frequency=4` considère que seul `snake` est peu fréquent, mais `max_categories=3`, force `dog` à être également peu fréquent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = preprocessing.OneHotEncoder(min_frequency=4, max_categories=3, sparse=False)\n",
    "enc = enc.fit(X)\n",
    "enc.transform([['dog'], ['cat'], ['rabbit'], ['snake']])\n",
    "# array([[0., 0., 1.],\n",
    "#        [1., 0., 0.],\n",
    "#        [0., 1., 0.],\n",
    "#        [0., 0., 1.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S'il existe des catégories peu fréquentes avec la même cardinalité au seuil de `max_categories`, alors les premières `max_categories` sont prises en fonction de l'ordre du lexique. Dans l'exemple suivant, « b », « c » et « d » ont la même cardinalité et avec `max_categories=2`, « b » et « c » sont peu fréquents car ils ont un ordre de lexique plus élevé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['b', 'c'], dtype=object)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.asarray([[\"a\"] * 20 + [\"b\"] * 10 + [\"c\"] * 10 + [\"d\"] * 10], dtype=object).T\n",
    "enc = preprocessing.OneHotEncoder(max_categories=3).fit(X)\n",
    "enc.infrequent_categories_\n",
    "# [array(['b', 'c'], dtype=object)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.5. Discrétisation\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9aff9e50adfaa9e30c910fb3872ffdc72747acb5f50803ca0504f00e980f7c25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
