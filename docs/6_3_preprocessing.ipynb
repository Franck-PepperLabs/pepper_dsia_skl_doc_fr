{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='preprocessing-data'></a> 6.3. [**Prétraitement des données**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_3_preprocessing.ipynb#preprocessing-data)</br>([*Preprocessing data*](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-data))\n",
    "\n",
    "Le package `sklearn.preprocessing` fournit plusieurs fonctions utilitaires courantes et des classes de transformeurs pour changer les vecteurs de caractéristiques brutes en une représentation plus adaptée aux estimateurs aval.\n",
    "\n",
    "En général, les algorithmes d'apprentissage bénéficient d'une standardisation de l'ensemble de données. Si des outliers sont présents dans l'ensemble, des redimensionneurs ou transformateurs robustes sont plus appropriés. Le comportement des différents redimensionneurs, transformeurs et normaliseurs sur un jeu de données contenant des outliers marginaux est mis en évidence dans [**Comparaison de l'effet de différents redimensionneurs sur des données avec des outliers**](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html).\n",
    "\n",
    "- ✔ 6.3. [**Prétraitement des données**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_3_preprocessing.ipynb#preprocessing-data)<br/>([*Preprocessing data*](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-data))\n",
    "    - **Volume** : 26 pages, 7 exemples, 5 papiers\n",
    "    - ✔ 6.3.1. [**Standardisation, ou suppression de la moyenne et mise à l'échelle de la variance**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_3_preprocessing.ipynb#standardization-or-mean-removal-and-variance-scaling)<br/>([*Standardization, or mean removal and variance scaling*](https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling))\n",
    "    - ✔ 6.3.2. [**Transformation non linéaire**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_3_preprocessing.ipynb#non-linear-transformation)<br/>([*Non-linear transformation*](https://scikit-learn.org/stable/modules/preprocessing.html#non-linear-transformation))\n",
    "    - ✔ 6.3.3. [**Normalisation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_3_preprocessing.ipynb#normalization)<br/>([*Normalization*](https://scikit-learn.org/stable/modules/preprocessing.html#normalization))\n",
    "    - ✔ 6.3.4. [**Encodage des caractéristiques catégorielles**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_3_preprocessing.ipynb#encoding-categorical-features)<br/>([*Encoding categorical features*](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features))\n",
    "    - ✔ 6.3.5. [**Discrétisation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_3_preprocessing.ipynb#discretization)<br/>([*Discretization*](https://scikit-learn.org/stable/modules/preprocessing.html#discretization))\n",
    "    - ✔ 6.3.6. [**Imputation des valeurs manquantes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_3_preprocessing.ipynb#imputation-of-missing-values)<br/>([*Imputation of missing values*](https://scikit-learn.org/stable/modules/preprocessing.html#imputation-of-missing-values))\n",
    "    - ✔ 6.3.7. [**Génération de caractéristiques polynomiales**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_3_preprocessing.ipynb#generating-polynomial-features)<br/>([*Generating polynomial features*](https://scikit-learn.org/stable/modules/preprocessing.html#generating-polynomial-features))\n",
    "    - ✔ 6.3.8. [**Transformateurs personnalisés**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_3_preprocessing.ipynb#custom-transformers)<br/>([*Custom transformers*](https://scikit-learn.org/stable/modules/preprocessing.html#custom-transformers))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='standardization-or-mean-removal-and-variance-scaling'></a> 6.3.1. Standardisation, ou suppression de la moyenne et normalisation de la variance\n",
    "\n",
    "La **standardisation** des ensembles de données est **une exigence courante pour de nombreux estimateurs d'apprentissage automatique** mis en œuvre dans scikit-learn; ils pourraient mal se comporter si les caractéristiques individuelles ne ressemblent pas plus ou moins à des données distribuées suivant la distribution normale standard : Gaussienne avec une **moyenne nulle et une variance unitaire**.\n",
    "\n",
    "En pratique, nous ignorons souvent la forme de la distribution et transformons simplement les données pour les centrer en supprimant la valeur moyenne de chaque caractéristique, puis en les dimensionnant en divisant les caractéristiques non constantes par leur écart-type.\n",
    "\n",
    "Par exemple, de nombreux éléments utilisés dans la fonction objectif d'un algorithme d'apprentissage (tels que le noyau RBF des Machines à Vecteurs de Support ou les régulariseurs $\\ell_1$ et $\\ell_2$ des modèles linéaires) peuvent supposer que toutes les caractéristiques sont centrées autour de zéro ou ont une variance de même ordre. Si une caractéristique a une variance qui est des ordres de grandeur plus grande que les autres, elle pourrait dominer la fonction objectif et empêcher l'estimateur d'apprendre correctement des autres caractéristiques comme prévu.\n",
    "\n",
    "Le module [**`preprocessing`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) fournit la classe utilitaire [**`StandardScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html), qui est un moyen rapide et facile pour effectuer les opérations suivantes sur un jeu de données de type tableau :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "scaler\n",
    "# StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.        , 0.33333333])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.mean_\n",
    "# array([1. ..., 0. ..., 0.33...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81649658, 0.81649658, 1.24721913])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.scale_\n",
    "#array([0.81..., 0.81..., 1.24...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled = scaler.transform(X_train)\n",
    "X_scaled\n",
    "# array([[ 0.  ..., -1.22...,  1.33...],\n",
    "#        [ 1.22...,  0.  ..., -0.26...],\n",
    "#        [-1.22...,  1.22..., -1.06...]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données mises à l'échelle ont une moyenne nulle et une variance unitaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled.mean(axis=0)\n",
    "# array([0., 0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled.std(axis=0)\n",
    "# array([1., 1., 1.])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette classe implémente l'API `Transformer` pour calculer la moyenne et l'écart-type sur un ensemble d'entraînement, afin de pouvoir réappliquer la même transformation sur l'ensemble de test. Cette classe est donc adaptée aux premières étapes d'un [**`Pipeline`**](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;logisticregression&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;logisticregression&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('logisticregression', LogisticRegression())])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y = make_classification(random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "pipe.fit(X_train, y_train)  # apply scaling on training data\n",
    "# Pipeline(steps=[('standardscaler', StandardScaler()),\n",
    "#                 ('logisticregression', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(X_test, y_test)  # apply scaling on testing data, without leaking training data.\n",
    "# 0.96"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible de désactiver le centrage ou la mise à l'échelle en passant respectivement les paramètres `with_mean=False` ou `with_std=False` au constructeur de [**`StandardScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='scaling-features-to-a-range'></a> 6.3.1.1. Mise à l'échelle des caractéristiques dans une plage donnée\n",
    "\n",
    "Une autre méthode de standardisation consiste à mettre à l'échelle les caractéristiques pour ramener entre des valeurs minimale et maximale données, souvent entre zéro et un, ou de manière à ce que la valeur absolue maximale de chaque caractéristique soit mise à l'échelle à une taille unitaire. Cela peut être réalisé en utilisant [**`MinMaxScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) ou [**`MaxAbsScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html), respectivement.\n",
    "\n",
    "La motivation pour utiliser cette mise à l'échelle comprend la robustesse par rapport aux écarts types très faibles des caractéristiques et la préservation des entrées nulles dans les données éparses.\n",
    "\n",
    "Voici un exemple pour mettre à l'échelle une matrice de données factices dans la plage `[0, 1]` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.        , 1.        ],\n",
       "       [1.        , 0.5       , 0.33333333],\n",
       "       [0.        , 1.        , 0.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "X_train_minmax\n",
    "# array([[0.5       , 0.        , 1.        ],\n",
    "#        [1.        , 0.5       , 0.33333333],\n",
    "#        [0.        , 1.        , 0.        ]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La même instance du transformeur peut ensuite être appliquée à de nouvelles données de test qui n'ont pas été vues lors de l'appel à la méthode `fit` : les mêmes opérations de mise à l'échelle et de translation seront appliquées pour assurer la cohérence avec la transformation effectuée sur les données d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5       ,  0.        ,  1.66666667]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.array([[-3., -1.,  4.]])\n",
    "X_test_minmax = min_max_scaler.transform(X_test)\n",
    "X_test_minmax\n",
    "# array([[-1.5       ,  0.        ,  1.66666667]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible d'introspecter les attributs du redimensionneur pour en savoir plus sur la nature exacte de la transformation apprise sur les données d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5       , 0.5       , 0.33333333])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_scaler.scale_\n",
    "# array([0.5       , 0.5       , 0.33...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.5       , 0.33333333])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_scaler.min_\n",
    "# array([0.        , 0.5       , 0.33...])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si [**`MinMaxScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) reçoit une plage explicite `feature_range=(min, max)`, la formule complète est :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "X_scaled = X_std * (max - min) + min"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`MaxAbsScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html) fonctionne de manière très similaire, mais met à l'échelle de manière à ce que les données d'entraînement se situent dans la plage `[-1, 1]` en divisant par la plus grande valeur maximale de chaque caractéristique. Il est conçu pour les données qui sont déjà centrées sur zéro ou des données creuses.\n",
    "\n",
    "Voici comment utiliser les données factices de l'exemple précédent avec ce redimensionneur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5, -1. ,  1. ],\n",
       "       [ 1. ,  0. ,  0. ],\n",
       "       [ 0. ,  1. , -0.5]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "X_train_maxabs = max_abs_scaler.fit_transform(X_train)\n",
    "X_train_maxabs\n",
    "# array([[ 0.5, -1. ,  1. ],\n",
    "#        [ 1. ,  0. ,  0. ],\n",
    "#        [ 0. ,  1. , -0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5, -1. ,  2. ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.array([[ -3., -1.,  4.]])\n",
    "X_test_maxabs = max_abs_scaler.transform(X_test)\n",
    "X_test_maxabs\n",
    "# array([[-1.5, -1. ,  2. ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 2.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_abs_scaler.scale_\n",
    "# array([2.,  1.,  2.])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='scaling-sparse-data'></a> 6.3.1.2. Mise à l'échelle des données creuses\n",
    "\n",
    "Recentrer des données creuses détruirait leur structure creuse, et donc cela n'est généralement pas pertinent. Cependant, il peut être judicieux de mettre à l'échelle des entrées creuses, en particulier si les caractéristiques sont sur des échelles différentes.\n",
    "\n",
    "[**`MaxAbsScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html) a été spécifiquement conçu pour la mise à l'échelle de données clairsemées et est la méthode recommandée pour cela. Cependant, [**`StandardScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) peut accepter des matrices creuses de `scipy.sparse` en entrée, à condition que `with_mean=False` soit explicitement passé au constructeur. Sinon, une `ValueError` sera levée car la recentralisation silencieuse romprait la cavité et pourrait souvent entraîner un crash de l'exécution en allouant accidentellement des quantités excessives de mémoire. [**`RobustScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) ne peut pas être adapté à des entrées creuses, mais vous pouvez utiliser la méthode `transform` sur des entrées creuses.\n",
    "\n",
    "Notez que les scalers acceptent à la fois le format CSR (Compressed Sparse Rows) et le format CSC (Compressed Sparse Columns) (voir `scipy.sparse.csr_matrix` et `scipy.sparse.csc_matrix`). Toute autre entrée creuse sera **convertie en représentation CSR**. Pour éviter des copies de mémoire inutiles, il est recommandé de choisir la représentation CSR ou CSC en amont.\n",
    "\n",
    "Enfin, si l'on s'attend à ce que les données recentrées soient suffisamment petites, une autre option consiste à convertir explicitement l'entrée en un tableau en utilisant la méthode `toarray` des matrices creuses.\n",
    "\n",
    "## <a id='scaling-data-with-outliers'></a> 6.3.1.3. Mise à l'échelle des données avec des valeurs aberrantes\n",
    "\n",
    "Si vos données contiennent de nombreuses valeurs aberrantes, une mise à l'échelle utilisant la moyenne et la variance des données risque de ne pas fonctionner très bien. Dans ces cas, vous pouvez utiliser [**`RobustScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) comme remplacement direct. Il utilise des estimations plus robustes pour le centre et l'étendue de vos données.\n",
    "\n",
    "### Références\n",
    "\n",
    "Une discussion plus approfondie sur l'importance du recentrage et de la mise à l'échelle des données est disponible dans cette FAQ : [Dois-je normaliser/standardiser/mettre à l'échelle les données ?](http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html)\n",
    "\n",
    "### Mise à l'échelle vs Blanchiment\n",
    "\n",
    "Il n'est parfois pas suffisant de centrer et mettre à l'échelle les caractéristiques indépendamment, car un modèle ultérieur peut faire des hypothèses supplémentaires sur l'indépendance linéaire des caractéristiques.\n",
    "\n",
    "Pour résoudre ce problème, vous pouvez utiliser [**`PCA`** (Analyse en Composantes Principales)](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA) avec `whiten=True` pour éliminer davantage la corrélation linéaire entre les caractéristiques."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='centering-kernel-matrices'></a> 6.3.1.4. Recentrage des matrices de noyau\n",
    "\n",
    "Si vous disposez d'une matrice de noyau de noyau $K$ qui calcule un produit scalaire dans un espace de caractéristiques (éventuellement de manière implicite) défini par une fonction $\\phi(\\cdot)$, un [**`KernelCenterer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KernelCenterer.html) peut transformer la matrice de noyau de manière à contenir des produits scalaires dans l'espace de caractéristiques défini par suivi de la suppression de la moyenne dans cet espace. En d'autres termes, le [**`KernelCenterer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KernelCenterer.html) calcule la matrice de Gram centrée associée à un noyau semi-défini positif $K$.\n",
    "\n",
    "### Formulation mathématique\n",
    "\n",
    "Nous pouvons examiner la formulation mathématique maintenant que nous avons l'intuition. Soit $K$\n",
    "une matrice de noyau de forme `(n_samples, n_samples)` calculée à partir de $X$, une matrice de données de forme `(n_samples, n_features)`, lors de l'étape de l'ajustement (`fit`). $K$ est défini par\n",
    "\n",
    "$$K(X, X) = \\phi(X) . \\phi(X)^{T}$$\n",
    "\n",
    "$\\phi(X)$ est une fonction qui mappe $X$ dans un espace de Hilbert. Un noyau centré $\\tilde{K}$ est défini comme :\n",
    "\n",
    "$$\\tilde{K}(X, X) = \\tilde{\\phi}(X) . \\tilde{\\phi}(X)^{T}$$\n",
    "\n",
    "où $\\tilde{\\phi}(X)$ résulte du centrage de $\\phi(X)$ dans l'espace de Hilbert.\n",
    "\n",
    "Ainsi, on pourrait calculer $\\tilde{K}$ en effectuant le mappage en utilisant la fonction $\\phi(\\cdot)$ et en recentrant les données dans ce nouvel espace. Cependant, les noyaux sont souvent utilisés car ils permettent des calculs algébriques qui évitent de calculer explicitement ce mappage en utilisant $\\phi(\\cdot)$. En effet, on peut implicitement centrer comme indiqué dans l'annexe B de [Scholkopf1998] :\n",
    "\n",
    "$$\\tilde{K} = K - 1_{\\text{n}_{samples}} K - K 1_{\\text{n}_{samples}} + 1_{\\text{n}_{samples}} K 1_{\\text{n}_{samples}}$$\n",
    "\n",
    "$1_{\\text{n}_{samples}}$ est une matrice de `(n_samples, n_samples)` où toutes les entrées sont égales à $\\frac{1}{\\text{n}_{samples}}$. Lors de l'étape `transform`, le noyau devient $K_{test}(X, Y)$ défini comme :\n",
    "\n",
    "$$K_{test}(X, Y) = \\phi(Y) . \\phi(X)^{T}$$\n",
    "\n",
    "$Y$ est l'ensemble de données de test de forme `(n_samples_test, n_features)` et donc $K_{test}$ de forme `(n_samples_test, n_samples)`. Dans ce cas, le centrage de $K_{test}$ est effectué comme suit :\n",
    "\n",
    "$$\\tilde{K}_{test}(X, Y) = K_{test} - 1'_{\\text{n}_{samples}} K - K_{test} 1_{\\text{n}_{samples}} + 1'_{\\text{n}_{samples}} K 1_{\\text{n}_{samples}}$$\n",
    "\n",
    "$1'_{\\text{n}_{samples}}$ est une matrice de forme `(n_samples_test, n_samples)` où toutes les entrées sont égales à $\\frac{1}{\\text{n}_{samples}}$.\n",
    "\n",
    "### Références\n",
    "\n",
    "[Scholkopf1998] B. Schölkopf, A. Smola, and K.R. Müller, [“**Nonlinear component analysis as a kernel eigenvalue problem**](https://www.mlpack.org/papers/kpca.pdf)[”](https://drive.google.com/file/d/1ApHX19KKGii4WNoGWJCStLa3zK1sTB3p/view?usp=drive_link). Neural computation 10.5 (1998): 1299-1319."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='non-linear-transformation'></a> 6.3.2. Transformation non linéaire\n",
    "\n",
    "Deux types de transformations sont disponibles : les transformations quantiles et les transformations de puissance. Les transformations quantiles et les transformations de puissance sont basées sur des transformations monotones des caractéristiques, ce qui préserve le rang des valeurs le long de chaque caractéristique.\n",
    "\n",
    "Les transformations quantiles placent toutes les caractéristiques dans la même distribution souhaitée en utilisant la formule $G^{-1}(F(X))$ où $F$ est la fonction de distribution cumulative de la caractéristique et $G^{-1}$ la [fonction quantile](https://en.wikipedia.org/wiki/Quantile_function) de la distribution de sortie souhaitée $G$. Cette formule utilise les deux faits suivants : (i) si $X$ est une variable aléatoire avec une fonction de distribution cumulative continue $F$, alors $F(X)$ est uniformément distribué sur $[0,1]$; (ii) si $U$ est une variable aléatoire avec une distribution uniforme sur $[0,1]$, alors $G^{-1}(U)$ a une distribution $G$. En effectuant une transformation de rang, une transformation quantile lisse les distributions inhabituelles et est moins influencée par les valeurs aberrantes que les méthodes de mise à l'échelle. Cependant, elle déforme les corrélations et les distances à l'intérieur et entre les caractéristiques.\n",
    "\n",
    "Les transformations de puissance sont une famille de transformations paramétriques qui visent à mapper les données de n'importe quelle distribution aussi proche que possible d'une distribution gaussienne."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='mapping-to-a-uniform-distribution'></a> 6.3.2.1. Mappage vers une distribution uniforme\n",
    "\n",
    "[**`QuantileTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html) fournit une transformation non paramétrique pour mapper les données vers une distribution uniforme avec des valeurs entre 0 et 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "quantile_transformer = preprocessing.QuantileTransformer(random_state=0)\n",
    "X_train_trans = quantile_transformer.fit_transform(X_train)\n",
    "X_test_trans = quantile_transformer.transform(X_test)\n",
    "np.percentile(X_train[:, 0], [0, 25, 50, 75, 100])\n",
    "# array([ 4.3,  5.1,  5.8,  6.5,  7.9])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette caractéristique correspond à la longueur des sépales en cm. Une fois la transformation quantile appliquée, ces points de repère se rapprochent étroitement des percentiles précédemment définis :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(X_train_trans[:, 0], [0, 25, 50, 75, 100])\n",
    "# array([ 0.00... ,  0.24...,  0.49...,  0.73...,  0.99... ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela peut être confirmé sur un ensemble de test indépendant avec des remarques similaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(X_test[:, 0], [0, 25, 50, 75, 100])\n",
    "# array([ 4.4  ,  5.125,  5.75 ,  6.175,  7.3  ])\n",
    "np.percentile(X_test_trans[:, 0], [0, 25, 50, 75, 100])\n",
    "# array([ 0.01...,  0.25...,  0.46...,  0.60... ,  0.94...])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='mapping-to-a-gaussian-distribution'></a> 6.3.2.2. Mappage vers une distribution gaussienne\n",
    "\n",
    "Dans de nombreux scénarios de modélisation, la normalité des caractéristiques d'un ensemble de données est souhaitable. Les transformations de puissance sont une famille de transformations paramétriques et monotones qui visent à mapper les données de n'importe quelle distribution aussi proche que possible d'une distribution gaussienne afin de stabiliser la variance et de minimiser l'asymétrie.\n",
    "\n",
    "[**`PowerTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html) fournit actuellement deux transformations de puissance, la transformation Yeo-Johnson et la transformation Box-Cox.\n",
    "\n",
    "La transformation Yeo-Johnson est donnée par :\n",
    "\n",
    "$$\\begin{split}x_i^{(\\lambda)} =\n",
    "\\begin{cases}\n",
    " [(x_i + 1)^\\lambda - 1] / \\lambda & \\text{if } \\lambda \\neq 0, x_i \\geq 0, \\\\[8pt]\n",
    "\\ln{(x_i + 1)} & \\text{if } \\lambda = 0, x_i \\geq 0 \\\\[8pt]\n",
    "-[(-x_i + 1)^{2 - \\lambda} - 1] / (2 - \\lambda) & \\text{if } \\lambda \\neq 2, x_i < 0, \\\\[8pt]\n",
    " - \\ln (- x_i + 1) & \\text{if } \\lambda = 2, x_i < 0\n",
    "\\end{cases}\\end{split}$$\n",
    "\n",
    "tandis que la transformation Box-Cox est donnée par :\n",
    "\n",
    "$$\\begin{split}x_i^{(\\lambda)} =\n",
    "\\begin{cases}\n",
    "\\dfrac{x_i^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0, \\\\[8pt]\n",
    "\\ln{(x_i)} & \\text{if } \\lambda = 0,\n",
    "\\end{cases}\\end{split}$$\n",
    "\n",
    "Box-Cox ne peut être appliqué qu'à des données strictement positives. Dans les deux méthodes, la transformation est paramétrée par $\\lambda$, qui est déterminé par une estimation du maximum de vraisemblance. Voici un exemple d'utilisation de Box-Cox pour mapper des échantillons tirés d'une distribution lognormale vers une distribution normale :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = preprocessing.PowerTransformer(method='box-cox', standardize=False)\n",
    "X_lognormal = np.random.RandomState(616).lognormal(size=(3, 3))\n",
    "X_lognormal\n",
    "# array([[1.28..., 1.18..., 0.84...],\n",
    "#        [0.94..., 1.60..., 0.38...],\n",
    "#        [1.35..., 0.21..., 1.09...]])\n",
    "pt.fit_transform(X_lognormal)\n",
    "# array([[ 0.49...,  0.17..., -0.15...],\n",
    "#        [-0.05...,  0.58..., -0.57...],\n",
    " #       [ 0.69..., -0.84...,  0.10...]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien que l'exemple ci-dessus définisse l'option `standardize` sur `False`, [**`PowerTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html) appliquera par défaut une normalisation à moyenne nulle et variance unitaire à la sortie transformée.\n",
    "\n",
    "Ci-dessous, des exemples de Box-Cox et de Yeo-Johnson appliqués à diverses distributions de probabilité. Notez que lorsqu'ils sont appliqués à certaines distributions, les transformations de puissance donnent des résultats très similaires à une distribution gaussienne, mais avec d'autres, elles sont inefficaces. Cela souligne l'importance de visualiser les données avant et après la transformation.\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_map_data_to_normal_001.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est également possible de mapper les données vers une distribution normale en utilisant [**`QuantileTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html) avec `output_distribution='normal'`. En utilisant l'exemple précédent avec l'ensemble de données Iris :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_transformer = preprocessing.QuantileTransformer(\n",
    "      output_distribution='normal', random_state=0)\n",
    "X_trans = quantile_transformer.fit_transform(X)\n",
    "quantile_transformer.quantiles_\n",
    "# array([[4.3, 2. , 1. , 0.1],\n",
    "#        [4.4, 2.2, 1.1, 0.1],\n",
    "#        [4.4, 2.2, 1.2, 0.1],\n",
    "#        ...,\n",
    "#        [7.7, 4.1, 6.7, 2.5],\n",
    "#        [7.7, 4.2, 6.7, 2.5],\n",
    "#        [7.9, 4.4, 6.9, 2.5]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi, la médiane de l'entrée devient la moyenne de la sortie, centrée sur 0. La sortie normale est tronquée de sorte que le minimum et le maximum de l'entrée - correspondant respectivement aux quantiles 1e-7 et 1 - 1e-7 - ne deviennent pas infinis sous la transformation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='normalization'></a> 6.3.3. Normalisation\n",
    "\n",
    "La **normalisation** est le processus de **mise à l'échelle des échantillons individuels pour avoir une norme unitaire**. Ce processus peut être utile si vous prévoyez d'utiliser une forme quadratique telle que le produit scalaire ou tout autre noyau pour quantifier la similarité de chaque paire d'échantillons.\n",
    "\n",
    "Cette hypothèse est à la base du [**modèle d'espace vectoriel** (VSM)](https://en.wikipedia.org/wiki/Vector_Space_Model) souvent utilisé dans les contextes de classification et de regroupement de texte.\n",
    "\n",
    "La fonction [**`normalize`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html) fournit un moyen rapide et facile d'effectuer cette opération sur un seul ensemble de données de type tableau, en utilisant les normes `l1`, `l2` ou `max` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[ 1., -1.,  2.],\n",
    "    [ 2.,  0.,  0.],\n",
    "    [ 0.,  1., -1.]]\n",
    "X_normalized = preprocessing.normalize(X, norm='l2')\n",
    "\n",
    "X_normalized\n",
    "# array([[ 0.40..., -0.40...,  0.81...],\n",
    "#        [ 1.  ...,  0.  ...,  0.  ...],\n",
    "#        [ 0.  ...,  0.70..., -0.70...]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le module `preprocessing` fournit également une classe utilitaire [**`Normalizer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer) qui implémente la même opération en utilisant l'API du `Transformer` (même si la méthode `fit` est inutile dans ce cas : la classe est sans état car cette opération traite les échantillons indépendamment).\n",
    "\n",
    "Cette classe convient donc à une utilisation dans les premières étapes d'un [**`Pipeline`**](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = preprocessing.Normalizer().fit(X)  # fit does nothing\n",
    "normalizer\n",
    "# Normalizer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'instance du normaliseur peut ensuite être utilisée sur des vecteurs d'échantillons comme n'importe quel transformeur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer.transform(X)\n",
    "# array([[ 0.40..., -0.40...,  0.81...],\n",
    "#        [ 1.  ...,  0.  ...,  0.  ...],\n",
    "#        [ 0.  ...,  0.70..., -0.70...]])\n",
    "\n",
    "normalizer.transform([[-1.,  1., 0.]])\n",
    "# array([[-0.70...,  0.70...,  0.  ...]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : La normalisation $\\ell_2$ est également connue sous le nom de prétraitement par signe spatial.\n",
    "\n",
    "### Entrée creuse\n",
    "\n",
    "[**`normalize`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html) et [**`Normalizer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html) acceptent **à la fois des tableaux denses et des matrices creuses de type array-like provenant de scipy.sparse en entrée.**\n",
    "\n",
    "Pour les entrées creuses, les données sont **converties en représentation Compressed Sparse Rows** (voir `scipy.sparse.csr_matrix`) avant d'être traitées par des routines Cython efficaces. Pour éviter des copies inutiles de mémoire, il est recommandé de choisir la représentation CSR en amont."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='encoding-categorical-features'></a> 6.3.4. Encodage des caractéristiques catégorielles\n",
    "\n",
    "Souvent, les caractéristiques ne sont pas données sous forme de valeurs continues mais catégorielles. Par exemple, une personne peut avoir des caractéristiques `[\"homme\", \"femme\"]`, `[\"from Europe\", \"from États-Unis\", \"from Asia\"]`, `[\"uses Firefox\", \"uses Chrome\", \"uses Safari\", \"uses Internet Explorer\"]`. De telles caractéristiques peuvent être efficacement codées sous forme d'entiers, par exemple `[\"male\", \"from États-Unis\", \"uses Internet Explorer\"]` pourrait être exprimé par `[0, 1, 3]` tandis que `[\"female\", \"from Asia\", \"uses Chrome\"]` serait `[1, 2, 1]`.\n",
    "\n",
    "Pour convertir des caractéristiques catégorielles en de tels codes entiers, nous pouvons utiliser [**`OrdinalEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html). Cet estimateur transforme chaque caractéristique catégorielle en une nouvelle caractéristique d'entiers ($0$ à $n_{\\text{categories}} - 1$) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "enc = preprocessing.OrdinalEncoder()\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)\n",
    "# OrdinalEncoder()\n",
    "enc.transform([['female', 'from US', 'uses Safari']])\n",
    "# array([[0., 1., 1.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une telle représentation entière ne peut cependant pas être utilisée directement avec tous les estimateurs scikit-learn, car ceux-ci attendent une entrée continue et interpréteraient les catégories en tant qu'ordinaux, ce qui n'est souvent pas souhaité (c'est-à-dire que l'ensemble des navigateurs a été ordonné arbitrairement).\n",
    "\n",
    "Par défaut, [**`OrdinalEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html) transmettra également les valeurs manquantes représentées par `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "enc = preprocessing.OrdinalEncoder()\n",
    "X = [['male'], ['female'], [np.nan], ['female']]\n",
    "enc.fit_transform(X)\n",
    "# array([[ 1.],\n",
    "#       [ 0.],\n",
    "#       [nan],\n",
    "#       [ 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`OrdinalEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html) fournit un paramètre `encoded_missing_value` pour encoder les valeurs manquantes sans avoir besoin de créer un pipeline et en utilisant [**`SimpleImputer`**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 0.],\n",
       "       [-1.],\n",
       "       [ 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc = preprocessing.OrdinalEncoder(encoded_missing_value=-1)\n",
    "X = [['male'], ['female'], [np.nan], ['female']]\n",
    "enc.fit_transform(X)\n",
    "# array([[ 1.],\n",
    "#        [ 0.],\n",
    "#        [-1.],\n",
    "#        [ 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le traitement ci-dessus est équivalent au pipeline suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "enc = Pipeline(steps=[\n",
    "        (\"encoder\", preprocessing.OrdinalEncoder()),\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=-1)),\n",
    "    ])\n",
    "enc.fit_transform(X)\n",
    "# array([[ 1.],\n",
    "#        [ 0.],\n",
    "#        [-1.],\n",
    "#        [ 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une autre possibilité de convertir des caractéristiques catégorielles en caractéristiques pouvant être utilisées avec les estimateurs scikit-learn consiste à utiliser un one-of-K, également connu sous le nom de codage one-hot ou factice. Ce type d'encodage peut être obtenu avec le [**`OneHotEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html), qui transforme chaque caractéristique catégorique avec `n_categories` valeurs possibles en `n_categories` caractéristiques binaires, avec l'une d'elles à 1, et toutes les autres à 0.\n",
    "\n",
    "Reprenons l'exemple ci-dessus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 1., 0., 0., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc = preprocessing.OneHotEncoder()\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)\n",
    "# OneHotEncoder()\n",
    "enc.transform([['female', 'from US', 'uses Safari'],\n",
    "               ['male', 'from Europe', 'uses Safari']]).toarray()\n",
    "# array([[1., 0., 0., 1., 0., 1.],\n",
    "#        [0., 1., 1., 0., 0., 1.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par défaut, les valeurs que chaque caractéristique peut prendre sont automatiquement déduites du jeu de données et peuvent être trouvées dans l'attribut `categories_` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['female', 'male'], dtype=object),\n",
       " array(['from Europe', 'from US'], dtype=object),\n",
       " array(['uses Firefox', 'uses Safari'], dtype=object)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc.categories_\n",
    "# [array(['female', 'male'], dtype=object),\n",
    "#  array(['from Europe', 'from US'], dtype=object),\n",
    "#  array(['uses Firefox', 'uses Safari'], dtype=object)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible de le spécifier explicitement à l'aide du paramètre `categories`. Il existe deux genres, quatre continents possibles et quatre navigateurs Web dans notre ensemble de données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 0., 1., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "genders = ['female', 'male']\n",
    "locations = ['from Africa', 'from Asia', 'from Europe', 'from US']\n",
    "browsers = ['uses Chrome', 'uses Firefox', 'uses IE', 'uses Safari']\n",
    "enc = preprocessing.OneHotEncoder(categories=[genders, locations, browsers])\n",
    "# Note that for there are missing categorical values for the 2nd and 3rd\n",
    "# feature\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)\n",
    "# OneHotEncoder(categories=[['female', 'male'],\n",
    "#                           ['from Africa', 'from Asia', 'from Europe',\n",
    "#                            'from US'],\n",
    "#                           ['uses Chrome', 'uses Firefox', 'uses IE',\n",
    "#                            'uses Safari']])\n",
    "enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray()\n",
    "# array([[1., 0., 0., 1., 0., 0., 1., 0., 0., 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S'il est possible que les données d'entraînement aient des caractéristiques catégorielles manquantes, il peut souvent être préférable de spécifier `handle_unknown='infrequent_if_exist'` au lieu de définir les catégories manuellement comme ci-dessus. Lorsque `handle_unknown='infrequent_if_exist'` est spécifié et que des catégories inconnues sont rencontrées lors de la transformation, aucune erreur n'est générée, mais les colonnes encodées à chaud résultantes pour cette caractéristique sont toutes nulles ou considérées comme une catégorie peu fréquente si cette option est activée. (`handle_unknown='infrequent_if_exist'` n'est pris en charge que pour l'encodage one-hot) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc = preprocessing.OneHotEncoder(handle_unknown='infrequent_if_exist')\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)\n",
    "# OneHotEncoder(handle_unknown='infrequent_if_exist')\n",
    "enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray()\n",
    "# array([[1., 0., 0., 0., 0., 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est également possible d'encoder chaque colonne en `n_categories - 1` colonnes au lieu de `n_categories` colonnes en utilisant le paramètre `drop`. Ce paramètre permet à l'utilisateur de spécifier une catégorie pour chaque caractéristique à supprimer. Ceci est utile pour éviter la colinéarité dans la matrice d'entrée dans certains classificateurs. Une telle fonctionnalité est utile, par exemple, lors de l'utilisation d'une régression non régularisée ([**`LinearRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)), car la colinéarité rendrait la matrice de covariance non inversible :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [['male', 'from US', 'uses Safari'],\n",
    "     ['female', 'from Europe', 'uses Firefox']]\n",
    "drop_enc = preprocessing.OneHotEncoder(drop='first').fit(X)\n",
    "drop_enc.categories_\n",
    "# [array(['female', 'male'], dtype=object), array(['from Europe', 'from US'], dtype=object),\n",
    "#  array(['uses Firefox', 'uses Safari'], dtype=object)]\n",
    "drop_enc.transform(X).toarray()\n",
    "# array([[1., 1., 1.],\n",
    "#        [0., 0., 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut vouloir supprimer l'une des deux colonnes uniquement pour les caractéristiques à 2 catégories. Dans ce cas, vous pouvez définir le paramètre `drop='if_binary'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 1., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [['male', 'US', 'Safari'],\n",
    "     ['female', 'Europe', 'Firefox'],\n",
    "     ['female', 'Asia', 'Chrome']]\n",
    "drop_enc = preprocessing.OneHotEncoder(drop='if_binary').fit(X)\n",
    "drop_enc.categories_\n",
    "# [array(['female', 'male'], dtype=object), array(['Asia', 'Europe', 'US'], dtype=object),\n",
    "#  array(['Chrome', 'Firefox', 'Safari'], dtype=object)]\n",
    "drop_enc.transform(X).toarray()\n",
    "# array([[1., 0., 0., 1., 0., 0., 1.],\n",
    "#        [0., 0., 1., 0., 0., 1., 0.],\n",
    "#        [0., 1., 0., 0., 1., 0., 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le `X` transformé, la première colonne encode la caractéristique avec les catégories \"masculin\"/\"féminin\", tandis que les 6 colonnes restantes encodent les 2 caractéristiques avec respectivement 3 catégories chacune.\n",
    "\n",
    "Lorsque `handle_unknown='ignore'` et que `drop` n'est pas `None`, les catégories inconnues seront encodées en zéros :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_enc = preprocessing.OneHotEncoder(drop='first',\n",
    "                                       handle_unknown='ignore').fit(X)\n",
    "X_test = [['unknown', 'America', 'IE']]\n",
    "drop_enc.transform(X_test).toarray()\n",
    "# array([[0., 0., 0., 0., 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toutes les catégories dans `X_test` sont inconnues lors de la transformation et seront mappées sur tous les zéros. Cela signifie que les catégories inconnues auront la même correspondance que la catégorie supprimée. [**`OneHotEncoder.inverse_transform`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) mappera tous les zéros sur la catégorie supprimée si une catégorie est supprimée et `None` si une catégorie n'est pas supprimée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:188: UserWarning: Found unknown categories in columns [0, 1, 2] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['female', None, None]], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drop_enc = preprocessing.OneHotEncoder(drop='if_binary', sparse=False,\n",
    "                                       handle_unknown='ignore').fit(X)\n",
    "X_test = [['unknown', 'America', 'IE']]\n",
    "X_trans = drop_enc.transform(X_test)\n",
    "X_trans\n",
    "# array([[0., 0., 0., 0., 0., 0., 0.]])\n",
    "drop_enc.inverse_transform(X_trans)\n",
    "# array([['female', None, None]], dtype=object)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`OneHotEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder) prend en charge les caractéristiques catégorielles avec des valeurs manquantes en considérant les valeurs manquantes comme une catégorie supplémentaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 1., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [['male', 'Safari'],\n",
    "     ['female', None],\n",
    "     [np.nan, 'Firefox']]\n",
    "enc = preprocessing.OneHotEncoder(handle_unknown='error').fit(X)\n",
    "enc.categories_\n",
    "# [array(['female', 'male', nan], dtype=object),\n",
    "#  array(['Firefox', 'Safari', None], dtype=object)]\n",
    "enc.transform(X).toarray()\n",
    "# array([[0., 1., 0., 0., 1., 0.],\n",
    "#        [1., 0., 0., 0., 0., 1.],\n",
    "#        [0., 0., 1., 1., 0., 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si une caractéristique contient à la fois `np.nan` et `None`, elles seront considérées comme des catégories distinctes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [['Safari'], [None], [np.nan], ['Firefox']]\n",
    "enc = preprocessing.OneHotEncoder(handle_unknown='error').fit(X)\n",
    "enc.categories_\n",
    "# [array(['Firefox', 'Safari', None, nan], dtype=object)]\n",
    "enc.transform(X).toarray()\n",
    "# array([[0., 1., 0., 0.],\n",
    "#        [0., 0., 1., 0.],\n",
    "#        [0., 0., 0., 1.],\n",
    "#        [1., 0., 0., 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir [**Chargement de caractéristiques à partir de dicts** (6.2.1)](https://scikit-learn.org/stable/modules/feature_extraction.html#dict-feature-extraction) pour des caractéristiques catégorielles représentées sous forme de dictionnaires, et non sous forme de scalaires."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='infrequent-categories'></a> 6.3.4.1. Catégories peu fréquentes\n",
    "\n",
    "[**`OneHotEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) et [**`OrdinalEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html) prennent en charge l'agrégation de catégories peu fréquentes en une seule sortie pour chaque caractéristique. Les paramètres permettant de rassembler les catégories peu fréquentes sont `min_frequency` et `max_categories`.\n",
    "\n",
    "1. `min_frequency` est soit un entier supérieur ou égal à 1, soit un flottant dans l'intervalle `(0.0, 1.0)`. Si `min_frequency` est un entier, les catégories avec une cardinalité inférieure à `min_frequency` seront considérées comme peu fréquentes. Si `min_frequency` est un flottant, les catégories avec une cardinalité inférieure à cette fraction du nombre total d'échantillons seront considérées comme peu fréquentes. La valeur par défaut est 1, ce qui signifie que chaque catégorie est encodée séparément.\n",
    "\n",
    "2. `max_categories` vaut `None` ou tout entier supérieur à 1. Ce paramètre définit une limite supérieure au nombre de caractéristiques en sortie pour chaque caractéristique en entrée. `max_categories` inclut la caractéristique qui combine des catégories peu fréquentes.\n",
    "\n",
    "Dans l'exemple suivant avec [**`OrdinalEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html), les catégories `dog` et `snake` sont considérées comme peu fréquentes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([['dog'] * 5 + ['cat'] * 20 + ['rabbit'] * 10 +\n",
    "              ['snake'] * 3], dtype=object).T\n",
    "enc = preprocessing.OrdinalEncoder(min_frequency=6).fit(X)\n",
    "enc.infrequent_categories_\n",
    "# [array(['dog', 'snake'], dtype=object)]\n",
    "enc.transform(np.array([['dog'], ['cat'], ['rabbit'], ['snake']]))\n",
    "# array([[2.],\n",
    "#        [0.],\n",
    "#        [1.],\n",
    "#        [2.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le paramètre `max_categories` de [**`OrdinalEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html) **ne tient pas** compte des catégories manquantes ou inconnues. En définissant `unknown_value` ou `encoded_missing_value` sur un entier, le nombre de codes entiers uniques augmente de un à chaque fois. Cela peut entraîner jusqu'à `max_categories + 2` codes entiers. Dans l'exemple suivant, \"a\" et \"d\" sont considérés comme peu fréquents et regroupés dans une seule catégorie, \"b\" et \"c\" sont leurs propres catégories, les valeurs inconnues sont codées comme 3 et les valeurs manquantes sont codées comme 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [3.],\n",
       "       [4.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array(\n",
    "    [[\"a\"] * 5 + [\"b\"] * 20 + [\"c\"] * 10 + [\"d\"] * 3 + [np.nan]],\n",
    "    dtype=object).T\n",
    "enc = preprocessing.OrdinalEncoder(\n",
    "    handle_unknown=\"use_encoded_value\", unknown_value=3,\n",
    "    max_categories=3, encoded_missing_value=4)\n",
    "_ = enc.fit(X_train)\n",
    "X_test = np.array([[\"a\"], [\"b\"], [\"c\"], [\"d\"], [\"e\"], [np.nan]], dtype=object)\n",
    "enc.transform(X_test)\n",
    "# array([[2.],\n",
    "#        [0.],\n",
    "#        [1.],\n",
    "#        [2.],\n",
    "#        [3.],\n",
    "#        [4.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De même, [**`OneHotEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) peut être configuré pour regrouper les catégories peu fréquentes ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = preprocessing.OneHotEncoder(min_frequency=6, sparse_output=False).fit(X)\n",
    "enc.infrequent_categories_\n",
    "# [array(['dog', 'snake'], dtype=object)]\n",
    "enc.transform(np.array([['dog'], ['cat'], ['rabbit'], ['snake']]))\n",
    "# array([[0., 0., 1.],\n",
    "#        [1., 0., 0.],\n",
    "#        [0., 1., 0.],\n",
    "#        [0., 0., 1.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En définissant `handle_unknown` sur `'infrequent_if_exist'`, les catégories inconnues seront considérées comme peu fréquentes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = preprocessing.OneHotEncoder(\n",
    "    handle_unknown='infrequent_if_exist', sparse_output=False, min_frequency=6)\n",
    "enc = enc.fit(X)\n",
    "enc.transform(np.array([['dragon']]))\n",
    "# array([[0., 0., 1.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`OneHotEncoder.get_feature_names_out`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder.get_feature_names_out) utilise `'infrequent'` pour désigner les caractéristiques peu fréquentes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x0_cat', 'x0_rabbit', 'x0_infrequent_sklearn'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.get_feature_names_out()\n",
    "# array(['x0_cat', 'x0_rabbit', 'x0_infrequent_sklearn'], dtype=object)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque `'handle_unknown'` est défini sur `'infrequent_if_exist'` et qu'une catégorie inconnue est rencontrée dans la transformation :\n",
    "\n",
    "1. Si la prise en charge de la catégorie peu fréquente n'a pas été configurée ou s'il n'y avait pas de catégorie peu fréquente pendant l'entraînement, les colonnes encodées à chaud résultantes pour cette caractéristique seront toutes des zéros. Dans la transformation inverse, une catégorie inconnue sera notée `None`.\n",
    "\n",
    "2. S'il y a une catégorie peu fréquente pendant l'entraînement, la catégorie inconnue sera considérée comme peu fréquente. Dans la transformation inverse, \"infrequent_sklearn\" sera utilisé pour représenter la catégorie peu fréquente.\n",
    "\n",
    "Les catégories peu fréquentes peuvent également être configurées à l'aide de `max_categories`. Dans l'exemple suivant, nous définissons `max_categories=2` pour limiter le nombre de caractéristiques dans la sortie. Il en résultera que toutes les catégories sauf la catégorie `'cat'` seront considérées comme peu fréquentes, ce qui conduira à deux caractéristiques, une pour les catégories `'cat'` et une pour les catégories peu fréquentes - qui sont toutes les autres :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = preprocessing.OneHotEncoder(max_categories=2, sparse_output=False)\n",
    "enc = enc.fit(X)\n",
    "enc.transform([['dog'], ['cat'], ['rabbit'], ['snake']])\n",
    "# array([[0., 1.],\n",
    "#        [1., 0.],\n",
    "#        [0., 1.],\n",
    "#        [0., 1.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si `max_categories` et `min_frequency` ne sont pas des valeurs par défaut, les catégories sont sélectionnées en fonction de `min_frequency` en premier et les catégories `max_categories` sont conservées. Dans l'exemple suivant, `min_frequency=4` considère que seul `snake` est peu fréquent, mais `max_categories=3`, force `dog` à être également peu fréquent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = preprocessing.OneHotEncoder(min_frequency=4, max_categories=3, sparse_output=False)\n",
    "enc = enc.fit(X)\n",
    "enc.transform([['dog'], ['cat'], ['rabbit'], ['snake']])\n",
    "# array([[0., 0., 1.],\n",
    "#        [1., 0., 0.],\n",
    "#        [0., 1., 0.],\n",
    "#        [0., 0., 1.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S'il existe des catégories peu fréquentes avec la même cardinalité au seuil de `max_categories`, alors les premières `max_categories` sont prises en fonction de l'ordre du lexique. Dans l'exemple suivant, « b », « c » et « d » ont la même cardinalité et avec `max_categories=2`, « b » et « c » sont peu fréquents car ils ont un ordre de lexique plus élevé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['b', 'c'], dtype=object)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.asarray([[\"a\"] * 20 + [\"b\"] * 10 + [\"c\"] * 10 + [\"d\"] * 10], dtype=object).T\n",
    "enc = preprocessing.OneHotEncoder(max_categories=3).fit(X)\n",
    "enc.infrequent_categories_\n",
    "# [array(['b', 'c'], dtype=object)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='target-encoder'></a> 6.3.4.1. Encodeur de cible\n",
    "\n",
    "Le [**`TargetEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html) utilise la moyenne de la cible conditionnée à la caractéristique catégorielle pour encoder les catégories non ordonnées, c'est-à-dire les catégories nominales [PAR] [MIC]. Ce schéma d'encodage est utile pour les caractéristiques catégorielles ayant une cardinalité élevée, où l'encodage one-hot augmenterait l'espace des caractéristiques, rendant ainsi le traitement plus coûteux pour un modèle aval. Un exemple classique de catégories à cardinalité élevée sont les données basées sur la localisation, telles que le code postal ou la région. Pour la cible de classification binaire, l'encodage de la cible est donné par :\n",
    "\n",
    "$$S_i = \\lambda_i\\frac{n_{iY}}{n_i} + (1 - \\lambda_i)\\frac{n_Y}{n}$$\n",
    "\n",
    "où $S_i$ est l'encodage pour la catégorie $i$, $n_{iY}$ est le nombre d'observations avec $Y=1$ et la catégorie $i$, $n_i$ est le nombre d'observations avec la catégorie $i$, $n_Y$ est le nombre d'observations avec $Y=1$, $n$ est le nombre d'observations et $\\lambda_i$ est un facteur de réduction pour la catégorie. Le facteur de réduction est donné par :\n",
    "\n",
    "$$\\lambda_i = \\frac{n_i}{m + n_i}$$\n",
    "\n",
    "où $m$ est un facteur de lissage, qui est contrôlé avec le paramètre `smooth` dans [**`TargetEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html). Les facteurs de lissage élevés mettront davantage l'accent sur la moyenne globale. Lorsque `smooth=\"auto\"`, le facteur de lissage est calculé comme une estimation de Bayes empirique : $m=\\sigma_i^2/\\tau^2$, où $\\sigma_i^2$  est la variance de `y` avec la catégorie $i$ et $r^2$ est la variance globale de `y`.\n",
    "\n",
    "Pour les cibles continues, la formulation est similaire à la classification binaire :\n",
    "\n",
    "$$S_i = \\lambda_i\\frac{\\sum_{k\\in L_i}Y_k}{n_i} + (1 - \\lambda_i)\\frac{\\sum_{k=1}^{n}Y_k}{n}$$\n",
    "\n",
    "où $L_i$ est l'ensemble des observations avec la catégorie $i$ et $n_i$ est le nombre d'observations avec la catégorie $i$.\n",
    "\n",
    "La méthode [**`fit_transform`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html#sklearn.preprocessing.TargetEncoder.fit_transform) utilise un schéma d'ajustement croisé pour éviter que les informations de la cible ne s'introduisent dans la représentation à l'entraînement, en particulier pour les variables catégorielles de grande cardinalité non informatives, et aide à prévenir le surajustement du modèle aval à des corrélations erronées. Notez qu'en conséquence, `fit(X, y).transform(X)` n'est pas équivalent à `fit_transform(X, y)`. Dans [**`fit_transform`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html#sklearn.preprocessing.TargetEncoder.fit_transform), les données d'entraînement sont divisées en $k$ plis (déterminés par le paramètre `cv`) et chaque pli est encodé en utilisant les encodages entraînés sur les autres $k-1$ plis. Le schéma d'ajustement croisé dans [**`fit_transform`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html#sklearn.preprocessing.TargetEncoder.fit_transform) avec la valeur par défaut `cv=5` est illustré dans le diagramme suivant :\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/target_encoder_cross_validation.svg)\n",
    "\n",
    "La méthode [**`fit_transform`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html#sklearn.preprocessing.TargetEncoder.fit_transform) apprend également un encodage \"données complètes\" en utilisant la totalité du jeu d'entraînement. Cela n'est jamais utilisé dans [**`fit_transform`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html#sklearn.preprocessing.TargetEncoder.fit_transform) mais est enregistré dans l'attribut `encodings_` pour une utilisation ultérieure lors de l'appel à la méthode [**`transform`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html#sklearn.preprocessing.TargetEncoder.transform). Notez que les encodages appris pour chaque pli lors du schéma d'ajustement croisé ne sont pas enregistrés dans un attribut.\n",
    "\n",
    "La méthode [**`fit`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html#sklearn.preprocessing.TargetEncoder.fit) **n'utilise aucun** schéma d'ajustement croisé et apprend un encodage sur la totalité du jeu d'entraînement, qui est utilisé pour encoder les catégories dans la méthode [**`transform`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html#sklearn.preprocessing.TargetEncoder.transform). Cet encodage est identique à l'encodage \"données complètes\" appris dans [**`fit_transform`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html#sklearn.preprocessing.TargetEncoder.fit_transform).\n",
    "\n",
    "> **Note** : Le [**`TargetEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html) considère les valeurs manquantes, telles que `np.nan` ou `None`, comme une autre catégorie et les encode comme n'importe quelle autre catégorie. Les catégories qui ne sont pas observées lors de l'ajustement (`fit`) sont encodées avec la moyenne de la cible, c'est-à-dire `target_mean_`.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Comparaison du TargetEncoder avec d'autres encodeurs**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/6_3_preprocessing/plot_target_encoder.ipynb)<br/>([_Comparing Target Encoder with Other Encoders_](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_target_encoder.html))\n",
    "\n",
    "#### [**Ajustement croisé interne du TargetEncoder**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/6_3_preprocessing/plot_target_encoder_cross_val.ipynb)<br/>([_Target Encoder’s Internal Cross fitting_](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_target_encoder_cross_val.html))\n",
    "\n",
    "### Références\n",
    "\n",
    "[MIC] Micci-Barreca, Daniele. [“**A preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems**](https://dl.acm.org/doi/10.1145/507533.507538)[”](https://drive.google.com/file/d/1BZZf6TCUb5F7H-7FmVPTcWrwrywP8w8o/view?usp=drive_link) SIGKDD Explor. Newsl. 3, 1 (July 2001), 27–32.\n",
    "\n",
    "[PAR] Pargent, F., Pfisterer, F., Thomas, J. et al. [“**Regularized target encoding outperforms traditional methods in supervised machine learning with high cardinality features**](https://link.springer.com/article/10.1007/s00180-022-01207-6)[”](https://drive.google.com/file/d/1-8PJVt5nSDRcQn3KXsbcSieAiKTqhJI2/view?usp=drive_link) Comput Stat 37, 2671–2692 (2022)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='discretization'></a> 6.3.5. Discrétisation\n",
    "\n",
    "La [**discrétisation**](https://en.wikipedia.org/wiki/Discretization_of_continuous_features) (également connue sous le nom de quantification ou regroupement) permet de diviser les caractéristiques continues en valeurs discrètes. Certains ensembles de données comportant des caractéristiques continues peuvent bénéficier d'une discrétisation, car celle-ci peut transformer l'ensemble de données en attributs nominaux seulement.\n",
    "\n",
    "Les caractéristiques discrétisées encodées en one-hot peuvent rendre un modèle plus expressif tout en préservant son interprétabilité. Par exemple, le prétraitement avec un discrétiseur peut introduire une non-linéarité dans les modèles linéaires. Pour des possibilités plus avancées, en particulier des fonctions lisses, consultez la section [**Génération de caractéristiques polynomiales** (6.3.7)](https://scikit-learn.org/stable/modules/preprocessing.html#generating-polynomial-features) ci-dessous.\n",
    "\n",
    "\n",
    "### <a id='k-bins-discretization'></a> 6.3.5.1. Discrétisation en K intervalles\n",
    "\n",
    "[**`KBinsDiscretizer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html#sklearn.preprocessing.KBinsDiscretizer) discrétise les caractéristiques en `k` intervalles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[ -3., 5., 15 ],\n",
    "              [  0., 6., 14 ],\n",
    "              [  6., 3., 11 ]])\n",
    "est = preprocessing.KBinsDiscretizer(n_bins=[3, 2, 2], encode='ordinal').fit(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par défaut, la sortie est encodée en one-hot dans une matrice creuse (voir [**Encodage des caractéristiques catégorielles** (6.3.4)](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-categorical-features)) et cela peut être configuré avec le paramètre `encode`. Pour chaque caractéristique, les limites des intervalles sont calculées lors de l'ajustement (`fit`) et, avec le nombre d'intervalles, elles définiront les intervalles. Ainsi, pour l'exemple actuel, ces intervalles sont définis comme suit :\n",
    "\n",
    "- caractéristique 1 : ${[-\\infty, -1), [-1, 2), [2, \\infty)}$\n",
    "- caractéristique 2 : ${[-\\infty, 5), [5, \\infty)}$\n",
    "- caractéristique 3 : ${[-\\infty, 14), [14, \\infty)}$\n",
    "\n",
    "En fonction de ces intervalles, `X` est transformé comme suit :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [2., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.transform(X)\n",
    "# array([[ 0., 1., 1.],\n",
    "#        [ 1., 1., 1.],\n",
    "#        [ 2., 0., 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'ensemble de données résultant contient des attributs ordinaux qui peuvent être utilisés ultérieurement dans un [**`Pipeline`**](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).\n",
    "\n",
    "La discrétisation est similaire à la construction d'histogrammes pour les données continues. Cependant, les histogrammes se concentrent sur le décompte des caractéristiques qui tombent dans des intervalles particuliers, tandis que la discrétisation se concentre sur l'assignation des valeurs des caractéristiques à ces intervalles.\n",
    "\n",
    "[**`KBinsDiscretizer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html) met en œuvre différentes stratégies de regroupement, qui peuvent être sélectionnées avec le paramètre `strategy`. La stratégie `'uniform'` utilise des intervalles de largeur constante. La stratégie `'quantile'` utilise les valeurs des quantiles pour avoir des intervalles également peuplés dans chaque caractéristique. La stratégie `'kmeans'` définit des intervalles en se basant sur une procédure de regroupement k-means effectuée sur chaque caractéristique de manière indépendante.\n",
    "\n",
    "Notez que vous pouvez spécifier des intervalles personnalisés en passant une fonction définissant la stratégie de discrétisation à [**`FunctionTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html). Par exemple, vous pouvez utiliser la fonction [**`pandas.cut`**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html) de Pandas :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['infant', 'kid', 'teen', 'adult', 'senior citizen']\n",
       "Categories (5, object): ['infant' < 'kid' < 'teen' < 'adult' < 'senior citizen']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "bins = [0, 1, 13, 20, 60, np.inf]\n",
    "labels = ['infant', 'kid', 'teen', 'adult', 'senior citizen']\n",
    "transformer = preprocessing.FunctionTransformer(\n",
    "    pd.cut, kw_args={'bins': bins, 'labels': labels, 'retbins': False}\n",
    ")\n",
    "X = np.array([0.2, 2, 15, 25, 97])\n",
    "transformer.fit_transform(X)\n",
    "# ['infant', 'kid', 'teen', 'adult', 'senior citizen']\n",
    "# Categories (5, object): ['infant' < 'kid' < 'teen' < 'adult' < 'senior citizen']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**Utilisation de KBinsDiscretizer pour discrétiser des caractéristiques continues**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/6_3_preprocessing/plot_discretization.ipynb)<br/>([_Using KBinsDiscretizer to discretize continuous features_](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_discretization.html))\n",
    "\n",
    "#### [**Discrétisation des caractéristiques**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/6_3_preprocessing/plot_discretization_classification.ipynb)<br/>([_Feature discretization_](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_discretization_classification.html))\n",
    "\n",
    "#### [**Démonstration des différentes stratégies de KBinsDiscretizer**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/6_3_preprocessing/plot_discretization_strategies.ipynb)<br/>([_Demonstrating the different strategies of KBinsDiscretizer_](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_discretization_strategies.html))\n",
    "\n",
    "### <a id='feature-binarization'></a> 6.3.5.2. Binarisation des caractéristiques\n",
    "\n",
    "La **binarisation des caractéristiques** consiste à **seuiller les caractéristiques numériques pour obtenir des valeurs booléennes**. Cela peut être utile pour les estimateurs probabilistes ultérieurs qui supposent que les données d'entrée sont distribuées selon une [**distribution de Bernoulli**](https://en.wikipedia.org/wiki/Bernoulli_distribution) multivariée. Par exemple, c'est le cas pour la classe [**`BernoulliRBM`**](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.BernoulliRBM.html).\n",
    "\n",
    "Il est également courant dans la communauté du traitement du texte d'utiliser des valeurs de caractéristiques binaires (probablement pour simplifier le raisonnement probabiliste), même si les comptages normalisés (également appelés fréquences de termes) ou les caractéristiques pondérées par TF-IDF sont souvent légèrement plus performants en pratique.\n",
    "\n",
    "Comme pour le [**`Normalizer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html), la classe d'utilité [**`Binarizer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html) est destinée à être utilisée aux premières étapes d'un [**`Pipeline`**](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). La méthode `fit` ne fait rien car chaque échantillon est traité indépendamment des autres :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[ 1., -1.,  2.],\n",
    "     [ 2.,  0.,  0.],\n",
    "     [ 0.,  1., -1.]]\n",
    "\n",
    "binarizer = preprocessing.Binarizer().fit(X)  # fit does nothing\n",
    "binarizer\n",
    "# Binarizer()\n",
    "\n",
    "binarizer.transform(X)\n",
    "# array([[1., 0., 1.],\n",
    "#        [1., 0., 0.],\n",
    "#        [0., 1., 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible d'ajuster le seuil du binariseur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarizer = preprocessing.Binarizer(threshold=1.1)\n",
    "binarizer.transform(X)\n",
    "# array([[0., 0., 1.],\n",
    "#        [1., 0., 0.],\n",
    "#        [0., 0., 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme pour la classe [**`Normalizer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html), le module de prétraitement fournit une fonction accompagnante [**`binarize`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.binarize.html) qui peut être utilisée lorsque l'API du transformateur n'est pas nécessaire.\n",
    "\n",
    "Notez que le [**`Binarizer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html) est similaire à [**`KBinsDiscretizer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html) lorsque `k = 2` et que la limite de l'intervalle se situe à la valeur du seuil (`threshold`).\n",
    "\n",
    "#### Entrée creuse\n",
    "\n",
    "[**`binarize`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.binarize.html) et [**`Binarizer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html) acceptent **à la fois des tableaux denses et des matrices creuses de type array-like et sparse de scipy.sparse**.\n",
    "\n",
    "Pour les entrées creuses, les données sont **converties en une représentation Compressed Sparse Rows** (voir `scipy.sparse.csr_matrix`). Pour éviter les copies inutiles de mémoire, il est recommandé de choisir la représentation CSR en amont.\n",
    "\n",
    "## <a id='imputation-of-missing-values'></a> 6.3.6. Imputation des valeurs manquantes\n",
    "\n",
    "Les outils pour l'imputation des valeurs manquantes sont discutés dans la section [**Imputation des valeurs manquantes** (6.4)](https://scikit-learn.org/stable/modules/impute.html).\n",
    "\n",
    "## <a id='generating-polynomial-features'></a> 6.3.7. Génération de caractéristiques polynomiales\n",
    "\n",
    "Il est souvent utile d'ajouter de la complexité à un modèle en considérant des caractéristiques non linéaires des données d'entrée. Nous présentons deux possibilités basées sur les polynômes : la première utilise des polynômes purs, la deuxième utilise des splines, c'est-à-dire des polynômes par morceaux.\n",
    "\n",
    "### <a id='polynomial-features'></a> 6.3.7.1. Caractéristiques polynomiales\n",
    "\n",
    "Une méthode simple et courante à utiliser est celle des caractéristiques polynomiales, qui permet d'obtenir les termes d'ordre supérieur et les termes d'interaction des caractéristiques. Elle est implémentée dans [**`PolynomialFeatures`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X = np.arange(6).reshape(3, 2)\n",
    "X\n",
    "# array([[0, 1],\n",
    "#        [2, 3],\n",
    "#        [4, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  1.,  0.,  0.,  1.],\n",
       "       [ 1.,  2.,  3.,  4.,  6.,  9.],\n",
       "       [ 1.,  4.,  5., 16., 20., 25.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(2)\n",
    "poly.fit_transform(X)\n",
    "# array([[ 1.,  0.,  1.,  0.,  0.,  1.],\n",
    "#        [ 1.,  2.,  3.,  4.,  6.,  9.],\n",
    "#        [ 1.,  4.,  5., 16., 20., 25.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les caractéristiques de `X` ont été transformées de $(X_1, X_2)$ en $(1, X_1, X_2, X_1^2, X_1X_2, X_2^2)$.\n",
    "\n",
    "Dans certains cas, seuls les termes d'interaction entre les caractéristiques sont nécessaires, et cela peut être obtenu en définissant `interaction_only=True` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.,   0.,   1.,   2.,   0.,   0.,   2.,   0.],\n",
       "       [  1.,   3.,   4.,   5.,  12.,  15.,  20.,  60.],\n",
       "       [  1.,   6.,   7.,   8.,  42.,  48.,  56., 336.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.arange(9).reshape(3, 3)\n",
    "X\n",
    "# array([[0, 1, 2],\n",
    "#        [3, 4, 5],\n",
    "#        [6, 7, 8]])\n",
    "poly = PolynomialFeatures(degree=3, interaction_only=True)\n",
    "poly.fit_transform(X)\n",
    "# array([[  1.,   0.,   1.,   2.,   0.,   0.,   2.,   0.],\n",
    "#        [  1.,   3.,   4.,   5.,  12.,  15.,  20.,  60.],\n",
    "#        [  1.,   6.,   7.,   8.,  42.,  48.,  56., 336.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les caractéristiques de `X` ont été transformées de $(X_1, X_2, X_3)$ en $(1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)$.\n",
    "\n",
    "Notez que les caractéristiques polynomiales sont utilisées implicitement dans les [**méthodes de noyau**](https://en.wikipedia.org/wiki/Kernel_method) (par exemple, [**`SVC`**](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html), [**`KernelPCA`**](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html)) lors de l'utilisation de [**fonctions de noyau** (1.4.6)](https://scikit-learn.org/stable/modules/svm.html#svm-kernels) polynomiales.\n",
    "\n",
    "Consultez **Interpolation polynomiale et par splines pour la régression Ridge** en utilisant les caractéristiques polynomiales créées.\n",
    "\n",
    "### <a id='spline-transformer'></a> 6.3.7.2. Transformateur Spline\n",
    "\n",
    "Une autre façon d'ajouter des termes non linéaires au lieu de polynômes purs de caractéristiques est de générer des fonctions de base spline pour chaque caractéristique avec [**`SplineTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.SplineTransformer.html). Les splines sont des polynômes par morceaux, paramétrés par leur degré polynomial et les positions des nœuds. [**`SplineTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.SplineTransformer.html) implémente une base de splines B, cf. les références ci-dessous.\n",
    "\n",
    "> **Note :** Le [**`SplineTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.SplineTransformer.html) traite chaque caractéristique séparément, c'est-à-dire qu'il ne vous donnera pas de termes d'interaction.\n",
    "\n",
    "Certains des avantages des splines par rapport aux polynômes sont les suivants :\n",
    "\n",
    "- Les splines B sont très flexibles et robustes si vous maintenez un degré faible fixe, généralement 3, et adaptez de manière parcimonieuse le nombre de nœuds. Les polynômes nécessiteraient un degré plus élevé, ce qui nous amène au point suivant.\n",
    "- Les splines B n'ont pas de comportement oscillatoire aux limites comme les polynômes (plus le degré est élevé, plus c'est mauvais). Cela est connu sous le nom de [**phénomène de Runge**](https://en.wikipedia.org/wiki/Runge%27s_phenomenon).\n",
    "- Les splines B offrent de bonnes options pour l'extrapolation au-delà des limites, c'est-à-dire au-delà de la plage des valeurs ajustées. Regardez l'option `extrapolation`.\n",
    "- Les splines B génèrent une matrice de caractéristiques avec une structure en bande. Pour une seule caractéristique, chaque ligne ne contient que `degré + 1` éléments non nuls, qui se produisent de manière consécutive et sont tous positifs. Cela donne une matrice avec de bonnes propriétés numériques, par exemple un faible nombre conditionnel, contrairement à une matrice de polynômes, qui porte le nom de [**matrice de Vandermonde**](https://en.wikipedia.org/wiki/Vandermonde_matrix). Un faible nombre conditionnel est important pour des algorithmes stables de modèles linéaires.\n",
    "\n",
    "Le code suivant montre les splines en action :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import SplineTransformer\n",
    "X = np.arange(5).reshape(5, 1)\n",
    "X\n",
    "# array([[0],\n",
    "#        [1],\n",
    "#        [2],\n",
    "#        [3],\n",
    "#        [4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5  , 0.5  , 0.   , 0.   ],\n",
       "       [0.125, 0.75 , 0.125, 0.   ],\n",
       "       [0.   , 0.5  , 0.5  , 0.   ],\n",
       "       [0.   , 0.125, 0.75 , 0.125],\n",
       "       [0.   , 0.   , 0.5  , 0.5  ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spline = SplineTransformer(degree=2, n_knots=3)\n",
    "spline.fit_transform(X)\n",
    "# array([[0.5  , 0.5  , 0.   , 0.   ],\n",
    "#        [0.125, 0.75 , 0.125, 0.   ],\n",
    "#        [0.   , 0.5  , 0.5  , 0.   ],\n",
    "#        [0.   , 0.125, 0.75 , 0.125],\n",
    "#        [0.   , 0.   , 0.5  , 0.5  ]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme `X` est trié, on peut facilement voir la matrice en bande générée. Seules les trois diagonales centrales sont non nulles pour `degré=2`. Plus le degré est élevé, plus les splines se chevauchent.\n",
    "\n",
    "Fait intéressant, un [**`SplineTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.SplineTransformer.html) avec `degré=0` est identique à [**`KBinsDiscretizer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html) avec `encode='onehot-dense'` et `n_bins = n_knots - 1` si `knots = strategy`.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Interpolation polynomiale et par spline**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/6_3_preprocessing/plot_polynomial_interpolation.ipynb)<br/>([_Polynomial and Spline interpolation_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_polynomial_interpolation.html))\n",
    "\n",
    "#### [**Ingénierie des caractéristiques liées au temps**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/6_3_preprocessing/plot_cyclical_feature_engineering.ipynb)<br/>([_Time-related feature engineering_](https://scikit-learn.org/stable/auto_examples/applications/plot_cyclical_feature_engineering.html))\n",
    "\n",
    "### Références\n",
    "\n",
    "- Eilers, P., & Marx, B. (1996). [“**Flexible Smoothing with B-splines and Penalties**](https://projecteuclid.org/journals/statistical-science/volume-11/issue-2/Flexible-smoothing-with-B-splines-and-penalties/10.1214/ss/1038425655.full)[”](https://drive.google.com/file/d/1JbQUYqM7iWJ4qC7oXx23AMM9L_Y5kSVa/view?usp=drive_link). Statist. Sci. 11 (1996), no. 2, 89–121.\n",
    "- Perperoglou, A., Sauerbrei, W., Abrahamowicz, M. et al. [“**A review of spline function procedures in R**](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-019-0666-3)[”](https://drive.google.com/file/d/1XKlipPMjuy-3Ug6P9w4_LzlkaMt6usgH/view?usp=drive_link). BMC Med Res Methodol 19, 46 (2019)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='custom-transformers'></a> 6.3.8. Transformateurs personnalisés\n",
    "\n",
    "Souvent, vous voudrez convertir une fonction Python existante en un transformateur pour aider à la préparation ou au traitement des données. Vous pouvez implémenter un transformateur à partir d'une fonction arbitraire avec [**`FunctionTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html). Par exemple, pour construire un transformateur qui applique une transformation logarithmique dans un pipeline, faites :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.69314718],\n",
       "       [1.09861229, 1.38629436]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "X = np.array([[0, 1], [2, 3]])\n",
    "# Since FunctionTransformer is no-op during fit, we can call transform directly\n",
    "transformer.transform(X)\n",
    "# array([[0.        , 0.69314718],\n",
    "#        [1.09861229, 1.38629436]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez vous assurer que `func` et `inverse_func` sont inverses l'un de l'autre en définissant `check_inverse=True` et en appelant `fit` avant `transform`. Veuillez noter qu'un avertissement est émis et peut être transformé en une erreur avec un `filterwarnings` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"error\", message=\".*check_inverse*.\",\n",
    "                        category=UserWarning, append=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour un exemple de code complet qui montre comment utiliser un [**`FunctionTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html) pour extraire des caractéristiques à partir de données textuelles, consultez **Transformation de colonnes avec des sources de données hétérogènes** et **Ingénierie des caractéristiques liées au temps**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9aff9e50adfaa9e30c910fb3872ffdc72747acb5f50803ca0504f00e980f7c25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
