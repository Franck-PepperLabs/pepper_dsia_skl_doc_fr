{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <a id='ensemble-methods'></a>  1.11. [**Méthodes ensemblistes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_11_ensemble.ipynb)<br/>([*Ensemble methods*](https://scikit-learn.org/stable/modules/ensemble.html))\n",
    "\n",
    "✔ 1.11.1. [**Méta-estimateur de bagging**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_11_ensemble.ipynb#bagging-meta-estimator)\n",
    "([*Bagging meta-estimator*](https://scikit-learn.org/stable/modules/ensemble.html#bagging-meta-estimator))\n",
    "\n",
    "1.11.2. Forêts d'arbres aléatoires | Forests of randomized trees\n",
    "\n",
    "1.11.3. AdaBoost | AdaBoost\n",
    "\n",
    "1.11.4. Amélioration de l'arbre par gradient | Gradient Tree Boosting\n",
    "\n",
    "1.11.5. Amplification du gradient basée sur l'histogramme | Histogram-Based Gradient Boosting\n",
    "\n",
    "1.11.6. Classifieur de vote | Voting Classifier\n",
    "\n",
    "1.11.7. Régresseur de vote | Voting Regressor\n",
    "\n",
    "1.11.8. Généralisation empilée | Stacked generalization\n",
    "\n",
    "Le but des **méthodes ensemblistes** est de combiner les prédictions de plusieurs estimateurs de base construits avec un algorithme d'apprentissage donné afin d'améliorer la généralisabilité/robustesse sur un seul estimateur.\n",
    "\n",
    "On distingue généralement deux familles de méthodes ensemblistes :\n",
    "\n",
    "* Dans les **méthodes de moyennage**, le principe directeur est de construire plusieurs estimateurs indépendamment, puis de faire la moyenne de leurs prédictions. En moyenne, l'estimateur combiné est généralement meilleur que n'importe quel estimateur à base unique car sa variance est réduite.\n",
    "\n",
    "Exemples : [Méthodes de bagging (1.11.1.)](), [Forêts d'arbres aléatoires (1.11.2.)](), …\n",
    "\n",
    "* Au contraire, dans les **méthodes de boosting**, les estimateurs de base sont construits séquentiellement et on essaie de réduire le biais de l'estimateur combiné. La motivation est de combiner plusieurs modèles faibles pour produire un ensemble puissant.\n",
    "\n",
    "Exemples : [AdaBoost (1.11.3.)](), [Gradient Tree Boosting (1.11.4.)](), …"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='bagging-meta-estimator'></a> 1.11.1. Méta-estimateur de bagging\n",
    "\n",
    "Dans les algorithmes ensemblistes, les méthodes de bagging forment une classe d'algorithmes qui construisent plusieurs instances d'un estimateur boîte noire sur des sous-ensembles aléatoires de l'ensemble d'apprentissage d'origine, puis agrègent leurs prédictions individuelles pour former une prédiction finale. Ces méthodes sont utilisées comme moyen de réduire la variance d'un estimateur de base (par exemple, un arbre de décision), en introduisant la randomisation dans sa procédure de construction, puis en en faisant un ensemble. Dans de nombreux cas, les méthodes de bagging constituent un moyen très simple d'amélioration par rapport à un modèle unique, sans nécessiter d'adaptation de l'algorithme de base sous-jacent. Comme elles offrent un moyen de réduire le surajustement, les méthodes de bagging fonctionnent mieux avec des modèles solides et complexes (par exemple, des arbres de décision entièrement développés), contrairement aux méthodes de renforcement qui fonctionnent généralement mieux avec des modèles faibles (par exemple, des arbres de décision peu profonds).\n",
    "\n",
    "Les méthodes de bagging se présentent sous de nombreuses formes, mais diffèrent principalement les unes des autres par la manière dont elles dessinent des sous-ensembles aléatoires de l'ensemble d'apprentissage :\n",
    "\n",
    "* Lorsque des sous-ensembles aléatoires de l'ensemble de données sont tirés en tant que sous-ensembles aléatoires des échantillons, cet algorithme est connu sous le nom de **Collage** (*Pasting*) [B1999].\n",
    "\n",
    "* Lorsque les échantillons sont tirés avec remise, la méthode est connue sous le nom de **Bagging** [B1996].\n",
    "\n",
    "* Lorsque des sous-ensembles aléatoires de l'ensemble de données sont tirés en tant que sous-ensembles aléatoires des caractéristiques, la méthode est connue sous le nom de **Sous-espaces aléatoires** (*Radom Subspaces*) [H1998].\n",
    "\n",
    "* Enfin, lorsque les estimateurs de base sont construits sur des sous-ensembles d'échantillons et de caractéristiques, la méthode est connue sous le nom de **Correctifs aléatoires** (*Random Patches*) [LG2012].\n",
    "\n",
    "Dans scikit-learn, les méthodes de bagging sont proposées sous la forme d'un méta-estimateur [`BaggingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier) unifié (resp. [`BaggingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html#sklearn.ensemble.BaggingRegressor)), prenant en entrée un estimateur de base spécifié par l'utilisateur ainsi que des paramètres spécifiant la stratégie pour tirer des sous-ensembles aléatoires. En particulier, `max_samples` et `max_features` contrôlent la taille des sous-ensembles (en termes d'échantillons et de caractéristiques), tandis que `bootstrap` et `bootstrap_features` contrôlent si les échantillons et les caractéristiques sont tirés avec ou sans remise. Lors de l'utilisation d'un sous-ensemble des échantillons disponibles, la précision de la généralisation peut être estimée avec les échantillons hors sac en définissant `oob_score=True`. À titre d'exemple, l'extrait ci-dessous illustre comment instancier un ensemble d'ensachage d'estimateurs de base `KNeighborsClassifier`, chacun construit sur des sous-ensembles aléatoires de 50 % des échantillons et 50 % des caractéristiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "bagging = BaggingClassifier(KNeighborsClassifier(),\n",
    "                            max_samples=0.5, max_features=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemples\n",
    "\n",
    "### [**Estimateur unique versus bagging : décomposition biais-variance**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_11_ensembles/plot_bias_variance.ipynb)<br/>([*Single estimator versus bagging: bias-variance decomposition*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Références\n",
    "\n",
    "[B1999] L. Breiman, [“Pasting small votes for classification in large databases and on-line](https://link.springer.com/content/pdf/10.1023/A:1007563306331.pdf)[”](https://drive.google.com/file/d/1fy7yUOaskUIWibIR7r3e_N3ZISVScJyZ/view?usp=share_link), Machine Learning, 36(1), 85-103, 1999.\n",
    "\n",
    "[B1996] L. Breiman, [“Bagging predictors](https://link.springer.com/content/pdf/10.1023/A:1018054314350.pdf)[”](https://drive.google.com/file/d/11ed-XZkMo-hO2z1Ow130-NfSvIrvPEb6/view?usp=share_link), Machine Learning, 24(2), 123-140, 1996.\n",
    "\n",
    "[H1998] T. Ho, [“The random subspace method for constructing decision forests](http://machine-learning.martinsewell.com/ensembles/rsm/Ho1998.pdf)[”](https://drive.google.com/file/d/1NjBrmBfga6u5rh5b25vmdT0soPWTzfUg/view?usp=share_link), Pattern Analysis and Machine Intelligence, 20(8), 832-844, 1998.\n",
    "\n",
    "[LG2012] G. Louppe and P. Geurts, [“Ensembles on Random Patches](https://link.springer.com/content/pdf/10.1007/978-3-642-33460-3_28.pdf)[”](https://drive.google.com/file/d/1TUxYyKc3uXr6V0Q1uMH_L7q-YVAynMGs/view?usp=share_link), Machine Learning and Knowledge Discovery in Databases, 346-361, 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='forests-of-randomized-trees'></a> 1.11.2. Forêts d'arbres aléatoires\n",
    "\n",
    "Le module [`sklearn.ensemble`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble) comprend deux algorithmes de moyennage basés sur des [**arbres de décision** (1.10.)](https://scikit-learn.org/stable/modules/tree.html#tree) aléatoires : l'algorithme RandomForest et la méthode Extra-Trees. Les deux algorithmes sont des techniques de perturbation et de combinaison [B1998] spécifiquement conçues pour les arbres. Cela signifie qu'un ensemble diversifié de classifieurs est créé en introduisant un caractère aléatoire dans la construction du classifieur. La prédiction de l'ensemble est donnée comme la prédiction moyenne des classifieurs individuels.\n",
    "\n",
    "Comme les autres classifieurs, les classifieurs forestiers doivent être équipés de deux tableaux : un tableau creux ou dense X de forme `(n_samples, n_features)` contenant les échantillons d'apprentissage, et un tableau Y de forme `(n_samples,)` contenant les valeurs cibles (étiquettes de classe) pour les échantillons d'apprentissage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X = [[0, 0], [1, 1]]\n",
    "Y = [0, 1]\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf = clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme les [**arbres de décision** (1.10.)](https://scikit-learn.org/stable/modules/tree.html#tree), les forêts d'arbres s'étendent également aux [**problèmes multi-sorties** (1.10.3.)](https://scikit-learn.org/stable/modules/tree.html#tree-multioutput) (si Y est un tableau de forme `(n_samples, n_features)`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='random-forests'></a> 1.11.2.1. Forêts aléatoires\n",
    "\n",
    "Dans les forêts aléatoires (voir les classes [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) et [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor)), chaque arbre de l'ensemble est construit à partir d'un échantillon tiré avec remise (c'est-à-dire un échantillon bootstrap) à partir de l'ensemble d'apprentissage.\n",
    "\n",
    "De plus, lors de la division de chaque nœud lors de la construction d'un arbre, la meilleure division est trouvée soit à partir de toutes les caractéristiques d'entrée, soit d'un sous-ensemble aléatoire de taille `max_features`. (Voir les [**directives de réglage des paramètres** (1.11.2.3.)](https://scikit-learn.org/stable/modules/ensemble.html#random-forest-parameters) pour plus de détails).\n",
    "\n",
    "Le but de ces deux sources d'aléa est de diminuer la variance de l'estimateur *forestier*. En effet, les arbres de décision individuels présentent généralement une variance élevée et ont tendance à sur-ajuster. Le caractère aléatoire injecté dans les forêts produit des arbres de décision avec des erreurs de prédiction quelque peu découplées. En prenant une moyenne de ces prédictions, certaines erreurs peuvent s'annuler. Les forêts aléatoires permettent d'obtenir une variance réduite en combinant divers arbres, parfois au prix d'une légère augmentation du biais. En pratique, la réduction de la variance est souvent significative, ce qui donne un meilleur modèle global.\n",
    "\n",
    "Contrairement à la publication originale [B2001], l'implémentation scikit-learn combine les classifieurs en faisant la moyenne de leur prédiction probabiliste, au lieu de laisser chaque classificateur voter pour une seule classe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='extremely-randomized-trees'></a> 1.11.2.2. Arbres extrêmement aléatoires\n",
    "\n",
    "Dans les arbres extrêmement aléatoires (voir les classes [`ExtraTreesClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier) et [`ExtraTreesRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html#sklearn.ensemble.ExtraTreesRegressor)), le caractère aléatoire va encore plus loin dans la façon dont les fractionnements sont calculés. Comme dans les forêts aléatoires, un sous-ensemble aléatoire de caractéristiques candidates est utilisé, mais au lieu de rechercher les seuils les plus discriminants, les seuils sont tirés au hasard pour chaque caractéristique candidate et le meilleur de ces seuils générés aléatoirement est choisi comme règle de division. Cela permet généralement de réduire un peu plus la variance du modèle, au prix d'une augmentation un peu plus importante du biais :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X, y = make_blobs(n_samples=10000, n_features=10, centers=100,\n",
    "                  random_state=0)\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,\n",
    "                             random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()\n",
    "# 0.98...\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "                             min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()\n",
    "# 0.999...\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,\n",
    "                           min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean() > 0.999\n",
    "# True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_forest_iris_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='parameters'></a> 1.11.2.3. Paramètres\n",
    "\n",
    "Les principaux paramètres à ajuster lors de l'utilisation de ces méthodes sont `n_estimators` et `max_features`. Le premier est le nombre d'arbres dans la forêt. Plus il est grand, mieux c'est, mais aussi plus il faudra de temps pour calculer. De plus, notez que les résultats cesseront de s'améliorer significativement au-delà d'un nombre critique d'arbres. Ce dernier est la taille des sous-ensembles aléatoires de caractéristiques à prendre en compte lors de la division d'un nœud. Plus la réduction de la variance est faible, plus l'augmentation du biais est importante. Les bonnes valeurs empiriques par défaut sont `max_features=1.0` ou de manière équivalente `max_features=None` (en considérant toujours toutes les caractéristiques au lieu d'un sous-ensemble aléatoire) pour les problèmes de régression, et `max_features=\"sqrt\"` (en utilisant un sous-ensemble aléatoire de taille `sqrt(n_features)`) pour les tâches de classification ( où `n_features` est le nombre d'entités dans les données). La valeur par défaut de `max_features=1.0` équivaut à des arbres ensachés et un caractère plus aléatoire peut être obtenu en définissant des valeurs plus petites (par exemple, 0,3 est une valeur par défaut typique dans la littérature). De bons résultats sont souvent obtenus en définissant `max_depth=None` en combinaison avec `min_samples_split=2` (c'est-à-dire lors du développement complet des arbres). Gardez cependant à l'esprit que ces valeurs ne sont généralement pas optimales et peuvent entraîner des modèles qui consomment beaucoup de RAM. Les meilleures valeurs de paramètres doivent toujours faire l'objet d'une validation croisée. De plus, notez que dans les forêts aléatoires, les échantillons bootstrap sont utilisés par défaut (`bootstrap=True`) tandis que la stratégie par défaut pour les arbres supplémentaires consiste à utiliser l'ensemble de données complet (`bootstrap=False`). Lors de l'utilisation de l'échantillonnage bootstrap, l'erreur de généralisation peut être estimée sur les échantillons laissés de côté ou hors sac. Cela peut être activé en définissant `oob_score=True`.\n",
    "\n",
    "**NB** - La taille du modèle avec les paramètres par défaut est $(O( M * N * log (N) )$, où $M$ est le nombre d'arbres et $N$ est le nombre d'échantillons. Afin de réduire la taille du modèle, vous pouvez modifier ces paramètres : `min_samples_split`, `max_leaf_nodes`, `max_depth` et `min_samples_leaf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='parallelization'></a> 1.11.2.4. Parallélisation\n",
    "\n",
    "Enfin, ce module propose également la construction parallèle des arbres et le calcul parallèle des prédictions via le paramètre `n_jobs`. Si `n_jobs=k` alors les calculs sont partitionnés en `k` jobs, et exécutés sur `k` cœurs de la machine. Si `n_jobs=-1` alors tous les cœurs disponibles sur la machine sont utilisés. Notez qu'en raison de la surcharge de communication inter-processus, l'accélération peut ne pas être linéaire (c'est-à-dire que l'utilisation de `k` travaux ne sera malheureusement pas `k` fois plus rapide). Une accélération significative peut encore être obtenue lors de la construction d'un grand nombre d'arbres, ou lorsque la construction d'un seul arbre nécessite une bonne quantité de temps (par exemple, sur de grands ensembles de données).\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Tracer les surfaces de décision d'ensembles d'arbres sur le jeu de données iris**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_11_ensembles/plot_forest_iris.ipynb)<br/>([*Plot the decision surfaces of ensembles of trees on the iris dataset*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_iris.html))\n",
    "\n",
    "#### [**Importances des pixels avec une forêt parallèle d'arbres**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_11_ensembles/plot_forest_importances_faces.ipynb)<br/>([*Pixel importances with a parallel forest of trees*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances_faces.html))\n",
    "\n",
    "#### [**Complétion de visages avec des estimateurs multi-sorties**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_11_ensembles/plot_multioutput_face_completion.ipynb)<br/>([*Face completion with multi-output estimators*](https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_multioutput_face_completion.html))\n",
    "\n",
    "### Références\n",
    "\n",
    "[B2001] Breiman, [“**Random Forests**](https://link.springer.com/content/pdf/10.1023/A:1010933404324.pdf)[”](https://drive.google.com/file/d/1Kb5Z81PDb2d_HGi0OBzJjWOzIq834G0y/view?usp=share_link), Machine Learning, 45(1), 5-32, 2001.\n",
    "\n",
    "[B1998] Breiman, [“**Arcing Classifiers**](https://www.stat.berkeley.edu/~breiman/arcall.pdf)[”](https://drive.google.com/file/d/1WjnqzK1xX1L1e5s627xKZdZPyUzgYUda/view?usp=share_link), Annals of Statistics 1998.\n",
    "\n",
    "[_] P. Geurts, D. Ernst., and L. Wehenkel, [“**Extremely randomized trees**](https://link.springer.com/content/pdf/10.1007/s10994-006-6226-1.pdf)[”](https://drive.google.com/file/d/1bh6mV8lIjy7isl_51XRcYaIvHgYB_d8W/view?usp=share_link), Machine Learning, 63(1), 3-42, 2006.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='feature-importance-evaluation'></a> 1.11.2.5. Évaluation de l'importance des caractéristiques\n",
    "\n",
    "Le rang relatif (c'est-à-dire la profondeur) d'une caractéristique utilisée comme nœud de décision dans un arbre peut être utilisé pour évaluer l'importance relative de cette caractéristique par rapport à la prévisibilité de la variable cible. Les caractéristiques utilisées en haut de l'arbre contribuent à la décision de prédiction finale d'une plus grande fraction des échantillons d'entrée. La **fraction attendue des échantillons** auxquels ils contribuent peut donc être utilisée comme une estimation de l'**importance relative des caractéristiques**. Dans scikit-learn, la fraction d'échantillons à laquelle une caractéristique contribue est combinée avec la diminution de l'impureté résultant de leur division pour créer une estimation normalisée du pouvoir prédictif de cette caractéristique.\n",
    "\n",
    "En **faisant la moyenne** des estimations de la capacité prédictive sur plusieurs arbres randomisés, on peut **réduire la variance** d'une telle estimation et l'utiliser pour la sélection des caractéristiques. C'est ce qu'on appelle la diminution moyenne des impuretés, ou MDI (*Mean Decrease in Impurity*). Se reporter à [L2014] pour plus d'informations sur MDI et l'évaluation de l'importance des caractéristiques avec Random Forests.\n",
    "\n",
    "**Avertissement** - Les importances des caractéristiques basées sur les impuretés calculées sur les modèles arborescents souffrent de deux défauts qui peuvent conduire à des conclusions trompeuses. Tout d'abord, ils sont calculés sur des statistiques dérivées de l'ensemble de données d'apprentissage et ne nous informent donc pas nécessairement sur les caractéristiques les plus importantes pour faire de bonnes prédictions sur l'ensemble de données retenu. Deuxièmement, ils favorisent les caractéristiques à cardinalité élevée, c'est-à-dire les caractéristiques avec de nombreuses valeurs uniques. L'[**importance des caractéristiques de permutation** (4.2)](https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-importance) est une alternative à l'importance des caractéristiques basée sur les impuretés qui ne souffre pas de ces défauts. Ces deux méthodes d'obtention de l'importance des caractéristiques sont explorées dans : [**Permutation Importance vs Random Forest Feature Importance (MDI)**](https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py).\n",
    "\n",
    "L'exemple suivant montre une représentation codée par couleur des importances relatives de chaque pixel individuel pour une tâche de reconnaissance faciale à l'aide d'un modèle [`ExtraTreesClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier).\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_forest_importances_faces_001.png)\n",
    "\n",
    "En pratique, ces estimations sont stockées sous la forme d'un attribut nommé `feature_importances_` sur le modèle ajusté. Il s'agit d'un tableau de forme `(n_features,)` dont les valeurs sont positives et dont la somme est égale à 1,0. Plus la valeur est élevée, plus la contribution de la caractéristique d'appariement à la fonction de prédiction est importante.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Importances des pixels avec une forêt parallèle d'arbres**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_11_ensembles/plot_forest_importances_faces.ipynb)<br/>([*Pixel importances with a parallel forest of trees*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances_faces.html))\n",
    "\n",
    "#### [**Importances des caractéristiques avec une forêt d'arbres**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_11_ensembles/plot_forest_importances.ipynb)<br/>([*Feature importances with a forest of trees*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html))\n",
    "\n",
    "### Références\n",
    "\n",
    "[L2014] G. Louppe, [“Understanding Random Forests: From Theory to Practice](https://arxiv.org/pdf/1407.7502.pdf)[”](https://drive.google.com/file/d/1qIVbGYe0ONLDXMA4dAzNxWLQSSiBAHlq/view?usp=share_link), PhD Thesis, U. of Liege, 2014."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='totally-random-trees-embedding'></a> 1.11.2.6. Intégration d'arbres totalement aléatoires\n",
    "\n",
    "[`RandomTreesEmbedding`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomTreesEmbedding.html) implémente une transformation non supervisée des données. En utilisant une forêt d'arbres complètement aléatoires, [`RandomTreesEmbedding`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomTreesEmbedding.html) code les données par les indices des feuilles dans lesquelles un point de données se retrouve. Cet index est ensuite codé suivant la méthode un-parmi-K, conduisant à un codage binaire creux de grande dimension. Ce codage peut être calculé très efficacement et peut ensuite être utilisé comme base pour d'autres tâches d'apprentissage. La taille et le creux du code peuvent être influencés en choisissant le nombre d'arbres et la profondeur maximale par arbre. Pour chaque arbre de l'ensemble, le codage contient une entrée parmi une. La taille du codage est au plus `n_estimators * 2 ** max_depth`, le nombre maximum de feuilles dans la forêt.\n",
    "\n",
    "Comme les points de données voisins sont plus susceptibles de se trouver dans la même feuille d'un arbre, la transformation effectue une estimation de densité implicite et non paramétrique.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Transformation des caractéristiques par hachage à l'aide d'arbres totalement aléatoires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_11_ensembles/plot_random_forest_embedding.ipynb)<br/>([*Hashing feature transformation using Totally Random Trees*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_random_forest_embedding.html))\n",
    "\n",
    "#### [**Apprentissage par variétés sur chiffres manuscrits : Locally Linear Embedding, Isomap…**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_2_manifold/plot_lle_digits.ipynb)<br/>([*Manifold learning on handwritten digits: Locally Linear Embedding, Isomap…*](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html))\n",
    "\n",
    "Compare des techniques de réduction de dimensionnalité non linéaire sur des chiffres manuscrits.\n",
    "\n",
    "#### [**Transformations de caractéristiques avec des ensembles d'arbres**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_11_ensemble/plot_feature_transformation.ipynb)<br/>([*Feature transformations with ensembles of trees*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html))\n",
    "\n",
    "Comparent les transformations de caractéristiques supervisées et non supervisées basées sur des arbres.\n",
    "\n",
    "### Voir également\n",
    "\n",
    "Les techniques d'[**apprentissage par variétés** (2.2)](https://scikit-learn.org/stable/modules/manifold.html#manifold) peuvent également être utiles pour dériver des représentations non linéaires de l'espace des caractéristiques, ces approches se concentrent également sur la réduction de la dimensionnalité.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='adaboost'></a> 1.11.3. AdaBoost\n",
    "\n",
    "Le module [`sklearn.ensemble`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble) inclut l'algorithme d'amplification populaire AdaBoost, introduit en 1995 par Freund et Schapire [FS1995].\n",
    "\n",
    "Le principe de base d'AdaBoost est d'adapter une séquence d'apprenants faibles (c'est-à-dire des modèles qui ne sont que légèrement meilleurs que les devinettes aléatoires, comme les petits arbres de décision) sur des versions modifiées à plusieurs reprises des données. Les prédictions de chacun d'eux sont ensuite combinées par un vote majoritaire pondéré (ou somme) pour produire la prédiction finale. Les modifications de données à chaque itération dite de boosting consistent à appliquer des poids\n",
    "$w_1, w_2, \\cdots, w_N$ à chacun des échantillons d'apprentissage. Initialement, ces pondérations sont toutes définies sur $w_i=1/N$, de sorte que la première étape entraîne simplement un apprenant faible sur les données d'origine. Pour chaque itération successive, les poids des échantillons sont modifiés individuellement et l'algorithme d'apprentissage est réappliqué aux données repondérées. A une étape donnée, les exemples d'apprentissage qui ont été mal prédits par le modèle boosté induit à l'étape précédente voient leur poids augmenté, alors que les poids sont diminués pour ceux qui ont été correctement prédits. Au fur et à mesure des itérations, les exemples difficiles à prédire reçoivent une influence de plus en plus grande. Chaque apprenant faible suivant est ainsi contraint de se concentrer sur les exemples manqués par les précédents dans la séquence [HTF].\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_adaboost_hastie_10_2_001.png)\n",
    "\n",
    "AdaBoost peut être utilisé à la fois pour les problèmes de classification et de régression :\n",
    "* Pour la classification multi-classes, [`AdaBoostClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier) implémente AdaBoost-SAMME et AdaBoost-SAMME.R [ZZRH2009].\n",
    "* Pour la régression, [`AdaBoostRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor) implémente AdaBoost.R2 [D1997]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.11.3.1. Utilisation\n",
    "\n",
    "L'exemple suivant montre comment ajuster un classificateur AdaBoost avec 100 apprenants faibles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()\n",
    "# 0.9..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre d'apprenants faibles est contrôlé par le paramètre `n_estimators`. Le paramètre `learning_rate` contrôle la contribution des apprenants faibles dans la combinaison finale. Par défaut, les apprenants faibles sont des souches de décision. Différents apprenants faibles peuvent être spécifiés via le paramètre `base_estimator`. Les principaux paramètres à régler pour obtenir de bons résultats sont les `n_estimators` et la complexité des estimateurs de base (par exemple, sa profondeur `max_depth` ou le nombre minimum d'échantillons requis pour considérer un fractionnement `min_samples_split`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**AdaBoost discret vs. réel**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_11_ensembles/plot_adaboost_hastie_10_2.ipynb)<br/>([*Discrete versus Real AdaBoost*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_hastie_10_2.html))\n",
    "\n",
    "Compare l'erreur de classification d'une souche de décision, d'un arbre de décision et d'une souche de décision amplifiée à l'aide d'AdaBoost-SAMME et d'AdaBoost-SAMME.R.\n",
    "\n",
    "#### [**Arbres de décision AdaBoosted multi-classes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_11_ensembles/plot_adaboost_multiclass.ipynb)<br/>([*Multi-class AdaBoosted Decision Trees*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_multiclass.html))\n",
    "\n",
    "Montre les performances d'AdaBoost-SAMME et d'AdaBoost-SAMME.R sur un problème multi-classes.\n",
    "\n",
    "#### [**AdaBoost à deux classes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_11_ensembles/plot_adaboost_twoclass.ipynb)<br/>([*Two-class AdaBoost*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_twoclass.html))\n",
    "\n",
    "Montre les valeurs de la limite de décision et de la fonction de décision pour un problème à deux classes non linéairement séparable à l'aide d'AdaBoost-SAMME.\n",
    "\n",
    "#### [**Régression d'arbre de décision avec AdaBoost**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_11_ensembles/plot_adaboost_regression.ipynb)<br/>([*Decision Tree Regression with AdaBoost*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_regression.html))\n",
    "\n",
    "Illustre la régression avec l'algorithme AdaBoost.R2.\n",
    "\n",
    "\n",
    "### Références\n",
    "\n",
    "[FS1995] Y. Freund, and R. Schapire, [“**A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting**](https://reader.elsevier.com/reader/sd/pii/S002200009791504X?token=AD1674DDF28DD408B7DDD9B0CDADA79A121E6BB81335081764FF30C2992E13A694C3122F25D23A16EB9B688BE2D01526&originRegion=eu-west-1&originCreation=20221106045206)[”](https://drive.google.com/file/d/1hSyiKvcDxWcDzjiDvRBUEUsniwiaOdun/view?usp=share_link), 1997.\n",
    "\n",
    "[ZZRH2009] J Zhu, H. Zou, S. Rosset, T. Hastie, [“**Multi-class AdaBoost**](https://www.intlpress.com/site/pub/files/_fulltext/journals/sii/2009/0002/0003/SII-2009-0002-0003-a008.pdf)[”](https://drive.google.com/file/d/1xqvN5UIc9VebtSAwMJrUI-3LPRKSaQ4g/view?usp=share_link), Statistics and Its Interface, 2009.\n",
    "\n",
    "[D1997] Drucker. [“**Improving Regressors using Boosting Techniques**](https://www.researchgate.net/profile/Harris-Drucker/publication/2424244_Improving_Regressors_Using_Boosting_Techniques/links/0deec51ae736538cec000000/Improving-Regressors-Using-Boosting-Techniques.pdf)[”](https://drive.google.com/file/d/1LpxXmeVx5UR34hPrWnHmFBp_jiNzMar_/view?usp=share_link), 1997.\n",
    "\n",
    "[HTF] (1,2,3) T. Hastie, R. Tibshirani et J. Friedman, [“**Elements of Statistical Learning Ed. 2**](https://hastie.su.domains/Papers/ESLII.pdf)[”](https://drive.google.com/file/d/1wbK7Ii8bQliX7D3rKyGoR0-gxqJZ11iN/view?usp=share_link), Springer, 2009."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='gradient-tree-boosting'></a> 1.11.4. Arbres de décision à gradient amplifié (*Gradient Tree Boosting*)\n",
    "\n",
    "Le [[WKP] **Gradient Tree Boosting**](https://en.wikipedia.org/wiki/Gradient_boosting) ou Gradient Boosted Decision Trees (GBDT) est une généralisation du boosting à toute fonctions de perte différentiables, voir le travail séminal de [Friedman2001]. GBDT est une procédure prête à l'emploi précise et efficace qui peut être utilisée à la fois pour les problèmes de régression et de classification dans une variété de domaines, y compris le classement de la recherche Web et l'écologie.\n",
    "\n",
    "Le module [`sklearn.ensemble`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble) fournit des méthodes de classification et de régression via des arbres de décision boostés par gradient.\n",
    "\n",
    "**NB** - Scikit-learn 0.21 introduit deux nouvelles implémentations d'arbres à gradient amplifié, à savoir [`HistGradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html) et [`HistGradientBoostingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html), inspirés de [[LIB] **LightGBM**](https://github.com/Microsoft/LightGBM) (voir [LightGBM]).\n",
    "\n",
    "Ces estimateurs basés sur des histogrammes peuvent être **des ordres de grandeur plus rapides** que [`GradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) et [`GradientBoostingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) lorsque le nombre d'échantillons est supérieur à des dizaines de milliers d'échantillons.\n",
    "\n",
    "Ils ont également un support intégré pour les valeurs manquantes, ce qui évite le besoin d'un \"imputer\".\n",
    "\n",
    "Ces estimateurs sont décrits plus en détail ci-dessous dans [**Amplification du gradient basée sur l'histogramme** (1.11.5)](https://scikit-learn.org/stable/modules/ensemble.html#histogram-based-gradient-boosting).\n",
    "\n",
    "Le guide suivant se concentre sur [`GradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) et [`GradientBoostingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html), qui peuvent être préférés pour les échantillons de petite taille, car le regroupement peut conduire à des points de partage trop approximatifs dans cette configuration.\n",
    "\n",
    "L'utilisation et les paramètres de [`GradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) et [`GradientBoostingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) sont décrits ci-dessous. Les 2 paramètres les plus importants de ces estimateurs sont `n_estimators` et `learning_rate`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='classification'></a> 1.11.4.1. Classification\n",
    "\n",
    "[`GradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) prend en charge la classification binaire et multiclasse. L'exemple suivant montre comment ajuster un classifieur amplificateur de gradient avec 100 souches de décision en tant qu'apprenants faibles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "# 0.913..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre d'apprenants faibles (c'est-à-dire des arbres de régression) est contrôlé par le paramètre `n_estimators`; La taille de chaque arbre peut être contrôlée soit en définissant la profondeur de l'arbre via `max_depth` soit en définissant le nombre de nœuds feuilles via `max_leaf_nodes`. Le `learning_rate` est un hyper-paramètre dans la plage (0.0, 1.0] qui contrôle le surajustement via le rétrécissement.\n",
    "\n",
    "**NB** - La classification avec plus de 2 classes nécessite l'induction d'arbres de régression de `n_classes` à chaque itération, ainsi, le nombre total d'arbres induits est égal à `n_classes * n_estimateurs`. Pour les ensembles de données avec un grand nombre de classes, nous recommandons fortement d'utiliser [`HistGradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html) et [`HistGradientBoostingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='regression'></a> 1.11.4.2. Régression\n",
    "\n",
    "[`GradientBoostingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) prend en charge un certain nombre de [**fonctions de perte** (1.11.4.6)](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting-loss) différentes pour la régression qui peuvent être spécifiées via l'argument `loss`; la fonction de perte par défaut pour la régression est l'erreur au carré (`'squared_error'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.009154859960321"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "est = GradientBoostingRegressor(\n",
    "    n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,\n",
    "    loss='squared_error'\n",
    ").fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))\n",
    "# 5.00..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La figure ci-dessous montre les résultats de l'application de [`GradientBoostingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) avec perte des moindres carrés et 500 apprenants de base à l'ensemble de données sur le diabète ([`sklearn.datasets.load_diabetes`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes)). Le tracé montre l'erreur d'entraînement et de test à chaque itération. L'erreur d'entraînement à chaque itération est stockée dans l'attribut `train_score_` du modèle de gradient boosting. L'erreur de test à chaque itération peut être obtenue via la méthode [`staged_predict`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) qui retourne un générateur qui donne les prédictions à chaque étape. Des graphiques comme ceux-ci peuvent être utilisés pour déterminer le nombre optimal d'arbres (c'est-à-dire `n_estimators`) avec un arrêt anticipé.\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_gradient_boosting_regression_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**Régression à amplification de gradient**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_11_ensembles/plot_gradient_boosting_regression.ipynb)<br/>([*Gradient Boosting regression*](https://scikit-learn.org/stable/auto_examples/ensembles/plot_gradient_boosting_regression.html))\n",
    "\n",
    "#### [**Estimations OOB de l'amplification de gradient**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_11_ensembles/plot_gradient_boosting_oob.ipynb)<br/>([*Gradient Boosting Out-of-Bag estimates*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_oob.html))\n",
    "\n",
    "<mark>Pb de maj. du code cf. dépréciation de .loss_</mark>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='fitting-additional-weak-learners'></a> 1.11.4.3. Ajustement d'apprenants faibles supplémentaires\n",
    "\n",
    "[`GradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) et [`GradientBoostingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) prennent en charge `warm_start=True`, ce qui vous permet d'ajouter plus d'estimateurs à un modèle déjà ajusté."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.840234741105356"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = est.set_params(n_estimators=200, warm_start=True)  # set warm_start and new nr of trees\n",
    "_ = est.fit(X_train, y_train) # fit additional 100 trees to est\n",
    "mean_squared_error(y_test, est.predict(X_test))\n",
    "# 3.84..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='controlling-the-tree-size'></a> 1.11.4.4. Contrôle de la taille de l'arbre\n",
    "\n",
    "La taille des apprenants de base de l'arbre de régression définit le niveau d'interactions variables qui peut être capturé par le modèle d'amplification du gradient. En général, un arbre de profondeur `h` peut capturer des interactions d'ordre `h`. Il existe deux manières de contrôler la taille des arbres de régression individuels.\n",
    "\n",
    "Si vous spécifiez `max_depth=h` alors des arbres binaires complets de profondeur `h` seront développés. De tels arbres auront (au plus) `2**h` nœuds feuilles et `2**h - 1` nœuds séparés.\n",
    "\n",
    "Alternativement, vous pouvez contrôler la taille de l'arbre en spécifiant le nombre de nœuds feuilles via le paramètre `max_leaf_nodes`. Dans ce cas, les arbres seront développés en utilisant la meilleure recherche en premier où les nœuds avec la plus grande amélioration d'impureté seront développés en premier. Un arbre avec `max_leaf_nodes=k` a `k - 1` nœuds divisés et peut donc modéliser des interactions allant jusqu'à `max_leaf_nodes - 1`.\n",
    "\n",
    "Nous avons constaté que `max_leaf_nodes=k` donne des résultats comparables à `max_depth=k-1` mais est beaucoup plus rapide à entraîner en contrepartie d'une erreur d'entraînement légèrement plus élevée. Le paramètre `max_leaf_nodes` correspond à la variable `J` dans le chapitre sur le gradient boosting dans [Friedman2001] et est lié au paramètre `interaction.depth` dans le package gbm de R où `max_leaf_nodes == interaction.depth + 1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='mathematical-formulation'></a> 1.11.4.5. Formulation mathématique\n",
    "\n",
    "Nous présentons d'abord le GBRT pour la régression, puis nous détaillons le cas de la classification.\n",
    "\n",
    "### Régression\n",
    "\n",
    "Les régresseurs GBRT sont des modèles additifs dont la prédiction $\\hat{y}_i$ pour une entrée donnée $x_i$ est de la forme suivante :\n",
    "\n",
    "$$\\hat{y}_i = F_M(x_i) = \\sum_{m=1}^{M} h_m(x_i)$$\n",
    "\n",
    "où les $h_m$ sont des estimateurs appelés *apprenants faibles* dans le contexte du boosting. Gradient Tree Boosting utilise des régresseurs d'arbre de décision de taille fixe en tant qu'apprenants faibles. La constante $M$ correspond au paramètre `n_estimators`.\n",
    "\n",
    "Semblable à d'autres algorithmes de boosting, un GBRT est construit de manière gourmande :\n",
    "\n",
    "$$F_m(x) = F_{m-1}(x) + h_m(x),$$\n",
    "\n",
    "où l'arbre $h_m$ nouvellement ajouté est ajusté afin de minimiser une somme de pertes $L_m$, compte tenu de l'ensemble précédent $F_{m-1}$ :\n",
    "\n",
    "$$h_m =  \\arg\\min_{h} L_m = \\arg\\min_{h} \\sum_{i=1}^{n} l(y_i, F_{m-1}(x_i) + h(x_i)),$$\n",
    "\n",
    "où $l(y_i, F(x_i))$ est défini par le paramètre de perte, détaillé dans la section suivante.\n",
    "\n",
    "Par défaut, le modèle initial $F_0$ est choisi comme la constante qui minimise la perte : pour une perte des moindres carrés, c'est la moyenne empirique des valeurs cibles. Le modèle initial peut également être spécifié via l'argument `init`.\n",
    "\n",
    "En utilisant une approximation de Taylor du premier ordre, la valeur de $l$ peut être approximée comme suit :\n",
    "\n",
    "$$\n",
    "l(y_i, F_{m-1}(x_i) + h_m(x_i)) \\approx\n",
    "l(y_i, F_{m-1}(x_i))\n",
    "+ h_m(x_i)\n",
    "\\left[ \\frac{\\partial l(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F=F_{m - 1}}.\n",
    "$$\n",
    "\n",
    "**NB** - En bref, une approximation de Taylor du premier ordre dit que $l(z) \\approx l(a) + (z - a) \\frac{\\partial l(a)}{\\partial a}$. Ici, $z$ correspond à $F_{m - 1}(x_i) + h_m(x_i)$, et $a$ correspond à $F_{m - 1}(x_i)$\n",
    "\n",
    "\n",
    "La quantité $\\left[ \\frac{\\partial l(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F=F_{m - 1}}$ est la dérivée de la perte par rapport à son deuxième paramètre, évalué à $F_{m-1}(x)$. Elle est facile à calculer pour tout $F_{m - 1}(x_i)$ donné sous une forme fermée puisque la perte est différentiable. Nous le noterons par $g_i$.\n",
    "\n",
    "En supprimant les termes constants, on a :\n",
    "\n",
    "$$h_m \\approx \\arg\\min_{h} \\sum_{i=1}^{n} h(x_i) g_i$$\n",
    "\n",
    "\n",
    "Ceci est minimisé si $h(x_i)$ est ajusté pour prédire une valeur qui est proportionnelle au gradient négatif $-g_i$. Par conséquent, à chaque itération, l'estimateur $h_m$ **est ajusté pour prédire les gradients négatifs des échantillons**. Les dégradés sont mis à jour à chaque itération. Cela peut être considéré comme une sorte de descente de gradient dans un espace fonctionnel.\n",
    "\n",
    "**NB** - Pour certaines pertes, par ex. le moindre écart absolu (LAD) où les gradients sont $\\pm 1$, les valeurs prédites par un $h_m$ ajusté ne sont pas assez précises : l'arbre ne peut produire que des valeurs entières. En conséquence, les valeurs des feuilles de l'arbre $h_m$ sont modifiées une fois l'arbre ajusté, de sorte que les valeurs des feuilles minimisent la perte $L_m$. La mise à jour dépend de la perte : pour la perte LAD, la valeur d'une feuille est mise à jour à la médiane des échantillons de cette feuille.\n",
    "\n",
    "### Classification\n",
    "\n",
    "L'amplification du gradient pour la classification est très similaire au cas de régression. Cependant, la somme des arbres $F_M(x_i) = \\sum_m h_m(x_i)$ n'est pas homogène à une prédiction : il ne peut pas s'agir d'une classe, puisque les arbres prédisent des valeurs continues.\n",
    "\n",
    "Le mappage à partir de la valeur $F_M(x_i)$ à une classe ou une probabilité dépend de la perte. Pour la perte logarithmique, la probabilité que $x_i$ appartienne à la classe positive est modélisée comme $p(y_i = 1 | x_i) = \\sigma(F_M(x_i))$ où $\\sigma$ est la fonction sigmoïde ou expit.\n",
    "\n",
    "Pour la classification multiclasse, les arbres K (pour les classes K) sont construits à chacune des\n",
    "$M$ itérations. La probabilité que $x_i$ appartienne à la classe k est modélisée comme un softmax des valeurs $F_{M,k}(x_i)$.\n",
    "\n",
    "Notez que même pour une tâche de classification, le sous-estimateur $h_m$ est toujours un régresseur, pas un classifieur. En effet, les sous-estimateurs sont entraînés pour prédire des *gradients* (négatifs), qui sont toujours des quantités continues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='loss-functions'></a> 1.11.4.6. Fonctions de perte\n",
    "\n",
    "Les fonctions de perte suivantes sont prises en charge et peuvent être spécifiées à l'aide du paramètre `loss` :\n",
    "\n",
    "* Régression\n",
    "    * Erreur au carré (`'squared_error'`) : le choix naturel pour la régression en raison de ses propriétés de calcul supérieures. Le modèle initial est donné par la moyenne des valeurs cibles.\n",
    "    * Plus petit écart absolu (`'lad'`) : une fonction de perte robuste pour la régression. Le modèle initial est donné par la médiane des valeurs cibles.\n",
    "    * Huber (`'huber'`) : autre fonction de perte robuste qui combine les moindres carrés et le moindre écart absolu ; utiliser `alpha` pour contrôler la sensibilité aux valeurs aberrantes (voir [Friedman2001] pour plus de détails).\n",
    "    * Quantile (`'quantile'`) : une fonction de perte pour la régression quantile. Utilisez `0 < alpha < 1` pour spécifier le quantile. Cette fonction de perte peut être utilisée pour créer des intervalles de prédiction (voir l'exemple [**Intervalles de prédiction pour la régression de renforcement de gradient**](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html)).\n",
    "\n",
    "* Classification\n",
    "    * Log-loss binaire (`'log-loss'`) : la fonction de perte de log-vraisemblance négative binomiale pour la classification binaire. Elle fournit des estimations de probabilité. Le modèle initial est donné par le log odds-ratio.\n",
    "    * Log-loss multi-classes (`'log-loss'`) : la fonction de perte de log-vraisemblance négative multinomiale pour la classification multi-classes avec des classes mutuellement exclusives `n_classes`. Elle fournit des estimations de probabilité. Le modèle initial est donné par la probabilité a priori de chaque classe. A chaque itération, des arbres de régression `n_classes` doivent être construits, ce qui rend le GBRT plutôt inefficace pour les ensembles de données avec un grand nombre de classes.\n",
    "    * Perte exponentielle (`'exponential'`) : la même fonction de perte qu'[`AdaBoostClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html). Moins robuste aux exemples mal étiquetés que `'log-loss'` ; ne peut être utilisé que pour la classification binaire.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='shrinkage-via-learning-rate'></a> 1.11.4.7. Réduction via le taux d'apprentissage\n",
    "\n",
    "[Friedman2001] a proposé une stratégie de régularisation simple qui échelonne la contribution de chaque apprenant faible par un facteur constant $\\nu$ :\n",
    "\n",
    "$$F_m(x) = F_{m-1}(x) + \\nu h_m(x)$$\n",
    "\n",
    "Le paramètre $\\nu$ est également appelé **taux d'apprentissage** car il met à l'échelle la longueur du pas de la procédure de descente de gradient ; il peut être défini via le paramètre `learning_rate`.\n",
    "\n",
    "Le paramètre `learning_rate` interagit fortement avec le paramètre `n_estimators`, le nombre d'apprenants faibles à adapter. Des valeurs plus petites de `learning_rate` nécessitent un plus grand nombre d'apprenants faibles pour maintenir une erreur d'entraînement constante. Des preuves empiriques suggèrent que de petites valeurs de `learning_rate` favorisent une meilleure erreur de test. [HTF] recommande de définir le taux d'apprentissage sur une petite constante (par exemple, `learning_rate <= 0.1`) et de choisir `n_estimators` en s'arrêtant tôt. Pour une discussion plus détaillée de l'interaction entre `learning_rate` et `n_estimators` voir [R2007]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='subsampling'></a> 1.11.4.8. Sous-échantillonnage\n",
    "\n",
    "[Friedman2002] a proposé le boosting de gradient stochastique, qui combine le boosting de gradient avec une moyennage bootstrap (bagging). A chaque itération, le classifieur de base est entraîné sur une fraction `subsample` des données d'entraînement disponibles. Le sous-échantillon est tiré sans remise. Une valeur typique de `subsample` est de 0.5.\n",
    "\n",
    "La figure ci-dessous illustre l'effet du rétrécissement et du sous-échantillonnage sur la qualité de l'ajustement du modèle. Nous pouvons clairement voir que le rétrécissement surpasse l'absence de rétrécissement. Le sous-échantillonnage avec rétrécissement peut encore augmenter la précision du modèle. Le sous-échantillonnage sans rétrécissement, en revanche, donne de mauvais résultats.\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_gradient_boosting_regularization_001.png)\n",
    "\n",
    "Une autre stratégie pour réduire la variance consiste à sous-échantillonner les caractéristiques analogues aux fractionnements aléatoires dans [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). Le nombre d'entités sous-échantillonnées peut être contrôlé via le paramètre `max_features`.\n",
    "\n",
    "**NB** - L'utilisation d'une petite valeur `max_features` peut réduire considérablement le temps d'exécution.\n",
    "\n",
    "L'amplification du gradient stochastique permet de calculer des estimations hors sac de la déviance du test en calculant l'amélioration de la déviance sur les exemples qui ne sont pas inclus dans l'échantillon bootstrap (c'est-à-dire les exemples hors sac). Les améliorations sont stockées dans l'attribut `oob_improvement_`. `oob_improvement_[i]` contient l'amélioration en termes de perte sur les échantillons OOB si vous ajoutez la ième étape aux prédictions actuelles. Les estimations hors sac peuvent être utilisées pour la sélection du modèle, par exemple pour déterminer le nombre optimal d'itérations. Les estimations OOB sont généralement très pessimistes, nous vous recommandons donc d'utiliser la validation croisée à la place et de n'utiliser OOB que si la validation croisée prend trop de temps.\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Régularisation de l'amplification de gradient**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_11_ensembles/plot_gradient_boosting_regularization.ipynb)<br/>([*Gradient Boosting regularization*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regularization.html))\n",
    "\n",
    "<mark>Pb de maj. du code cf. dépréciation de .loss_</mark>\n",
    "\n",
    "#### [**Estimations OOB de l'amplification de gradient**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_11_ensembles/plot_gradient_boosting_oob.ipynb)<br/>([*Gradient Boosting Out-of-Bag estimates*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_oob.html))\n",
    "\n",
    "\n",
    "#### [**Erreurs OOB pour les forêts aléatoires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_11_ensembles/plot_gradient_boosting_regularization.ipynb)<br/>([*OOB Errors for Random Forests*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regularization.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='interpretation-with-feature-importance'></a> 1.11.4.9. Interprétation avec importance des caractéristiques\n",
    "\n",
    "Les arbres de décision individuels peuvent être interprétés facilement en visualisant simplement la structure arborescente. Les modèles d'amplification de gradient, cependant, comprennent des centaines d'arbres de régression et ne peuvent donc pas être facilement interprétés par une inspection visuelle des arbres individuels. Heureusement, un certain nombre de techniques ont été proposées pour résumer et interpréter les modèles de gradient boosting.\n",
    "\n",
    "Souvent, les caractéristiques ne contribuent pas de manière égale à prédire la réponse cible ; dans de nombreuses situations, la majorité des caractéristiques ne sont en fait pas pertinentes. Lors de l'interprétation d'un modèle, la première question est généralement : quelles sont ces caractéristiques importantes et comment contribuent-elles à prédire la réponse cible ?\n",
    "\n",
    "Les arbres de décision individuels effectuent intrinsèquement la sélection des caractéristiques en sélectionnant les points de partage appropriés. Ces informations peuvent être utilisées pour mesurer l'importance de chaque caractéristique ; l'idée de base est la suivante : plus une caractéristique est souvent utilisée dans les points de division d'un arbre, plus cette caractéristique est importante. Cette notion d'importance peut être étendue aux ensembles d'arbres de décision en faisant simplement la moyenne de l'importance des caractéristiques basée sur les impuretés de chaque arbre (voir [**Évaluation de l'importance des caractéristiques** (1.11.2.5)](https://scikit-learn.org/stable/modules/ensemble.html#random-forest-feature-importance) pour plus de détails).\n",
    "\n",
    "Les scores d'importance des caractéristiques d'un modèle d'amplification du gradient ajusté sont accessibles via la propriété feature_importances_ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10684213, 0.10461707, 0.11265447, 0.09863589, 0.09469133,\n",
       "       0.10729306, 0.09163753, 0.09718194, 0.09581415, 0.09063242])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                 max_depth=1, random_state=0).fit(X, y)\n",
    "clf.feature_importances_\n",
    "# array([0.10..., 0.10..., 0.11..., ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez que ce calcul de l'importance des caractéristiques est basé sur l'entropie et qu'il est distinct de [`sklearn.inspection.permutation_importance`](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html) qui est basé sur la permutation des caractéristiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**Régression à amplification de gradient**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_11_ensembles/plot_gradient_boosting_regression.ipynb)<br/>([*Gradient Boosting regression*](https://scikit-learn.org/stable/auto_examples/ensembles/plot_gradient_boosting_regression.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Références\n",
    "\n",
    "[Friedman2001] (1,2,3,4) Friedman, J.H. (2001). [“**Greedy function approximation: A gradient boosting machine**](https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.full)[”](https://drive.google.com/file/d/1voZldAUTto1QS8z459_ZAEKalwo-uaE6/view?usp=share_link). Annals of Statistics, 29, 1189-1232.\n",
    "\n",
    "[Friedman2002] Friedman, J.H. (2002). [“**Stochastic gradient boosting.**](https://jerryfriedman.su.domains/ftp/stobst.pdf)[”](https://drive.google.com/file/d/1FXSz7-PvPPKxGMmr9aU0gRELhJzV0yyX/view?usp=share_link) Computational Statistics & Data Analysis, 38, 367-378.\n",
    "\n",
    "[R2007] G. Ridgeway (2006). [“**Generalized Boosted Models: A guide to the gbm package**](https://cran.r-project.org/web/packages/gbm/vignettes/gbm.pdf)[”](https://drive.google.com/file/d/1ZqCUt3KbiZQkfyVHHIykj3KCGAeTY7j5/view?usp=share_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='histogram-based-gradient-boosting'></a> 1.11.5. Amplification du gradient basée sur l'histogramme\n",
    "\n",
    "Scikit-learn 0.21 a introduit deux nouvelles implémentations d'arbres de renforcement de gradient, à savoir [`HistGradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html) et [`HistGradientBoostingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html), inspirés de [MS **LightGBM**](https://github.com/Microsoft/LightGBM) (voir [LightGBM]).\n",
    "\n",
    "Ces estimateurs basés sur des histogrammes peuvent être des **ordres de grandeur plus rapides** que [`GradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) et [`GradientBoostingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) lorsque le nombre d'échantillons est supérieur à des dizaines de milliers d'échantillons.\n",
    "\n",
    "Ils ont également un support intégré pour les valeurs manquantes, ce qui évite le besoin d'un imputer.\n",
    "\n",
    "Ces estimateurs rapides regroupent d'abord les échantillons d'entrée `X` en groupes de valeurs entières (généralement 256 groupes), ce qui réduit considérablement le nombre de points de division à prendre en compte et permet à l'algorithme de tirer parti des structures de données basées sur des nombres entiers (histogrammes) au lieu de s'appuyer sur des valeurs continues triées lors de la construction des arbres. L'API de ces estimateurs est légèrement différente et certaines des fonctionnalités de [`GradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) et [`GradientBoostingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) ne sont pas encore prises en charge, par exemple certaines fonctions de perte.\n",
    "\n",
    "## Exemple\n",
    "\n",
    "### [**Graphiques de dépendance partielle et d'espérance conditionnelle individuelle**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/4_inspection/plot_partial_dependence.ipynb)<br/>([*Partial Dependence and Individual Conditional Expectation Plots*](https://scikit-learn.org/stable/auto_examples/inspection/plot_partial_dependence.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img alt=\"wkpd\" src=\"https://upload.wikimedia.org/wikipedia/en/thumb/8/80/Wikipedia-logo-v2.svg/1200px-Wikipedia-logo-v2.svg.png\" style=\"width: 30.0px; height: 30.0px\" /></a> Wikipedia\n",
    "\n",
    "[**Ensemble learning**](https://en.wikipedia.org/wiki/Ensemble_learning) ([*Apprentissage ensembliste*](https://fr.wikipedia.org/wiki/Apprentissage_ensembliste))\n",
    "\n",
    "[**Bootstrap aggregating (~Bagging)**](https://en.wikipedia.org/wiki/Bootstrap_aggregating) ([***B****ootstrap* ***agg****regat****ing*** *(~Ensachage)*](https://fr.wikipedia.org/wiki/Bootstrap_aggregating))\n",
    "\n",
    "[**Boosting (machine learning)**](https://en.wikipedia.org/wiki/Boosting_(machine_learning)) ([*Boosting*](https://fr.wikipedia.org/wiki/Boosting))\n",
    "\n",
    "[**Random forest**](https://en.wikipedia.org/wiki/Random_forest) ([*Forêt aléatoire*](https://fr.wikipedia.org/wiki/For%C3%AAt_d%27arbres_d%C3%A9cisionnels))\n",
    "\n",
    "[**AdaBoost**](https://en.wikipedia.org/wiki/AdaBoost) ([*AdaBoost*](https://fr.wikipedia.org/wiki/AdaBoost))\n",
    "* [**Decision stump**](https://en.wikipedia.org/wiki/Decision_stump) ([*Souche de décision*]())\n",
    "\n",
    "[**Gradient boosting**](https://en.wikipedia.org/wiki/Gradient_boosting) ([*Gradient boosting*]())\n",
    "\n",
    "[**Out-of-bag error (OOB)**](https://en.wikipedia.org/wiki/Out-of-bag_error) ([*Erreur hors sac*]())\n",
    "\n",
    "[****]() ([**]())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e03b612d84ba21ce95ed447e81b3062e1eb99b56c6d885cdab4aaa12f1b8e240"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
