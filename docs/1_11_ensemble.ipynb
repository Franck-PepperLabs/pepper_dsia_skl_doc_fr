{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='supervised-learning'></a> 1. [**Apprentissage supervis√©**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_supervised_learning.ipynb#supervised-learning)</br>([*Supervised learning*](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning))\n",
    "\n",
    "# <a id='ensembles-gradient-boosting-random-forests-bagging-voting-stacking'></a> 1.11. [**M√©thodes ensemblistes : Gradient boosting, random forests, bagging, voting, stacking**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb)<br/>([_Ensemble methods: Gradient boosting, random forests, bagging, voting, stacking_](https://scikit-learn.org/stable/modules/ensemble.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 31 pages, 24 exemples, 19 papiers\n",
    "- 1.11.1. [**Arbres de d√©cision √† gradient amplifi√©**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#gradient-boosted-trees)<br/>([_Gradient-boosted trees_](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosted-trees))\n",
    "- 1.11.1.1. [**Amplification de gradient bas√©e sur un histogramme**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#histogram-based-gradient-boosting)<br/>([_Histogram-Based Gradient Boosting_](https://scikit-learn.org/stable/modules/ensemble.html#histogram-based-gradient-boosting))\n",
    "    - 1.11.1.1.1. [**Utilisation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#usage)<br/>([_Usage_](https://scikit-learn.org/stable/modules/ensemble.html#usage))\n",
    "    - 1.11.1.1.2. [**Prise en charge des valeurs manquantes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#missing-values-support)<br/>([_Missing values support_](https://scikit-learn.org/stable/modules/ensemble.html#missing-values-support))\n",
    "    - 1.11.1.1.3. [**Prise en charge du poids des √©chantillons**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#sample-weight-support)<br/>([_Missing values support_](https://scikit-learn.org/stable/modules/ensemble.html#sample-weight-support))\n",
    "    - 1.11.1.1.4. [**Prise en charge des caract√©ristiques cat√©gorielles**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#categorical-features-support)<br/>([_Categorical Features Support_](https://scikit-learn.org/stable/modules/ensemble.html#categorical-features-support))\n",
    "    - 1.11.1.1.5. [**Contraintes monotones**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#monotonic-constraints)<br/>([_Monotonic Constraints_](https://scikit-learn.org/stable/modules/ensemble.html#monotonic-constraints))\n",
    "    - 1.11.1.1.6. [**Contraintes d'interaction**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#interaction-constraints)<br/>([_Interaction constraints_](https://scikit-learn.org/stable/modules/ensemble.html#interaction-constraints))\n",
    "    - 1.11.1.1.7. [**Parall√©lisme de bas niveau**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#low-level-parallelism)<br/>([_Low-level parallelism_](https://scikit-learn.org/stable/modules/ensemble.html#low-level-parallelism))\n",
    "    - 1.11.1.1.8. [**Pourquoi c'est plus rapide**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#why-it-s-faster)<br/>([_Why it‚Äôs faster_](https://scikit-learn.org/stable/modules/ensemble.html#why-it-s-faster))\n",
    "- 1.11.1.2. [**`GradientBoostingClassifier` et `GradientBoostingRegressor`**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#gradientboostingclassifier-and-gradientboostingregressor)<br/>([_**`GradientBoostingClassifier`** and **`GradientBoostingRegressor`**_](https://scikit-learn.org/stable/modules/ensemble.html#gradientboostingclassifier-and-gradientboostingregressor))\n",
    "    - 1.11.1.2.1. [**Classification**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#classification)<br/>([_Classification_](https://scikit-learn.org/stable/modules/ensemble.html#classification))\n",
    "    - 1.11.1.2.2. [**R√©gression**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#regression)<br/>([_Regression_](https://scikit-learn.org/stable/modules/ensemble.html#regression))\n",
    "    - 1.11.1.2.3. [**Ajustement d'apprenants faibles suppl√©mentaires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#fitting-additional-weak-learners)<br/>([_Fitting additional weak-learners_](https://scikit-learn.org/stable/modules/ensemble.html#fitting-additional-weak-learners))\n",
    "    - 1.11.1.2.4. [**Contr√¥le de la taille de l'arbre**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#controlling-the-tree-size)<br/>([_Controlling the tree size_](https://scikit-learn.org/stable/modules/ensemble.html#controlling-the-tree-size))\n",
    "    - 1.11.1.2.5. [**Formulation math√©matique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#mathematical-formulation)<br/>([_Mathematical formulation_](https://scikit-learn.org/stable/modules/ensemble.html#mathematical-formulation))\n",
    "    - 1.11.1.2.6. [**Fonctions de perte**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#loss-functions)<br/>([_Loss Functions_](https://scikit-learn.org/stable/modules/ensemble.html#loss-functions))\n",
    "    - 1.11.1.2.7. [**R√©duction via le taux d'apprentissage**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#shrinkage-via-learning-rate)<br/>([_Shrinkage via learning rate_](https://scikit-learn.org/stable/modules/ensemble.html#shrinkage-via-learning-rate))\n",
    "    - 1.11.1.2.8. [**Sous-√©chantillonnage**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#subsampling)<br/>([_Subsampling_](https://scikit-learn.org/stable/modules/ensemble.html#subsampling))\n",
    "    - 1.11.1.2.9. [**Interpr√©tation avec importance des caract√©ristiques**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#interpretation-with-feature-importance)<br/>([_Interpretation with feature importance_](https://scikit-learn.org/stable/modules/ensemble.html#interpretation-with-feature-importance))\n",
    "- 1.11.2. [**For√™ts al√©atoires et autres ensembles d'arbres al√©atoires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#random-forests-and-other-randomized-tree-ensembles)<br/>([_Random forests and other randomized tree ensembles_](https://scikit-learn.org/stable/modules/ensemble.html#random-forests-and-other-randomized-tree-ensembles))\n",
    "    - 1.11.2.1. [**For√™ts al√©atoires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#random-forests)<br/>([_Random Forests_](https://scikit-learn.org/stable/modules/ensemble.html#random-forests))\n",
    "    - 1.11.2.2. [**Arbres extr√™mement al√©atoires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#extremely-randomized-trees)<br/>([_Extremely Randomized Trees_](https://scikit-learn.org/stable/modules/ensemble.html#extremely-randomized-trees))\n",
    "    - 1.11.2.3. [**Param√®tres**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#parameters)<br/>([_Parameters_](https://scikit-learn.org/stable/modules/ensemble.html#parameters))\n",
    "    - 1.11.2.4. [**Parall√©lisation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#parallelization)<br/>([_Parallelization_](https://scikit-learn.org/stable/modules/ensemble.html#parallelization))\n",
    "    - 1.11.2.5. [**√âvaluation de l'importance des caract√©ristiques**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#feature-importance-evaluation)<br/>([_Feature importance evaluation_](https://scikit-learn.org/stable/modules/ensemble.html#feature-importance-evaluation))\n",
    "    - 1.11.2.6. [**Int√©gration d'arbres totalement al√©atoires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#totally-random-trees-embedding)<br/>([_Totally Random Trees Embedding_](https://scikit-learn.org/stable/modules/ensemble.html#totally-random-trees-embedding))\n",
    "- 1.11.3. [**Meta-estimation par bagging (ensachage)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#bagging-meta-estimator)<br/>([_Bagging meta-estimator_](https://scikit-learn.org/stable/modules/ensemble.html#bagging-meta-estimator))\n",
    "- 1.11.4. [**Classifieur de vote**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#voting-classifier)<br/>([_Voting Classifier_](https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier))\n",
    "    - 1.11.4.1. [**√âtiquettes de classe majoritaires (Vote majoritaire/Dur)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#majority-class-labels-majority-hard-voting)<br/>([_Majority Class Labels (Majority/Hard Voting)_](https://scikit-learn.org/stable/modules/ensemble.html#majority-class-labels-majority-hard-voting))\n",
    "    - 1.11.4.2. [**Utilisation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#id24)<br/>([_Usage_](https://scikit-learn.org/stable/modules/ensemble.html#id24))\n",
    "    - 1.11.4.3. [**Probabilit√©s moyennes pond√©r√©es (Vote doux)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#weighted-average-probabilities-soft-voting)<br/>([_Weighted Average Probabilities (Soft Voting)_](https://scikit-learn.org/stable/modules/ensemble.html#weighted-average-probabilities-soft-voting))\n",
    "    - 1.11.4.4. [**Utilisation du `VotingClassifier` avec `GridSearchCV`**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#using-the-votingclassifier-with-gridsearchcv)<br/>([_Using the `VotingClassifier` with `GridSearchCV`_](https://scikit-learn.org/stable/modules/ensemble.html#using-the-votingclassifier-with-gridsearchcv))\n",
    "    - 1.11.4.5. [**Utilisation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#id25)<br/>([_Usage_](https://scikit-learn.org/stable/modules/ensemble.html#id25))\n",
    "- 1.11.5. [**R√©gresseur de vote**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#voting-regressor)<br/>([_Voting Regressor_](https://scikit-learn.org/stable/modules/ensemble.html#voting-regressor))\n",
    "    - 1.11.5.5. [**Utilisation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#id27)<br/>([_Usage_](https://scikit-learn.org/stable/modules/ensemble.html#id27))\n",
    "- 1.11.6. [**Empilement g√©n√©ralis√©**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#stacked-generalization)<br/>([_Stacked generalization_](https://scikit-learn.org/stable/modules/ensemble.html#stacked-generalization))\n",
    "- 1.11.7. [**AdaBoost**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#adaboost)<br/>([_AdaBoost_](https://scikit-learn.org/stable/modules/ensemble.html#adaboost))\n",
    "    - 1.11.7.1. [**Utilisation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#id35)<br/>([_Usage_](https://scikit-learn.org/stable/modules/ensemble.html#id35))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img alt=\"wkpd\" src=\"https://upload.wikimedia.org/wikipedia/en/thumb/8/80/Wikipedia-logo-v2.svg/1200px-Wikipedia-logo-v2.svg.png\" style=\"width: 30.0px; height: 30.0px\" /></a> Wikipedia\n",
    "\n",
    "[**Ensemble learning**](https://en.wikipedia.org/wiki/Ensemble_learning) ([*Apprentissage ensembliste*](https://fr.wikipedia.org/wiki/Apprentissage_ensembliste))\n",
    "\n",
    "[**Bootstrap aggregating (~Bagging)**](https://en.wikipedia.org/wiki/Bootstrap_aggregating) ([***B****ootstrap* ***agg****regat****ing*** *(~Ensachage)*](https://fr.wikipedia.org/wiki/Bootstrap_aggregating))\n",
    "\n",
    "[**Boosting (machine learning)**](https://en.wikipedia.org/wiki/Boosting_(machine_learning)) ([*Boosting*](https://fr.wikipedia.org/wiki/Boosting))\n",
    "\n",
    "[**Random forest**](https://en.wikipedia.org/wiki/Random_forest) ([*For√™t al√©atoire*](https://fr.wikipedia.org/wiki/For%C3%AAt_d%27arbres_d%C3%A9cisionnels))\n",
    "\n",
    "[**AdaBoost**](https://en.wikipedia.org/wiki/AdaBoost) ([*AdaBoost*](https://fr.wikipedia.org/wiki/AdaBoost))\n",
    "* [**Decision stump**](https://en.wikipedia.org/wiki/Decision_stump) ([*Souche de d√©cision*]())\n",
    "\n",
    "[**Gradient boosting**](https://en.wikipedia.org/wiki/Gradient_boosting) ([*Gradient boosting*]())\n",
    "\n",
    "[**Out-of-bag error (OOB)**](https://en.wikipedia.org/wiki/Out-of-bag_error) ([*Erreur hors sac*]())\n",
    "\n",
    "[****]() ([**]())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='ensembles-gradient-boosting-random-forests-bagging-voting-stacking'></a> 1.11. **M√©thodes ensemblistes : Gradient boosting, random forests, bagging, voting, stacking**<br/>([_Ensemble methods: Gradient boosting, random forests, bagging, voting, stacking_](https://scikit-learn.org/stable/modules/ensemble.html))\n",
    "\n",
    "Les **m√©thodes ensemblistes** combinent les pr√©dictions de plusieurs estimateurs de base construits avec un algorithme d'apprentissage donn√© afin d'am√©liorer la g√©n√©ralisation et la robustesse par rapport √† un seul estimateur.\n",
    "\n",
    "Deux exemples tr√®s c√©l√®bres de m√©thodes ensemblistes sont les [**arbres √† gradient amplifi√©** (1.11.1)](#gradient-boosted-trees) et les [**for√™ts al√©atoires** (1.11.2)](#random-forests-and-other-randomized-tree-ensembles).\n",
    "\n",
    "De mani√®re plus g√©n√©rale, les mod√®les ensemblistes peuvent √™tre appliqu√©s √† tout apprenant de base au-del√† des arbres, dans des m√©thodes de moyenne telles que les [**m√©thodes de Bagging** (1.11.3)](#bagging-meta-estimator), le [**stacking de mod√®les** (1.11.6)](#stacked-generalization) ou le [**Vote** (1.11.4)](#voting-classifier), ou dans des m√©thodes de renforcement, comme [**AdaBoost** (1.11.7)](#adaboost)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='gradient-boosted-trees'></a> 1.11.1. **Arbres de d√©cision √† gradient amplifi√©**<br/>([_Gradient-boosted trees_](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosted-trees))\n",
    "\n",
    "[**üåê Gradient Tree Boosting**](https://en.wikipedia.org/wiki/Gradient_boosting), ou arbres de d√©cision √† gradient amplifi√© (GBDT), est une g√©n√©ralisation de l'amplification qui s'applique √† des fonctions de perte arbitrairement diff√©rentiables. Pour en savoir plus, consultez le travail fondateur de [Friedman2001]. GBDT est un mod√®le exceptionnel pour la r√©gression et la classification, notamment pour les donn√©es tabulaires.\n",
    "\n",
    "### [**`GradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) vs [**`HistGradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier)\n",
    "\n",
    "Scikit-learn propose deux impl√©mentations d'arbres √† gradient amplifi√© : [**`HistGradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier) vs [**`GradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) pour la classification, ainsi que les classes correspondantes pour la r√©gression. La premi√®re peut √™tre **des ordres de grandeur plus rapide** que la seconde lorsque le nombre d'√©chantillons est sup√©rieur √† plusieurs dizaines de milliers.\n",
    "\n",
    "Les valeurs manquantes et les donn√©es cat√©gorielles sont prises en charge nativement par la version `Hist...`, √©liminant ainsi le besoin de pr√©traitements suppl√©mentaires tels que l'imputation.\n",
    "\n",
    "[**`GradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) et [**`GradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor) pourraient √™tre pr√©f√©r√©s pour de petits jeux de donn√©es, car la discr√©tisation peut entra√Æner des points de division qui sont trop approximatifs dans ce contexte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='histogram-based-gradient-boosting'></a> 1.11.1.1. **Amplification de gradient bas√©e sur un histogramme**<br/>([_Histogram-Based Gradient Boosting_](https://scikit-learn.org/stable/modules/ensemble.html#histogram-based-gradient-boosting))\n",
    "\n",
    "Scikit-learn 0.21 a introduit deux nouvelles impl√©mentations d'arbres √† gradient amplifi√©, √† savoir [**`HistGradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier) et HistGradientBoostingRegressor, inspir√©es de [**LightGBM**](https://github.com/Microsoft/LightGBM) (voir [LightGBM]).\n",
    "\n",
    "Ces estimateurs bas√©s sur des histogrammes peuvent √™tre **plus rapides de plusieurs ordres de grandeur** que [**`GradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) et [**`GradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor) lorsque le nombre d'√©chantillons est sup√©rieur √† plusieurs dizaines de milliers.\n",
    "\n",
    "Ils prennent √©galement en charge nativement les valeurs manquantes, ce qui √©vite la n√©cessit√© d'un outil de remplacement.\n",
    "\n",
    "Ces estimateurs rapides regroupent d'abord les √©chantillons d'entr√©e `X` en bacs √† valeurs enti√®res (typiquement 256 bacs), ce qui r√©duit consid√©rablement le nombre de points de division √† consid√©rer, et permet √† l'algorithme d'utiliser des structures de donn√©es bas√©es sur des entiers (histogrammes) au lieu de d√©pendre de valeurs continues tri√©es lors de la construction des arbres. L'API de ces estimateurs est l√©g√®rement diff√©rente, et certaines des fonctionnalit√©s de [**`GradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) et [**`GradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor) ne sont pas encore prises en charge, par exemple certaines fonctions de perte.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Graphiques de d√©pendance partielle et d'esp√©rance conditionnelle individuelle**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/4_inspection/plot_partial_dependence.ipynb)<br/>([*Partial Dependence and Individual Conditional Expectation Plots*](https://scikit-learn.org/stable/auto_examples/inspection/plot_partial_dependence.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='usage'></a> 1.11.1.1.1. **Utilisation**<br/>([_Usage_](https://scikit-learn.org/stable/modules/ensemble.html#usage))\n",
    "\n",
    "La plupart des param√®tres sont inchang√©s par rapport √† [**`GradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) et [**`GradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor). Une exception est le param√®tre `max_iter` qui remplace `n_estimators` et contr√¥le le nombre d'it√©rations du processus d'amplification :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8965"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "# 0.8965"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fonctions de perte disponibles pour la r√©gression sont `'squared_error'`, `'absolute_error'`, qui est moins sensible aux valeurs aberrantes, et `'poisson'`, qui convient bien pour mod√©liser les comptages et les fr√©quences. Pour la classification, la seule option est `'log_loss'`. Pour la classification binaire, elle utilise la perte logarithmique binaire, √©galement connue sous le nom de d√©viance binomiale ou entropie crois√©e binaire. Pour `n_classes >= 3`, elle utilise la fonction de perte logarithmique multi-classe, avec d√©viance multinomiale et entropie crois√©e cat√©gorielle comme noms alternatifs. La version appropri√©e de la fonction de perte est s√©lectionn√©e en fonction de [**`y`**](https://scikit-learn.org/stable/glossary.html#term-y) transmis √† [**`fit`**](https://scikit-learn.org/stable/glossary.html#term-fit).\n",
    "\n",
    "La taille des arbres peut √™tre contr√¥l√©e √† l'aide des param√®tres `max_leaf_nodes`, `max_depth` et `min_samples_leaf`.\n",
    "\n",
    "Le nombre de bacs utilis√©s pour regrouper les donn√©es est contr√¥l√© par le param√®tre `max_bins`. Utiliser moins de bacs agit comme une forme de r√©gularisation. Il est g√©n√©ralement recommand√© d'utiliser autant de bacs que possible (256), ce qui est la valeur par d√©faut.\n",
    "\n",
    "Le param√®tre de `l2_regularization` est un r√©gularisateur sur la fonction de perte et correspond √† $\\lambda$ dans l'√©quation (2) de [XGBoost].\n",
    "\n",
    "Notez que **l'arr√™t pr√©coce est activ√© par d√©faut si le nombre d'√©chantillons est sup√©rieur √† 10 000**. Le comportement de l'arr√™t pr√©coce est contr√¥l√© via les param√®tres `early_stopping`, `scoring`, `validation_fraction`, `n_iter_no_change`, et `tol`. Il est possible de mettre fin pr√©matur√©ment en utilisant un [**scorer**](https://scikit-learn.org/stable/glossary.html#term-scorer) arbitraire, ou simplement la perte d'entra√Ænement ou de validation. Notez que pour des raisons techniques, l'utilisation d'un scorer est significativement plus lente que l'utilisation de la perte. Par d√©faut, l'arr√™t pr√©coce est effectu√© s'il y a au moins 10 000 √©chantillons dans l'ensemble d'entra√Ænement et utilise la perte de validation.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Arr√™t anticip√© du Gradient Boosting**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_gradient_boosting_early_stopping.ipynb)<br/>([_Early stopping of Gradient Boosting_](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_early_stopping.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='missing-values-support'></a> 1.11.1.1.2. **Prise en charge des valeurs manquantes**<br/>([_Missing values support_](https://scikit-learn.org/stable/modules/ensemble.html#missing-values-support))\n",
    "\n",
    "[**`HistGradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier) et [**`HistGradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor) prennent en charge les valeurs manquantes (NaN).\n",
    "\n",
    "Pendant l'entra√Ænement, l'algorithme d'arbre d√©cide √† chaque point de division si les √©chantillons avec des valeurs manquantes doivent aller vers l'enfant gauche ou droit, en se basant sur le gain potentiel. Lors de la pr√©diction, les √©chantillons avec des valeurs manquantes sont assign√©s en cons√©quence √† l'enfant gauche ou droit :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([0, 1, 2, np.nan]).reshape(-1, 1)\n",
    "y = [0, 0, 1, 1]\n",
    "\n",
    "gbdt = HistGradientBoostingClassifier(min_samples_leaf=1).fit(X, y)\n",
    "gbdt.predict(X)\n",
    "# array([0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque le mod√®le pr√©voit un sch√©ma de donn√©es manquantes, les divisions peuvent √™tre effectu√©es en fonction de l'absence ou de la pr√©sence des valeurs manquantes dans la caract√©ristique :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([0, np.nan, 1, 2, np.nan]).reshape(-1, 1)\n",
    "y = [0, 1, 0, 0, 1]\n",
    "gbdt = HistGradientBoostingClassifier(\n",
    "    min_samples_leaf=1,\n",
    "    max_depth=2,\n",
    "    learning_rate=1,\n",
    "    max_iter=1\n",
    ").fit(X, y)\n",
    "gbdt.predict(X)\n",
    "# array([0, 1, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si aucune valeur manquante n'a √©t√© rencontr√©e pour une caract√©ristique donn√©e pendant l'entra√Ænement, alors les √©chantillons avec des valeurs manquantes sont attribu√©s √† l'enfant qui a le plus d'√©chantillons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='sample-weight-support'></a> 1.11.1.1.3. **Prise en charge du poids des √©chantillons**<br/>([_Sample weight support_](https://scikit-learn.org/stable/modules/ensemble.html#sample-weight-support))\n",
    "\n",
    "[**`HistGradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier) et [**`HistGradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor) prennent en charge les poids d'√©chantillons lors de l'entra√Ænement.\n",
    "\n",
    "L'exemple suivant montre que les √©chantillons avec un poids d'√©chantillon de z√©ro sont ignor√©s :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9990209190235209"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[1, 0], [1, 0], [1, 0], [0, 1]]\n",
    "y = [0, 0, 1, 0]\n",
    "# ignore the first 2 training samples by setting their weight to 0\n",
    "sample_weight = [0, 0, 1, 1]\n",
    "gb = HistGradientBoostingClassifier(min_samples_leaf=1)\n",
    "gb.fit(X, y, sample_weight=sample_weight)\n",
    "# HistGradientBoostingClassifier(...)\n",
    "gb.predict([[1, 0]])\n",
    "# array([1])\n",
    "gb.predict_proba([[1, 0]])[0, 1]\n",
    "# 0.99..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='categorical-features-support'></a> 1.11.1.1.4. **Prise en charge des caract√©ristiques cat√©gorielles**<br/>([_Categorical Features Support_](https://scikit-learn.org/stable/modules/ensemble.html#categorical-features-support))\n",
    "\n",
    "[**`HistGradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier) et [**`HistGradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor) prennent en charge nativement les caract√©ristiques cat√©gorielles : ils peuvent consid√©rer des divisions sur des donn√©es cat√©gorielles non ordonn√©es.\n",
    "\n",
    "Pour les ensembles de donn√©es avec des caract√©ristiques cat√©gorielles, il est souvent pr√©f√©rable d'utiliser la prise en charge cat√©gorielle native plut√¥t que de s'appuyer sur un encodage one-hot ([**`OneHotEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder)), car l'encodage one-hot n√©cessite une profondeur d'arbre plus grande pour atteindre des divisions √©quivalentes. Il est √©galement g√©n√©ralement pr√©f√©rable de s'appuyer sur la prise en charge cat√©gorielle native plut√¥t que de traiter les caract√©ristiques cat√©gorielles comme continues (ordinales), ce qui se produit pour les donn√©es cat√©gorielles encod√©es de mani√®re ordonn√©e, car les cat√©gories sont des quantit√©s nominales o√π l'ordre n'a pas d'importance.\n",
    "\n",
    "Pour activer la prise en charge cat√©gorielle, un masque bool√©en peut √™tre pass√© au param√®tre `categorical_features`, indiquant quelles caract√©ristiques sont cat√©gorielles. Dans l'exemple suivant, la premi√®re caract√©ristique est trait√©e comme cat√©gorielle et la deuxi√®me comme num√©rique :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt = HistGradientBoostingClassifier(categorical_features=[True, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De mani√®re √©quivalente, il est possible de passer une liste d'entiers indiquant les indices des caract√©ristiques cat√©gorielles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt = HistGradientBoostingClassifier(categorical_features=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cardinalit√© de chaque caract√©ristique cat√©gorielle doit √™tre inf√©rieure au param√®tre `max_bins`, et chaque caract√©ristique cat√©gorielle doit √™tre encod√©e dans `[0, max_bins - 1]`. √Ä cette fin, il peut √™tre utile de pr√©traiter les donn√©es avec un [**`OrdinalEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder) comme indiqu√© dans [**Support des caract√©ristiques cat√©gorielles dans l'amplification de gradient**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_gradient_boosting_categorical.ipynb).\n",
    "\n",
    "S'il y a des valeurs manquantes pendant l'apprentissage, les valeurs manquantes seront trait√©es comme une cat√©gorie distincte. S'il n'y a pas de valeurs manquantes pendant l'apprentissage, alors au moment de la pr√©diction, les valeurs manquantes sont mapp√©es vers le n≈ìud enfant qui poss√®de le plus d'√©chantillons (tout comme pour les caract√©ristiques continues). Lors de la pr√©diction, les cat√©gories qui n'ont pas √©t√© observ√©es au moment de l'apprentissage seront trait√©es comme des valeurs manquantes.\n",
    "\n",
    "**Recherche de divisions avec des caract√©ristiques cat√©gorielles :** La mani√®re canonique de consid√©rer des divisions cat√©gorielles dans un arbre consiste √† prendre en compte toutes les $2^{K - 1} - 1$ partitions, o√π $K$ est le nombre de cat√©gories. Cela peut rapidement devenir prohibitif lorsque $K$ est grand. Heureusement, √©tant donn√© que les arbres de gradient boosting sont toujours des arbres de r√©gression (m√™me pour les probl√®mes de classification), il existe une strat√©gie plus rapide qui peut fournir des divisions √©quivalentes. Tout d'abord, les cat√©gories d'une caract√©ristique sont tri√©es en fonction de la variance de la cible, pour chaque cat√©gorie `k`. Une fois que les cat√©gories sont tri√©es, on peut consid√©rer des partitions continues, c'est-√†-dire traiter les cat√©gories comme si elles √©taient des valeurs continues ordonn√©es (voir Fisher [Fisher1958] pour une preuve formelle). En cons√©quence, seules $K - 1$ divisions doivent √™tre prises en compte au lieu de $2^{K - 1} - 1$. Le tri initial est une op√©ration en $\\mathcal{O}(K \\log(K))$, ce qui donne une complexit√© totale en $\\mathcal{O}(K \\log(K) + K)$, au lieu de $\\mathcal{O}(2^K)$.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Support des caract√©ristiques cat√©gorielles dans l'amplification de gradient**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_gradient_boosting_categorical.ipynb)<br/>([_Categorical Feature Support in Gradient Boosting_](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_categorical.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='monotonic-constraints'></a> 1.11.1.1.5. **Contraintes monotones**<br/>([_Monotonic Constraints_](https://scikit-learn.org/stable/modules/ensemble.html#monotonic-constraints))\n",
    "\n",
    "En fonction du probl√®me √† r√©soudre, il se peut que vous ayez des connaissances pr√©alables indiquant qu'une certaine caract√©ristique devrait en g√©n√©ral avoir un effet positif (ou n√©gatif) sur la valeur cible. Par exemple, toutes choses √©gales par ailleurs, un score de cr√©dit plus √©lev√© devrait augmenter la probabilit√© d'obtenir l'approbation d'un pr√™t. Les contraintes monotones vous permettent d'incorporer de telles connaissances pr√©alables dans le mod√®le.\n",
    "\n",
    "Pour un pr√©dicteur $F$ avec deux caract√©ristiques :**\n",
    "\n",
    "- **Une contrainte d'augmentation monotone est une contrainte de la forme :**\n",
    "\n",
    "$$x_1 \\leq x_1' \\implies F(x_1, x_2) \\leq F(x_1', x_2)$$\n",
    "\n",
    "- **Une contrainte de diminution monotone est une contrainte de la forme :**\n",
    "\n",
    "$$x_1 \\leq x_1' \\implies F(x_1, x_2) \\geq F(x_1', x_2)$$\n",
    "\n",
    "Vous pouvez sp√©cifier une contrainte monotone sur chaque caract√©ristique en utilisant le param√®tre `monotonic_cst`. Pour chaque caract√©ristique, une valeur de 0 indique qu'il n'y a aucune contrainte, tandis que 1 et -1 indiquent une contrainte d'augmentation monotone et de diminution monotone, respectivement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "# monotonic increase, monotonic decrease, and no constraint on the 3 features\n",
    "gbdt = HistGradientBoostingRegressor(monotonic_cst=[1, -1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un contexte de classification binaire, l'imposition d'une contrainte d'augmentation monotone (de diminution) signifie que des valeurs plus √©lev√©es de la caract√©ristique sont cens√©es avoir un effet positif (n√©gatif) sur la probabilit√© que les √©chantillons appartiennent √† la classe positive.\n",
    "\n",
    "Cependant, les contraintes monotones ne limitent que marginalement les effets des caract√©ristiques sur la sortie. Par exemple, les contraintes d'augmentation et de diminution monotones ne peuvent pas √™tre utilis√©es pour imposer la contrainte de mod√©lisation suivante :\n",
    "\n",
    "$$x_1 \\leq x_1' \\implies F(x_1, x_2) \\leq F(x_1', x_2')$$\n",
    "\n",
    "De plus, les contraintes monotones ne sont pas prises en charge pour la classification multiclasse.\n",
    "\n",
    "> **Remarque :** Comme les cat√©gories sont des quantit√©s non ordonn√©es, il n'est pas possible d'imposer des contraintes monotones sur les caract√©ristiques cat√©gorielles.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Contraintes monotones**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_monotonic_constraints.ipynb)<br/>([_Monotonic Constraints_](https://scikit-learn.org/stable/auto_examples/ensemble/plot_monotonic_constraints.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='interaction-constraints'></a> 1.11.1.1.6. **Contraintes d'interaction**<br/>([_Interaction constraints_](https://scikit-learn.org/stable/modules/ensemble.html#interaction-constraints))\n",
    "\n",
    "Les arbres √† gradient bas√©s sur des histogrammes sont a priori autoris√©s √† utiliser n'importe quelle caract√©ristique pour diviser un n≈ìud en n≈ìuds enfants. Cela cr√©e ce que l'on appelle des interactions entre les caract√©ristiques, c'est-√†-dire l'utilisation de diff√©rentes caract√©ristiques pour effectuer une division le long d'une branche. Parfois, on souhaite restreindre les interactions possibles, voir [Mayer2022]. Cela peut √™tre fait gr√¢ce au param√®tre `interaction_cst`, o√π l'on peut sp√©cifier les indices des caract√©ristiques autoris√©es √† interagir. Par exemple, avec un total de 3 caract√©ristiques, `interaction_cst=[{0}, {1}, {2}]` interdit toutes les interactions. Les contraintes `[{0, 1}, {1, 2}]` sp√©cifient deux groupes de caract√©ristiques pouvant √©ventuellement interagir. Les caract√©ristiques 0 et 1 peuvent interagir entre elles, tout comme les caract√©ristiques 1 et 2. Cependant, notez que les caract√©ristiques 0 et 2 sont interdites d'interaction. Voici un arbre et les divisions possibles de l'arbre :\n",
    "\n",
    "```\n",
    "   1      <- Les deux groupes de contraintes pourraient √™tre appliqu√©s √† partir de maintenant\n",
    "  / \\\n",
    " 1   2    <- La division √† gauche remplit toujours les deux groupes de contraintes.\n",
    "/ \\ / \\      La division √† droite √† la caract√©ristique 2 a seulement le groupe {1, 2} √† partir de maintenant.\n",
    "```\n",
    "\n",
    "LightGBM utilise la m√™me logique pour les groupes chevauchants.\n",
    "\n",
    "Notez que les caract√©ristiques qui ne sont pas r√©pertori√©es dans `interaction_cst` se voient automatiquement attribuer un groupe d'interaction propre. Avec √† nouveau 3 caract√©ristiques, cela signifie que `[{0}]` est √©quivalent √† `[{0}, {1, 2}]`.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Graphiques de d√©pendance partielle et d'esp√©rance conditionnelle individuelle**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/4_inspection/plot_partial_dependence.ipynb)<br/>([*Partial Dependence and Individual Conditional Expectation Plots*](https://scikit-learn.org/stable/auto_examples/inspection/plot_partial_dependence.html))\n",
    "\n",
    "#### R√©f√©rences\n",
    "\n",
    "üî¨ [Mayer2022] M. Mayer, S.C. Bourassa, M. Hoesli, and D.F. Scognamiglio. 2022. [**‚ÄúMachine Learning Applications to Land and Structure Valuation‚Äù**](https://EconPapers.repec.org/RePEc:gam:jjrfmx:v:15:y:2022:i:5:p:193-:d:797960). Journal of Risk and Financial Management 15, no. 5: 193"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='low-level-parallelism'></a> 1.11.1.1.7. **Parall√©lisme de bas niveau**<br/>([_Low-level parallelism_](https://scikit-learn.org/stable/modules/ensemble.html#low-level-parallelism))\n",
    "\n",
    "[**`HistGradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier) et [**`HistGradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor) utilisent OpenMP pour la parall√©lisation via Cython. Pour plus de d√©tails sur la mani√®re de contr√¥ler le nombre de threads, veuillez vous r√©f√©rer √† nos notes sur la parall√©lisation.\n",
    "\n",
    "Les parties suivantes sont parall√©lis√©es :\n",
    "- la mise en correspondance des √©chantillons des valeurs r√©elles aux bacs de valeurs enti√®res (toutefois, la recherche des seuils des bacs est s√©quentielle)\n",
    "- la construction des histogrammes est parall√©lis√©e sur les caract√©ristiques\n",
    "- la recherche du meilleur point de division dans un n≈ìud est parall√©lis√©e sur les caract√©ristiques\n",
    "- pendant l'apprentissage, la mise en correspondance des √©chantillons dans les n≈ìuds enfants de gauche et de droite est parall√©lis√©e sur les √©chantillons\n",
    "- le calcul des gradients et hessiens est parall√©lis√© sur les √©chantillons\n",
    "- la pr√©diction est parall√©lis√©e sur les √©chantillons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='why-it-s-faster'></a> 1.11.1.1.8. **Pourquoi c'est plus rapide**<br/>([_Why it‚Äôs faster_](https://scikit-learn.org/stable/modules/ensemble.html#why-it-s-faster))\n",
    "\n",
    "La principale limitation d'une proc√©dure de boosting par gradient r√©side dans la construction des arbres de d√©cision. La construction d'un arbre de d√©cision traditionnel (comme dans les autres GBDTs [**`GradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) et [**`GradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)) n√©cessite de trier les √©chantillons √† chaque n≈ìud (pour chaque caract√©ristique). Le tri est n√©cessaire pour que le gain potentiel d'un point de division puisse √™tre calcul√© de mani√®re efficace. Par cons√©quent, la complexit√© de la division d'un n≈ìud unique est de $\\mathcal{O}(n_\\text{features} \\times n \\log(n))$ o√π $n$ est le nombre d'√©chantillons du n≈ìud.\n",
    "\n",
    "[**`HistGradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier) et [**`HistGradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor), en revanche, n'exigent pas de trier les valeurs des caract√©ristiques et utilisent √† la place une structure de donn√©es appel√©e histogramme, o√π les √©chantillons sont implicitement ordonn√©s. La construction d'un histogramme a une complexit√© de $\\mathcal{O}(n)$, de sorte que la proc√©dure de division du n≈ìud a une complexit√© de $\\mathcal{O}(n_\\text{features} \\times n)$, bien inf√©rieure √† la pr√©c√©dente. De plus, au lieu de consid√©rer $n$ points de division, nous n'en consid√©rons que `max_bins`, ce qui peut √™tre beaucoup plus petit.\n",
    "\n",
    "Afin de construire les histogrammes, les donn√©es d'entr√©e X doivent √™tre regroup√©es en bins √† valeurs enti√®res. Cette proc√©dure de regroupement n√©cessite de trier les valeurs des caract√©ristiques, mais cela se produit uniquement une fois au tout d√©but du processus de boosting (et non √† chaque n≈ìud, comme dans [**`GradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) et [**`GradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)).\n",
    "\n",
    "Enfin, de nombreuses parties de la mise en ≈ìuvre de [**`HistGradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier) et [**`HistGradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor) sont parall√©lis√©es.\n",
    "\n",
    "#### R√©f√©rences\n",
    "\n",
    "üî¨ [XGBoost] Tianqi Chen, Carlos Guestrin, [**‚ÄúXGBoost: A Scalable Tree Boosting System‚Äù**](https://arxiv.org/pdf/1603.02754.pdf)\n",
    "\n",
    "üî¨ [LightGBM] Ke et. al. [**‚ÄúLightGBM: A Highly Efficient Gradient BoostingDecision Tree‚Äù**](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/11/lightgbm.pdf)\n",
    "\n",
    "üî¨ [Fisher1958] Fisher, W.D. (1958). [**‚ÄúOn Grouping for Maximum Homogeneity‚Äù**](https://www.semanticscholar.org/paper/On-Grouping-for-Maximum-Homogeneity-Fisher/040c3e7d4baac625b6072cf9bf6be697f26d3cab) Journal of the American Statistical Association, 53, 789-798."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='gradientboostingclassifier-and-gradientboostingregressor'></a> 1.11.1.2. **[**`GradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) et [**`GradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)**<br/>([_**`GradientBoostingClassifier`** and **`GradientBoostingRegressor`**_](https://scikit-learn.org/stable/modules/ensemble.html#gradientboostingclassifier-and-gradientboostingregressor))\n",
    "\n",
    "L'utilisation et les param√®tres de [**`GradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) et [**`GradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor) sont d√©crits ci-dessous. Les 2 param√®tres les plus importants de ces estimateurs sont `n_estimators` et `learning_rate`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='classification'></a> 1.11.1.2.1. **Classification**<br/>([_Classification_](https://scikit-learn.org/stable/modules/ensemble.html#classification))\n",
    "\n",
    "[**`GradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) prend en charge √† la fois la classification binaire et la classification multi-classe. L'exemple suivant montre comment ajuster un classifieur de gradient boosting avec 100 souches (arbres de d√©cision de profondeur 1) en tant que classifieurs faibles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(\n",
    "    n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "# 0.913..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre de classifieurs faibles (c'est-√†-dire les arbres de r√©gression) est contr√¥l√© par le param√®tre `n_estimators`. [**La taille de chaque arbre**](#taille-de-l'arbre-gradient-boosting) peut √™tre contr√¥l√©e soit en d√©finissant la profondeur de l'arbre via `max_depth`, soit en d√©finissant le nombre de feuilles via `max_leaf_nodes`. Le `learning_rate` est un hyperparam√®tre dans la plage (0.0, 1.0] qui contr√¥le le surapprentissage via [**le taux de r√©tr√©cissement**](#r√©tr√©cissement-gradient-boosting).\n",
    "\n",
    "> **Remarque:** La classification avec plus de 2 classes n√©cessite l'induction de `n_classes` arbres de r√©gression √† chaque it√©ration, donc le nombre total d'arbres induits √©quivaut √† `n_classes * n_estimators`. Pour les ensembles de donn√©es avec un grand nombre de classes, nous recommandons fortement d'utiliser [**`HistGradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier) comme alternative √† [**`GradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='regression'></a> 1.11.1.2.2. **R√©gression**<br/>([_Regression_](https://scikit-learn.org/stable/modules/ensemble.html#regression))\n",
    "\n",
    "[**`GradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor) prend en charge diverses fonctions de perte pour la r√©gression, qui peuvent √™tre sp√©cifi√©es via l'argument `loss`. La fonction de perte par d√©faut pour la r√©gression est l'erreur quadratique (`'squared_error'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.009154859960321"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "est = GradientBoostingRegressor(\n",
    "    n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,\n",
    "    loss='squared_error'\n",
    ").fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))\n",
    "# 5.00..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'illustration ci-dessous montre les r√©sultats de l'application de [**`GradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor) avec une perte des moindres carr√©s et 500 apprenants de base sur l'ensemble de donn√©es sur le diab√®te ([**`sklearn.datasets.load_diabetes`**](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes)). Le graphique affiche l'erreur d'entra√Ænement et d'essai √† chaque it√©ration. L'erreur d'entra√Ænement √† chaque it√©ration est stock√©e dans l'attribut `train_score_` du mod√®le de gradient boosting. L'erreur d'essai √† chaque it√©ration peut √™tre obtenue via la m√©thode `staged_predict`, qui renvoie un g√©n√©rateur fournissant les pr√©dictions √† chaque √©tape. Des graphiques de ce type peuvent √™tre utilis√©s pour d√©terminer le nombre optimal d'arbres (c'est-√†-dire `n_estimators`) en utilisant l'arr√™t pr√©coce.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_gradient_boosting_regression_001.png\"\n",
    "    alt=\"Gradient Boosting regression\"\n",
    "    style=\"max-width: 30%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**R√©gression √† amplification de gradient**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_gradient_boosting_regression.ipynb)<br/>([_Gradient Boosting regression_](https://scikit-learn.org/stable/auto_examples/ensembles/plot_gradient_boosting_regression.html))\n",
    "\n",
    "##### [**Estimations hors sac (OOB) de l'amplification de gradient**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_gradient_boosting_oob.ipynb)<br/>([*Gradient Boosting Out-of-Bag estimates*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_oob.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='fitting-additional-weak-learners'></a> 1.11.1.2.3. **Ajustement d'apprenants faibles suppl√©mentaires**<br/>([_Fitting additional weak-learners_](https://scikit-learn.org/stable/modules/ensemble.html#fitting-additional-weak-learners))\n",
    "\n",
    "[**`GradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) et [**`GradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) prennent en charge `warm_start=True` qui permet d'ajouter des estimateurs suppl√©mentaires √† un mod√®le d√©j√† ajust√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.840234741105356"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = est.set_params(n_estimators=200, warm_start=True)  # set warm_start and new nr of trees\n",
    "_ = est.fit(X_train, y_train) # fit additional 100 trees to est\n",
    "mean_squared_error(y_test, est.predict(X_test))\n",
    "# 3.84..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='controlling-the-tree-size'></a> 1.11.1.2.4. **Contr√¥le de la taille de l'arbre**<br/>([_Controlling the tree size_](https://scikit-learn.org/stable/modules/ensemble.html#controlling-the-tree-size))\n",
    "\n",
    "La taille des apprenants de base de l'arbre de r√©gression d√©finit le niveau d'interactions variables qui peut √™tre captur√© par le mod√®le d'amplification du gradient. En g√©n√©ral, un arbre de profondeur `h` peut capturer des interactions d'ordre `h`. Il existe deux mani√®res de contr√¥ler la taille des arbres de r√©gression individuels.\n",
    "\n",
    "Si vous sp√©cifiez `max_depth=h` alors des arbres binaires complets de profondeur `h` seront d√©velopp√©s. De tels arbres auront (au plus) `2**h` n≈ìuds feuilles et `2**h - 1` n≈ìuds s√©par√©s.\n",
    "\n",
    "Alternativement, vous pouvez contr√¥ler la taille de l'arbre en sp√©cifiant le nombre de n≈ìuds feuilles via le param√®tre `max_leaf_nodes`. Dans ce cas, les arbres seront d√©velopp√©s en utilisant la meilleure recherche en premier o√π les n≈ìuds avec la plus grande am√©lioration d'impuret√© seront d√©velopp√©s en premier. Un arbre avec `max_leaf_nodes=k` a `k - 1` n≈ìuds divis√©s et peut donc mod√©liser des interactions allant jusqu'√† `max_leaf_nodes - 1`.\n",
    "\n",
    "Nous avons constat√© que `max_leaf_nodes=k` donne des r√©sultats comparables √† `max_depth=k-1` mais est beaucoup plus rapide √† entra√Æner en contrepartie d'une erreur d'entra√Ænement l√©g√®rement plus √©lev√©e. Le param√®tre `max_leaf_nodes` correspond √† la variable `J` dans le chapitre sur le gradient boosting dans [Friedman2001] et est li√© au param√®tre `interaction.depth` dans le package gbm de R o√π `max_leaf_nodes == interaction.depth + 1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='mathematical-formulation'></a> 1.11.1.2.5. **Formulation math√©matique**<br/>([_Mathematical formulation_](https://scikit-learn.org/stable/modules/ensemble.html#mathematical-formulation))\n",
    "\n",
    "Nous pr√©sentons d'abord le GBRT pour la r√©gression, puis nous d√©taillons le cas de la classification.\n",
    "\n",
    "#### R√©gression\n",
    "\n",
    "Les r√©gresseurs GBRT sont des mod√®les additifs dont la pr√©diction $\\hat{y}_i$ pour une entr√©e donn√©e $x_i$ est de la forme suivante :\n",
    "\n",
    "$$\\hat{y}_i = F_M(x_i) = \\sum_{m=1}^{M} h_m(x_i)$$\n",
    "\n",
    "o√π les $h_m$ sont des estimateurs appel√©s *apprenants faibles* dans le contexte du boosting. Gradient Tree Boosting utilise des [**r√©gresseurs √† arbre de d√©cision** (1.10)](https://scikit-learn.org/stable/modules/tree.html#tree) de taille fixe en tant qu'apprenants faibles. La constante $M$ correspond au param√®tre `n_estimators`.\n",
    "\n",
    "Semblable √† d'autres algorithmes de boosting, un GBRT est construit de mani√®re gourmande¬†:\n",
    "\n",
    "$$F_m(x) = F_{m-1}(x) + h_m(x),$$\n",
    "\n",
    "o√π l'arbre $h_m$ nouvellement ajout√© est ajust√© afin de minimiser une somme de pertes $L_m$, compte tenu de l'ensemble pr√©c√©dent $F_{m-1}$ :\n",
    "\n",
    "$$h_m =  \\arg\\min_{h} L_m = \\arg\\min_{h} \\sum_{i=1}^{n} l(y_i, F_{m-1}(x_i) + h(x_i)),$$\n",
    "\n",
    "o√π $l(y_i, F(x_i))$ est d√©fini par le param√®tre de perte `loss`, d√©taill√© dans la section suivante.\n",
    "\n",
    "Par d√©faut, le mod√®le initial $F_0$ est choisi comme la constante qui minimise la perte : pour une perte des moindres carr√©s, c'est la moyenne empirique des valeurs cibles. Le mod√®le initial peut √©galement √™tre sp√©cifi√© via l'argument `init`.\n",
    "\n",
    "En utilisant une approximation de Taylor du premier ordre, la valeur de $l$ peut √™tre approch√©e comme suit :\n",
    "\n",
    "$$\n",
    "l(y_i, F_{m-1}(x_i) + h_m(x_i)) \\approx\n",
    "l(y_i, F_{m-1}(x_i))\n",
    "+ h_m(x_i)\n",
    "\\left[ \\frac{\\partial l(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F=F_{m - 1}}.\n",
    "$$\n",
    "\n",
    "> **Note :** En bref, une approximation de Taylor du premier ordre dit que $l(z) \\approx l(a) + (z - a) \\frac{\\partial l(a)}{\\partial a}$. Ici, $z$ correspond √† $F_{m - 1}(x_i) + h_m(x_i)$, et $a$ correspond √† $F_{m - 1}(x_i)$\n",
    "\n",
    "La quantit√© $\\left[ \\frac{\\partial l(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F=F_{m - 1}}$ est la d√©riv√©e de la perte par rapport √† son deuxi√®me param√®tre, √©valu√© √† $F_{m-1}(x)$. Elle est facile √† calculer pour tout $F_{m - 1}(x_i)$ donn√© sous une forme ferm√©e puisque la perte est diff√©rentiable. Nous le noterons par $g_i$.\n",
    "\n",
    "En supprimant les termes constants, on a :\n",
    "\n",
    "$$h_m \\approx \\arg\\min_{h} \\sum_{i=1}^{n} h(x_i) g_i$$\n",
    "\n",
    "\n",
    "Ceci est minimis√© si $h(x_i)$ est ajust√© pour pr√©dire une valeur qui est proportionnelle au gradient n√©gatif $-g_i$. Par cons√©quent, √† chaque it√©ration, l'estimateur $h_m$ **est ajust√© pour pr√©dire les gradients n√©gatifs des √©chantillons**. Les gradients sont mis √† jour √† chaque it√©ration. Cela peut √™tre consid√©r√© comme une sorte de descente de gradient dans un espace fonctionnel.\n",
    "\n",
    "> **Note :** Pour certaines pertes, par ex. le moindre √©cart absolu (LAD, `absolute_error`) o√π les gradients sont $\\pm 1$, les valeurs pr√©dites par un $h_m$ ajust√© ne sont pas assez pr√©cises : l'arbre ne peut produire que des valeurs enti√®res. En cons√©quence, les valeurs des feuilles de l'arbre $h_m$ sont modifi√©es une fois l'arbre ajust√©, de sorte que les valeurs des feuilles minimisent la perte $L_m$. La mise √† jour d√©pend de la perte¬†: pour la perte LAD, la valeur d'une feuille est mise √† jour √† la m√©diane des √©chantillons de cette feuille.\n",
    "\n",
    "#### Classification\n",
    "\n",
    "L'amplification du gradient pour la classification est tr√®s similaire √† celle de la r√©gression. Cependant, la somme des arbres $F_M(x_i) = \\sum_m h_m(x_i)$ n'est pas homog√®ne √† une pr√©diction : il ne peut pas s'agir d'une classe, puisque les arbres pr√©disent des valeurs continues.\n",
    "\n",
    "Le mappage √† partir de la valeur $F_M(x_i)$ √† une classe ou une probabilit√© d√©pend de la perte. Pour la perte logarithmique, la probabilit√© que $x_i$ appartienne √† la classe positive est mod√©lis√©e comme $p(y_i = 1 | x_i) = \\sigma(F_M(x_i))$ o√π $\\sigma$ est la fonction sigmo√Øde ou expit.\n",
    "\n",
    "Pour la classification multiclasse, les arbres K (pour les classes K) sont construits √† chacune des\n",
    "$M$ it√©rations. La probabilit√© que $x_i$ appartienne √† la classe $k$ est mod√©lis√©e comme un softmax des valeurs $F_{M,k}(x_i)$.\n",
    "\n",
    "Notez que m√™me pour une t√¢che de classification, le sous-estimateur $h_m$ est toujours un r√©gresseur, pas un classifieur. En effet, les sous-estimateurs sont entra√Æn√©s pour pr√©dire des *gradients* (n√©gatifs), qui sont toujours des quantit√©s continues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='loss-functions'></a> 1.11.1.2.6. **Fonctions de perte**<br/>([_Loss Functions_](https://scikit-learn.org/stable/modules/ensemble.html#loss-functions))\n",
    "\n",
    "Les fonctions de perte suivantes sont prises en charge et peuvent √™tre sp√©cifi√©es √† l'aide du param√®tre `loss`¬†:\n",
    "\n",
    "* R√©gression\n",
    "    * Erreur au carr√© (`'squared_error'`)¬†: le choix naturel pour la r√©gression en raison de ses propri√©t√©s de calcul sup√©rieures. Le mod√®le initial est donn√© par la moyenne des valeurs cibles.\n",
    "    * Plus petit √©cart absolu (`'lad'`)¬†: une fonction de perte robuste pour la r√©gression. Le mod√®le initial est donn√© par la m√©diane des valeurs cibles.\n",
    "    * Huber (`'huber'`)¬†: autre fonction de perte robuste qui combine les moindres carr√©s et le moindre √©cart absolu¬†; utiliser `alpha` pour contr√¥ler la sensibilit√© aux valeurs aberrantes (voir [Friedman2001] pour plus de d√©tails).\n",
    "    * Quantile (`'quantile'`)¬†: une fonction de perte pour la r√©gression quantile. Utilisez `0 < alpha < 1` pour sp√©cifier le quantile. Cette fonction de perte peut √™tre utilis√©e pour cr√©er des intervalles de pr√©diction (voir l'exemple [**Intervalles de pr√©diction pour la r√©gression √† amplification de gradient**](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html)).\n",
    "* Classification\n",
    "    * Log-loss binaire (`'log-loss'`)¬†: la fonction de perte de log-vraisemblance n√©gative binomiale pour la classification binaire. Elle fournit des estimations de probabilit√©. Le mod√®le initial est donn√© par le log odds-ratio.\n",
    "    * Log-loss multi-classes (`'log-loss'`)¬†: la fonction de perte de log-vraisemblance n√©gative multinomiale pour la classification multi-classes avec des classes mutuellement exclusives `n_classes`. Elle fournit des estimations de probabilit√©. Le mod√®le initial est donn√© par la probabilit√© a priori de chaque classe. A chaque it√©ration, des arbres de r√©gression `n_classes` doivent √™tre construits, ce qui rend le GBRT plut√¥t inefficace pour les ensembles de donn√©es avec un grand nombre de classes.\n",
    "    * Perte exponentielle (`'exponential'`)¬†: la m√™me fonction de perte qu'[**`AdaBoostClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html). Moins robuste aux exemples mal √©tiquet√©s que `'log-loss'`¬†; ne peut √™tre utilis√© que pour la classification binaire.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='shrinkage-via-learning-rate'></a> 1.11.1.2.7. **R√©duction via le taux d'apprentissage**<br/>([_Shrinkage via learning rate_](https://scikit-learn.org/stable/modules/ensemble.html#shrinkage-via-learning-rate))\n",
    "\n",
    "[Friedman2001] a propos√© une strat√©gie de r√©gularisation simple qui √©chelonne la contribution de chaque apprenant faible par un facteur constant $\\nu$ :\n",
    "\n",
    "$$F_m(x) = F_{m-1}(x) + \\nu h_m(x)$$\n",
    "\n",
    "Le param√®tre $\\nu$ est √©galement appel√© **taux d'apprentissage** car il met √† l'√©chelle la longueur du pas de la proc√©dure de descente de gradient¬†; il peut √™tre d√©fini via le param√®tre `learning_rate`.\n",
    "\n",
    "Le param√®tre `learning_rate` interagit fortement avec le param√®tre `n_estimators`, le nombre d'apprenants faibles √† adapter. Des valeurs plus petites de `learning_rate` n√©cessitent un plus grand nombre d'apprenants faibles pour maintenir une erreur d'entra√Ænement constante. Des preuves empiriques sugg√®rent que de petites valeurs de `learning_rate` favorisent une meilleure erreur de test. [HTF] recommande de d√©finir le taux d'apprentissage sur une petite constante (par exemple, `learning_rate <= 0.1`) et de choisir `n_estimators` en s'arr√™tant t√¥t. Pour une discussion plus d√©taill√©e de l'interaction entre `learning_rate` et `n_estimators` voir [R2007]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='subsampling'></a> 1.11.1.2.8. **Sous-√©chantillonnage**<br/>([_Subsampling_](https://scikit-learn.org/stable/modules/ensemble.html#subsampling))\n",
    "\n",
    "[Friedman2002] a propos√© le boosting de gradient stochastique, qui combine le boosting de gradient avec un moyennage bootstrap (bagging). A chaque it√©ration, le classifieur de base est entra√Æn√© sur une fraction `subsample` des donn√©es d'entra√Ænement disponibles. Le sous-√©chantillon est tir√© sans remise. Une valeur typique de `subsample` est 0.5.\n",
    "\n",
    "La figure ci-dessous illustre l'effet du r√©tr√©cissement et du sous-√©chantillonnage sur la qualit√© de l'ajustement du mod√®le. Nous pouvons clairement voir que le r√©tr√©cissement surpasse l'absence de r√©tr√©cissement. Le sous-√©chantillonnage avec r√©tr√©cissement peut encore augmenter la pr√©cision du mod√®le. Le sous-√©chantillonnage sans r√©tr√©cissement, en revanche, donne de mauvais r√©sultats.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_gradient_boosting_regularization_001.png\"\n",
    "    alt=\"Effet du r√©tr√©cissement et du sous-√©chantillonnage\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Une autre strat√©gie pour r√©duire la variance consiste √† sous-√©chantillonner les caract√©ristiques analogues aux fractionnements al√©atoires dans [**`RandomForestClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). Le nombre de caract√©ristiques sous-√©chantillonn√©es peut √™tre contr√¥l√© via le param√®tre `max_features`.\n",
    "\n",
    "> **Note :** - L'utilisation d'une petite valeur `max_features` peut r√©duire consid√©rablement le temps d'ex√©cution.\n",
    "\n",
    "L'amplification du gradient stochastique permet de calculer des estimations hors sac de la d√©viance du test en calculant l'am√©lioration de la d√©viance sur les exemples qui ne sont pas inclus dans l'√©chantillon bootstrap (c'est-√†-dire les exemples hors sac). Les am√©liorations sont stock√©es dans l'attribut `oob_improvement_`. `oob_improvement_[i]` contient l'am√©lioration en termes de perte sur les √©chantillons OOB si vous ajoutez la i√®me √©tape aux pr√©dictions actuelles. Les estimations hors sac peuvent √™tre utilis√©es pour la s√©lection du mod√®le, par exemple pour d√©terminer le nombre optimal d'it√©rations. Les estimations OOB sont g√©n√©ralement tr√®s pessimistes, nous vous recommandons donc d'utiliser la validation crois√©e √† la place et de n'utiliser OOB que si la validation crois√©e prend trop de temps.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**R√©gularisation de l'amplification de gradient**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_gradient_boosting_regularization.ipynb)<br/>([*Gradient Boosting regularization*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regularization.html))\n",
    "\n",
    "##### [**Estimations OOB de l'amplification de gradient**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_gradient_boosting_oob.ipynb)<br/>([*Gradient Boosting Out-of-Bag estimates*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_oob.html))\n",
    "\n",
    "##### [**Erreurs OOB pour les for√™ts al√©atoires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_gradient_boosting_regularization.ipynb)<br/>([*OOB Errors for Random Forests*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regularization.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='interpretation-with-feature-importance'></a> 1.11.1.2.9. **Interpr√©tation avec importance des caract√©ristiques**<br/>([_Interpretation with feature importance_](https://scikit-learn.org/stable/modules/ensemble.html#interpretation-with-feature-importance))\n",
    "\n",
    "Les arbres de d√©cision individuels peuvent √™tre interpr√©t√©s facilement en visualisant simplement la structure arborescente. Les mod√®les d'amplification de gradient, cependant, comprennent des centaines d'arbres de r√©gression et ne peuvent donc pas √™tre facilement interpr√©t√©s par une inspection visuelle des arbres individuels. Heureusement, un certain nombre de techniques ont √©t√© propos√©es pour r√©sumer et interpr√©ter les mod√®les de gradient boosting.\n",
    "\n",
    "Souvent, les caract√©ristiques ne contribuent pas de mani√®re √©gale √† pr√©dire la r√©ponse cible¬†; dans de nombreuses situations, la majorit√© des caract√©ristiques ne sont en fait pas pertinentes. Lors de l'interpr√©tation d'un mod√®le, la premi√®re question est g√©n√©ralement¬†: quelles sont ces caract√©ristiques importantes et comment contribuent-elles √† pr√©dire la r√©ponse cible¬†?\n",
    "\n",
    "Les arbres de d√©cision individuels effectuent intrins√®quement la s√©lection des caract√©ristiques en s√©lectionnant les points de partage appropri√©s. Ces informations peuvent √™tre utilis√©es pour mesurer l'importance de chaque caract√©ristique ; l'id√©e de base est la suivante¬†: plus une caract√©ristique est souvent utilis√©e dans les points de division d'un arbre, plus cette caract√©ristique est importante. Cette notion d'importance peut √™tre √©tendue aux ensembles d'arbres de d√©cision en faisant simplement la moyenne de l'importance des caract√©ristiques bas√©e sur les impuret√©s de chaque arbre (voir [**√âvaluation de l'importance des caract√©ristiques** (1.11.2.5)](https://scikit-learn.org/stable/modules/ensemble.html#random-forest-feature-importance) pour plus de d√©tails).\n",
    "\n",
    "Les scores d'importance des caract√©ristiques d'un mod√®le d'amplification du gradient ajust√© sont accessibles via la propri√©t√© feature_importances_¬†:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10684213, 0.10461707, 0.11265447, 0.09863589, 0.09469133,\n",
       "       0.10729306, 0.09163753, 0.09718194, 0.09581415, 0.09063242])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                 max_depth=1, random_state=0).fit(X, y)\n",
    "clf.feature_importances_\n",
    "# array([0.10..., 0.10..., 0.11..., ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez que ce calcul de l'importance des caract√©ristiques est bas√© sur l'entropie et qu'il est distinct de [**`sklearn.inspection.permutation_importance`**](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html) qui est bas√© sur la permutation des caract√©ristiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemples\n",
    "\n",
    "##### [**R√©gression √† amplification de gradient**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_gradient_boosting_regression.ipynb)<br/>([*Gradient Boosting regression*](https://scikit-learn.org/stable/auto_examples/ensembles/plot_gradient_boosting_regression.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R√©f√©rences\n",
    "\n",
    "üî¨ [Friedman2001] (1,2,3,4) Friedman, J.H. (2001). [‚Äú**Greedy function approximation: A gradient boosting machine**‚Äù](https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.full). Annals of Statistics, 29, 1189-1232.\n",
    "\n",
    "üî¨ [Friedman2002] Friedman, J.H. (2002). [‚Äú**Stochastic gradient boosting**‚Äù](https://jerryfriedman.su.domains/ftp/stobst.pdf). Computational Statistics & Data Analysis, 38, 367-378.\n",
    "\n",
    "üî¨ [R2007] G. Ridgeway (2006). [‚Äú**Generalized Boosted Models: A guide to the gbm package‚Äù**](https://cran.r-project.org/web/packages/gbm/vignettes/gbm.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='forests-of-randomized-trees'></a> 1.11.2. **For√™ts al√©atoires et autres ensembles d'arbres al√©atoires**<br/>([_Random forests and other randomized tree ensembles_](https://scikit-learn.org/stable/modules/ensemble.html#random-forests-and-other-randomized-tree-ensembles))\n",
    "\n",
    "Le module [**`sklearn.ensemble`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble) comprend deux algorithmes de moyennage bas√©s sur des [**arbres de d√©cision** (1.10.)](https://scikit-learn.org/stable/modules/tree.html#tree) al√©atoires : l'algorithme RandomForest et la m√©thode Extra-Trees. Les deux algorithmes sont des techniques de perturbation et de combinaison [B1998] sp√©cifiquement con√ßues pour les arbres. Cela signifie qu'un ensemble diversifi√© de classifieurs est cr√©√© en introduisant un caract√®re al√©atoire dans la construction du classifieur. La pr√©diction de l'ensemble est donn√©e comme la pr√©diction moyenne des classifieurs individuels.\n",
    "\n",
    "Comme les autres classifieurs, les classifieurs for√™ts doivent √™tre √©quip√©s de deux tableaux¬†: un tableau creux ou dense `X` de forme `(n_samples, n_features)` contenant les √©chantillons d'entra√Ænement, et un tableau `Y` de forme `(n_samples,)` contenant les valeurs cibles (√©tiquettes de classe) pour les √©chantillons d'entra√Ænement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X = [[0, 0], [1, 1]]\n",
    "Y = [0, 1]\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf = clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme les [**arbres de d√©cision** (1.10.)](https://scikit-learn.org/stable/modules/tree.html#tree), les for√™ts d'arbres s'√©tendent √©galement aux [**probl√®mes multi-sorties** (1.10.3.)](https://scikit-learn.org/stable/modules/tree.html#tree-multioutput) (si `Y` est un tableau de forme `(n_samples, n_features)`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='random-forests'></a> 1.11.2.1. **For√™ts al√©atoires**<br/>([_Random Forests_](https://scikit-learn.org/stable/modules/ensemble.html#random-forests))\n",
    "\n",
    "\n",
    "Dans les for√™ts al√©atoires (voir les classes [**`RandomForestClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) et [**`RandomForestRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor)), chaque arbre de l'ensemble est construit √† partir d'un √©chantillon tir√© avec remise (c'est-√†-dire un √©chantillon bootstrap) √† partir de l'ensemble d'entra√Ænement.\n",
    "\n",
    "De plus, lors de la division de chaque n≈ìud lors de la construction d'un arbre, la meilleure division est trouv√©e soit √† partir de toutes les caract√©ristiques d'entr√©e, soit d'un sous-ensemble al√©atoire de taille `max_features`. (Voir les [**consignes de r√©glage des param√®tres** (1.11.2.3.)](https://scikit-learn.org/stable/modules/ensemble.html#random-forest-parameters) pour plus de d√©tails).\n",
    "\n",
    "Le but de ces deux sources d'al√©a est de diminuer la variance de l'estimateur for√™t. En effet, les arbres de d√©cision individuels pr√©sentent g√©n√©ralement une variance √©lev√©e et ont tendance √† sur-ajuster. Le caract√®re al√©atoire inject√© dans les for√™ts produit des arbres de d√©cision avec des erreurs de pr√©diction quelque peu d√©coupl√©es. En prenant une moyenne de ces pr√©dictions, certaines erreurs peuvent s'annuler. Les for√™ts al√©atoires permettent d'obtenir une variance r√©duite en combinant divers arbres, parfois au prix d'une l√©g√®re augmentation du biais. En pratique, la r√©duction de la variance est souvent significative, ce qui donne un meilleur mod√®le global.\n",
    "\n",
    "Contrairement √† la publication originale [B2001], l'impl√©mentation scikit-learn combine les classifieurs en faisant la moyenne de leur pr√©diction probabiliste, au lieu de laisser chaque classificateur voter pour une seule classe.\n",
    "\n",
    "Une alternative comp√©titive aux for√™ts al√©atoires est repr√©sent√©e par les mod√®les d'[**amplification de gradient bas√©s sur des histogrammes** (1.11.1)](#histogram-based-gradient-boosting) (HGBT) :\n",
    "\n",
    "- **Construction d'arbres** : Les for√™ts al√©atoires s'appuient g√©n√©ralement sur des arbres profonds (qui surajustent individuellement) et n√©cessitent beaucoup de ressources computationnelles, car elles n√©cessitent plusieurs divisions et √©valuations de divisions candidates. Les mod√®les de renforcement construisent des arbres peu profonds (qui sous-ajustent individuellement) qui sont plus rapides √† ajuster et √† pr√©dire.\n",
    "\n",
    "- **Renforcement s√©quentiel** : Dans HGBT, les arbres de d√©cision sont construits s√©quentiellement, o√π chaque arbre est form√© pour corriger les erreurs commises par les pr√©c√©dents. Cela leur permet d'am√©liorer it√©rativement les performances du mod√®le en utilisant relativement peu d'arbres. En revanche, les for√™ts al√©atoires utilisent un vote √† la majorit√© pour pr√©dire le r√©sultat, ce qui peut n√©cessiter un plus grand nombre d'arbres pour atteindre le m√™me niveau de pr√©cision.\n",
    "\n",
    "- **Binning efficace** : HGBT utilise un algorithme de binning efficace qui peut g√©rer de grands ensembles de donn√©es avec un grand nombre de caract√©ristiques. L'algorithme de binning peut pr√©traiter les donn√©es pour acc√©l√©rer la construction ult√©rieure de l'arbre (voir Pourquoi c'est plus rapide). En revanche, l'impl√©mentation de scikit-learn des for√™ts al√©atoires n'utilise pas le binning et repose sur des divisions exactes, ce qui peut √™tre co√ªteux en termes de calcul.\n",
    "\n",
    "Dans l'ensemble, le co√ªt de calcul d'HGBT par rapport aux for√™ts al√©atoires d√©pend des caract√©ristiques sp√©cifiques de l'ensemble de donn√©es et de la t√¢che de mod√©lisation. Il est judicieux d'essayer les deux mod√®les et de comparer leurs performances et leur efficacit√© de calcul sur votre probl√®me sp√©cifique pour d√©terminer lequel est le mieux adapt√©.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Comparaison des mod√®les de for√™ts al√©atoires et d'amplification de gradient bas√©s sur des histogrammes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_forest_hist_grad_boosting_comparison.ipynb)<br/>([_Comparing Random Forests and Histogram Gradient Boosting models_](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_hist_grad_boosting_comparison.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='extremely-randomized-trees'></a> 1.11.2.2. **Arbres extr√™mement al√©atoires**<br/>([_Extremely Randomized Trees_](https://scikit-learn.org/stable/modules/ensemble.html#extremely-randomized-trees))\n",
    "\n",
    "Dans les arbres extr√™mement al√©atoires (voir les classes [**`ExtraTreesClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier) et [**`ExtraTreesRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html#sklearn.ensemble.ExtraTreesRegressor)), le caract√®re al√©atoire va encore plus loin dans la fa√ßon dont les fractionnements sont calcul√©s. Comme dans les for√™ts al√©atoires, un sous-ensemble al√©atoire de caract√©ristiques candidates est utilis√©, mais au lieu de rechercher les seuils les plus discriminants, les seuils sont tir√©s au hasard pour chaque caract√©ristique candidate et le meilleur de ces seuils g√©n√©r√©s al√©atoirement est choisi comme r√®gle de division. Cela permet g√©n√©ralement de r√©duire un peu plus la variance du mod√®le, au prix d'une augmentation un peu plus importante du biais :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X, y = make_blobs(n_samples=10000, n_features=10, centers=100,\n",
    "                  random_state=0)\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,\n",
    "                             random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()\n",
    "# 0.98...\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "                             min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()\n",
    "# 0.999...\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,\n",
    "                           min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean() > 0.999\n",
    "# True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_forest_iris_001.png\"\n",
    "    alt=\"Classifieurs sur des sous-ensembles de caract√©ristiques du jeu de donn√©es Iris\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='parameters'></a> 1.11.2.3. **Param√®tres**<br/>([_Parameters_](https://scikit-learn.org/stable/modules/ensemble.html#parameters))\n",
    "\n",
    "Les principaux param√®tres √† ajuster lors de l'utilisation de ces m√©thodes sont `n_estimators` et `max_features`. Le premier est le nombre d'arbres dans la for√™t. Plus il est grand, mieux c'est, mais aussi plus il faudra de temps pour calculer. De plus, notez que les r√©sultats cesseront de s'am√©liorer significativement au-del√† d'un nombre critique d'arbres. Ce dernier est la taille des sous-ensembles al√©atoires de caract√©ristiques √† prendre en compte lors de la division d'un n≈ìud. Plus la r√©duction de la variance est faible, plus l'augmentation du biais est importante. Les bonnes valeurs empiriques par d√©faut sont `max_features=1.0` ou de mani√®re √©quivalente `max_features=None` (en consid√©rant toujours toutes les caract√©ristiques au lieu d'un sous-ensemble al√©atoire) pour les probl√®mes de r√©gression, et `max_features=\"sqrt\"` (en utilisant un sous-ensemble al√©atoire de taille `sqrt(n_features)`) pour les t√¢ches de classification ( o√π `n_features` est le nombre d'entit√©s dans les donn√©es). La valeur par d√©faut de `max_features=1.0` √©quivaut √† des arbres ensach√©s et un caract√®re plus al√©atoire peut √™tre obtenu en d√©finissant des valeurs plus petites (par exemple, 0,3 est une valeur par d√©faut typique dans la litt√©rature). De bons r√©sultats sont souvent obtenus en d√©finissant `max_depth=None` en combinaison avec `min_samples_split=2` (c'est-√†-dire lors du d√©veloppement complet des arbres). Gardez cependant √† l'esprit que ces valeurs ne sont g√©n√©ralement pas optimales et peuvent entra√Æner des mod√®les qui consomment beaucoup de RAM. Les meilleures valeurs de param√®tres doivent toujours faire l'objet d'une validation crois√©e. De plus, notez que dans les for√™ts al√©atoires, les √©chantillons bootstrap sont utilis√©s par d√©faut (`bootstrap=True`) tandis que la strat√©gie par d√©faut pour les arbres suppl√©mentaires consiste √† utiliser l'ensemble de donn√©es complet (`bootstrap=False`). Lors de l'utilisation de l'√©chantillonnage bootstrap, l'erreur de g√©n√©ralisation peut √™tre estim√©e sur les √©chantillons laiss√©s de c√¥t√© ou hors sac. Cela peut √™tre activ√© en d√©finissant `oob_score=True`.\n",
    "\n",
    "**NB** - La taille du mod√®le avec les param√®tres par d√©faut est $\\mathcal{O}(M \\times N \\times \\log(N))$, o√π $M$ est le nombre d'arbres et $N$ est le nombre d'√©chantillons. Afin de r√©duire la taille du mod√®le, vous pouvez modifier ces param√®tres¬†: `min_samples_split`, `max_leaf_nodes`, `max_depth` et `min_samples_leaf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='parallelization'></a> 1.11.2.4. **Parall√©lisation**<br/>([_Parallelization_](https://scikit-learn.org/stable/modules/ensemble.html#parallelization))\n",
    "\n",
    "Enfin, ce module propose √©galement la construction parall√®le des arbres et le calcul parall√®le des pr√©dictions via le param√®tre `n_jobs`. Si `n_jobs=k` alors les calculs sont partitionn√©s en `k` jobs, et ex√©cut√©s sur `k` c≈ìurs de la machine. Si `n_jobs=-1` alors tous les c≈ìurs disponibles sur la machine sont utilis√©s. Notez qu'en raison de la surcharge de communication inter-processus, l'acc√©l√©ration peut ne pas √™tre lin√©aire (c'est-√†-dire que l'utilisation de `k` travaux ne sera malheureusement pas `k` fois plus rapide). Une acc√©l√©ration significative peut encore √™tre obtenue lors de la construction d'un grand nombre d'arbres, ou lorsque la construction d'un seul arbre n√©cessite une bonne quantit√© de temps (par exemple, sur de grands ensembles de donn√©es).\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Tracer les surfaces de d√©cision d'ensembles d'arbres sur le jeu de donn√©es iris**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_forest_iris.ipynb)<br/>([*Plot the decision surfaces of ensembles of trees on the iris dataset*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_iris.html))\n",
    "\n",
    "#### [**Importances des pixels avec une for√™t parall√®le d'arbres**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_forest_importances_faces.ipynb)<br/>([*Pixel importances with a parallel forest of trees*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances_faces.html))\n",
    "\n",
    "#### [**Compl√©tion de visages avec des estimateurs multi-sorties**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/misc/plot_multioutput_face_completion.ipynb)<br/>([*Face completion with multi-output estimators*](https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_multioutput_face_completion.html))\n",
    "\n",
    "### R√©f√©rences\n",
    "\n",
    "üî¨ [B2001] Breiman, [‚Äú**Random Forests‚Äù**](https://link.springer.com/content/pdf/10.1023/A:1010933404324.pdf), Machine Learning, 45(1), 5-32, 2001.\n",
    "\n",
    "üî¨ [B1998] Breiman, [‚Äú**Arcing Classifiers‚Äù**](https://www.stat.berkeley.edu/~breiman/arcall.pdf), Annals of Statistics 1998.\n",
    "\n",
    "üî¨  P. Geurts, D. Ernst., and L. Wehenkel, [‚Äú**Extremely randomized trees‚Äù**](https://link.springer.com/content/pdf/10.1007/s10994-006-6226-1.pdf), Machine Learning, 63(1), 3-42, 2006."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='feature-importance-evaluation'></a> 1.11.2.5. **√âvaluation de l'importance des caract√©ristiques**<br/>([_Feature importance evaluation_](https://scikit-learn.org/stable/modules/ensemble.html#feature-importance-evaluation))\n",
    "\n",
    "Le rang relatif (c'est-√†-dire la profondeur) d'une caract√©ristique utilis√©e comme n≈ìud de d√©cision dans un arbre peut √™tre utilis√© pour √©valuer l'importance relative de cette caract√©ristique par rapport √† la pr√©visibilit√© de la variable cible. Les caract√©ristiques utilis√©es en haut de l'arbre contribuent √† la d√©cision de pr√©diction finale d'une plus grande fraction des √©chantillons d'entr√©e. La **fraction attendue des √©chantillons** auxquels ils contribuent peut donc √™tre utilis√©e comme une estimation de l'**importance relative des caract√©ristiques**. Dans scikit-learn, la fraction d'√©chantillons √† laquelle une caract√©ristique contribue est combin√©e avec la diminution de l'impuret√© r√©sultant de leur division pour cr√©er une estimation normalis√©e du pouvoir pr√©dictif de cette caract√©ristique.\n",
    "\n",
    "En **faisant la moyenne** des estimations de la capacit√© pr√©dictive sur plusieurs arbres randomis√©s, on peut **r√©duire la variance** d'une telle estimation et l'utiliser pour la s√©lection des caract√©ristiques. C'est ce qu'on appelle la diminution moyenne des impuret√©s, ou MDI (*Mean Decrease in Impurity*). Se reporter √† [L2014] pour plus d'informations sur MDI et l'√©valuation de l'importance des caract√©ristiques avec Random Forests.\n",
    "\n",
    "> **Avertissement :** Les importances de caract√©ristiques bas√©es sur les impuret√©s calcul√©es sur les mod√®les arborescents souffrent de deux d√©fauts qui peuvent conduire √† des conclusions erron√©es. Tout d'abord, ils sont calcul√©s sur des statistiques d√©riv√©es de l'ensemble de donn√©es d'entra√Ænement et ne nous informent donc pas n√©cessairement sur les caract√©ristiques les plus importantes pour faire de bonnes pr√©dictions sur l'ensemble de donn√©es retenu. Deuxi√®mement, ils favorisent les caract√©ristiques √† cardinalit√© √©lev√©e, c'est-√†-dire les caract√©ristiques avec de nombreuses valeurs uniques. L'[**importance des caract√©ristiques de permutation** (4.2)](https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-importance) est une alternative √† l'importance des caract√©ristiques bas√©e sur les impuret√©s qui ne souffre pas de ces d√©fauts. Ces deux m√©thodes d'obtention de l'importance des caract√©ristiques sont explor√©es dans¬†: [**Permutation Importance vs Random Forest Feature Importance (MDI)**](https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py).\n",
    "\n",
    "L'exemple suivant montre une repr√©sentation cod√©e par couleur des importances relatives de chaque pixel individuel pour une t√¢che de reconnaissance faciale √† l'aide d'un mod√®le [`ExtraTreesClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier).\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_forest_importances_faces_001.png\"\n",
    "    alt=\"Importance des pixels √† partir des valeurs d'impuret√©\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "En pratique, ces estimations sont stock√©es sous la forme d'un attribut nomm√© `feature_importances_` sur le mod√®le ajust√©. Il s'agit d'un tableau de forme `(n_features,)` dont les valeurs sont positives et dont la somme est √©gale √† 1,0. Plus la valeur est √©lev√©e, plus la contribution de la caract√©ristique correspondante √† la fonction de pr√©diction est importante.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Importances des pixels avec une for√™t parall√®le d'arbres**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_forest_importances_faces.ipynb)<br/>([*Pixel importances with a parallel forest of trees*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances_faces.html))\n",
    "\n",
    "##### [**Importances des caract√©ristiques avec une for√™t d'arbres**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_forest_importances.ipynb)<br/>([*Feature importances with a forest of trees*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html))\n",
    "\n",
    "#### R√©f√©rences\n",
    "\n",
    "üìö [L2014] G. Louppe, [**‚ÄúUnderstanding Random Forests: From Theory to Practice‚Äù**](https://arxiv.org/pdf/1407.7502.pdf), PhD Thesis, U. of Liege, 2014."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='totally-random-trees-embedding'></a> 1.11.2.6. **Int√©gration d'arbres totalement al√©atoires**<br/>([_Totally Random Trees Embedding_](https://scikit-learn.org/stable/modules/ensemble.html#totally-random-trees-embedding))\n",
    "\n",
    "[**`RandomTreesEmbedding`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomTreesEmbedding.html) impl√©mente une transformation non supervis√©e des donn√©es. En utilisant une for√™t d'arbres compl√®tement al√©atoires, [**`RandomTreesEmbedding`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomTreesEmbedding.html) code les donn√©es par les indices des feuilles dans lesquelles un point de donn√©es se retrouve. Cet index est ensuite cod√© suivant la m√©thode un-parmi-K, conduisant √† un codage binaire creux de grande dimension. Ce codage peut √™tre calcul√© tr√®s efficacement et peut ensuite √™tre utilis√© comme base pour d'autres t√¢ches d'apprentissage. La taille et le creux du code peuvent √™tre influenc√©s en choisissant le nombre d'arbres et la profondeur maximale par arbre. Pour chaque arbre de l'ensemble, le codage contient une entr√©e parmi une. La taille du codage est au plus `n_estimators * 2 ** max_depth`, le nombre maximum de feuilles dans la for√™t.\n",
    "\n",
    "Comme les points de donn√©es voisins sont plus susceptibles de se trouver dans la m√™me feuille d'un arbre, la transformation effectue une estimation de densit√© implicite et non param√©trique.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Transformation des caract√©ristiques par hachage √† l'aide d'arbres totalement al√©atoires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_random_forest_embedding.ipynb)<br/>([*Hashing feature transformation using Totally Random Trees*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_random_forest_embedding.html))\n",
    "\n",
    "##### [**Apprentissage par vari√©t√©s sur chiffres manuscrits : Locally Linear Embedding, Isomap‚Ä¶**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_2_manifold/plot_lle_digits.ipynb)<br/>([*Manifold learning on handwritten digits: Locally Linear Embedding, Isomap‚Ä¶*](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html))\n",
    "\n",
    "Compare des techniques de r√©duction de dimensionnalit√© non lin√©aire sur des chiffres manuscrits.\n",
    "\n",
    "##### [**Transformations de caract√©ristiques avec des ensembles d'arbres**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensemble/plot_feature_transformation.ipynb)<br/>([*Feature transformations with ensembles of trees*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html))\n",
    "\n",
    "Comparent les transformations de caract√©ristiques supervis√©es et non supervis√©es bas√©es sur des arbres.\n",
    "\n",
    "> **Voir √©galement :** Les techniques d'[**Apprentissage des vari√©t√©s** (2.2)](https://scikit-learn.org/stable/modules/manifold.html#manifold) peuvent √©galement √™tre utiles pour d√©river des repr√©sentations non lin√©aires de l'espace des caract√©ristiques, ces approches se concentrent √©galement sur la r√©duction de la dimensionnalit√©."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='bagging-meta-estimator'></a> 1.11.3. **Meta-estimation par bagging (ensachage)**<br/>([_Bagging meta-estimator_](https://scikit-learn.org/stable/modules/ensemble.html#bagging-meta-estimator))\n",
    "\n",
    "Dans les algorithmes ensemblistes, les m√©thodes de bagging forment une classe d'algorithmes qui construisent plusieurs instances d'un estimateur bo√Æte noire sur des sous-ensembles al√©atoires de l'ensemble d'entra√Ænement d'origine, puis agr√®gent leurs pr√©dictions individuelles pour former une pr√©diction finale. Ces m√©thodes sont utilis√©es comme moyen de r√©duire la variance d'un estimateur de base (par exemple, un arbre de d√©cision), en introduisant de l'al√©atoire dans sa proc√©dure de construction, puis en en faisant un ensemble. Dans de nombreux cas, les m√©thodes de bagging constituent un moyen tr√®s simple d'am√©lioration par rapport √† un mod√®le unique, sans n√©cessiter d'adaptation de l'algorithme de base sous-jacent. Comme elles offrent un moyen de r√©duire le surajustement, les m√©thodes de bagging fonctionnent mieux avec des mod√®les solides et complexes (par exemple, des arbres de d√©cision enti√®rement d√©velopp√©s), contrairement aux m√©thodes d'amplification qui fonctionnent g√©n√©ralement mieux avec des mod√®les faibles (par exemple, des arbres de d√©cision peu profonds).\n",
    "\n",
    "Les m√©thodes de bagging se pr√©sentent sous de nombreuses formes, mais diff√®rent principalement les unes des autres par la mani√®re dont elles dessinent des sous-ensembles al√©atoires de l'ensemble d'entra√Ænement :\n",
    "\n",
    "* Lorsque des sous-ensembles al√©atoires de l'ensemble de donn√©es sont tir√©s en tant que sous-ensembles al√©atoires des √©chantillons, cet algorithme est connu sous le nom de **Collage** (_Pasting_) [B1999].\n",
    "\n",
    "* Lorsque les √©chantillons sont tir√©s avec remise, la m√©thode est connue sous le nom de **Bagging** [B1996].\n",
    "\n",
    "* Lorsque des sous-ensembles al√©atoires de l'ensemble de donn√©es sont tir√©s en tant que sous-ensembles al√©atoires des caract√©ristiques, la m√©thode est connue sous le nom de **Sous-espaces al√©atoires** (_Radom Subspaces_) [H1998].\n",
    "\n",
    "* Enfin, lorsque les estimateurs de base sont construits sur des sous-ensembles d'√©chantillons et de caract√©ristiques, la m√©thode est connue sous le nom de **Correctifs al√©atoires** (_Random Patches_) [LG2012].\n",
    "\n",
    "Dans scikit-learn, les m√©thodes de bagging sont propos√©es sous la forme d'un m√©ta-estimateur [**`BaggingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier) unifi√© (resp. [**`BaggingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html#sklearn.ensemble.BaggingRegressor)), prenant en entr√©e un estimateur de base sp√©cifi√© par l'utilisateur ainsi que des param√®tres sp√©cifiant la strat√©gie pour tirer des sous-ensembles al√©atoires. En particulier, `max_samples` et `max_features` contr√¥lent la taille des sous-ensembles (en termes d'√©chantillons et de caract√©ristiques), tandis que `bootstrap` et `bootstrap_features` contr√¥lent si les √©chantillons et les caract√©ristiques sont tir√©s avec ou sans remise. Lors de l'utilisation d'un sous-ensemble des √©chantillons disponibles, la pr√©cision de la g√©n√©ralisation peut √™tre estim√©e avec les √©chantillons hors sac en d√©finissant `oob_score=True`. √Ä titre d'exemple, l'extrait ci-dessous illustre comment instancier un ensemble d'ensachage d'estimateurs de base [**`KNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier), chacun construit sur des sous-ensembles al√©atoires de 50¬†% des √©chantillons et 50¬†% des caract√©ristiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "bagging = BaggingClassifier(KNeighborsClassifier(),\n",
    "                            max_samples=0.5, max_features=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemples\n",
    "\n",
    "##### [**Estimateur unique versus bagging¬†: d√©composition biais-variance**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_bias_variance.ipynb)<br/>([_Single estimator versus bagging: bias-variance decomposition_](https://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R√©f√©rences\n",
    "\n",
    "Z üî¨ [B1999] L. Breiman, [**‚ÄúPasting Small Votes for Classification in Large Databases and On-Line‚Äù**](https://link.springer.com/content/pdf/10.1023/A:1007563306331.pdf), Machine Learning, 36(1), 85-103, 1999.\n",
    "\n",
    "Z üî¨ [B1996] L. Breiman, [**‚ÄúBagging predictors‚Äù**](https://link.springer.com/content/pdf/10.1023/A:1018054314350.pdf), Machine Learning, 24(2), 123-140, 1996.\n",
    "\n",
    "Z üî¨ [H1998] T. Ho, [**‚ÄúThe Random Subspace Method for Constructing Decision Forests‚Äù**](http://machine-learning.martinsewell.com/ensembles/rsm/Ho1998.pdf), Pattern Analysis and Machine Intelligence, 20(8), 832-844, 1998.\n",
    "\n",
    "Z üî¨ [LG2012] G. Louppe and P. Geurts, [**‚ÄúEnsembles on Random Patches‚Äù**](https://link.springer.com/content/pdf/10.1007/978-3-642-33460-3_28.pdf), Machine Learning and Knowledge Discovery in Databases, 346-361, 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='voting-classifier'></a> 1.11.4. **Classifieur de vote**<br/>([_Voting Classifier_](https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier))\n",
    "\n",
    "L'id√©e derri√®re le **Classifieur de vote** ([**`VotingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier)) est de combiner des classifieurs d'apprentissage automatique conceptuellement diff√©rents et d'utiliser un vote √† la majorit√© ou les probabilit√©s pr√©dites moyennes (vote doux) pour pr√©dire les √©tiquettes de classe. Un tel classifieur peut √™tre utile pour un ensemble de mod√®les qui performe de mani√®re √©quivalente afin d'√©quilibrer leurs faiblesses individuelles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='voting-classifier'></a> 1.11.4.1. **√âtiquettes de classe majoritaires (Vote majoritaire/Dur)**<br/>([_Majority Class Labels (Majority/Hard Voting)_](https://scikit-learn.org/stable/modules/ensemble.html#majority-class-labels-majority-hard-voting))\n",
    "\n",
    "Dans le vote majoritaire, l'√©tiquette de classe pr√©dite pour un √©chantillon particulier est l'√©tiquette de classe qui repr√©sente la majorit√© (mode) des √©tiquettes de classe pr√©dites par chaque classifieur individuel.\n",
    "\n",
    "Par exemple, si la pr√©diction pour un √©chantillon donn√© est la suivante :\n",
    "- classifieur 1 -> classe 1\n",
    "- classifieur 2 -> classe 1\n",
    "- classifieur 3 -> classe 2\n",
    "\n",
    "Le [**`VotingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier) (avec `voting='hard'`) classerait l'√©chantillon comme \"classe 1\" en se basant sur l'√©tiquette de classe majoritaire.\n",
    "\n",
    "En cas d'√©galit√©, le [**`VotingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier) s√©lectionnera la classe en fonction de l'ordre de tri ascendant. Par exemple, dans le sc√©nario suivant :\n",
    "- classifieur 1 -> classe 2\n",
    "- classifieur 2 -> classe 1\n",
    "\n",
    "L'√©tiquette de classe 1 sera attribu√©e √† l'√©chantillon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='id24'></a> 1.11.4.2. **Utilisation**<br/>([_Usage_](https://scikit-learn.org/stable/modules/ensemble.html#id24))\n",
    "\n",
    "L'exemple suivant montre comment ajuster le classifieur de r√®gle de la majorit√© :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95 (+/- 0.04) [Logistic Regression]\n",
      "Accuracy: 0.94 (+/- 0.04) [Random Forest]\n",
      "Accuracy: 0.91 (+/- 0.04) [naive Bayes]\n",
      "Accuracy: 0.95 (+/- 0.04) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "\n",
    "eclf = VotingClassifier(\n",
    "    estimators=[(\"lr\", clf1), (\"rf\", clf2), (\"gnb\", clf3)],\n",
    "    voting=\"hard\"\n",
    ")\n",
    "\n",
    "for clf, label in zip(\n",
    "    [clf1, clf2, clf3, eclf],\n",
    "    ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']\n",
    "):\n",
    "    scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "\n",
    "# Accuracy: 0.95 (+/- 0.04) [Logistic Regression]\n",
    "# Accuracy: 0.94 (+/- 0.04) [Random Forest]\n",
    "# Accuracy: 0.91 (+/- 0.04) [naive Bayes]\n",
    "# Accuracy: 0.95 (+/- 0.04) [Ensemble]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='weighted-average-probabilities-soft-voting'></a> 1.11.4.3. **Probabilit√©s moyennes pond√©r√©es (Vote doux)**<br/>([_Weighted Average Probabilities (Soft Voting)_](https://scikit-learn.org/stable/modules/ensemble.html#weighted-average-probabilities-soft-voting))\n",
    "\n",
    "Contrairement au vote majoritaire (hard voting), le vote doux renvoie l'√©tiquette de classe comme √©tant l'argmax de la somme des probabilit√©s pr√©dites.\n",
    "\n",
    "Des poids sp√©cifiques peuvent √™tre attribu√©s √† chaque classifieur via le param√®tre `weights`. Lorsque des poids sont fournis, les probabilit√©s de classe pr√©dites pour chaque classifieur sont collect√©es, multipli√©es par le poids du classifieur, et moyenn√©es. L'√©tiquette de classe finale est ensuite d√©riv√©e de l'√©tiquette de classe avec la probabilit√© moyenne la plus √©lev√©e.\n",
    "\n",
    "Pour illustrer cela avec un exemple simple, supposons que nous ayons 3 classifieurs et un probl√®me de classification √† 3 classes o√π nous attribuons des poids √©gaux √† tous les classifieurs : w1=1, w2=1, w3=1.\n",
    "\n",
    "Les probabilit√©s moyennes pond√©r√©es pour un √©chantillon seraient alors calcul√©es comme suit :\n",
    "\n",
    "| classifieur       | classe 1  | classe 2  | classe 3  |\n",
    "| ----------------- | --------- | --------- | --------- |\n",
    "| classifieur 1     | w1 \\* 0,2 | w1 \\* 0,5 | w1 \\* 0,3 |\n",
    "| classifieur 2     | w2 \\* 0,6 | w2 \\* 0,3 | w2 \\* 0,1 |\n",
    "| classifieur 3     | w3 \\* 0,3 | w3 \\* 0,4 | w3 \\* 0,3 |\n",
    "| moyenne pond√©r√©e  | 0,37      | 0,4       | 0,23      |\n",
    "\n",
    "Ici, l'√©tiquette de classe pr√©dite est 2, car elle a la probabilit√© moyenne la plus √©lev√©e.\n",
    "\n",
    "L'exemple suivant illustre comment les r√©gions de d√©cision peuvent changer lorsqu'un [**`VotingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier) doux est utilis√© en se basant sur une machine √† vecteurs de support lin√©aire, un arbre de d√©cision et un classifieur des k plus proches voisins :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from itertools import product\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Loading some example data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [0, 2]]\n",
    "y = iris.target\n",
    "\n",
    "# Training classifiers\n",
    "clf1 = DecisionTreeClassifier(max_depth=4)\n",
    "clf2 = KNeighborsClassifier(n_neighbors=7)\n",
    "clf3 = SVC(kernel='rbf', probability=True)\n",
    "eclf = VotingClassifier(\n",
    "    estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)],\n",
    "    voting='soft', weights=[2, 1, 2]\n",
    ")\n",
    "\n",
    "clf1 = clf1.fit(X, y)\n",
    "clf2 = clf2.fit(X, y)\n",
    "clf3 = clf3.fit(X, y)\n",
    "eclf = eclf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_voting_decision_regions_001.png\"\n",
    "    alt=\"Fronti√®res de d√©cision d'un classifieur de vote\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "### <a id='using-the-votingclassifier-with-gridsearchcv'></a> 1.11.4.4. **Utilisation du `VotingClassifier` avec `GridSearchCV`**<br/>([_Using the `VotingClassifier` with `GridSearchCV`_](https://scikit-learn.org/stable/modules/ensemble.html#using-the-votingclassifier-with-gridsearchcv))\n",
    "\n",
    "Le [**`VotingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier) peut √©galement √™tre utilis√© en conjonction avec [**`GridSearchCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) afin d'ajuster les hyperparam√®tres des estimateurs individuels :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "eclf = VotingClassifier(\n",
    "    estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "params = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200]}\n",
    "\n",
    "grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\n",
    "grid = grid.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='id25'></a> 1.11.4.5. **Utilisation**<br/>([_Usage_](https://scikit-learn.org/stable/modules/ensemble.html#id25))\n",
    "\n",
    "Afin de pr√©dire les √©tiquettes de classe en fonction des probabilit√©s de classe pr√©dites (les estimateurs de scikit-learn dans le [**`VotingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier) doivent prendre en charge la m√©thode `predict_proba`) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclf = VotingClassifier(\n",
    "    estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
    "    voting='soft'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionnellement, des poids peuvent √™tre fournis pour les classificateurs individuels :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclf = VotingClassifier(\n",
    "    estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
    "    voting='soft', weights=[2,5,1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**Visualiser les fronti√®res de d√©cision d'un `VotingClassifier`**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_voting_decision_regions.ipynb)<br/>([_Plot the decision boundaries of a `VotingClassifier`_](https://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='voting-regressor'></a> 1.11.5. **R√©gresseur de vote**<br/>([_Voting Regressor_](https://scikit-learn.org/stable/modules/ensemble.html#voting-regressor))\n",
    "\n",
    "L'id√©e derri√®re le [**`VotingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html#sklearn.ensemble.VotingRegressor) est de combiner des r√©gresseurs d'apprentissage automatique conceptuellement diff√©rents et de renvoyer les valeurs pr√©dites moyennes. Un tel r√©gresseur peut √™tre utile pour un ensemble de mod√®les de performances √©quivalentes afin d'√©quilibrer leurs faiblesses individuelles.\n",
    "\n",
    "### <a id='id27'></a> 1.11.5.5. **Utilisation**<br/>([_Usage_](https://scikit-learn.org/stable/modules/ensemble.html#id27))\n",
    "\n",
    "L'exemple suivant montre comment ajuster le [**`VotingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html#sklearn.ensemble.VotingRegressor) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# Loading some example data\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "\n",
    "# Training classifiers\n",
    "reg1 = GradientBoostingRegressor(random_state=1)\n",
    "reg2 = RandomForestRegressor(random_state=1)\n",
    "reg3 = LinearRegression()\n",
    "ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])\n",
    "ereg = ereg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_voting_regressor_001.png\"\n",
    "    alt=\"Pr√©dictions individuelles et de vote\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Visualisation des pr√©dictions individuelles et de r√©gression par vote**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensemble/plot_bias_variance.ipynb)<br/>([_Plot individual and voting regression predictions_](https://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='stacked-generalization'></a> 1.11.6. **Empilement g√©n√©ralis√©**<br/>([_Stacked generalization_](https://scikit-learn.org/stable/modules/ensemble.html#stacked-generalization))\n",
    "\n",
    "L'empilement g√©n√©ralis√© est une m√©thode pour combiner des estimateurs afin de r√©duire leurs biais [W1992] [HTF]. Plus pr√©cis√©ment, les pr√©dictions de chaque estimateur individuel sont empil√©es ensemble et utilis√©es en entr√©e pour un estimateur final afin de calculer la pr√©diction. Cet estimateur final est entra√Æn√© par validation crois√©e.\n",
    "\n",
    "Le [**`StackingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) et le [**`StackingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html#sklearn.ensemble.StackingRegressor) fournissent de telles strat√©gies qui peuvent √™tre appliqu√©es aux probl√®mes de classification et de r√©gression.\n",
    "\n",
    "Le param√®tre `estimators` correspond √† la liste des estimateurs qui sont empil√©s en parall√®le sur les donn√©es d'entr√©e. Il doit √™tre fourni sous forme d'une liste de noms et d'estimateurs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "estimators = [\n",
    "    (\"ridge\", RidgeCV()),\n",
    "    (\"lasso\", LassoCV(random_state=42)),\n",
    "    (\"knr\", KNeighborsRegressor(n_neighbors=20, metric=\"euclidean\"))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le `final_estimator` utilisera comme entr√©e les pr√©dictions des `estimators`. Il doit √™tre un classifieur ou un r√©gresseur lors de l'utilisation de [**`StackingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) ou [**`StackingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html#sklearn.ensemble.StackingRegressor), respectivement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "final_estimator = GradientBoostingRegressor(\n",
    "    n_estimators=25, subsample=0.5, min_samples_leaf=25, max_features=1,\n",
    "    random_state=42\n",
    ")\n",
    "reg = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=final_estimator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour entra√Æner les `estimators` et le `final_estimator`, la m√©thode `fit` doit √™tre appel√©e sur les donn√©es d'entra√Ænement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StackingRegressor(estimators=[(&#x27;ridge&#x27;, RidgeCV()),\n",
       "                              (&#x27;lasso&#x27;, LassoCV(random_state=42)),\n",
       "                              (&#x27;knr&#x27;,\n",
       "                               KNeighborsRegressor(metric=&#x27;euclidean&#x27;,\n",
       "                                                   n_neighbors=20))],\n",
       "                  final_estimator=GradientBoostingRegressor(max_features=1,\n",
       "                                                            min_samples_leaf=25,\n",
       "                                                            n_estimators=25,\n",
       "                                                            random_state=42,\n",
       "                                                            subsample=0.5))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StackingRegressor</label><div class=\"sk-toggleable__content\"><pre>StackingRegressor(estimators=[(&#x27;ridge&#x27;, RidgeCV()),\n",
       "                              (&#x27;lasso&#x27;, LassoCV(random_state=42)),\n",
       "                              (&#x27;knr&#x27;,\n",
       "                               KNeighborsRegressor(metric=&#x27;euclidean&#x27;,\n",
       "                                                   n_neighbors=20))],\n",
       "                  final_estimator=GradientBoostingRegressor(max_features=1,\n",
       "                                                            min_samples_leaf=25,\n",
       "                                                            n_estimators=25,\n",
       "                                                            random_state=42,\n",
       "                                                            subsample=0.5))</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>ridge</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RidgeCV</label><div class=\"sk-toggleable__content\"><pre>RidgeCV()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lasso</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LassoCV</label><div class=\"sk-toggleable__content\"><pre>LassoCV(random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>knr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsRegressor</label><div class=\"sk-toggleable__content\"><pre>KNeighborsRegressor(metric=&#x27;euclidean&#x27;, n_neighbors=20)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(max_features=1, min_samples_leaf=25, n_estimators=25,\n",
       "                          random_state=42, subsample=0.5)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "StackingRegressor(estimators=[('ridge', RidgeCV()),\n",
       "                              ('lasso', LassoCV(random_state=42)),\n",
       "                              ('knr',\n",
       "                               KNeighborsRegressor(metric='euclidean',\n",
       "                                                   n_neighbors=20))],\n",
       "                  final_estimator=GradientBoostingRegressor(max_features=1,\n",
       "                                                            min_samples_leaf=25,\n",
       "                                                            n_estimators=25,\n",
       "                                                            random_state=42,\n",
       "                                                            subsample=0.5))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "reg.fit(X_train, y_train)\n",
    "# StackingRegressor(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pendant l'entra√Ænement, les `estimators` sont ajust√©s sur l'ensemble complet des donn√©es d'entra√Ænement `X_train`. Ils seront utilis√©s lors de l'appel √† `predict` ou `predict_proba`. Pour g√©n√©raliser et √©viter le sur-ajustement, le `final_estimator` est entra√Æn√© sur des donn√©es hors-√©chantillon en utilisant [**`sklearn.model_selection.cross_val_predict`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html#sklearn.model_selection.cross_val_predict) en interne.\n",
    "\n",
    "Pour [**`StackingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier), notez que la sortie des `estimators` est contr√¥l√©e par le param√®tre `stack_method` et est appel√©e par chaque estimateur. Ce param√®tre est soit une cha√Æne de caract√®res, √©tant des noms de m√©thodes d'estimateur, soit `'auto'`, qui identifiera automatiquement une m√©thode disponible en fonction de la disponibilit√©, test√©e dans l'ordre de pr√©f√©rence : `predict_proba`, `decision_function` et `predict`.\n",
    "\n",
    "Un [**`StackingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html#sklearn.ensemble.StackingRegressor) et [**`StackingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) peuvent √™tre utilis√©s comme tout autre r√©gresseur ou classifieur, exposant des m√©thodes `predict`, `predict_proba` et `decision_function`, par exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.53\n"
     ]
    }
   ],
   "source": [
    "y_pred = reg.predict(X_test)\n",
    "from sklearn.metrics import r2_score\n",
    "print(f\"R2 score: {r2_score(y_test, y_pred):.2f}\")\n",
    "# R2 score: 0.53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez qu'il est √©galement possible d'obtenir la sortie des `estimators` empil√©s en utilisant la m√©thode `transform` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[142.36209608, 138.30724927, 146.1       ],\n",
       "       [179.700576  , 182.89812552, 151.75      ],\n",
       "       [139.89817956, 132.46803343, 158.25      ],\n",
       "       [286.95180286, 292.65695767, 225.4       ],\n",
       "       [126.88317154, 124.1215975 , 164.65      ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.transform(X_test[:5])\n",
    "# array([[142..., 138..., 146...],\n",
    "#        [179..., 182..., 151...],\n",
    "#        [139..., 132..., 158...],\n",
    "#        [286..., 292..., 225...],\n",
    "#        [126..., 124..., 164...]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En pratique, un pr√©dicteur empil√© pr√©dit aussi bien que le meilleur pr√©dicteur de la couche de base et parfois le surpasse en combinant les diff√©rentes forces de ces pr√©dicteurs. Cependant, l'entra√Ænement d'un pr√©dicteur empil√© est co√ªteux en termes de calcul.\n",
    "\n",
    "> **Note :** Pour [**`StackingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier), lors de l'utilisation de `stack_method_='predict_proba'`, la premi√®re colonne est supprim√©e lorsque le probl√®me est un probl√®me de classification binaire. En effet, les deux colonnes de probabilit√© pr√©dites par chaque estimateur sont parfaitement colin√©aires.\n",
    "\n",
    "> **Note :** Plusieurs couches d'empilement peuvent √™tre r√©alis√©es en attribuant le `final_estimator` √† un [**`StackingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) ou [**`StackingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html#sklearn.ensemble.StackingRegressor) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.53\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "final_layer_rfr = RandomForestRegressor(\n",
    "    n_estimators=10, max_features=1, max_leaf_nodes=5,\n",
    "    random_state=42\n",
    ")\n",
    "final_layer_gbr = GradientBoostingRegressor(\n",
    "    n_estimators=10, max_features=1, max_leaf_nodes=5,\n",
    "    random_state=42\n",
    ")\n",
    "final_layer = StackingRegressor(\n",
    "    estimators=[\n",
    "        (\"rf\", final_layer_rfr),\n",
    "        (\"gbrt\", final_layer_gbr)\n",
    "    ],\n",
    "    final_estimator=RidgeCV()\n",
    ")\n",
    "multi_layer_regressor = StackingRegressor(\n",
    "    estimators=[\n",
    "        (\"ridge\", RidgeCV()),\n",
    "        (\"lasso\", LassoCV(random_state=42)),\n",
    "        (\"knr\", KNeighborsRegressor(n_neighbors=20, metric=\"euclidean\"))\n",
    "    ],\n",
    "    final_estimator=final_layer\n",
    ")\n",
    "multi_layer_regressor.fit(X_train, y_train)\n",
    "# StackingRegressor(...)\n",
    "print(f\"R2 score: {multi_layer_regressor.score(X_test, y_test):.2f}\")\n",
    "# R2 score: 0.53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R√©f√©rences\n",
    "\n",
    "üî¨ [W1992] Wolpert, David H. [**‚ÄúStacked generalization‚Äù**](http://machine-learning.martinsewell.com/ensembles/stacking/Wolpert1992.pdf). Neural networks 5.2 (1992): 241-259."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='adaboost'></a> 1.11.7. **AdaBoost**<br/>([_AdaBoost_](https://scikit-learn.org/stable/modules/ensemble.html#adaboost))\n",
    "\n",
    "\n",
    "Le module [**`sklearn.ensemble`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble) comprend l'algorithme de renforcement populaire AdaBoost, introduit en 1995 par Freund et Schapire [FS1995].\n",
    "\n",
    "Le principe fondamental d'AdaBoost est de s'adapter √† une s√©quence de mod√®les faibles (c'est-√†-dire des mod√®les qui sont l√©g√®rement meilleurs que de simples pr√©dictions al√©atoires, tels que de petits arbres de d√©cision) sur des versions modifi√©es √† plusieurs reprises des donn√©es. Les pr√©dictions de chacun d'entre eux sont ensuite combin√©es par le biais d'un vote majoritaire pond√©r√© (ou d'une somme) pour produire la pr√©diction finale. Les modifications des donn√©es √† chaque it√©ration d'amplification, appel√©e it√©ration de boosting, consistent √† appliquer des poids $w_1, w_2, \\cdots, w_N$ √† chacun des √©chantillons d'entra√Ænement. Initialement, ces poids sont tous d√©finis √† $w_i=1/N$, de sorte que la premi√®re √©tape consiste simplement √† former un mod√®le faible sur les donn√©es d'origine. √Ä chaque it√©ration successive, les poids des √©chantillons sont modifi√©s individuellement, et l'algorithme d'apprentissage est r√©appliqu√© aux donn√©es pond√©r√©es. √Ä une √©tape donn√©e, les exemples d'entra√Ænement qui ont √©t√© pr√©dits de mani√®re incorrecte par le mod√®le renforc√© induit √† l'√©tape pr√©c√©dente voient leurs poids augmenter, tandis que les poids sont diminu√©s pour ceux qui ont √©t√© pr√©dits correctement. √Ä mesure que les it√©rations avancent, les exemples difficiles √† pr√©dire re√ßoivent une influence de plus en plus importante. Chaque mod√®le faible subs√©quent est ainsi contraint de se concentrer sur les exemples que les mod√®les pr√©c√©dents de la s√©quence ont manqu√©s [HTF].\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_adaboost_hastie_10_2_001.png\"\n",
    "    alt=\"AdaBoost discret vs r√©el\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "AdaBoost peut √™tre utilis√© √† la fois pour les probl√®mes de classification et de r√©gression¬†:\n",
    "* Pour la classification multi-classe, [**`AdaBoostClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier) impl√©mente AdaBoost-SAMME et AdaBoost-SAMME.R [ZZRH2009].\n",
    "* Pour la r√©gression, [**`AdaBoostRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor) impl√©mente AdaBoost.R2 [D1997]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='id35'></a> 1.11.7.1. **Utilisation**<br/>([_Usage_](https://scikit-learn.org/stable/modules/ensemble.html#id35))\n",
    "\n",
    "L'exemple suivant montre comment ajuster un classificateur AdaBoost avec 100 apprenants faibles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9466666666666665"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()\n",
    "# 0.9..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre d'apprenants faibles est contr√¥l√© par le param√®tre `n_estimators`. Le param√®tre `learning_rate` contr√¥le la contribution des apprenants faibles dans la combinaison finale. Par d√©faut, les apprenants faibles sont des souches de d√©cision. Diff√©rents apprenants faibles peuvent √™tre sp√©cifi√©s via le param√®tre `base_estimator`. Les principaux param√®tres √† r√©gler pour obtenir de bons r√©sultats sont les `n_estimators` et la complexit√© des estimateurs de base (par exemple, sa profondeur `max_depth` ou le nombre minimum d'√©chantillons requis pour consid√©rer un fractionnement `min_samples_split`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**AdaBoost discret vs. r√©el**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_adaboost_hastie_10_2.ipynb)<br/>([*Discrete versus Real AdaBoost*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_hastie_10_2.html))\n",
    "\n",
    "Compare l'erreur de classification d'une souche de d√©cision, d'un arbre de d√©cision et d'une souche de d√©cision amplifi√©e √† l'aide d'AdaBoost-SAMME et d'AdaBoost-SAMME.R.\n",
    "\n",
    "#### [**Arbres de d√©cision AdaBoosted multi-classe**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_adaboost_multiclass.ipynb)<br/>([*Multi-class AdaBoosted Decision Trees*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_multiclass.html))\n",
    "\n",
    "Montre les performances d'AdaBoost-SAMME et d'AdaBoost-SAMME.R sur un probl√®me multi-classe.\n",
    "\n",
    "#### [**AdaBoost √† deux classes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_adaboost_twoclass.ipynb)<br/>([*Two-class AdaBoost*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_twoclass.html))\n",
    "\n",
    "Montre les valeurs de la limite de d√©cision et de la fonction de d√©cision pour un probl√®me √† deux classes non lin√©airement s√©parable √† l'aide d'AdaBoost-SAMME.\n",
    "\n",
    "#### [**R√©gression d'arbre de d√©cision avec AdaBoost**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_adaboost_regression.ipynb)<br/>([*Decision Tree Regression with AdaBoost*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_regression.html))\n",
    "\n",
    "Illustre la r√©gression avec l'algorithme AdaBoost.R2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R√©f√©rences\n",
    "\n",
    "üî¨ [FS1995] Y. Freund, and R. Schapire, [‚Äú**A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting‚Äù**](https://reader.elsevier.com/reader/sd/pii/S002200009791504X?token=AD1674DDF28DD408B7DDD9B0CDADA79A121E6BB81335081764FF30C2992E13A694C3122F25D23A16EB9B688BE2D01526&originRegion=eu-west-1&originCreation=20221106045206), 1997.\n",
    "\n",
    "üî¨ [ZZRH2009] J Zhu, H. Zou, S. Rosset, T. Hastie, [**‚ÄúMulti-class AdaBoost‚Äù**](https://www.intlpress.com/site/pub/files/_fulltext/journals/sii/2009/0002/0003/SII-2009-0002-0003-a008.pdf), Statistics and Its Interface, 2009.\n",
    "\n",
    "üî¨ [D1997] Drucker. [‚Äú**Improving Regressors using Boosting Techniques‚Äù**](https://www.researchgate.net/profile/Harris-Drucker/publication/2424244_Improving_Regressors_Using_Boosting_Techniques/links/0deec51ae736538cec000000/Improving-Regressors-Using-Boosting-Techniques.pdf), 1997.\n",
    "\n",
    "üìö [HTF] (1,2,3)  T. Hastie, R. Tibshirani et J. Friedman, [**‚ÄúElements of Statistical Learning Ed. 2‚Äù**](https://hastie.su.domains/Papers/ESLII.pdf), Springer, 2009."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e03b612d84ba21ce95ed447e81b3062e1eb99b56c6d885cdab4aaa12f1b8e240"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
