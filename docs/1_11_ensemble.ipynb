{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='supervised-learning'></a> 1. [**Apprentissage supervisé**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_supervised_learning.ipynb#supervised-learning)</br>([*Supervised learning*](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning))\n",
    "\n",
    "# <a id='ensembles-gradient-boosting-random-forests-bagging-voting-stacking'></a> 1.11. [**Méthodes ensemblistes : Gradient boosting, random forests, bagging, voting, stacking**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb)<br/>([_Ensemble methods: Gradient boosting, random forests, bagging, voting, stacking_](https://scikit-learn.org/stable/modules/ensemble.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 31 pages, 24 exemples, 19 papiers\n",
    "- 1.11.1. [**Arbres de décision à gradient amplifié**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#gradient-boosted-trees)<br/>([_Gradient-boosted trees_](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosted-trees))\n",
    "- 1.11.1.1. [**Amplification de gradient basée sur un histogramme**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#histogram-based-gradient-boosting)<br/>([_Histogram-Based Gradient Boosting_](https://scikit-learn.org/stable/modules/ensemble.html#histogram-based-gradient-boosting))\n",
    "    - 1.11.1.1.1. [**Utilisation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#usage)<br/>([_Usage_](https://scikit-learn.org/stable/modules/ensemble.html#usage))\n",
    "    - 1.11.1.1.2. [**Prise en charge des valeurs manquantes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#missing-values-support)<br/>([_Missing values support_](https://scikit-learn.org/stable/modules/ensemble.html#missing-values-support))\n",
    "    - 1.11.1.1.3. [**Prise en charge du poids des échantillons**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#sample-weight-support)<br/>([_Missing values support_](https://scikit-learn.org/stable/modules/ensemble.html#sample-weight-support))\n",
    "    - 1.11.1.1.4. [**Prise en charge des caractéristiques catégorielles**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#categorical-features-support)<br/>([_Categorical Features Support_](https://scikit-learn.org/stable/modules/ensemble.html#categorical-features-support))\n",
    "    - 1.11.1.1.5. [**Contraintes monotones**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#monotonic-constraints)<br/>([_Monotonic Constraints_](https://scikit-learn.org/stable/modules/ensemble.html#monotonic-constraints))\n",
    "    - 1.11.1.1.6. [**Contraintes d'interaction**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#interaction-constraints)<br/>([_Interaction constraints_](https://scikit-learn.org/stable/modules/ensemble.html#interaction-constraints))\n",
    "    - 1.11.1.1.7. [**Parallélisme de bas niveau**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#low-level-parallelism)<br/>([_Low-level parallelism_](https://scikit-learn.org/stable/modules/ensemble.html#low-level-parallelism))\n",
    "    - 1.11.1.1.8. [**Pourquoi c'est plus rapide**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#why-it-s-faster)<br/>([_Why it’s faster_](https://scikit-learn.org/stable/modules/ensemble.html#why-it-s-faster))\n",
    "- 1.11.1.2. [**`GradientBoostingClassifier` et `GradientBoostingRegressor`**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#gradientboostingclassifier-and-gradientboostingregressor)<br/>([_**`GradientBoostingClassifier`** and **`GradientBoostingRegressor`**_](https://scikit-learn.org/stable/modules/ensemble.html#gradientboostingclassifier-and-gradientboostingregressor))\n",
    "    - 1.11.1.2.1. [**Classification**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#classification)<br/>([_Classification_](https://scikit-learn.org/stable/modules/ensemble.html#classification))\n",
    "    - 1.11.1.2.2. [**Régression**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#regression)<br/>([_Regression_](https://scikit-learn.org/stable/modules/ensemble.html#regression))\n",
    "    - 1.11.1.2.3. [**Ajustement d'apprenants faibles supplémentaires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#fitting-additional-weak-learners)<br/>([_Fitting additional weak-learners_](https://scikit-learn.org/stable/modules/ensemble.html#fitting-additional-weak-learners))\n",
    "    - 1.11.1.2.4. [**Contrôle de la taille de l'arbre**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#controlling-the-tree-size)<br/>([_Controlling the tree size_](https://scikit-learn.org/stable/modules/ensemble.html#controlling-the-tree-size))\n",
    "    - 1.11.1.2.5. [**Formulation mathématique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#mathematical-formulation)<br/>([_Mathematical formulation_](https://scikit-learn.org/stable/modules/ensemble.html#mathematical-formulation))\n",
    "    - 1.11.1.2.6. [**Fonctions de perte**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#loss-functions)<br/>([_Loss Functions_](https://scikit-learn.org/stable/modules/ensemble.html#loss-functions))\n",
    "    - 1.11.1.2.7. [**Réduction via le taux d'apprentissage**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#shrinkage-via-learning-rate)<br/>([_Shrinkage via learning rate_](https://scikit-learn.org/stable/modules/ensemble.html#shrinkage-via-learning-rate))\n",
    "    - 1.11.1.2.8. [**Sous-échantillonnage**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#subsampling)<br/>([_Subsampling_](https://scikit-learn.org/stable/modules/ensemble.html#subsampling))\n",
    "    - 1.11.1.2.9. [**Interprétation avec importance des caractéristiques**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#interpretation-with-feature-importance)<br/>([_Interpretation with feature importance_](https://scikit-learn.org/stable/modules/ensemble.html#interpretation-with-feature-importance))\n",
    "- 1.11.2. [**Forêts aléatoires et autres ensembles d'arbres aléatoires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#random-forests-and-other-randomized-tree-ensembles)<br/>([_Random forests and other randomized tree ensembles_](https://scikit-learn.org/stable/modules/ensemble.html#random-forests-and-other-randomized-tree-ensembles))\n",
    "    - 1.11.2.1. [**Forêts aléatoires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#random-forests)<br/>([_Random Forests_](https://scikit-learn.org/stable/modules/ensemble.html#random-forests))\n",
    "    - 1.11.2.2. [**Arbres extrêmement aléatoires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#extremely-randomized-trees)<br/>([_Extremely Randomized Trees_](https://scikit-learn.org/stable/modules/ensemble.html#extremely-randomized-trees))\n",
    "    - 1.11.2.3. [**Paramètres**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#parameters)<br/>([_Parameters_](https://scikit-learn.org/stable/modules/ensemble.html#parameters))\n",
    "    - 1.11.2.4. [**Parallélisation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#parallelization)<br/>([_Parallelization_](https://scikit-learn.org/stable/modules/ensemble.html#parallelization))\n",
    "    - 1.11.2.5. [**Évaluation de l'importance des caractéristiques**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#feature-importance-evaluation)<br/>([_Feature importance evaluation_](https://scikit-learn.org/stable/modules/ensemble.html#feature-importance-evaluation))\n",
    "    - 1.11.2.6. [**Intégration d'arbres totalement aléatoires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#totally-random-trees-embedding)<br/>([_Totally Random Trees Embedding_](https://scikit-learn.org/stable/modules/ensemble.html#totally-random-trees-embedding))\n",
    "- 1.11.3. [**Meta-estimation par bagging (ensachage)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#bagging-meta-estimator)<br/>([_Bagging meta-estimator_](https://scikit-learn.org/stable/modules/ensemble.html#bagging-meta-estimator))\n",
    "- 1.11.4. [**Classifieur de vote**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#voting-classifier)<br/>([_Voting Classifier_](https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier))\n",
    "    - 1.11.4.1. [**Étiquettes de classe majoritaires (Vote majoritaire/Dur)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#majority-class-labels-majority-hard-voting)<br/>([_Majority Class Labels (Majority/Hard Voting)_](https://scikit-learn.org/stable/modules/ensemble.html#majority-class-labels-majority-hard-voting))\n",
    "    - 1.11.4.2. [**Utilisation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#id24)<br/>([_Usage_](https://scikit-learn.org/stable/modules/ensemble.html#id24))\n",
    "    - 1.11.4.3. [**Probabilités moyennes pondérées (Vote doux)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#weighted-average-probabilities-soft-voting)<br/>([_Weighted Average Probabilities (Soft Voting)_](https://scikit-learn.org/stable/modules/ensemble.html#weighted-average-probabilities-soft-voting))\n",
    "    - 1.11.4.4. [**Utilisation du `VotingClassifier` avec `GridSearchCV`**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#using-the-votingclassifier-with-gridsearchcv)<br/>([_Using the `VotingClassifier` with `GridSearchCV`_](https://scikit-learn.org/stable/modules/ensemble.html#using-the-votingclassifier-with-gridsearchcv))\n",
    "    - 1.11.4.5. [**Utilisation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#id25)<br/>([_Usage_](https://scikit-learn.org/stable/modules/ensemble.html#id25))\n",
    "- 1.11.5. [**Régresseur de vote**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#voting-regressor)<br/>([_Voting Regressor_](https://scikit-learn.org/stable/modules/ensemble.html#voting-regressor))\n",
    "    - 1.11.5.5. [**Utilisation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#id27)<br/>([_Usage_](https://scikit-learn.org/stable/modules/ensemble.html#id27))\n",
    "- 1.11.6. [**Empilement généralisé**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#stacked-generalization)<br/>([_Stacked generalization_](https://scikit-learn.org/stable/modules/ensemble.html#stacked-generalization))\n",
    "- 1.11.7. [**AdaBoost**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#adaboost)<br/>([_AdaBoost_](https://scikit-learn.org/stable/modules/ensemble.html#adaboost))\n",
    "    - 1.11.7.1. [**Utilisation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_11_ensemble.ipynb#id35)<br/>([_Usage_](https://scikit-learn.org/stable/modules/ensemble.html#id35))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img alt=\"wkpd\" src=\"https://upload.wikimedia.org/wikipedia/en/thumb/8/80/Wikipedia-logo-v2.svg/1200px-Wikipedia-logo-v2.svg.png\" style=\"width: 30.0px; height: 30.0px\" /></a> Wikipedia\n",
    "\n",
    "[**Ensemble learning**](https://en.wikipedia.org/wiki/Ensemble_learning) ([*Apprentissage ensembliste*](https://fr.wikipedia.org/wiki/Apprentissage_ensembliste))\n",
    "\n",
    "[**Bootstrap aggregating (~Bagging)**](https://en.wikipedia.org/wiki/Bootstrap_aggregating) ([***B****ootstrap* ***agg****regat****ing*** *(~Ensachage)*](https://fr.wikipedia.org/wiki/Bootstrap_aggregating))\n",
    "\n",
    "[**Boosting (machine learning)**](https://en.wikipedia.org/wiki/Boosting_(machine_learning)) ([*Boosting*](https://fr.wikipedia.org/wiki/Boosting))\n",
    "\n",
    "[**Random forest**](https://en.wikipedia.org/wiki/Random_forest) ([*Forêt aléatoire*](https://fr.wikipedia.org/wiki/For%C3%AAt_d%27arbres_d%C3%A9cisionnels))\n",
    "\n",
    "[**AdaBoost**](https://en.wikipedia.org/wiki/AdaBoost) ([*AdaBoost*](https://fr.wikipedia.org/wiki/AdaBoost))\n",
    "* [**Decision stump**](https://en.wikipedia.org/wiki/Decision_stump) ([*Souche de décision*]())\n",
    "\n",
    "[**Gradient boosting**](https://en.wikipedia.org/wiki/Gradient_boosting) ([*Gradient boosting*]())\n",
    "\n",
    "[**Out-of-bag error (OOB)**](https://en.wikipedia.org/wiki/Out-of-bag_error) ([*Erreur hors sac*]())\n",
    "\n",
    "[****]() ([**]())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='ensembles-gradient-boosting-random-forests-bagging-voting-stacking'></a> 1.11. **Méthodes ensemblistes : Gradient boosting, random forests, bagging, voting, stacking**<br/>([_Ensemble methods: Gradient boosting, random forests, bagging, voting, stacking_](https://scikit-learn.org/stable/modules/ensemble.html))\n",
    "\n",
    "Les **méthodes ensemblistes** combinent les prédictions de plusieurs estimateurs de base construits avec un algorithme d'apprentissage donné afin d'améliorer la généralisation et la robustesse par rapport à un seul estimateur.\n",
    "\n",
    "Deux exemples très célèbres de méthodes ensemblistes sont les [**arbres à gradient amplifié** (1.11.1)](#gradient-boosted-trees) et les [**forêts aléatoires** (1.11.2)](#random-forests-and-other-randomized-tree-ensembles).\n",
    "\n",
    "De manière plus générale, les modèles ensemblistes peuvent être appliqués à tout apprenant de base au-delà des arbres, dans des méthodes de moyenne telles que les [**méthodes de Bagging** (1.11.3)](#bagging-meta-estimator), le [**stacking de modèles** (1.11.6)](#stacked-generalization) ou le [**Vote** (1.11.4)](#voting-classifier), ou dans des méthodes de renforcement, comme [**AdaBoost** (1.11.7)](#adaboost)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='gradient-boosted-trees'></a> 1.11.1. **Arbres de décision à gradient amplifié**<br/>([_Gradient-boosted trees_](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosted-trees))\n",
    "\n",
    "[**🌐 Gradient Tree Boosting**](https://en.wikipedia.org/wiki/Gradient_boosting), ou arbres de décision à gradient amplifié (GBDT), est une généralisation de l'amplification qui s'applique à des fonctions de perte arbitrairement différentiables. Pour en savoir plus, consultez le travail fondateur de [Friedman2001]. GBDT est un modèle exceptionnel pour la régression et la classification, notamment pour les données tabulaires.\n",
    "\n",
    "### [**`GradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) vs [**`HistGradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier)\n",
    "\n",
    "Scikit-learn propose deux implémentations d'arbres à gradient amplifié : [**`HistGradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier) vs [**`GradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) pour la classification, ainsi que les classes correspondantes pour la régression. La première peut être **des ordres de grandeur plus rapide** que la seconde lorsque le nombre d'échantillons est supérieur à plusieurs dizaines de milliers.\n",
    "\n",
    "Les valeurs manquantes et les données catégorielles sont prises en charge nativement par la version `Hist...`, éliminant ainsi le besoin de prétraitements supplémentaires tels que l'imputation.\n",
    "\n",
    "[**`GradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) et [**`GradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor) pourraient être préférés pour de petits jeux de données, car la discrétisation peut entraîner des points de division qui sont trop approximatifs dans ce contexte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='histogram-based-gradient-boosting'></a> 1.11.1.1. **Amplification de gradient basée sur un histogramme**<br/>([_Histogram-Based Gradient Boosting_](https://scikit-learn.org/stable/modules/ensemble.html#histogram-based-gradient-boosting))\n",
    "\n",
    "Scikit-learn 0.21 a introduit deux nouvelles implémentations d'arbres à gradient amplifié, à savoir [**`HistGradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier) et HistGradientBoostingRegressor, inspirées de [**LightGBM**](https://github.com/Microsoft/LightGBM) (voir [LightGBM]).\n",
    "\n",
    "Ces estimateurs basés sur des histogrammes peuvent être **plus rapides de plusieurs ordres de grandeur** que [**`GradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) et [**`GradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor) lorsque le nombre d'échantillons est supérieur à plusieurs dizaines de milliers.\n",
    "\n",
    "Ils prennent également en charge nativement les valeurs manquantes, ce qui évite la nécessité d'un outil de remplacement.\n",
    "\n",
    "Ces estimateurs rapides regroupent d'abord les échantillons d'entrée `X` en bacs à valeurs entières (typiquement 256 bacs), ce qui réduit considérablement le nombre de points de division à considérer, et permet à l'algorithme d'utiliser des structures de données basées sur des entiers (histogrammes) au lieu de dépendre de valeurs continues triées lors de la construction des arbres. L'API de ces estimateurs est légèrement différente, et certaines des fonctionnalités de [**`GradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) et [**`GradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor) ne sont pas encore prises en charge, par exemple certaines fonctions de perte.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Graphiques de dépendance partielle et d'espérance conditionnelle individuelle**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/4_inspection/plot_partial_dependence.ipynb)<br/>([*Partial Dependence and Individual Conditional Expectation Plots*](https://scikit-learn.org/stable/auto_examples/inspection/plot_partial_dependence.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='usage'></a> 1.11.1.1.1. **Utilisation**<br/>([_Usage_](https://scikit-learn.org/stable/modules/ensemble.html#usage))\n",
    "\n",
    "La plupart des paramètres sont inchangés par rapport à [**`GradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) et [**`GradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor). Une exception est le paramètre `max_iter` qui remplace `n_estimators` et contrôle le nombre d'itérations du processus d'amplification :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8965"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "# 0.8965"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fonctions de perte disponibles pour la régression sont `'squared_error'`, `'absolute_error'`, qui est moins sensible aux valeurs aberrantes, et `'poisson'`, qui convient bien pour modéliser les comptages et les fréquences. Pour la classification, la seule option est `'log_loss'`. Pour la classification binaire, elle utilise la perte logarithmique binaire, également connue sous le nom de déviance binomiale ou entropie croisée binaire. Pour `n_classes >= 3`, elle utilise la fonction de perte logarithmique multi-classe, avec déviance multinomiale et entropie croisée catégorielle comme noms alternatifs. La version appropriée de la fonction de perte est sélectionnée en fonction de [**`y`**](https://scikit-learn.org/stable/glossary.html#term-y) transmis à [**`fit`**](https://scikit-learn.org/stable/glossary.html#term-fit).\n",
    "\n",
    "La taille des arbres peut être contrôlée à l'aide des paramètres `max_leaf_nodes`, `max_depth` et `min_samples_leaf`.\n",
    "\n",
    "Le nombre de bacs utilisés pour regrouper les données est contrôlé par le paramètre `max_bins`. Utiliser moins de bacs agit comme une forme de régularisation. Il est généralement recommandé d'utiliser autant de bacs que possible (256), ce qui est la valeur par défaut.\n",
    "\n",
    "Le paramètre de `l2_regularization` est un régularisateur sur la fonction de perte et correspond à $\\lambda$ dans l'équation (2) de [XGBoost].\n",
    "\n",
    "Notez que **l'arrêt précoce est activé par défaut si le nombre d'échantillons est supérieur à 10 000**. Le comportement de l'arrêt précoce est contrôlé via les paramètres `early_stopping`, `scoring`, `validation_fraction`, `n_iter_no_change`, et `tol`. Il est possible de mettre fin prématurément en utilisant un [**scorer**](https://scikit-learn.org/stable/glossary.html#term-scorer) arbitraire, ou simplement la perte d'entraînement ou de validation. Notez que pour des raisons techniques, l'utilisation d'un scorer est significativement plus lente que l'utilisation de la perte. Par défaut, l'arrêt précoce est effectué s'il y a au moins 10 000 échantillons dans l'ensemble d'entraînement et utilise la perte de validation.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Arrêt anticipé du Gradient Boosting**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_gradient_boosting_early_stopping.ipynb)<br/>([_Early stopping of Gradient Boosting_](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_early_stopping.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='missing-values-support'></a> 1.11.1.1.2. **Prise en charge des valeurs manquantes**<br/>([_Missing values support_](https://scikit-learn.org/stable/modules/ensemble.html#missing-values-support))\n",
    "\n",
    "[**`HistGradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier) et [**`HistGradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor) prennent en charge les valeurs manquantes (NaN).\n",
    "\n",
    "Pendant l'entraînement, l'algorithme d'arbre décide à chaque point de division si les échantillons avec des valeurs manquantes doivent aller vers l'enfant gauche ou droit, en se basant sur le gain potentiel. Lors de la prédiction, les échantillons avec des valeurs manquantes sont assignés en conséquence à l'enfant gauche ou droit :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([0, 1, 2, np.nan]).reshape(-1, 1)\n",
    "y = [0, 0, 1, 1]\n",
    "\n",
    "gbdt = HistGradientBoostingClassifier(min_samples_leaf=1).fit(X, y)\n",
    "gbdt.predict(X)\n",
    "# array([0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque le modèle prévoit un schéma de données manquantes, les divisions peuvent être effectuées en fonction de l'absence ou de la présence des valeurs manquantes dans la caractéristique :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([0, np.nan, 1, 2, np.nan]).reshape(-1, 1)\n",
    "y = [0, 1, 0, 0, 1]\n",
    "gbdt = HistGradientBoostingClassifier(\n",
    "    min_samples_leaf=1,\n",
    "    max_depth=2,\n",
    "    learning_rate=1,\n",
    "    max_iter=1\n",
    ").fit(X, y)\n",
    "gbdt.predict(X)\n",
    "# array([0, 1, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si aucune valeur manquante n'a été rencontrée pour une caractéristique donnée pendant l'entraînement, alors les échantillons avec des valeurs manquantes sont attribués à l'enfant qui a le plus d'échantillons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='sample-weight-support'></a> 1.11.1.1.3. **Prise en charge du poids des échantillons**<br/>([_Sample weight support_](https://scikit-learn.org/stable/modules/ensemble.html#sample-weight-support))\n",
    "\n",
    "[**`HistGradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier) et [**`HistGradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor) prennent en charge les poids d'échantillons lors de l'entraînement.\n",
    "\n",
    "L'exemple suivant montre que les échantillons avec un poids d'échantillon de zéro sont ignorés :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9990209190235209"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[1, 0], [1, 0], [1, 0], [0, 1]]\n",
    "y = [0, 0, 1, 0]\n",
    "# ignore the first 2 training samples by setting their weight to 0\n",
    "sample_weight = [0, 0, 1, 1]\n",
    "gb = HistGradientBoostingClassifier(min_samples_leaf=1)\n",
    "gb.fit(X, y, sample_weight=sample_weight)\n",
    "# HistGradientBoostingClassifier(...)\n",
    "gb.predict([[1, 0]])\n",
    "# array([1])\n",
    "gb.predict_proba([[1, 0]])[0, 1]\n",
    "# 0.99..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='categorical-features-support'></a> 1.11.1.1.4. **Prise en charge des caractéristiques catégorielles**<br/>([_Categorical Features Support_](https://scikit-learn.org/stable/modules/ensemble.html#categorical-features-support))\n",
    "\n",
    "[**`HistGradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier) et [**`HistGradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor) prennent en charge nativement les caractéristiques catégorielles : ils peuvent considérer des divisions sur des données catégorielles non ordonnées.\n",
    "\n",
    "Pour les ensembles de données avec des caractéristiques catégorielles, il est souvent préférable d'utiliser la prise en charge catégorielle native plutôt que de s'appuyer sur un encodage one-hot ([**`OneHotEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder)), car l'encodage one-hot nécessite une profondeur d'arbre plus grande pour atteindre des divisions équivalentes. Il est également généralement préférable de s'appuyer sur la prise en charge catégorielle native plutôt que de traiter les caractéristiques catégorielles comme continues (ordinales), ce qui se produit pour les données catégorielles encodées de manière ordonnée, car les catégories sont des quantités nominales où l'ordre n'a pas d'importance.\n",
    "\n",
    "Pour activer la prise en charge catégorielle, un masque booléen peut être passé au paramètre `categorical_features`, indiquant quelles caractéristiques sont catégorielles. Dans l'exemple suivant, la première caractéristique est traitée comme catégorielle et la deuxième comme numérique :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt = HistGradientBoostingClassifier(categorical_features=[True, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De manière équivalente, il est possible de passer une liste d'entiers indiquant les indices des caractéristiques catégorielles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt = HistGradientBoostingClassifier(categorical_features=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cardinalité de chaque caractéristique catégorielle doit être inférieure au paramètre `max_bins`, et chaque caractéristique catégorielle doit être encodée dans `[0, max_bins - 1]`. À cette fin, il peut être utile de prétraiter les données avec un [**`OrdinalEncoder`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder) comme indiqué dans [**Support des caractéristiques catégorielles dans l'amplification de gradient**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_gradient_boosting_categorical.ipynb).\n",
    "\n",
    "S'il y a des valeurs manquantes pendant l'apprentissage, les valeurs manquantes seront traitées comme une catégorie distincte. S'il n'y a pas de valeurs manquantes pendant l'apprentissage, alors au moment de la prédiction, les valeurs manquantes sont mappées vers le nœud enfant qui possède le plus d'échantillons (tout comme pour les caractéristiques continues). Lors de la prédiction, les catégories qui n'ont pas été observées au moment de l'apprentissage seront traitées comme des valeurs manquantes.\n",
    "\n",
    "**Recherche de divisions avec des caractéristiques catégorielles :** La manière canonique de considérer des divisions catégorielles dans un arbre consiste à prendre en compte toutes les $2^{K - 1} - 1$ partitions, où $K$ est le nombre de catégories. Cela peut rapidement devenir prohibitif lorsque $K$ est grand. Heureusement, étant donné que les arbres de gradient boosting sont toujours des arbres de régression (même pour les problèmes de classification), il existe une stratégie plus rapide qui peut fournir des divisions équivalentes. Tout d'abord, les catégories d'une caractéristique sont triées en fonction de la variance de la cible, pour chaque catégorie `k`. Une fois que les catégories sont triées, on peut considérer des partitions continues, c'est-à-dire traiter les catégories comme si elles étaient des valeurs continues ordonnées (voir Fisher [Fisher1958] pour une preuve formelle). En conséquence, seules $K - 1$ divisions doivent être prises en compte au lieu de $2^{K - 1} - 1$. Le tri initial est une opération en $\\mathcal{O}(K \\log(K))$, ce qui donne une complexité totale en $\\mathcal{O}(K \\log(K) + K)$, au lieu de $\\mathcal{O}(2^K)$.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Support des caractéristiques catégorielles dans l'amplification de gradient**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_gradient_boosting_categorical.ipynb)<br/>([_Categorical Feature Support in Gradient Boosting_](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_categorical.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='monotonic-constraints'></a> 1.11.1.1.5. **Contraintes monotones**<br/>([_Monotonic Constraints_](https://scikit-learn.org/stable/modules/ensemble.html#monotonic-constraints))\n",
    "\n",
    "En fonction du problème à résoudre, il se peut que vous ayez des connaissances préalables indiquant qu'une certaine caractéristique devrait en général avoir un effet positif (ou négatif) sur la valeur cible. Par exemple, toutes choses égales par ailleurs, un score de crédit plus élevé devrait augmenter la probabilité d'obtenir l'approbation d'un prêt. Les contraintes monotones vous permettent d'incorporer de telles connaissances préalables dans le modèle.\n",
    "\n",
    "Pour un prédicteur $F$ avec deux caractéristiques :**\n",
    "\n",
    "- **Une contrainte d'augmentation monotone est une contrainte de la forme :**\n",
    "\n",
    "$$x_1 \\leq x_1' \\implies F(x_1, x_2) \\leq F(x_1', x_2)$$\n",
    "\n",
    "- **Une contrainte de diminution monotone est une contrainte de la forme :**\n",
    "\n",
    "$$x_1 \\leq x_1' \\implies F(x_1, x_2) \\geq F(x_1', x_2)$$\n",
    "\n",
    "Vous pouvez spécifier une contrainte monotone sur chaque caractéristique en utilisant le paramètre `monotonic_cst`. Pour chaque caractéristique, une valeur de 0 indique qu'il n'y a aucune contrainte, tandis que 1 et -1 indiquent une contrainte d'augmentation monotone et de diminution monotone, respectivement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "# monotonic increase, monotonic decrease, and no constraint on the 3 features\n",
    "gbdt = HistGradientBoostingRegressor(monotonic_cst=[1, -1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un contexte de classification binaire, l'imposition d'une contrainte d'augmentation monotone (de diminution) signifie que des valeurs plus élevées de la caractéristique sont censées avoir un effet positif (négatif) sur la probabilité que les échantillons appartiennent à la classe positive.\n",
    "\n",
    "Cependant, les contraintes monotones ne limitent que marginalement les effets des caractéristiques sur la sortie. Par exemple, les contraintes d'augmentation et de diminution monotones ne peuvent pas être utilisées pour imposer la contrainte de modélisation suivante :\n",
    "\n",
    "$$x_1 \\leq x_1' \\implies F(x_1, x_2) \\leq F(x_1', x_2')$$\n",
    "\n",
    "De plus, les contraintes monotones ne sont pas prises en charge pour la classification multiclasse.\n",
    "\n",
    "> **Remarque :** Comme les catégories sont des quantités non ordonnées, il n'est pas possible d'imposer des contraintes monotones sur les caractéristiques catégorielles.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Contraintes monotones**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_monotonic_constraints.ipynb)<br/>([_Monotonic Constraints_](https://scikit-learn.org/stable/auto_examples/ensemble/plot_monotonic_constraints.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='interaction-constraints'></a> 1.11.1.1.6. **Contraintes d'interaction**<br/>([_Interaction constraints_](https://scikit-learn.org/stable/modules/ensemble.html#interaction-constraints))\n",
    "\n",
    "Les arbres à gradient basés sur des histogrammes sont a priori autorisés à utiliser n'importe quelle caractéristique pour diviser un nœud en nœuds enfants. Cela crée ce que l'on appelle des interactions entre les caractéristiques, c'est-à-dire l'utilisation de différentes caractéristiques pour effectuer une division le long d'une branche. Parfois, on souhaite restreindre les interactions possibles, voir [Mayer2022]. Cela peut être fait grâce au paramètre `interaction_cst`, où l'on peut spécifier les indices des caractéristiques autorisées à interagir. Par exemple, avec un total de 3 caractéristiques, `interaction_cst=[{0}, {1}, {2}]` interdit toutes les interactions. Les contraintes `[{0, 1}, {1, 2}]` spécifient deux groupes de caractéristiques pouvant éventuellement interagir. Les caractéristiques 0 et 1 peuvent interagir entre elles, tout comme les caractéristiques 1 et 2. Cependant, notez que les caractéristiques 0 et 2 sont interdites d'interaction. Voici un arbre et les divisions possibles de l'arbre :\n",
    "\n",
    "```\n",
    "   1      <- Les deux groupes de contraintes pourraient être appliqués à partir de maintenant\n",
    "  / \\\n",
    " 1   2    <- La division à gauche remplit toujours les deux groupes de contraintes.\n",
    "/ \\ / \\      La division à droite à la caractéristique 2 a seulement le groupe {1, 2} à partir de maintenant.\n",
    "```\n",
    "\n",
    "LightGBM utilise la même logique pour les groupes chevauchants.\n",
    "\n",
    "Notez que les caractéristiques qui ne sont pas répertoriées dans `interaction_cst` se voient automatiquement attribuer un groupe d'interaction propre. Avec à nouveau 3 caractéristiques, cela signifie que `[{0}]` est équivalent à `[{0}, {1, 2}]`.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Graphiques de dépendance partielle et d'espérance conditionnelle individuelle**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/4_inspection/plot_partial_dependence.ipynb)<br/>([*Partial Dependence and Individual Conditional Expectation Plots*](https://scikit-learn.org/stable/auto_examples/inspection/plot_partial_dependence.html))\n",
    "\n",
    "#### Références\n",
    "\n",
    "🔬 [Mayer2022] M. Mayer, S.C. Bourassa, M. Hoesli, and D.F. Scognamiglio. 2022. [**“Machine Learning Applications to Land and Structure Valuation”**](https://EconPapers.repec.org/RePEc:gam:jjrfmx:v:15:y:2022:i:5:p:193-:d:797960). Journal of Risk and Financial Management 15, no. 5: 193"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='low-level-parallelism'></a> 1.11.1.1.7. **Parallélisme de bas niveau**<br/>([_Low-level parallelism_](https://scikit-learn.org/stable/modules/ensemble.html#low-level-parallelism))\n",
    "\n",
    "[**`HistGradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier) et [**`HistGradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor) utilisent OpenMP pour la parallélisation via Cython. Pour plus de détails sur la manière de contrôler le nombre de threads, veuillez vous référer à nos notes sur la parallélisation.\n",
    "\n",
    "Les parties suivantes sont parallélisées :\n",
    "- la mise en correspondance des échantillons des valeurs réelles aux bacs de valeurs entières (toutefois, la recherche des seuils des bacs est séquentielle)\n",
    "- la construction des histogrammes est parallélisée sur les caractéristiques\n",
    "- la recherche du meilleur point de division dans un nœud est parallélisée sur les caractéristiques\n",
    "- pendant l'apprentissage, la mise en correspondance des échantillons dans les nœuds enfants de gauche et de droite est parallélisée sur les échantillons\n",
    "- le calcul des gradients et hessiens est parallélisé sur les échantillons\n",
    "- la prédiction est parallélisée sur les échantillons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='why-it-s-faster'></a> 1.11.1.1.8. **Pourquoi c'est plus rapide**<br/>([_Why it’s faster_](https://scikit-learn.org/stable/modules/ensemble.html#why-it-s-faster))\n",
    "\n",
    "La principale limitation d'une procédure de boosting par gradient réside dans la construction des arbres de décision. La construction d'un arbre de décision traditionnel (comme dans les autres GBDTs [**`GradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) et [**`GradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)) nécessite de trier les échantillons à chaque nœud (pour chaque caractéristique). Le tri est nécessaire pour que le gain potentiel d'un point de division puisse être calculé de manière efficace. Par conséquent, la complexité de la division d'un nœud unique est de $\\mathcal{O}(n_\\text{features} \\times n \\log(n))$ où $n$ est le nombre d'échantillons du nœud.\n",
    "\n",
    "[**`HistGradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier) et [**`HistGradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor), en revanche, n'exigent pas de trier les valeurs des caractéristiques et utilisent à la place une structure de données appelée histogramme, où les échantillons sont implicitement ordonnés. La construction d'un histogramme a une complexité de $\\mathcal{O}(n)$, de sorte que la procédure de division du nœud a une complexité de $\\mathcal{O}(n_\\text{features} \\times n)$, bien inférieure à la précédente. De plus, au lieu de considérer $n$ points de division, nous n'en considérons que `max_bins`, ce qui peut être beaucoup plus petit.\n",
    "\n",
    "Afin de construire les histogrammes, les données d'entrée X doivent être regroupées en bins à valeurs entières. Cette procédure de regroupement nécessite de trier les valeurs des caractéristiques, mais cela se produit uniquement une fois au tout début du processus de boosting (et non à chaque nœud, comme dans [**`GradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) et [**`GradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)).\n",
    "\n",
    "Enfin, de nombreuses parties de la mise en œuvre de [**`HistGradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier) et [**`HistGradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor) sont parallélisées.\n",
    "\n",
    "#### Références\n",
    "\n",
    "🔬 [XGBoost] Tianqi Chen, Carlos Guestrin, [**“XGBoost: A Scalable Tree Boosting System”**](https://arxiv.org/pdf/1603.02754.pdf)\n",
    "\n",
    "🔬 [LightGBM] Ke et. al. [**“LightGBM: A Highly Efficient Gradient BoostingDecision Tree”**](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/11/lightgbm.pdf)\n",
    "\n",
    "🔬 [Fisher1958] Fisher, W.D. (1958). [**“On Grouping for Maximum Homogeneity”**](https://www.semanticscholar.org/paper/On-Grouping-for-Maximum-Homogeneity-Fisher/040c3e7d4baac625b6072cf9bf6be697f26d3cab) Journal of the American Statistical Association, 53, 789-798."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='gradientboostingclassifier-and-gradientboostingregressor'></a> 1.11.1.2. **[**`GradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) et [**`GradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)**<br/>([_**`GradientBoostingClassifier`** and **`GradientBoostingRegressor`**_](https://scikit-learn.org/stable/modules/ensemble.html#gradientboostingclassifier-and-gradientboostingregressor))\n",
    "\n",
    "L'utilisation et les paramètres de [**`GradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) et [**`GradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor) sont décrits ci-dessous. Les 2 paramètres les plus importants de ces estimateurs sont `n_estimators` et `learning_rate`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='classification'></a> 1.11.1.2.1. **Classification**<br/>([_Classification_](https://scikit-learn.org/stable/modules/ensemble.html#classification))\n",
    "\n",
    "[**`GradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) prend en charge à la fois la classification binaire et la classification multi-classe. L'exemple suivant montre comment ajuster un classifieur de gradient boosting avec 100 souches (arbres de décision de profondeur 1) en tant que classifieurs faibles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(\n",
    "    n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "# 0.913..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre de classifieurs faibles (c'est-à-dire les arbres de régression) est contrôlé par le paramètre `n_estimators`. [**La taille de chaque arbre**](#taille-de-l'arbre-gradient-boosting) peut être contrôlée soit en définissant la profondeur de l'arbre via `max_depth`, soit en définissant le nombre de feuilles via `max_leaf_nodes`. Le `learning_rate` est un hyperparamètre dans la plage (0.0, 1.0] qui contrôle le surapprentissage via [**le taux de rétrécissement**](#rétrécissement-gradient-boosting).\n",
    "\n",
    "> **Remarque:** La classification avec plus de 2 classes nécessite l'induction de `n_classes` arbres de régression à chaque itération, donc le nombre total d'arbres induits équivaut à `n_classes * n_estimators`. Pour les ensembles de données avec un grand nombre de classes, nous recommandons fortement d'utiliser [**`HistGradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier) comme alternative à [**`GradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='regression'></a> 1.11.1.2.2. **Régression**<br/>([_Regression_](https://scikit-learn.org/stable/modules/ensemble.html#regression))\n",
    "\n",
    "[**`GradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor) prend en charge diverses fonctions de perte pour la régression, qui peuvent être spécifiées via l'argument `loss`. La fonction de perte par défaut pour la régression est l'erreur quadratique (`'squared_error'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.009154859960321"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "est = GradientBoostingRegressor(\n",
    "    n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,\n",
    "    loss='squared_error'\n",
    ").fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))\n",
    "# 5.00..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'illustration ci-dessous montre les résultats de l'application de [**`GradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor) avec une perte des moindres carrés et 500 apprenants de base sur l'ensemble de données sur le diabète ([**`sklearn.datasets.load_diabetes`**](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes)). Le graphique affiche l'erreur d'entraînement et d'essai à chaque itération. L'erreur d'entraînement à chaque itération est stockée dans l'attribut `train_score_` du modèle de gradient boosting. L'erreur d'essai à chaque itération peut être obtenue via la méthode `staged_predict`, qui renvoie un générateur fournissant les prédictions à chaque étape. Des graphiques de ce type peuvent être utilisés pour déterminer le nombre optimal d'arbres (c'est-à-dire `n_estimators`) en utilisant l'arrêt précoce.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_gradient_boosting_regression_001.png\"\n",
    "    alt=\"Gradient Boosting regression\"\n",
    "    style=\"max-width: 30%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Régression à amplification de gradient**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_gradient_boosting_regression.ipynb)<br/>([_Gradient Boosting regression_](https://scikit-learn.org/stable/auto_examples/ensembles/plot_gradient_boosting_regression.html))\n",
    "\n",
    "##### [**Estimations hors sac (OOB) de l'amplification de gradient**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_gradient_boosting_oob.ipynb)<br/>([*Gradient Boosting Out-of-Bag estimates*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_oob.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='fitting-additional-weak-learners'></a> 1.11.1.2.3. **Ajustement d'apprenants faibles supplémentaires**<br/>([_Fitting additional weak-learners_](https://scikit-learn.org/stable/modules/ensemble.html#fitting-additional-weak-learners))\n",
    "\n",
    "[**`GradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) et [**`GradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) prennent en charge `warm_start=True` qui permet d'ajouter des estimateurs supplémentaires à un modèle déjà ajusté."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.840234741105356"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = est.set_params(n_estimators=200, warm_start=True)  # set warm_start and new nr of trees\n",
    "_ = est.fit(X_train, y_train) # fit additional 100 trees to est\n",
    "mean_squared_error(y_test, est.predict(X_test))\n",
    "# 3.84..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='controlling-the-tree-size'></a> 1.11.1.2.4. **Contrôle de la taille de l'arbre**<br/>([_Controlling the tree size_](https://scikit-learn.org/stable/modules/ensemble.html#controlling-the-tree-size))\n",
    "\n",
    "La taille des apprenants de base de l'arbre de régression définit le niveau d'interactions variables qui peut être capturé par le modèle d'amplification du gradient. En général, un arbre de profondeur `h` peut capturer des interactions d'ordre `h`. Il existe deux manières de contrôler la taille des arbres de régression individuels.\n",
    "\n",
    "Si vous spécifiez `max_depth=h` alors des arbres binaires complets de profondeur `h` seront développés. De tels arbres auront (au plus) `2**h` nœuds feuilles et `2**h - 1` nœuds séparés.\n",
    "\n",
    "Alternativement, vous pouvez contrôler la taille de l'arbre en spécifiant le nombre de nœuds feuilles via le paramètre `max_leaf_nodes`. Dans ce cas, les arbres seront développés en utilisant la meilleure recherche en premier où les nœuds avec la plus grande amélioration d'impureté seront développés en premier. Un arbre avec `max_leaf_nodes=k` a `k - 1` nœuds divisés et peut donc modéliser des interactions allant jusqu'à `max_leaf_nodes - 1`.\n",
    "\n",
    "Nous avons constaté que `max_leaf_nodes=k` donne des résultats comparables à `max_depth=k-1` mais est beaucoup plus rapide à entraîner en contrepartie d'une erreur d'entraînement légèrement plus élevée. Le paramètre `max_leaf_nodes` correspond à la variable `J` dans le chapitre sur le gradient boosting dans [Friedman2001] et est lié au paramètre `interaction.depth` dans le package gbm de R où `max_leaf_nodes == interaction.depth + 1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='mathematical-formulation'></a> 1.11.1.2.5. **Formulation mathématique**<br/>([_Mathematical formulation_](https://scikit-learn.org/stable/modules/ensemble.html#mathematical-formulation))\n",
    "\n",
    "Nous présentons d'abord le GBRT pour la régression, puis nous détaillons le cas de la classification.\n",
    "\n",
    "#### Régression\n",
    "\n",
    "Les régresseurs GBRT sont des modèles additifs dont la prédiction $\\hat{y}_i$ pour une entrée donnée $x_i$ est de la forme suivante :\n",
    "\n",
    "$$\\hat{y}_i = F_M(x_i) = \\sum_{m=1}^{M} h_m(x_i)$$\n",
    "\n",
    "où les $h_m$ sont des estimateurs appelés *apprenants faibles* dans le contexte du boosting. Gradient Tree Boosting utilise des [**régresseurs à arbre de décision** (1.10)](https://scikit-learn.org/stable/modules/tree.html#tree) de taille fixe en tant qu'apprenants faibles. La constante $M$ correspond au paramètre `n_estimators`.\n",
    "\n",
    "Semblable à d'autres algorithmes de boosting, un GBRT est construit de manière gourmande :\n",
    "\n",
    "$$F_m(x) = F_{m-1}(x) + h_m(x),$$\n",
    "\n",
    "où l'arbre $h_m$ nouvellement ajouté est ajusté afin de minimiser une somme de pertes $L_m$, compte tenu de l'ensemble précédent $F_{m-1}$ :\n",
    "\n",
    "$$h_m =  \\arg\\min_{h} L_m = \\arg\\min_{h} \\sum_{i=1}^{n} l(y_i, F_{m-1}(x_i) + h(x_i)),$$\n",
    "\n",
    "où $l(y_i, F(x_i))$ est défini par le paramètre de perte `loss`, détaillé dans la section suivante.\n",
    "\n",
    "Par défaut, le modèle initial $F_0$ est choisi comme la constante qui minimise la perte : pour une perte des moindres carrés, c'est la moyenne empirique des valeurs cibles. Le modèle initial peut également être spécifié via l'argument `init`.\n",
    "\n",
    "En utilisant une approximation de Taylor du premier ordre, la valeur de $l$ peut être approchée comme suit :\n",
    "\n",
    "$$\n",
    "l(y_i, F_{m-1}(x_i) + h_m(x_i)) \\approx\n",
    "l(y_i, F_{m-1}(x_i))\n",
    "+ h_m(x_i)\n",
    "\\left[ \\frac{\\partial l(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F=F_{m - 1}}.\n",
    "$$\n",
    "\n",
    "> **Note :** En bref, une approximation de Taylor du premier ordre dit que $l(z) \\approx l(a) + (z - a) \\frac{\\partial l(a)}{\\partial a}$. Ici, $z$ correspond à $F_{m - 1}(x_i) + h_m(x_i)$, et $a$ correspond à $F_{m - 1}(x_i)$\n",
    "\n",
    "La quantité $\\left[ \\frac{\\partial l(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F=F_{m - 1}}$ est la dérivée de la perte par rapport à son deuxième paramètre, évalué à $F_{m-1}(x)$. Elle est facile à calculer pour tout $F_{m - 1}(x_i)$ donné sous une forme fermée puisque la perte est différentiable. Nous le noterons par $g_i$.\n",
    "\n",
    "En supprimant les termes constants, on a :\n",
    "\n",
    "$$h_m \\approx \\arg\\min_{h} \\sum_{i=1}^{n} h(x_i) g_i$$\n",
    "\n",
    "\n",
    "Ceci est minimisé si $h(x_i)$ est ajusté pour prédire une valeur qui est proportionnelle au gradient négatif $-g_i$. Par conséquent, à chaque itération, l'estimateur $h_m$ **est ajusté pour prédire les gradients négatifs des échantillons**. Les gradients sont mis à jour à chaque itération. Cela peut être considéré comme une sorte de descente de gradient dans un espace fonctionnel.\n",
    "\n",
    "> **Note :** Pour certaines pertes, par ex. le moindre écart absolu (LAD, `absolute_error`) où les gradients sont $\\pm 1$, les valeurs prédites par un $h_m$ ajusté ne sont pas assez précises : l'arbre ne peut produire que des valeurs entières. En conséquence, les valeurs des feuilles de l'arbre $h_m$ sont modifiées une fois l'arbre ajusté, de sorte que les valeurs des feuilles minimisent la perte $L_m$. La mise à jour dépend de la perte : pour la perte LAD, la valeur d'une feuille est mise à jour à la médiane des échantillons de cette feuille.\n",
    "\n",
    "#### Classification\n",
    "\n",
    "L'amplification du gradient pour la classification est très similaire à celle de la régression. Cependant, la somme des arbres $F_M(x_i) = \\sum_m h_m(x_i)$ n'est pas homogène à une prédiction : il ne peut pas s'agir d'une classe, puisque les arbres prédisent des valeurs continues.\n",
    "\n",
    "Le mappage à partir de la valeur $F_M(x_i)$ à une classe ou une probabilité dépend de la perte. Pour la perte logarithmique, la probabilité que $x_i$ appartienne à la classe positive est modélisée comme $p(y_i = 1 | x_i) = \\sigma(F_M(x_i))$ où $\\sigma$ est la fonction sigmoïde ou expit.\n",
    "\n",
    "Pour la classification multiclasse, les arbres K (pour les classes K) sont construits à chacune des\n",
    "$M$ itérations. La probabilité que $x_i$ appartienne à la classe $k$ est modélisée comme un softmax des valeurs $F_{M,k}(x_i)$.\n",
    "\n",
    "Notez que même pour une tâche de classification, le sous-estimateur $h_m$ est toujours un régresseur, pas un classifieur. En effet, les sous-estimateurs sont entraînés pour prédire des *gradients* (négatifs), qui sont toujours des quantités continues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='loss-functions'></a> 1.11.1.2.6. **Fonctions de perte**<br/>([_Loss Functions_](https://scikit-learn.org/stable/modules/ensemble.html#loss-functions))\n",
    "\n",
    "Les fonctions de perte suivantes sont prises en charge et peuvent être spécifiées à l'aide du paramètre `loss` :\n",
    "\n",
    "* Régression\n",
    "    * Erreur au carré (`'squared_error'`) : le choix naturel pour la régression en raison de ses propriétés de calcul supérieures. Le modèle initial est donné par la moyenne des valeurs cibles.\n",
    "    * Plus petit écart absolu (`'lad'`) : une fonction de perte robuste pour la régression. Le modèle initial est donné par la médiane des valeurs cibles.\n",
    "    * Huber (`'huber'`) : autre fonction de perte robuste qui combine les moindres carrés et le moindre écart absolu ; utiliser `alpha` pour contrôler la sensibilité aux valeurs aberrantes (voir [Friedman2001] pour plus de détails).\n",
    "    * Quantile (`'quantile'`) : une fonction de perte pour la régression quantile. Utilisez `0 < alpha < 1` pour spécifier le quantile. Cette fonction de perte peut être utilisée pour créer des intervalles de prédiction (voir l'exemple [**Intervalles de prédiction pour la régression à amplification de gradient**](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html)).\n",
    "* Classification\n",
    "    * Log-loss binaire (`'log-loss'`) : la fonction de perte de log-vraisemblance négative binomiale pour la classification binaire. Elle fournit des estimations de probabilité. Le modèle initial est donné par le log odds-ratio.\n",
    "    * Log-loss multi-classes (`'log-loss'`) : la fonction de perte de log-vraisemblance négative multinomiale pour la classification multi-classes avec des classes mutuellement exclusives `n_classes`. Elle fournit des estimations de probabilité. Le modèle initial est donné par la probabilité a priori de chaque classe. A chaque itération, des arbres de régression `n_classes` doivent être construits, ce qui rend le GBRT plutôt inefficace pour les ensembles de données avec un grand nombre de classes.\n",
    "    * Perte exponentielle (`'exponential'`) : la même fonction de perte qu'[**`AdaBoostClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html). Moins robuste aux exemples mal étiquetés que `'log-loss'` ; ne peut être utilisé que pour la classification binaire.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='shrinkage-via-learning-rate'></a> 1.11.1.2.7. **Réduction via le taux d'apprentissage**<br/>([_Shrinkage via learning rate_](https://scikit-learn.org/stable/modules/ensemble.html#shrinkage-via-learning-rate))\n",
    "\n",
    "[Friedman2001] a proposé une stratégie de régularisation simple qui échelonne la contribution de chaque apprenant faible par un facteur constant $\\nu$ :\n",
    "\n",
    "$$F_m(x) = F_{m-1}(x) + \\nu h_m(x)$$\n",
    "\n",
    "Le paramètre $\\nu$ est également appelé **taux d'apprentissage** car il met à l'échelle la longueur du pas de la procédure de descente de gradient ; il peut être défini via le paramètre `learning_rate`.\n",
    "\n",
    "Le paramètre `learning_rate` interagit fortement avec le paramètre `n_estimators`, le nombre d'apprenants faibles à adapter. Des valeurs plus petites de `learning_rate` nécessitent un plus grand nombre d'apprenants faibles pour maintenir une erreur d'entraînement constante. Des preuves empiriques suggèrent que de petites valeurs de `learning_rate` favorisent une meilleure erreur de test. [HTF] recommande de définir le taux d'apprentissage sur une petite constante (par exemple, `learning_rate <= 0.1`) et de choisir `n_estimators` en s'arrêtant tôt. Pour une discussion plus détaillée de l'interaction entre `learning_rate` et `n_estimators` voir [R2007]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='subsampling'></a> 1.11.1.2.8. **Sous-échantillonnage**<br/>([_Subsampling_](https://scikit-learn.org/stable/modules/ensemble.html#subsampling))\n",
    "\n",
    "[Friedman2002] a proposé le boosting de gradient stochastique, qui combine le boosting de gradient avec un moyennage bootstrap (bagging). A chaque itération, le classifieur de base est entraîné sur une fraction `subsample` des données d'entraînement disponibles. Le sous-échantillon est tiré sans remise. Une valeur typique de `subsample` est 0.5.\n",
    "\n",
    "La figure ci-dessous illustre l'effet du rétrécissement et du sous-échantillonnage sur la qualité de l'ajustement du modèle. Nous pouvons clairement voir que le rétrécissement surpasse l'absence de rétrécissement. Le sous-échantillonnage avec rétrécissement peut encore augmenter la précision du modèle. Le sous-échantillonnage sans rétrécissement, en revanche, donne de mauvais résultats.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_gradient_boosting_regularization_001.png\"\n",
    "    alt=\"Effet du rétrécissement et du sous-échantillonnage\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Une autre stratégie pour réduire la variance consiste à sous-échantillonner les caractéristiques analogues aux fractionnements aléatoires dans [**`RandomForestClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). Le nombre de caractéristiques sous-échantillonnées peut être contrôlé via le paramètre `max_features`.\n",
    "\n",
    "> **Note :** - L'utilisation d'une petite valeur `max_features` peut réduire considérablement le temps d'exécution.\n",
    "\n",
    "L'amplification du gradient stochastique permet de calculer des estimations hors sac de la déviance du test en calculant l'amélioration de la déviance sur les exemples qui ne sont pas inclus dans l'échantillon bootstrap (c'est-à-dire les exemples hors sac). Les améliorations sont stockées dans l'attribut `oob_improvement_`. `oob_improvement_[i]` contient l'amélioration en termes de perte sur les échantillons OOB si vous ajoutez la ième étape aux prédictions actuelles. Les estimations hors sac peuvent être utilisées pour la sélection du modèle, par exemple pour déterminer le nombre optimal d'itérations. Les estimations OOB sont généralement très pessimistes, nous vous recommandons donc d'utiliser la validation croisée à la place et de n'utiliser OOB que si la validation croisée prend trop de temps.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Régularisation de l'amplification de gradient**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_gradient_boosting_regularization.ipynb)<br/>([*Gradient Boosting regularization*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regularization.html))\n",
    "\n",
    "##### [**Estimations OOB de l'amplification de gradient**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_gradient_boosting_oob.ipynb)<br/>([*Gradient Boosting Out-of-Bag estimates*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_oob.html))\n",
    "\n",
    "##### [**Erreurs OOB pour les forêts aléatoires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_gradient_boosting_regularization.ipynb)<br/>([*OOB Errors for Random Forests*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regularization.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='interpretation-with-feature-importance'></a> 1.11.1.2.9. **Interprétation avec importance des caractéristiques**<br/>([_Interpretation with feature importance_](https://scikit-learn.org/stable/modules/ensemble.html#interpretation-with-feature-importance))\n",
    "\n",
    "Les arbres de décision individuels peuvent être interprétés facilement en visualisant simplement la structure arborescente. Les modèles d'amplification de gradient, cependant, comprennent des centaines d'arbres de régression et ne peuvent donc pas être facilement interprétés par une inspection visuelle des arbres individuels. Heureusement, un certain nombre de techniques ont été proposées pour résumer et interpréter les modèles de gradient boosting.\n",
    "\n",
    "Souvent, les caractéristiques ne contribuent pas de manière égale à prédire la réponse cible ; dans de nombreuses situations, la majorité des caractéristiques ne sont en fait pas pertinentes. Lors de l'interprétation d'un modèle, la première question est généralement : quelles sont ces caractéristiques importantes et comment contribuent-elles à prédire la réponse cible ?\n",
    "\n",
    "Les arbres de décision individuels effectuent intrinsèquement la sélection des caractéristiques en sélectionnant les points de partage appropriés. Ces informations peuvent être utilisées pour mesurer l'importance de chaque caractéristique ; l'idée de base est la suivante : plus une caractéristique est souvent utilisée dans les points de division d'un arbre, plus cette caractéristique est importante. Cette notion d'importance peut être étendue aux ensembles d'arbres de décision en faisant simplement la moyenne de l'importance des caractéristiques basée sur les impuretés de chaque arbre (voir [**Évaluation de l'importance des caractéristiques** (1.11.2.5)](https://scikit-learn.org/stable/modules/ensemble.html#random-forest-feature-importance) pour plus de détails).\n",
    "\n",
    "Les scores d'importance des caractéristiques d'un modèle d'amplification du gradient ajusté sont accessibles via la propriété feature_importances_ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10684213, 0.10461707, 0.11265447, 0.09863589, 0.09469133,\n",
       "       0.10729306, 0.09163753, 0.09718194, 0.09581415, 0.09063242])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                 max_depth=1, random_state=0).fit(X, y)\n",
    "clf.feature_importances_\n",
    "# array([0.10..., 0.10..., 0.11..., ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez que ce calcul de l'importance des caractéristiques est basé sur l'entropie et qu'il est distinct de [**`sklearn.inspection.permutation_importance`**](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html) qui est basé sur la permutation des caractéristiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemples\n",
    "\n",
    "##### [**Régression à amplification de gradient**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_gradient_boosting_regression.ipynb)<br/>([*Gradient Boosting regression*](https://scikit-learn.org/stable/auto_examples/ensembles/plot_gradient_boosting_regression.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Références\n",
    "\n",
    "🔬 [Friedman2001] (1,2,3,4) Friedman, J.H. (2001). [“**Greedy function approximation: A gradient boosting machine**”](https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.full). Annals of Statistics, 29, 1189-1232.\n",
    "\n",
    "🔬 [Friedman2002] Friedman, J.H. (2002). [“**Stochastic gradient boosting**”](https://jerryfriedman.su.domains/ftp/stobst.pdf). Computational Statistics & Data Analysis, 38, 367-378.\n",
    "\n",
    "🔬 [R2007] G. Ridgeway (2006). [“**Generalized Boosted Models: A guide to the gbm package”**](https://cran.r-project.org/web/packages/gbm/vignettes/gbm.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='forests-of-randomized-trees'></a> 1.11.2. **Forêts aléatoires et autres ensembles d'arbres aléatoires**<br/>([_Random forests and other randomized tree ensembles_](https://scikit-learn.org/stable/modules/ensemble.html#random-forests-and-other-randomized-tree-ensembles))\n",
    "\n",
    "Le module [**`sklearn.ensemble`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble) comprend deux algorithmes de moyennage basés sur des [**arbres de décision** (1.10.)](https://scikit-learn.org/stable/modules/tree.html#tree) aléatoires : l'algorithme RandomForest et la méthode Extra-Trees. Les deux algorithmes sont des techniques de perturbation et de combinaison [B1998] spécifiquement conçues pour les arbres. Cela signifie qu'un ensemble diversifié de classifieurs est créé en introduisant un caractère aléatoire dans la construction du classifieur. La prédiction de l'ensemble est donnée comme la prédiction moyenne des classifieurs individuels.\n",
    "\n",
    "Comme les autres classifieurs, les classifieurs forêts doivent être équipés de deux tableaux : un tableau creux ou dense `X` de forme `(n_samples, n_features)` contenant les échantillons d'entraînement, et un tableau `Y` de forme `(n_samples,)` contenant les valeurs cibles (étiquettes de classe) pour les échantillons d'entraînement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X = [[0, 0], [1, 1]]\n",
    "Y = [0, 1]\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf = clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme les [**arbres de décision** (1.10.)](https://scikit-learn.org/stable/modules/tree.html#tree), les forêts d'arbres s'étendent également aux [**problèmes multi-sorties** (1.10.3.)](https://scikit-learn.org/stable/modules/tree.html#tree-multioutput) (si `Y` est un tableau de forme `(n_samples, n_features)`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='random-forests'></a> 1.11.2.1. **Forêts aléatoires**<br/>([_Random Forests_](https://scikit-learn.org/stable/modules/ensemble.html#random-forests))\n",
    "\n",
    "\n",
    "Dans les forêts aléatoires (voir les classes [**`RandomForestClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) et [**`RandomForestRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor)), chaque arbre de l'ensemble est construit à partir d'un échantillon tiré avec remise (c'est-à-dire un échantillon bootstrap) à partir de l'ensemble d'entraînement.\n",
    "\n",
    "De plus, lors de la division de chaque nœud lors de la construction d'un arbre, la meilleure division est trouvée soit à partir de toutes les caractéristiques d'entrée, soit d'un sous-ensemble aléatoire de taille `max_features`. (Voir les [**consignes de réglage des paramètres** (1.11.2.3.)](https://scikit-learn.org/stable/modules/ensemble.html#random-forest-parameters) pour plus de détails).\n",
    "\n",
    "Le but de ces deux sources d'aléa est de diminuer la variance de l'estimateur forêt. En effet, les arbres de décision individuels présentent généralement une variance élevée et ont tendance à sur-ajuster. Le caractère aléatoire injecté dans les forêts produit des arbres de décision avec des erreurs de prédiction quelque peu découplées. En prenant une moyenne de ces prédictions, certaines erreurs peuvent s'annuler. Les forêts aléatoires permettent d'obtenir une variance réduite en combinant divers arbres, parfois au prix d'une légère augmentation du biais. En pratique, la réduction de la variance est souvent significative, ce qui donne un meilleur modèle global.\n",
    "\n",
    "Contrairement à la publication originale [B2001], l'implémentation scikit-learn combine les classifieurs en faisant la moyenne de leur prédiction probabiliste, au lieu de laisser chaque classificateur voter pour une seule classe.\n",
    "\n",
    "Une alternative compétitive aux forêts aléatoires est représentée par les modèles d'[**amplification de gradient basés sur des histogrammes** (1.11.1)](#histogram-based-gradient-boosting) (HGBT) :\n",
    "\n",
    "- **Construction d'arbres** : Les forêts aléatoires s'appuient généralement sur des arbres profonds (qui surajustent individuellement) et nécessitent beaucoup de ressources computationnelles, car elles nécessitent plusieurs divisions et évaluations de divisions candidates. Les modèles de renforcement construisent des arbres peu profonds (qui sous-ajustent individuellement) qui sont plus rapides à ajuster et à prédire.\n",
    "\n",
    "- **Renforcement séquentiel** : Dans HGBT, les arbres de décision sont construits séquentiellement, où chaque arbre est formé pour corriger les erreurs commises par les précédents. Cela leur permet d'améliorer itérativement les performances du modèle en utilisant relativement peu d'arbres. En revanche, les forêts aléatoires utilisent un vote à la majorité pour prédire le résultat, ce qui peut nécessiter un plus grand nombre d'arbres pour atteindre le même niveau de précision.\n",
    "\n",
    "- **Binning efficace** : HGBT utilise un algorithme de binning efficace qui peut gérer de grands ensembles de données avec un grand nombre de caractéristiques. L'algorithme de binning peut prétraiter les données pour accélérer la construction ultérieure de l'arbre (voir Pourquoi c'est plus rapide). En revanche, l'implémentation de scikit-learn des forêts aléatoires n'utilise pas le binning et repose sur des divisions exactes, ce qui peut être coûteux en termes de calcul.\n",
    "\n",
    "Dans l'ensemble, le coût de calcul d'HGBT par rapport aux forêts aléatoires dépend des caractéristiques spécifiques de l'ensemble de données et de la tâche de modélisation. Il est judicieux d'essayer les deux modèles et de comparer leurs performances et leur efficacité de calcul sur votre problème spécifique pour déterminer lequel est le mieux adapté.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Comparaison des modèles de forêts aléatoires et d'amplification de gradient basés sur des histogrammes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_forest_hist_grad_boosting_comparison.ipynb)<br/>([_Comparing Random Forests and Histogram Gradient Boosting models_](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_hist_grad_boosting_comparison.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='extremely-randomized-trees'></a> 1.11.2.2. **Arbres extrêmement aléatoires**<br/>([_Extremely Randomized Trees_](https://scikit-learn.org/stable/modules/ensemble.html#extremely-randomized-trees))\n",
    "\n",
    "Dans les arbres extrêmement aléatoires (voir les classes [**`ExtraTreesClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier) et [**`ExtraTreesRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html#sklearn.ensemble.ExtraTreesRegressor)), le caractère aléatoire va encore plus loin dans la façon dont les fractionnements sont calculés. Comme dans les forêts aléatoires, un sous-ensemble aléatoire de caractéristiques candidates est utilisé, mais au lieu de rechercher les seuils les plus discriminants, les seuils sont tirés au hasard pour chaque caractéristique candidate et le meilleur de ces seuils générés aléatoirement est choisi comme règle de division. Cela permet généralement de réduire un peu plus la variance du modèle, au prix d'une augmentation un peu plus importante du biais :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X, y = make_blobs(n_samples=10000, n_features=10, centers=100,\n",
    "                  random_state=0)\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,\n",
    "                             random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()\n",
    "# 0.98...\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "                             min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()\n",
    "# 0.999...\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,\n",
    "                           min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean() > 0.999\n",
    "# True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_forest_iris_001.png\"\n",
    "    alt=\"Classifieurs sur des sous-ensembles de caractéristiques du jeu de données Iris\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='parameters'></a> 1.11.2.3. **Paramètres**<br/>([_Parameters_](https://scikit-learn.org/stable/modules/ensemble.html#parameters))\n",
    "\n",
    "Les principaux paramètres à ajuster lors de l'utilisation de ces méthodes sont `n_estimators` et `max_features`. Le premier est le nombre d'arbres dans la forêt. Plus il est grand, mieux c'est, mais aussi plus il faudra de temps pour calculer. De plus, notez que les résultats cesseront de s'améliorer significativement au-delà d'un nombre critique d'arbres. Ce dernier est la taille des sous-ensembles aléatoires de caractéristiques à prendre en compte lors de la division d'un nœud. Plus la réduction de la variance est faible, plus l'augmentation du biais est importante. Les bonnes valeurs empiriques par défaut sont `max_features=1.0` ou de manière équivalente `max_features=None` (en considérant toujours toutes les caractéristiques au lieu d'un sous-ensemble aléatoire) pour les problèmes de régression, et `max_features=\"sqrt\"` (en utilisant un sous-ensemble aléatoire de taille `sqrt(n_features)`) pour les tâches de classification ( où `n_features` est le nombre d'entités dans les données). La valeur par défaut de `max_features=1.0` équivaut à des arbres ensachés et un caractère plus aléatoire peut être obtenu en définissant des valeurs plus petites (par exemple, 0,3 est une valeur par défaut typique dans la littérature). De bons résultats sont souvent obtenus en définissant `max_depth=None` en combinaison avec `min_samples_split=2` (c'est-à-dire lors du développement complet des arbres). Gardez cependant à l'esprit que ces valeurs ne sont généralement pas optimales et peuvent entraîner des modèles qui consomment beaucoup de RAM. Les meilleures valeurs de paramètres doivent toujours faire l'objet d'une validation croisée. De plus, notez que dans les forêts aléatoires, les échantillons bootstrap sont utilisés par défaut (`bootstrap=True`) tandis que la stratégie par défaut pour les arbres supplémentaires consiste à utiliser l'ensemble de données complet (`bootstrap=False`). Lors de l'utilisation de l'échantillonnage bootstrap, l'erreur de généralisation peut être estimée sur les échantillons laissés de côté ou hors sac. Cela peut être activé en définissant `oob_score=True`.\n",
    "\n",
    "**NB** - La taille du modèle avec les paramètres par défaut est $\\mathcal{O}(M \\times N \\times \\log(N))$, où $M$ est le nombre d'arbres et $N$ est le nombre d'échantillons. Afin de réduire la taille du modèle, vous pouvez modifier ces paramètres : `min_samples_split`, `max_leaf_nodes`, `max_depth` et `min_samples_leaf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='parallelization'></a> 1.11.2.4. **Parallélisation**<br/>([_Parallelization_](https://scikit-learn.org/stable/modules/ensemble.html#parallelization))\n",
    "\n",
    "Enfin, ce module propose également la construction parallèle des arbres et le calcul parallèle des prédictions via le paramètre `n_jobs`. Si `n_jobs=k` alors les calculs sont partitionnés en `k` jobs, et exécutés sur `k` cœurs de la machine. Si `n_jobs=-1` alors tous les cœurs disponibles sur la machine sont utilisés. Notez qu'en raison de la surcharge de communication inter-processus, l'accélération peut ne pas être linéaire (c'est-à-dire que l'utilisation de `k` travaux ne sera malheureusement pas `k` fois plus rapide). Une accélération significative peut encore être obtenue lors de la construction d'un grand nombre d'arbres, ou lorsque la construction d'un seul arbre nécessite une bonne quantité de temps (par exemple, sur de grands ensembles de données).\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Tracer les surfaces de décision d'ensembles d'arbres sur le jeu de données iris**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_forest_iris.ipynb)<br/>([*Plot the decision surfaces of ensembles of trees on the iris dataset*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_iris.html))\n",
    "\n",
    "#### [**Importances des pixels avec une forêt parallèle d'arbres**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_forest_importances_faces.ipynb)<br/>([*Pixel importances with a parallel forest of trees*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances_faces.html))\n",
    "\n",
    "#### [**Complétion de visages avec des estimateurs multi-sorties**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/misc/plot_multioutput_face_completion.ipynb)<br/>([*Face completion with multi-output estimators*](https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_multioutput_face_completion.html))\n",
    "\n",
    "### Références\n",
    "\n",
    "🔬 [B2001] Breiman, [“**Random Forests”**](https://link.springer.com/content/pdf/10.1023/A:1010933404324.pdf), Machine Learning, 45(1), 5-32, 2001.\n",
    "\n",
    "🔬 [B1998] Breiman, [“**Arcing Classifiers”**](https://www.stat.berkeley.edu/~breiman/arcall.pdf), Annals of Statistics 1998.\n",
    "\n",
    "🔬  P. Geurts, D. Ernst., and L. Wehenkel, [“**Extremely randomized trees”**](https://link.springer.com/content/pdf/10.1007/s10994-006-6226-1.pdf), Machine Learning, 63(1), 3-42, 2006."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='feature-importance-evaluation'></a> 1.11.2.5. **Évaluation de l'importance des caractéristiques**<br/>([_Feature importance evaluation_](https://scikit-learn.org/stable/modules/ensemble.html#feature-importance-evaluation))\n",
    "\n",
    "Le rang relatif (c'est-à-dire la profondeur) d'une caractéristique utilisée comme nœud de décision dans un arbre peut être utilisé pour évaluer l'importance relative de cette caractéristique par rapport à la prévisibilité de la variable cible. Les caractéristiques utilisées en haut de l'arbre contribuent à la décision de prédiction finale d'une plus grande fraction des échantillons d'entrée. La **fraction attendue des échantillons** auxquels ils contribuent peut donc être utilisée comme une estimation de l'**importance relative des caractéristiques**. Dans scikit-learn, la fraction d'échantillons à laquelle une caractéristique contribue est combinée avec la diminution de l'impureté résultant de leur division pour créer une estimation normalisée du pouvoir prédictif de cette caractéristique.\n",
    "\n",
    "En **faisant la moyenne** des estimations de la capacité prédictive sur plusieurs arbres randomisés, on peut **réduire la variance** d'une telle estimation et l'utiliser pour la sélection des caractéristiques. C'est ce qu'on appelle la diminution moyenne des impuretés, ou MDI (*Mean Decrease in Impurity*). Se reporter à [L2014] pour plus d'informations sur MDI et l'évaluation de l'importance des caractéristiques avec Random Forests.\n",
    "\n",
    "> **Avertissement :** Les importances de caractéristiques basées sur les impuretés calculées sur les modèles arborescents souffrent de deux défauts qui peuvent conduire à des conclusions erronées. Tout d'abord, ils sont calculés sur des statistiques dérivées de l'ensemble de données d'entraînement et ne nous informent donc pas nécessairement sur les caractéristiques les plus importantes pour faire de bonnes prédictions sur l'ensemble de données retenu. Deuxièmement, ils favorisent les caractéristiques à cardinalité élevée, c'est-à-dire les caractéristiques avec de nombreuses valeurs uniques. L'[**importance des caractéristiques de permutation** (4.2)](https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-importance) est une alternative à l'importance des caractéristiques basée sur les impuretés qui ne souffre pas de ces défauts. Ces deux méthodes d'obtention de l'importance des caractéristiques sont explorées dans : [**Permutation Importance vs Random Forest Feature Importance (MDI)**](https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py).\n",
    "\n",
    "L'exemple suivant montre une représentation codée par couleur des importances relatives de chaque pixel individuel pour une tâche de reconnaissance faciale à l'aide d'un modèle [`ExtraTreesClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier).\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_forest_importances_faces_001.png\"\n",
    "    alt=\"Importance des pixels à partir des valeurs d'impureté\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "En pratique, ces estimations sont stockées sous la forme d'un attribut nommé `feature_importances_` sur le modèle ajusté. Il s'agit d'un tableau de forme `(n_features,)` dont les valeurs sont positives et dont la somme est égale à 1,0. Plus la valeur est élevée, plus la contribution de la caractéristique correspondante à la fonction de prédiction est importante.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Importances des pixels avec une forêt parallèle d'arbres**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_forest_importances_faces.ipynb)<br/>([*Pixel importances with a parallel forest of trees*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances_faces.html))\n",
    "\n",
    "##### [**Importances des caractéristiques avec une forêt d'arbres**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_forest_importances.ipynb)<br/>([*Feature importances with a forest of trees*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html))\n",
    "\n",
    "#### Références\n",
    "\n",
    "📚 [L2014] G. Louppe, [**“Understanding Random Forests: From Theory to Practice”**](https://arxiv.org/pdf/1407.7502.pdf), PhD Thesis, U. of Liege, 2014."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='totally-random-trees-embedding'></a> 1.11.2.6. **Intégration d'arbres totalement aléatoires**<br/>([_Totally Random Trees Embedding_](https://scikit-learn.org/stable/modules/ensemble.html#totally-random-trees-embedding))\n",
    "\n",
    "[**`RandomTreesEmbedding`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomTreesEmbedding.html) implémente une transformation non supervisée des données. En utilisant une forêt d'arbres complètement aléatoires, [**`RandomTreesEmbedding`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomTreesEmbedding.html) code les données par les indices des feuilles dans lesquelles un point de données se retrouve. Cet index est ensuite codé suivant la méthode un-parmi-K, conduisant à un codage binaire creux de grande dimension. Ce codage peut être calculé très efficacement et peut ensuite être utilisé comme base pour d'autres tâches d'apprentissage. La taille et le creux du code peuvent être influencés en choisissant le nombre d'arbres et la profondeur maximale par arbre. Pour chaque arbre de l'ensemble, le codage contient une entrée parmi une. La taille du codage est au plus `n_estimators * 2 ** max_depth`, le nombre maximum de feuilles dans la forêt.\n",
    "\n",
    "Comme les points de données voisins sont plus susceptibles de se trouver dans la même feuille d'un arbre, la transformation effectue une estimation de densité implicite et non paramétrique.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Transformation des caractéristiques par hachage à l'aide d'arbres totalement aléatoires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_random_forest_embedding.ipynb)<br/>([*Hashing feature transformation using Totally Random Trees*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_random_forest_embedding.html))\n",
    "\n",
    "##### [**Apprentissage par variétés sur chiffres manuscrits : Locally Linear Embedding, Isomap…**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_2_manifold/plot_lle_digits.ipynb)<br/>([*Manifold learning on handwritten digits: Locally Linear Embedding, Isomap…*](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html))\n",
    "\n",
    "Compare des techniques de réduction de dimensionnalité non linéaire sur des chiffres manuscrits.\n",
    "\n",
    "##### [**Transformations de caractéristiques avec des ensembles d'arbres**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensemble/plot_feature_transformation.ipynb)<br/>([*Feature transformations with ensembles of trees*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html))\n",
    "\n",
    "Comparent les transformations de caractéristiques supervisées et non supervisées basées sur des arbres.\n",
    "\n",
    "> **Voir également :** Les techniques d'[**Apprentissage des variétés** (2.2)](https://scikit-learn.org/stable/modules/manifold.html#manifold) peuvent également être utiles pour dériver des représentations non linéaires de l'espace des caractéristiques, ces approches se concentrent également sur la réduction de la dimensionnalité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='bagging-meta-estimator'></a> 1.11.3. **Meta-estimation par bagging (ensachage)**<br/>([_Bagging meta-estimator_](https://scikit-learn.org/stable/modules/ensemble.html#bagging-meta-estimator))\n",
    "\n",
    "Dans les algorithmes ensemblistes, les méthodes de bagging forment une classe d'algorithmes qui construisent plusieurs instances d'un estimateur boîte noire sur des sous-ensembles aléatoires de l'ensemble d'entraînement d'origine, puis agrègent leurs prédictions individuelles pour former une prédiction finale. Ces méthodes sont utilisées comme moyen de réduire la variance d'un estimateur de base (par exemple, un arbre de décision), en introduisant de l'aléatoire dans sa procédure de construction, puis en en faisant un ensemble. Dans de nombreux cas, les méthodes de bagging constituent un moyen très simple d'amélioration par rapport à un modèle unique, sans nécessiter d'adaptation de l'algorithme de base sous-jacent. Comme elles offrent un moyen de réduire le surajustement, les méthodes de bagging fonctionnent mieux avec des modèles solides et complexes (par exemple, des arbres de décision entièrement développés), contrairement aux méthodes d'amplification qui fonctionnent généralement mieux avec des modèles faibles (par exemple, des arbres de décision peu profonds).\n",
    "\n",
    "Les méthodes de bagging se présentent sous de nombreuses formes, mais diffèrent principalement les unes des autres par la manière dont elles dessinent des sous-ensembles aléatoires de l'ensemble d'entraînement :\n",
    "\n",
    "* Lorsque des sous-ensembles aléatoires de l'ensemble de données sont tirés en tant que sous-ensembles aléatoires des échantillons, cet algorithme est connu sous le nom de **Collage** (_Pasting_) [B1999].\n",
    "\n",
    "* Lorsque les échantillons sont tirés avec remise, la méthode est connue sous le nom de **Bagging** [B1996].\n",
    "\n",
    "* Lorsque des sous-ensembles aléatoires de l'ensemble de données sont tirés en tant que sous-ensembles aléatoires des caractéristiques, la méthode est connue sous le nom de **Sous-espaces aléatoires** (_Radom Subspaces_) [H1998].\n",
    "\n",
    "* Enfin, lorsque les estimateurs de base sont construits sur des sous-ensembles d'échantillons et de caractéristiques, la méthode est connue sous le nom de **Correctifs aléatoires** (_Random Patches_) [LG2012].\n",
    "\n",
    "Dans scikit-learn, les méthodes de bagging sont proposées sous la forme d'un méta-estimateur [**`BaggingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier) unifié (resp. [**`BaggingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html#sklearn.ensemble.BaggingRegressor)), prenant en entrée un estimateur de base spécifié par l'utilisateur ainsi que des paramètres spécifiant la stratégie pour tirer des sous-ensembles aléatoires. En particulier, `max_samples` et `max_features` contrôlent la taille des sous-ensembles (en termes d'échantillons et de caractéristiques), tandis que `bootstrap` et `bootstrap_features` contrôlent si les échantillons et les caractéristiques sont tirés avec ou sans remise. Lors de l'utilisation d'un sous-ensemble des échantillons disponibles, la précision de la généralisation peut être estimée avec les échantillons hors sac en définissant `oob_score=True`. À titre d'exemple, l'extrait ci-dessous illustre comment instancier un ensemble d'ensachage d'estimateurs de base [**`KNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier), chacun construit sur des sous-ensembles aléatoires de 50 % des échantillons et 50 % des caractéristiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "bagging = BaggingClassifier(KNeighborsClassifier(),\n",
    "                            max_samples=0.5, max_features=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemples\n",
    "\n",
    "##### [**Estimateur unique versus bagging : décomposition biais-variance**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_bias_variance.ipynb)<br/>([_Single estimator versus bagging: bias-variance decomposition_](https://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Références\n",
    "\n",
    "Z 🔬 [B1999] L. Breiman, [**“Pasting Small Votes for Classification in Large Databases and On-Line”**](https://link.springer.com/content/pdf/10.1023/A:1007563306331.pdf), Machine Learning, 36(1), 85-103, 1999.\n",
    "\n",
    "Z 🔬 [B1996] L. Breiman, [**“Bagging predictors”**](https://link.springer.com/content/pdf/10.1023/A:1018054314350.pdf), Machine Learning, 24(2), 123-140, 1996.\n",
    "\n",
    "Z 🔬 [H1998] T. Ho, [**“The Random Subspace Method for Constructing Decision Forests”**](http://machine-learning.martinsewell.com/ensembles/rsm/Ho1998.pdf), Pattern Analysis and Machine Intelligence, 20(8), 832-844, 1998.\n",
    "\n",
    "Z 🔬 [LG2012] G. Louppe and P. Geurts, [**“Ensembles on Random Patches”**](https://link.springer.com/content/pdf/10.1007/978-3-642-33460-3_28.pdf), Machine Learning and Knowledge Discovery in Databases, 346-361, 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='voting-classifier'></a> 1.11.4. **Classifieur de vote**<br/>([_Voting Classifier_](https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier))\n",
    "\n",
    "L'idée derrière le **Classifieur de vote** ([**`VotingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier)) est de combiner des classifieurs d'apprentissage automatique conceptuellement différents et d'utiliser un vote à la majorité ou les probabilités prédites moyennes (vote doux) pour prédire les étiquettes de classe. Un tel classifieur peut être utile pour un ensemble de modèles qui performe de manière équivalente afin d'équilibrer leurs faiblesses individuelles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='voting-classifier'></a> 1.11.4.1. **Étiquettes de classe majoritaires (Vote majoritaire/Dur)**<br/>([_Majority Class Labels (Majority/Hard Voting)_](https://scikit-learn.org/stable/modules/ensemble.html#majority-class-labels-majority-hard-voting))\n",
    "\n",
    "Dans le vote majoritaire, l'étiquette de classe prédite pour un échantillon particulier est l'étiquette de classe qui représente la majorité (mode) des étiquettes de classe prédites par chaque classifieur individuel.\n",
    "\n",
    "Par exemple, si la prédiction pour un échantillon donné est la suivante :\n",
    "- classifieur 1 -> classe 1\n",
    "- classifieur 2 -> classe 1\n",
    "- classifieur 3 -> classe 2\n",
    "\n",
    "Le [**`VotingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier) (avec `voting='hard'`) classerait l'échantillon comme \"classe 1\" en se basant sur l'étiquette de classe majoritaire.\n",
    "\n",
    "En cas d'égalité, le [**`VotingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier) sélectionnera la classe en fonction de l'ordre de tri ascendant. Par exemple, dans le scénario suivant :\n",
    "- classifieur 1 -> classe 2\n",
    "- classifieur 2 -> classe 1\n",
    "\n",
    "L'étiquette de classe 1 sera attribuée à l'échantillon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='id24'></a> 1.11.4.2. **Utilisation**<br/>([_Usage_](https://scikit-learn.org/stable/modules/ensemble.html#id24))\n",
    "\n",
    "L'exemple suivant montre comment ajuster le classifieur de règle de la majorité :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95 (+/- 0.04) [Logistic Regression]\n",
      "Accuracy: 0.94 (+/- 0.04) [Random Forest]\n",
      "Accuracy: 0.91 (+/- 0.04) [naive Bayes]\n",
      "Accuracy: 0.95 (+/- 0.04) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "\n",
    "eclf = VotingClassifier(\n",
    "    estimators=[(\"lr\", clf1), (\"rf\", clf2), (\"gnb\", clf3)],\n",
    "    voting=\"hard\"\n",
    ")\n",
    "\n",
    "for clf, label in zip(\n",
    "    [clf1, clf2, clf3, eclf],\n",
    "    ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']\n",
    "):\n",
    "    scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "\n",
    "# Accuracy: 0.95 (+/- 0.04) [Logistic Regression]\n",
    "# Accuracy: 0.94 (+/- 0.04) [Random Forest]\n",
    "# Accuracy: 0.91 (+/- 0.04) [naive Bayes]\n",
    "# Accuracy: 0.95 (+/- 0.04) [Ensemble]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='weighted-average-probabilities-soft-voting'></a> 1.11.4.3. **Probabilités moyennes pondérées (Vote doux)**<br/>([_Weighted Average Probabilities (Soft Voting)_](https://scikit-learn.org/stable/modules/ensemble.html#weighted-average-probabilities-soft-voting))\n",
    "\n",
    "Contrairement au vote majoritaire (hard voting), le vote doux renvoie l'étiquette de classe comme étant l'argmax de la somme des probabilités prédites.\n",
    "\n",
    "Des poids spécifiques peuvent être attribués à chaque classifieur via le paramètre `weights`. Lorsque des poids sont fournis, les probabilités de classe prédites pour chaque classifieur sont collectées, multipliées par le poids du classifieur, et moyennées. L'étiquette de classe finale est ensuite dérivée de l'étiquette de classe avec la probabilité moyenne la plus élevée.\n",
    "\n",
    "Pour illustrer cela avec un exemple simple, supposons que nous ayons 3 classifieurs et un problème de classification à 3 classes où nous attribuons des poids égaux à tous les classifieurs : w1=1, w2=1, w3=1.\n",
    "\n",
    "Les probabilités moyennes pondérées pour un échantillon seraient alors calculées comme suit :\n",
    "\n",
    "| classifieur       | classe 1  | classe 2  | classe 3  |\n",
    "| ----------------- | --------- | --------- | --------- |\n",
    "| classifieur 1     | w1 \\* 0,2 | w1 \\* 0,5 | w1 \\* 0,3 |\n",
    "| classifieur 2     | w2 \\* 0,6 | w2 \\* 0,3 | w2 \\* 0,1 |\n",
    "| classifieur 3     | w3 \\* 0,3 | w3 \\* 0,4 | w3 \\* 0,3 |\n",
    "| moyenne pondérée  | 0,37      | 0,4       | 0,23      |\n",
    "\n",
    "Ici, l'étiquette de classe prédite est 2, car elle a la probabilité moyenne la plus élevée.\n",
    "\n",
    "L'exemple suivant illustre comment les régions de décision peuvent changer lorsqu'un [**`VotingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier) doux est utilisé en se basant sur une machine à vecteurs de support linéaire, un arbre de décision et un classifieur des k plus proches voisins :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from itertools import product\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Loading some example data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [0, 2]]\n",
    "y = iris.target\n",
    "\n",
    "# Training classifiers\n",
    "clf1 = DecisionTreeClassifier(max_depth=4)\n",
    "clf2 = KNeighborsClassifier(n_neighbors=7)\n",
    "clf3 = SVC(kernel='rbf', probability=True)\n",
    "eclf = VotingClassifier(\n",
    "    estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)],\n",
    "    voting='soft', weights=[2, 1, 2]\n",
    ")\n",
    "\n",
    "clf1 = clf1.fit(X, y)\n",
    "clf2 = clf2.fit(X, y)\n",
    "clf3 = clf3.fit(X, y)\n",
    "eclf = eclf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_voting_decision_regions_001.png\"\n",
    "    alt=\"Frontières de décision d'un classifieur de vote\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "### <a id='using-the-votingclassifier-with-gridsearchcv'></a> 1.11.4.4. **Utilisation du `VotingClassifier` avec `GridSearchCV`**<br/>([_Using the `VotingClassifier` with `GridSearchCV`_](https://scikit-learn.org/stable/modules/ensemble.html#using-the-votingclassifier-with-gridsearchcv))\n",
    "\n",
    "Le [**`VotingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier) peut également être utilisé en conjonction avec [**`GridSearchCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) afin d'ajuster les hyperparamètres des estimateurs individuels :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "eclf = VotingClassifier(\n",
    "    estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "params = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200]}\n",
    "\n",
    "grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\n",
    "grid = grid.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='id25'></a> 1.11.4.5. **Utilisation**<br/>([_Usage_](https://scikit-learn.org/stable/modules/ensemble.html#id25))\n",
    "\n",
    "Afin de prédire les étiquettes de classe en fonction des probabilités de classe prédites (les estimateurs de scikit-learn dans le [**`VotingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier) doivent prendre en charge la méthode `predict_proba`) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclf = VotingClassifier(\n",
    "    estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
    "    voting='soft'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionnellement, des poids peuvent être fournis pour les classificateurs individuels :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclf = VotingClassifier(\n",
    "    estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
    "    voting='soft', weights=[2,5,1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**Visualiser les frontières de décision d'un `VotingClassifier`**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_voting_decision_regions.ipynb)<br/>([_Plot the decision boundaries of a `VotingClassifier`_](https://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='voting-regressor'></a> 1.11.5. **Régresseur de vote**<br/>([_Voting Regressor_](https://scikit-learn.org/stable/modules/ensemble.html#voting-regressor))\n",
    "\n",
    "L'idée derrière le [**`VotingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html#sklearn.ensemble.VotingRegressor) est de combiner des régresseurs d'apprentissage automatique conceptuellement différents et de renvoyer les valeurs prédites moyennes. Un tel régresseur peut être utile pour un ensemble de modèles de performances équivalentes afin d'équilibrer leurs faiblesses individuelles.\n",
    "\n",
    "### <a id='id27'></a> 1.11.5.5. **Utilisation**<br/>([_Usage_](https://scikit-learn.org/stable/modules/ensemble.html#id27))\n",
    "\n",
    "L'exemple suivant montre comment ajuster le [**`VotingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html#sklearn.ensemble.VotingRegressor) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# Loading some example data\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "\n",
    "# Training classifiers\n",
    "reg1 = GradientBoostingRegressor(random_state=1)\n",
    "reg2 = RandomForestRegressor(random_state=1)\n",
    "reg3 = LinearRegression()\n",
    "ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])\n",
    "ereg = ereg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_voting_regressor_001.png\"\n",
    "    alt=\"Prédictions individuelles et de vote\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Visualisation des prédictions individuelles et de régression par vote**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensemble/plot_bias_variance.ipynb)<br/>([_Plot individual and voting regression predictions_](https://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='stacked-generalization'></a> 1.11.6. **Empilement généralisé**<br/>([_Stacked generalization_](https://scikit-learn.org/stable/modules/ensemble.html#stacked-generalization))\n",
    "\n",
    "L'empilement généralisé est une méthode pour combiner des estimateurs afin de réduire leurs biais [W1992] [HTF]. Plus précisément, les prédictions de chaque estimateur individuel sont empilées ensemble et utilisées en entrée pour un estimateur final afin de calculer la prédiction. Cet estimateur final est entraîné par validation croisée.\n",
    "\n",
    "Le [**`StackingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) et le [**`StackingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html#sklearn.ensemble.StackingRegressor) fournissent de telles stratégies qui peuvent être appliquées aux problèmes de classification et de régression.\n",
    "\n",
    "Le paramètre `estimators` correspond à la liste des estimateurs qui sont empilés en parallèle sur les données d'entrée. Il doit être fourni sous forme d'une liste de noms et d'estimateurs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "estimators = [\n",
    "    (\"ridge\", RidgeCV()),\n",
    "    (\"lasso\", LassoCV(random_state=42)),\n",
    "    (\"knr\", KNeighborsRegressor(n_neighbors=20, metric=\"euclidean\"))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le `final_estimator` utilisera comme entrée les prédictions des `estimators`. Il doit être un classifieur ou un régresseur lors de l'utilisation de [**`StackingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) ou [**`StackingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html#sklearn.ensemble.StackingRegressor), respectivement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "final_estimator = GradientBoostingRegressor(\n",
    "    n_estimators=25, subsample=0.5, min_samples_leaf=25, max_features=1,\n",
    "    random_state=42\n",
    ")\n",
    "reg = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=final_estimator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour entraîner les `estimators` et le `final_estimator`, la méthode `fit` doit être appelée sur les données d'entraînement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StackingRegressor(estimators=[(&#x27;ridge&#x27;, RidgeCV()),\n",
       "                              (&#x27;lasso&#x27;, LassoCV(random_state=42)),\n",
       "                              (&#x27;knr&#x27;,\n",
       "                               KNeighborsRegressor(metric=&#x27;euclidean&#x27;,\n",
       "                                                   n_neighbors=20))],\n",
       "                  final_estimator=GradientBoostingRegressor(max_features=1,\n",
       "                                                            min_samples_leaf=25,\n",
       "                                                            n_estimators=25,\n",
       "                                                            random_state=42,\n",
       "                                                            subsample=0.5))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StackingRegressor</label><div class=\"sk-toggleable__content\"><pre>StackingRegressor(estimators=[(&#x27;ridge&#x27;, RidgeCV()),\n",
       "                              (&#x27;lasso&#x27;, LassoCV(random_state=42)),\n",
       "                              (&#x27;knr&#x27;,\n",
       "                               KNeighborsRegressor(metric=&#x27;euclidean&#x27;,\n",
       "                                                   n_neighbors=20))],\n",
       "                  final_estimator=GradientBoostingRegressor(max_features=1,\n",
       "                                                            min_samples_leaf=25,\n",
       "                                                            n_estimators=25,\n",
       "                                                            random_state=42,\n",
       "                                                            subsample=0.5))</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>ridge</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RidgeCV</label><div class=\"sk-toggleable__content\"><pre>RidgeCV()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lasso</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LassoCV</label><div class=\"sk-toggleable__content\"><pre>LassoCV(random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>knr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsRegressor</label><div class=\"sk-toggleable__content\"><pre>KNeighborsRegressor(metric=&#x27;euclidean&#x27;, n_neighbors=20)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(max_features=1, min_samples_leaf=25, n_estimators=25,\n",
       "                          random_state=42, subsample=0.5)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "StackingRegressor(estimators=[('ridge', RidgeCV()),\n",
       "                              ('lasso', LassoCV(random_state=42)),\n",
       "                              ('knr',\n",
       "                               KNeighborsRegressor(metric='euclidean',\n",
       "                                                   n_neighbors=20))],\n",
       "                  final_estimator=GradientBoostingRegressor(max_features=1,\n",
       "                                                            min_samples_leaf=25,\n",
       "                                                            n_estimators=25,\n",
       "                                                            random_state=42,\n",
       "                                                            subsample=0.5))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "reg.fit(X_train, y_train)\n",
    "# StackingRegressor(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pendant l'entraînement, les `estimators` sont ajustés sur l'ensemble complet des données d'entraînement `X_train`. Ils seront utilisés lors de l'appel à `predict` ou `predict_proba`. Pour généraliser et éviter le sur-ajustement, le `final_estimator` est entraîné sur des données hors-échantillon en utilisant [**`sklearn.model_selection.cross_val_predict`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html#sklearn.model_selection.cross_val_predict) en interne.\n",
    "\n",
    "Pour [**`StackingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier), notez que la sortie des `estimators` est contrôlée par le paramètre `stack_method` et est appelée par chaque estimateur. Ce paramètre est soit une chaîne de caractères, étant des noms de méthodes d'estimateur, soit `'auto'`, qui identifiera automatiquement une méthode disponible en fonction de la disponibilité, testée dans l'ordre de préférence : `predict_proba`, `decision_function` et `predict`.\n",
    "\n",
    "Un [**`StackingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html#sklearn.ensemble.StackingRegressor) et [**`StackingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) peuvent être utilisés comme tout autre régresseur ou classifieur, exposant des méthodes `predict`, `predict_proba` et `decision_function`, par exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.53\n"
     ]
    }
   ],
   "source": [
    "y_pred = reg.predict(X_test)\n",
    "from sklearn.metrics import r2_score\n",
    "print(f\"R2 score: {r2_score(y_test, y_pred):.2f}\")\n",
    "# R2 score: 0.53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez qu'il est également possible d'obtenir la sortie des `estimators` empilés en utilisant la méthode `transform` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[142.36209608, 138.30724927, 146.1       ],\n",
       "       [179.700576  , 182.89812552, 151.75      ],\n",
       "       [139.89817956, 132.46803343, 158.25      ],\n",
       "       [286.95180286, 292.65695767, 225.4       ],\n",
       "       [126.88317154, 124.1215975 , 164.65      ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.transform(X_test[:5])\n",
    "# array([[142..., 138..., 146...],\n",
    "#        [179..., 182..., 151...],\n",
    "#        [139..., 132..., 158...],\n",
    "#        [286..., 292..., 225...],\n",
    "#        [126..., 124..., 164...]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En pratique, un prédicteur empilé prédit aussi bien que le meilleur prédicteur de la couche de base et parfois le surpasse en combinant les différentes forces de ces prédicteurs. Cependant, l'entraînement d'un prédicteur empilé est coûteux en termes de calcul.\n",
    "\n",
    "> **Note :** Pour [**`StackingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier), lors de l'utilisation de `stack_method_='predict_proba'`, la première colonne est supprimée lorsque le problème est un problème de classification binaire. En effet, les deux colonnes de probabilité prédites par chaque estimateur sont parfaitement colinéaires.\n",
    "\n",
    "> **Note :** Plusieurs couches d'empilement peuvent être réalisées en attribuant le `final_estimator` à un [**`StackingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) ou [**`StackingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html#sklearn.ensemble.StackingRegressor) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.53\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "final_layer_rfr = RandomForestRegressor(\n",
    "    n_estimators=10, max_features=1, max_leaf_nodes=5,\n",
    "    random_state=42\n",
    ")\n",
    "final_layer_gbr = GradientBoostingRegressor(\n",
    "    n_estimators=10, max_features=1, max_leaf_nodes=5,\n",
    "    random_state=42\n",
    ")\n",
    "final_layer = StackingRegressor(\n",
    "    estimators=[\n",
    "        (\"rf\", final_layer_rfr),\n",
    "        (\"gbrt\", final_layer_gbr)\n",
    "    ],\n",
    "    final_estimator=RidgeCV()\n",
    ")\n",
    "multi_layer_regressor = StackingRegressor(\n",
    "    estimators=[\n",
    "        (\"ridge\", RidgeCV()),\n",
    "        (\"lasso\", LassoCV(random_state=42)),\n",
    "        (\"knr\", KNeighborsRegressor(n_neighbors=20, metric=\"euclidean\"))\n",
    "    ],\n",
    "    final_estimator=final_layer\n",
    ")\n",
    "multi_layer_regressor.fit(X_train, y_train)\n",
    "# StackingRegressor(...)\n",
    "print(f\"R2 score: {multi_layer_regressor.score(X_test, y_test):.2f}\")\n",
    "# R2 score: 0.53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Références\n",
    "\n",
    "🔬 [W1992] Wolpert, David H. [**“Stacked generalization”**](http://machine-learning.martinsewell.com/ensembles/stacking/Wolpert1992.pdf). Neural networks 5.2 (1992): 241-259."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='adaboost'></a> 1.11.7. **AdaBoost**<br/>([_AdaBoost_](https://scikit-learn.org/stable/modules/ensemble.html#adaboost))\n",
    "\n",
    "\n",
    "Le module [**`sklearn.ensemble`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble) comprend l'algorithme de renforcement populaire AdaBoost, introduit en 1995 par Freund et Schapire [FS1995].\n",
    "\n",
    "Le principe fondamental d'AdaBoost est de s'adapter à une séquence de modèles faibles (c'est-à-dire des modèles qui sont légèrement meilleurs que de simples prédictions aléatoires, tels que de petits arbres de décision) sur des versions modifiées à plusieurs reprises des données. Les prédictions de chacun d'entre eux sont ensuite combinées par le biais d'un vote majoritaire pondéré (ou d'une somme) pour produire la prédiction finale. Les modifications des données à chaque itération d'amplification, appelée itération de boosting, consistent à appliquer des poids $w_1, w_2, \\cdots, w_N$ à chacun des échantillons d'entraînement. Initialement, ces poids sont tous définis à $w_i=1/N$, de sorte que la première étape consiste simplement à former un modèle faible sur les données d'origine. À chaque itération successive, les poids des échantillons sont modifiés individuellement, et l'algorithme d'apprentissage est réappliqué aux données pondérées. À une étape donnée, les exemples d'entraînement qui ont été prédits de manière incorrecte par le modèle renforcé induit à l'étape précédente voient leurs poids augmenter, tandis que les poids sont diminués pour ceux qui ont été prédits correctement. À mesure que les itérations avancent, les exemples difficiles à prédire reçoivent une influence de plus en plus importante. Chaque modèle faible subséquent est ainsi contraint de se concentrer sur les exemples que les modèles précédents de la séquence ont manqués [HTF].\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_adaboost_hastie_10_2_001.png\"\n",
    "    alt=\"AdaBoost discret vs réel\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "AdaBoost peut être utilisé à la fois pour les problèmes de classification et de régression :\n",
    "* Pour la classification multi-classe, [**`AdaBoostClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier) implémente AdaBoost-SAMME et AdaBoost-SAMME.R [ZZRH2009].\n",
    "* Pour la régression, [**`AdaBoostRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor) implémente AdaBoost.R2 [D1997]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='id35'></a> 1.11.7.1. **Utilisation**<br/>([_Usage_](https://scikit-learn.org/stable/modules/ensemble.html#id35))\n",
    "\n",
    "L'exemple suivant montre comment ajuster un classificateur AdaBoost avec 100 apprenants faibles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9466666666666665"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()\n",
    "# 0.9..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre d'apprenants faibles est contrôlé par le paramètre `n_estimators`. Le paramètre `learning_rate` contrôle la contribution des apprenants faibles dans la combinaison finale. Par défaut, les apprenants faibles sont des souches de décision. Différents apprenants faibles peuvent être spécifiés via le paramètre `base_estimator`. Les principaux paramètres à régler pour obtenir de bons résultats sont les `n_estimators` et la complexité des estimateurs de base (par exemple, sa profondeur `max_depth` ou le nombre minimum d'échantillons requis pour considérer un fractionnement `min_samples_split`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**AdaBoost discret vs. réel**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_adaboost_hastie_10_2.ipynb)<br/>([*Discrete versus Real AdaBoost*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_hastie_10_2.html))\n",
    "\n",
    "Compare l'erreur de classification d'une souche de décision, d'un arbre de décision et d'une souche de décision amplifiée à l'aide d'AdaBoost-SAMME et d'AdaBoost-SAMME.R.\n",
    "\n",
    "#### [**Arbres de décision AdaBoosted multi-classe**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_adaboost_multiclass.ipynb)<br/>([*Multi-class AdaBoosted Decision Trees*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_multiclass.html))\n",
    "\n",
    "Montre les performances d'AdaBoost-SAMME et d'AdaBoost-SAMME.R sur un problème multi-classe.\n",
    "\n",
    "#### [**AdaBoost à deux classes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_adaboost_twoclass.ipynb)<br/>([*Two-class AdaBoost*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_twoclass.html))\n",
    "\n",
    "Montre les valeurs de la limite de décision et de la fonction de décision pour un problème à deux classes non linéairement séparable à l'aide d'AdaBoost-SAMME.\n",
    "\n",
    "#### [**Régression d'arbre de décision avec AdaBoost**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_11_ensembles/plot_adaboost_regression.ipynb)<br/>([*Decision Tree Regression with AdaBoost*](https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_regression.html))\n",
    "\n",
    "Illustre la régression avec l'algorithme AdaBoost.R2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Références\n",
    "\n",
    "🔬 [FS1995] Y. Freund, and R. Schapire, [“**A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting”**](https://reader.elsevier.com/reader/sd/pii/S002200009791504X?token=AD1674DDF28DD408B7DDD9B0CDADA79A121E6BB81335081764FF30C2992E13A694C3122F25D23A16EB9B688BE2D01526&originRegion=eu-west-1&originCreation=20221106045206), 1997.\n",
    "\n",
    "🔬 [ZZRH2009] J Zhu, H. Zou, S. Rosset, T. Hastie, [**“Multi-class AdaBoost”**](https://www.intlpress.com/site/pub/files/_fulltext/journals/sii/2009/0002/0003/SII-2009-0002-0003-a008.pdf), Statistics and Its Interface, 2009.\n",
    "\n",
    "🔬 [D1997] Drucker. [“**Improving Regressors using Boosting Techniques”**](https://www.researchgate.net/profile/Harris-Drucker/publication/2424244_Improving_Regressors_Using_Boosting_Techniques/links/0deec51ae736538cec000000/Improving-Regressors-Using-Boosting-Techniques.pdf), 1997.\n",
    "\n",
    "📚 [HTF] (1,2,3)  T. Hastie, R. Tibshirani et J. Friedman, [**“Elements of Statistical Learning Ed. 2”**](https://hastie.su.domains/Papers/ESLII.pdf), Springer, 2009."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e03b612d84ba21ce95ed447e81b3062e1eb99b56c6d885cdab4aaa12f1b8e240"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
