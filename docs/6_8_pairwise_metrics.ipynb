{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='pairwise-metrics-affinities-and-kernels'></a> 6.8. [**Mesures par paires, Affinit√©s et Noyaux**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_8_pairwise_metrics.ipynb#pairwise-metrics-affinities-and-kernels)</br>([_Pairwise metrics, Affinities and Kernels_](https://scikit-learn.org/stable/modules/metrics.html#pairwise-metrics-affinities-and-kernels))\n",
    "\n",
    "Le sous-module [**`sklearn.metrics.pairwise`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics.pairwise) impl√©mente des utilitaires pour √©valuer les distances ou affinit√©s par paires d'ensembles d'√©chantillons.\n",
    "\n",
    "Ce module contient √† la fois des m√©triques de distance et des noyaux. Un bref r√©sum√© est donn√© ici pour les deux.\n",
    "\n",
    "‚úî 6.8. [**M√©triques par paires, affinit√©s et noyaux**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_8_pairwise_metrics.ipynb#pairwise-metrics-affinities-and-kernels)\n",
    "([*Pairwise metrics, Affinities and Kernels*](https://scikit-learn.org/stable/modules/metrics.html#pairwise-metrics-affinities-and-kernels))\n",
    "* **Volume** : 5 pages, 0 exemples, 3 papiers\n",
    "* ‚úî 6.8.1. [**Similitude cosinus**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_8_pairwise_metrics.ipynb#cosine-similarity)\n",
    "([*Cosine similarity*](https://scikit-learn.org/stable/modules/metrics.html#cosine-similarity))\n",
    "* ‚úî 6.8.2. [**Noyau lin√©aire**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_8_pairwise_metrics.ipynb#linear-kernel)\n",
    "([*Linear kernel*](https://scikit-learn.org/stable/modules/metrics.html#linear-kernel))\n",
    "* ‚úî 6.8.3. [**Noyau polynomial**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_8_pairwise_metrics.ipynb#polynomial-kernel)\n",
    "([*Polynomial kernel*](https://scikit-learn.org/stable/modules/metrics.html#polynomial-kernel))\n",
    "* ‚úî 6.8.4. [**Noyau sigmo√Øde**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_8_pairwise_metrics.ipynb#sigmoid-kernel)\n",
    "([*Sigmoid kernel*](https://scikit-learn.org/stable/modules/metrics.html#sigmoid-kernel))\n",
    "* ‚úî 6.8.5. [**Noyau RBF**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_8_pairwise_metrics.ipynb#rbf-kernel)\n",
    "([*RBF kernel*](https://scikit-learn.org/stable/modules/metrics.html#rbf-kernel))\n",
    "* ‚úî 6.8.6. [**Noyau laplacien**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_8_pairwise_metrics.ipynb#laplacian-kernel)\n",
    "([*Laplacian kernel*](https://scikit-learn.org/stable/modules/metrics.html#laplacian-kernel))\n",
    "* ‚úî 6.8.7. [**Noyau du chi carr√©**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_8_pairwise_metrics.ipynb#chi-squared-kernel)\n",
    "([*Chi-squared kernel*](https://scikit-learn.org/stable/modules/metrics.html#chi-squared-kernel))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les m√©triques de distance sont des fonctions $d(a, b)$ telles que $d(a, b) < d(a, c)$ si les objets $a$ et $b$ sont consid√©r√©s comme \"plus similaires\" que les objets $a$ et $c$. Deux objets identiques auraient une distance de z√©ro. L'un des exemples les plus populaires est la distance euclidienne. Pour √™tre une \"vraie\" m√©trique, elle doit respecter les quatre conditions suivantes :\n",
    "\n",
    "1. $d(a, b) \\geq 0$, pour tous les $a$ et $b$\n",
    "2. $d(a, b) = 0$, si et seulement si $a = b$ (d√©finition positive)\n",
    "3. $d(a, b) = d(b, a)$ (sym√©trie)\n",
    "4. $d(a, c) \\leq d(a, b) + d(b, c)$ (in√©galit√© triangulaire)\n",
    "\n",
    "Les noyaux sont des mesures de similarit√©, c'est-√†-dire que $s(a, b) < s(a, c)$ si les objets $a$ et $b$ sont consid√©r√©s comme \"plus similaires\" que les objets $a$ et $c$. Un noyau doit √©galement √™tre semi-d√©fini positif.\n",
    "\n",
    "Il existe plusieurs fa√ßons de convertir une m√©trique de distance en une mesure de similarit√©, telle qu'un noyau. Soit $D$ la distance et $S$ le noyau :\n",
    "\n",
    "1. $S = \\exp(-D \\gamma)$, o√π une heuristique pour choisir $\\gamma$ est $\\frac{1}{n_\\text{features}}$\n",
    "2. $S = \\frac{1}{\\left(\\frac{D}{\\max(D)}\\right)}$\n",
    "\n",
    "Les distances entre les vecteurs ligne de `X` et les vecteurs ligne de `Y` peuvent √™tre √©valu√©es √† l'aide de [**`pairwise_distances`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html#sklearn.metrics.pairwise_distances). Si Y est omis, les distances par paires des vecteurs ligne de `X` sont calcul√©es. De m√™me, [**`pairwise.pairwise_kernels`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_kernels.html#sklearn.metrics.pairwise.pairwise_kernels) peut √™tre utilis√© pour calculer le noyau entre `X` et `Y` en utilisant diff√©rentes fonctions de noyau. Consultez la r√©f√©rence de l'API pour plus de d√©tails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  7.],\n",
       "       [ 3., 11.],\n",
       "       [ 5., 18.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "X = np.array([[2, 3], [3, 5], [5, 8]])\n",
    "Y = np.array([[1, 0], [2, 1]])\n",
    "pairwise_distances(X, Y, metric='manhattan')\n",
    "# array([[ 4.,  2.],\n",
    "#        [ 7.,  5.],\n",
    "#        [12., 10.]])\n",
    "pairwise_distances(X, metric='manhattan')\n",
    "# array([[0., 3., 8.],\n",
    "#        [3., 0., 5.],\n",
    "#        [8., 5., 0.]])\n",
    "pairwise_kernels(X, Y, metric='linear')\n",
    "# array([[ 2.,  7.],\n",
    "#        [ 3., 11.],\n",
    "#        [ 5., 18.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='cosine-similarity'></a> 6.8.1. Similarit√© cosinus\n",
    "\n",
    "[**`cosine_similarity`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html#sklearn.metrics.pairwise.cosine_similarity) calcule le produit scalaire normalis√© $\\ell_2$ de vecteurs. Autrement dit, si $x$ et $y$ sont des vecteurs ligne, leur similarit√© cosinus $k$ est d√©finie comme suit :\n",
    "\n",
    "$$k(x, y) = \\frac{x y^\\top}{\\|x\\| \\|y\\|}$$\n",
    "\n",
    "Cela s'appelle similarit√© cosinus, car la normalisation euclidienne ($\\ell_2$) projette les vecteurs sur la sph√®re unit√©, et leur produit scalaire est alors le cosinus de l'angle entre les points repr√©sent√©s par les vecteurs.\n",
    "\n",
    "Ce noyau est un choix populaire pour calculer la similarit√© entre des documents repr√©sent√©s sous forme de vecteurs tf-idf. [**`cosine_similarity`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html#sklearn.metrics.pairwise.cosine_similarity) accepte des matrices `scipy.sparse`. (Notez que la fonctionnalit√© tf-idf dans `sklearn.feature_extraction.text` peut produire des vecteurs normalis√©s, auquel cas [**`cosine_similarity`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html) est √©quivalent √† [**`linear_kernel`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.linear_kernel.html), mais plus lent.)\n",
    "\n",
    "### R√©f√©rences\n",
    "\n",
    "üìö C.D. Manning, P. Raghavan and H. Sch√ºtze (2008). [‚Äú**Introduction to Information Retrieval**](https://www.cambridge.org/highereducation/books/introduction-to-information-retrieval/669D108D20F556C5C30957D63B5AB65C#overview)[‚Äù](https://drive.google.com/file/d/1Gr8HxMaFYIjK6MbsFNKfdzf3HrNLTSQF/view?usp=drive_link). Cambridge University Press."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='linear-kernel'></a> 6.8.2. Noyau lin√©aire\n",
    "\n",
    "La fonction [**`linear_kernel`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.linear_kernel.html) calcule le noyau lin√©aire, qui est un cas particulier de [**`polynomial_kernel`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.polynomial_kernel.html) avec `degree=1` et `coef0=0` (homog√®ne). Si $x$ et $y$ sont des vecteurs colonnes, leur noyau lin√©aire est :\n",
    "\n",
    "$$k(x, y) = x^\\top y$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='polynomial-kernel'></a> 6.8.3. Noyau polynomial\n",
    "\n",
    "La fonction [**`polynomial_kernel`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.polynomial_kernel.html) calcule le noyau polynomial de degr√© $d$ entre deux vecteurs. Le noyau polynomial repr√©sente la similarit√© entre deux vecteurs. Conceptuellement, les noyaux polynomiaux prennent en compte non seulement la similarit√© entre les vecteurs dans la m√™me dimension, mais aussi entre les dimensions. Lorsqu'ils sont utilis√©s dans des algorithmes d'apprentissage automatique, cela permet de tenir compte de l'interaction entre les caract√©ristiques.\n",
    "\n",
    "Le noyau polynomial est d√©fini comme suit :\n",
    "\n",
    "$$k(x, y) = (\\gamma x^\\top y +c_0)^d$$\n",
    "\n",
    "o√π :\n",
    "- $x$, $y$ sont les vecteurs d'entr√©e\n",
    "- $d$ est le degr√© du noyau\n",
    "\n",
    "Si $c_0$ = 0 le noyau est dit homog√®ne."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='sigmoid-kernel'></a> 6.8.4. Noyau sigmo√Øde\n",
    "\n",
    "La fonction [**`sigmoid_kernel`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.sigmoid_kernel.html#sklearn.metrics.pairwise.sigmoid_kernel) calcule le noyau sigmo√Øde entre deux vecteurs. Le noyau sigmo√Øde est √©galement connu sous le nom de tangente hyperbolique, ou Perceptron multicouche (parce que, dans le domaine des r√©seaux neuronaux, il est souvent utilis√© comme fonction d'activation des neurones). Il est d√©fini comme suit :\n",
    "\n",
    "$$k(x, y) = \\tanh( \\gamma x^\\top y + c_0)$$\n",
    "\n",
    "o√π :\n",
    "- $x$, $y$ sont les vecteurs d'entr√©e\n",
    "- $\\gamma$ est appel√© pente\n",
    "- $c_0$ est appel√© intercept"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='rbf-kernel'></a> 6.8.5. Noyau RBF\n",
    "\n",
    "La fonction [**`rbf_kernel`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.rbf_kernel.html) calcule le noyau de fonction de base radiale (RBF) entre deux vecteurs. Ce noyau est d√©fini comme suit :\n",
    "\n",
    "$$k(x, y) = \\exp( -\\gamma \\| x-y \\|^2)$$\n",
    "\n",
    "o√π $x$ et $y$ sont les vecteurs d'entr√©e. Si $\\gamma = \\sigma^{-2}$ le noyau est connu sous le nom de noyau gaussien de variance $\\sigma^2$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='laplacian-kernel'></a> 6.8.6. Noyau Laplacien\n",
    "\n",
    "La fonction [laplacian_kernel](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.laplacian_kernel.html) est une variante du noyau de fonction de base radiale d√©finie comme suit :\n",
    "\n",
    "$$k(x, y) = \\exp( -\\gamma \\| x-y \\|_1)$$\n",
    "\n",
    "o√π $x$ et $y$ sont les vecteurs d'entr√©e et $\\|x-y\\|_1$ est la distance de Manhattan entre les vecteurs d'entr√©e.\n",
    "\n",
    "Il s'est av√©r√© utile en ML appliqu√© aux donn√©es sans bruit. Voir par exemple üî¨ M. Rupp (2015). [‚Äú**Machine Learning for Quantum Mechanics in a Nutshell**](https://onlinelibrary.wiley.com/doi/epdf/10.1002/qua.24954)[‚Äù](https://drive.google.com/file/d/1rX5YODlxxWFiYz3tgNxCbLVO9e_JZ7_W/view?usp=drive_link)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='chi-squared-kernel'></a> 6.8.7. Noyau du chi carr√©\n",
    "\n",
    "Le noyau du chi carr√© est un choix tr√®s populaire pour l'entra√Ænement des SVM non lin√©aires dans les applications de vision par ordinateur. Il peut √™tre calcul√© √† l'aide de [**`chi2_kernel`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.chi2_kernel.html) puis transmis √† un [**`SVC`**](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) avec `kernel=\"precomputed\"` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics.pairwise import chi2_kernel\n",
    "X = [[0, 1], [1, 0], [.2, .8], [.7, .3]]\n",
    "y = [0, 1, 0, 1]\n",
    "K = chi2_kernel(X, gamma=.5)\n",
    "K\n",
    "# array([[1.        , 0.36787944, 0.89483932, 0.58364548],\n",
    "#        [0.36787944, 1.        , 0.51341712, 0.83822343],\n",
    "#        [0.89483932, 0.51341712, 1.        , 0.7768366 ],\n",
    "#        [0.58364548, 0.83822343, 0.7768366 , 1.        ]])\n",
    "\n",
    "svm = SVC(kernel='precomputed').fit(K, y)\n",
    "svm.predict(K)\n",
    "# array([0, 1, 0, 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il peut √©galement √™tre utilis√© directement comme argument `kernel` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel=chi2_kernel).fit(X, y)\n",
    "svm.predict(X)\n",
    "# array([0, 1, 0, 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le noyau du chi carr√© est donn√© par :\n",
    "\n",
    "$$k(x, y) = \\exp \\left (-\\gamma \\sum_i \\frac{(x[i] - y[i]) ^ 2}{x[i] + y[i]} \\right )$$\n",
    "\n",
    "Les donn√©es sont suppos√©es √™tre non n√©gatives et sont souvent normalis√©es de mani√®re √† avoir une norme $\\ell_1$ √©gale √† un. La normalisation est rationalis√©e par le lien avec la distance du chi carr√©, qui est une distance entre des distributions de probabilit√© discr√®tes.\n",
    "\n",
    "Le noyau du chi carr√© est le plus souvent utilis√© sur des histogrammes (ensembles) de mots visuels.\n",
    "\n",
    "### R√©f√©rences\n",
    "\n",
    "üî¨ Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C. [‚Äú**Local features and kernels for classification of texture and object categories: A comprehensive study**](https://hal.science/hal-00171412/document)[‚Äù](https://drive.google.com/file/d/1mSwzIOjCQrQS52CsKeZHYJpme_gZamld/view?usp=drive_link) International Journal of Computer Vision 200"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
