{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='imputation-of-missing-values'></a> 6.4. [**Imputation des valeurs manquantes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_4_impute.ipynb)</br>([*Imputation of missing values*](https://scikit-learn.org/stable/modules/impute.html))\n",
    "\n",
    "Pour diverses raisons, de nombreux ensembles de données du monde réel contiennent des valeurs manquantes, souvent codées comme des blancs, des NaN ou d'autres indicateurs de position. Cependant, de tels ensembles de données sont incompatibles avec les estimateurs de scikit-learn qui supposent que toutes les valeurs dans un tableau sont numériques et ont une signification. Une stratégie de base pour utiliser des ensembles de données incomplets consiste à supprimer les lignes et/ou les colonnes entières contenant des valeurs manquantes. Cependant, cela se fait au prix de la perte de données qui peuvent être précieuses (même si elles sont incomplètes). Une meilleure stratégie consiste à imputer les valeurs manquantes, c'est-à-dire à les inférer à partir de la partie connue des données. Voir l'entrée du glossaire sur l'[imputation](https://scikit-learn.org/stable/glossary.html#term-imputation).\n",
    "\n",
    "- ✔ 6.4. [**Imputation des valeurs manquantes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_4_impute.ipynb#imputation-of-missing-values)<br/>([*Imputation of missing values*](https://scikit-learn.org/stable/modules/impute.html))\n",
    "    - **Volume** : 8 pages, 2 exemples, 3 papiers\n",
    "    - ✔ 6.4.1. [**Imputation univariée vs imputation multivariée**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_4_impute.ipynb#univariate-vs-multivariate-imputation)<br/>([*Univariate vs. Multivariate Imputation*](https://scikit-learn.org/stable/modules/impute.html#univariate-vs-multivariate-imputation))\n",
    "    - ✔ 6.4.2. [**Imputation de caractéristique univariée**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_4_impute.ipynb#univariate-feature-imputation)<br/>([*Univariate feature imputation*](https://scikit-learn.org/stable/modules/impute.html#univariate-feature-imputation))\n",
    "    - ✔ 6.4.3. [**Imputation de caractéristiques multivariées**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_4_impute.ipynb#multivariate-feature-imputation)<br/>([*Multivariate feature imputation*](https://scikit-learn.org/stable/modules/impute.html#multivariate-feature-imputation))\n",
    "    - ✔ 6.4.4. [**Imputation des plus proches voisins**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_4_impute.ipynb#nearest-neighbors-imputation)<br/>([*Nearest neighbors imputation*](https://scikit-learn.org/stable/modules/impute.html#nearest-neighbors-imputation))\n",
    "    - ✔ 6.4.5. [**Maintenir le nombre de caractéristiques constant**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_4_impute.ipynb#keeping-the-number-of-features-constants)<br/>([*Keeping the number of features constants*](https://scikit-learn.org/stable/modules/impute.html#keeping-the-number-of-features-constants))\n",
    "    - ✔ 6.4.6. [**Marquage des valeurs imputées**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_4_impute.ipynb#marking-imputed-values)<br/>([*Marking imputed values*](https://scikit-learn.org/stable/modules/impute.html#marking-imputed-values))\n",
    "    - ✔ 6.4.7. [**Estimateurs qui gèrent les valeurs NaN**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_4_impute.ipynb#estimators-that-handle-nan-values)<br/>([*Estimators that handle NaN values*](https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='univariate-vs-multivariate-imputation'></a> 6.4.1. Imputation univariée vs. Imputation multivariée\n",
    "\n",
    "Un type d'algorithme d'imputation est univarié, ce qui impute des valeurs dans la dimension de la i-ème caractéristique en n'utilisant que les valeurs non manquantes dans cette dimension (par exemple, [**`impute.SimpleImputer`**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)). En revanche, les algorithmes d'imputation multivariée utilisent l'ensemble complet des dimensions de caractéristiques disponibles pour estimer les valeurs manquantes (par exemple, [**`impute.IterativeImputer`**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html))."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='univariate-feature-imputation'></a> 6.4.2. Imputation univariée des caractéristiques\n",
    "\n",
    "La classe [**`SimpleImputer`**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer) propose des stratégies de base pour l'imputation des valeurs manquantes. Les valeurs manquantes peuvent être imputées avec une valeur constante fournie, ou en utilisant les statistiques (_moyenne_, _médiane_ ou _plus fréquente_) de chaque colonne dans laquelle se trouvent les valeurs manquantes. Cette classe permet également différentes codifications des valeurs manquantes.\n",
    "\n",
    "L'exemple suivant montre comment remplacer les valeurs manquantes, codées en tant que `np.nan`, en utilisant la valeur moyenne des colonnes (axe 0) contenant les valeurs manquantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.         2.        ]\n",
      " [6.         3.66666667]\n",
      " [7.         6.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit([[1, 2], [np.nan, 3], [7, 6]])\n",
    "# SimpleImputer()\n",
    "X = [[np.nan, 2], [6, np.nan], [7, 6]]\n",
    "print(imp.transform(X))\n",
    "# [[4.          2.        ]\n",
    "#  [6.          3.666...]\n",
    "#  [7.          6.        ]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classe [**`SimpleImputer`**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer) prend également en charge les matrices creuses :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 2.]\n",
      " [6. 3.]\n",
      " [7. 6.]]\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "X = sp.csc_matrix([[1, 2], [0, -1], [8, 4]])\n",
    "imp = SimpleImputer(missing_values=-1, strategy='mean')\n",
    "imp.fit(X)\n",
    "# SimpleImputer(missing_values=-1)\n",
    "X_test = sp.csc_matrix([[-1, 2], [6, -1], [7, 6]])\n",
    "print(imp.transform(X_test).toarray())\n",
    "# [[3. 2.]\n",
    "#  [6. 3.]\n",
    "#  [7. 6.]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez que ce format n'est pas destiné à être utilisé pour stocker implicitement les valeurs manquantes dans la matrice, car cela la densifierait lors de la transformation. Les valeurs manquantes codées par 0 doivent être utilisées avec une entrée dense.\n",
    "\n",
    "La classe [**`SimpleImputer`**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer) prend également en charge les données catégorielles représentées sous forme de valeurs de chaîne de caractères ou de catégories pandas lors de l'utilisation de la stratégie `most_frequent` ou `constant` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a' 'x']\n",
      " ['a' 'y']\n",
      " ['a' 'y']\n",
      " ['b' 'y']]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([[\"a\", \"x\"],\n",
    "                   [np.nan, \"y\"],\n",
    "                   [\"a\", np.nan],\n",
    "                   [\"b\", \"y\"]], dtype=\"category\")\n",
    "\n",
    "imp = SimpleImputer(strategy=\"most_frequent\")\n",
    "print(imp.fit_transform(df))\n",
    "# [['a' 'x']\n",
    "#  ['a' 'y']\n",
    "#  ['a' 'y']\n",
    "#  ['b' 'y']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='multivariate-feature-imputation'></a> 6.4.3. Imputation multivariée des caractéristiques\n",
    "\n",
    "Une approche plus sophistiquée consiste à utiliser la classe [**`IterativeImputer`**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html), qui modélise chaque caractéristique avec des valeurs manquantes comme une fonction des autres caractéristiques et utilise cette estimation pour l'imputation. Cela se fait de manière itérative : à chaque étape, une colonne de caractéristiques est désignée comme sortie `y` et les autres colonnes de caractéristiques sont traitées comme des entrées `X`. Un régresseur est ajusté sur `(X, y)` pour `y` connu. Ensuite, le régresseur est utilisé pour prédire les valeurs manquantes de `y`. Cela est fait pour chaque caractéristique de manière itérative, puis répété pour un certain nombre d'itérations d'imputation (`max_iter`). Les résultats de la dernière itération d'imputation sont renvoyés.\n",
    "\n",
    "**Remarque** Cet estimateur est encore **expérimental** pour le moment : les paramètres par défaut ou les détails du comportement peuvent changer sans aucun cycle de dépréciation. Résoudre les problèmes suivants aiderait à stabiliser [**`IterativeImputer`**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html) : critères de convergence ([#14338](https://github.com/scikit-learn/scikit-learn/issues/14338)), estimateurs par défaut ([#13286](https://github.com/scikit-learn/scikit-learn/issues/13286)) et utilisation de l'état aléatoire ([#15611](https://github.com/scikit-learn/scikit-learn/issues/15611)). Pour l'utiliser, vous devez importer explicitement `enable_iterative_imputer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.]\n",
      " [ 6. 12.]\n",
      " [ 3.  6.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "imp.fit([[1, 2], [3, 6], [4, 8], [np.nan, 3], [7, np.nan]])\n",
    "# IterativeImputer(random_state=0)\n",
    "X_test = [[np.nan, 2], [6, np.nan], [np.nan, 6]]\n",
    "# the model learns that the second feature is double the first\n",
    "print(np.round(imp.transform(X_test)))\n",
    "# [[ 1.  2.]\n",
    "#  [ 6. 12.]\n",
    "#  [ 3.  6.]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À la fois [**`SimpleImputer`**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer) et [**`IterativeImputer`**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html) peuvent être utilisés dans un pipeline pour construire un estimateur composite prenant en charge l'imputation. Voir [**Imputation des valeurs manquantes avant la construction d'un estimateur**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/6_4_impute/plot_missing_values.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='flexibility-of-iterativeimputer'></a> 6.4.3.1. Souplesse d'IterativeImputer\n",
    "\n",
    "Il existe de nombreux packages d'imputation bien établis dans l'écosystème R de la science des données : Amelia, mi, mice, missForest, etc. missForest est populaire et s'avère être une instance particulière de différents algorithmes d'imputation séquentiels qui peuvent tous être implémentés avec [**`IterativeImputer`**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html) en passant différents régresseurs pour prédire les valeurs manquantes des caractéristiques. Dans le cas de missForest, ce régresseur est un RandomForest. Voir [**Imputation des valeurs manquantes avec des variantes de l'IterativeImputer**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/6_4_impute/plot_iterative_imputer_variants_comparison.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='multiple-vs-single-imputation'></a> 6.4.3.2. Imputation multiple vs. Imputation unique\n",
    "\n",
    "Dans la communauté statistique, il est courant de réaliser des imputations multiples, générant par exemple `m` imputations distinctes pour une seule matrice de caractéristiques. Chacune de ces `m` imputations est ensuite soumise au pipeline d'analyse ultérieur (par exemple, l'ingénierie des caractéristiques, le regroupement, la régression, la classification). Les `m` résultats finaux de l'analyse (par exemple, les erreurs de validation rétentionnée) permettent au scientifique des données de comprendre comment les résultats analytiques peuvent différer en raison de l'incertitude inhérente causée par les valeurs manquantes. Cette pratique est appelée imputation multiple.\n",
    "\n",
    "Notre implémentation d'[**`IterativeImputer`**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html) s'est inspirée du package R MICE (Multivariate Imputation by Chained Equations) [1], mais diffère de celui-ci en renvoyant une seule imputation au lieu de plusieurs imputations. Cependant, [**`IterativeImputer`**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html) peut également être utilisé pour des imputations multiples en l'appliquant de manière répétée au même jeu de données avec différentes graines aléatoires lorsque `sample_posterior=True`. Voir [2], chapitre 4 pour plus de discussions sur les imputations multiples par rapport aux imputations uniques.\n",
    "\n",
    "Il reste un problème ouvert concernant l'utilité de l'imputation unique par rapport à l'imputation multiple dans le contexte de la prédiction et de la classification lorsque l'utilisateur n'est pas intéressé par la mesure de l'incertitude due aux valeurs manquantes.\n",
    "\n",
    "Notez qu'un appel à la méthode `transform` de [**`IterativeImputer`**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html) n'est pas autorisé à modifier le nombre d'échantillons. Par conséquent, les imputations multiples ne peuvent pas être réalisées par un seul appel à `transform`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Références\n",
    "\n",
    "[1] Stef van Buuren, Karin Groothuis-Oudshoorn (2011). [“**mice: Multivariate Imputation by Chained Equations in R**](https://www.jstatsoft.org/article/view/v045i03)[”](https://drive.google.com/file/d/1TCGl_roQnvBQp-OntDPQZ-edvrQkQPxN/view?usp=drive_link). Journal of Statistical Software 45: 1-67.Stef van Buuren, Karin Groothuis-Oudshoorn (2011). “**mice: Multivariate Imputation by Chained Equations in R**”. Journal of Statistical Software 45: 1-67.\n",
    "\n",
    "[2] [**BOOK**] Roderick J A Little and Donald B Rubin (1986). [“**Statistical Analysis with Missing Data**](https://onlinelibrary.wiley.com/doi/book/10.1002/9781119013563)[”](https://drive.google.com/file/d/1eGVHvjaFN5nEthRMHdTD3CXhQJPFmFKc/view?usp=drive_link). John Wiley & Sons, Inc., New York, NY, USA."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.3.4. Exemples\n",
    "\n",
    "#### [**Imputation des valeurs manquantes avant la construction d'un estimateur**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/6_4_impute/plot_missing_values.ipynb)<br/>([_Imputing missing values before building an estimator_](https://scikit-learn.org/stable/auto_examples/impute/plot_missing_values.html))\n",
    "\n",
    "#### [**Imputation des valeurs manquantes avec des variantes de l'IterativeImputer**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/6_4_impute/plot_iterative_imputer_variants_comparison.ipynb)<br/>([_Imputing missing values with variants of IterativeImputer_](https://scikit-learn.org/stable/auto_examples/impute/plot_iterative_imputer_variants_comparison.html))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='nearest-neighbors-imputation'></a> 6.4.4. Imputation par les plus proches voisins\n",
    "\n",
    "La classe [**`KNNImputer`**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html) permet d'imputer les valeurs manquantes en utilisant l'approche des k-plus proches voisins. Par défaut, une mesure de distance euclidienne prenant en compte les valeurs manquantes, `nan_euclidean_distances`, est utilisée pour trouver les voisins les plus proches. Chaque caractéristique manquante est imputée en utilisant les valeurs des `n_neighbors` voisins les plus proches qui ont une valeur pour la caractéristique. Les caractéristiques des voisins sont moyennées de manière uniforme ou pondérée par la distance de chaque voisin. Si un échantillon a plusieurs caractéristiques manquantes, les voisins pour cet échantillon peuvent être différents en fonction de la caractéristique particulière à imputer. Lorsque le nombre de voisins disponibles est inférieur à `n_neighbors` et qu'il n'y a pas de distances définies par rapport à l'ensemble d'entraînement, la moyenne de l'ensemble d'entraînement pour cette caractéristique est utilisée pendant l'imputation. S'il y a au moins un voisin avec une distance définie, la moyenne pondérée ou non pondérée des voisins restants sera utilisée pendant l'imputation. Si une caractéristique est toujours manquante dans l'ensemble d'entraînement, elle est supprimée lors de la transformation. Pour plus d'informations sur la méthodologie, voir la référence [OL2001].\n",
    "\n",
    "L'extrait suivant montre comment remplacer les valeurs manquantes, codées en tant que `np.nan`, en utilisant la valeur moyenne des deux voisins les plus proches des échantillons avec des valeurs manquantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 2. , 4. ],\n",
       "       [3. , 4. , 3. ],\n",
       "       [5.5, 6. , 5. ],\n",
       "       [8. , 8. , 7. ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "nan = np.nan\n",
    "X = [[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]]\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "imputer.fit_transform(X)\n",
    "# array([[1. , 2. , 4. ],\n",
    "#        [3. , 4. , 3. ],\n",
    "#        [5.5, 6. , 5. ],\n",
    "#        [8. , 8. , 7. ]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Références\n",
    "\n",
    "[OL2001] Olga Troyanskaya, Michael Cantor, Gavin Sherlock, Pat Brown, Trevor Hastie, Robert Tibshirani, David Botstein and Russ B. Altman, [“**Missing value estimation methods for DNA microarrays**](https://sci2s.ugr.es/keel/pdf/algorithm/articulo/troyanskaya_cantor_sherlock01.pdf)[”](https://drive.google.com/file/d/1Lz7bcRMKRhXJV-SjF7BGjAhRMSFvx4IO/view?usp=drive_link), BIOINFORMATICS Vol. 17 no. 6, 2001 Pages 520-525."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='keeping-the-number-of-features-constants'></a> 6.4.5. Maintenir le nombre de caractéristiques constant\n",
    "\n",
    "Par défaut, les imputeurs de scikit-learn supprimeront les caractéristiques entièrement vides, c'est-à-dire les colonnes ne contenant que des valeurs manquantes. Par exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [2.],\n",
       "       [3.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer = SimpleImputer()\n",
    "X = np.array([[np.nan, 1], [np.nan, 2], [np.nan, 3]])\n",
    "imputer.fit_transform(X)\n",
    "# array([[1.],\n",
    "#        [2.],\n",
    "#        [3.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première caractéristique dans `X` ne contenant que des `np.nan` a été supprimée après l'imputation. Bien que cette caractéristique ne soit pas utile dans un contexte prédictif, supprimer les colonnes modifie la forme de `X`, ce qui peut poser problème lors de l'utilisation d'imputeurs dans un pipeline d'apprentissage automatique plus complexe. Le paramètre `keep_empty_features` offre la possibilité de conserver les caractéristiques vides en les imputant avec une valeur constante. Dans la plupart des cas, cette valeur constante est zéro :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 2.],\n",
       "       [0., 3.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer.set_params(keep_empty_features=True)\n",
    "# SimpleImputer(keep_empty_features=True)\n",
    "imputer.fit_transform(X)\n",
    "# array([[0., 1.],\n",
    "#        [0., 2.],\n",
    "#        [0., 3.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='marking-imputed-values'></a> 6.4.6. Marquage des valeurs imputées\n",
    "\n",
    "Le transformateur [**`MissingIndicator`**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.MissingIndicator.html#sklearn.impute.MissingIndicator) est utile pour transformer un ensemble de données en une matrice binaire correspondante indiquant la présence de valeurs manquantes dans l'ensemble de données. Cette transformation est utile en conjonction avec l'imputation. Lors de l'utilisation de l'imputation, conserver l'information sur les valeurs manquantes peut être informatif. Notez que les classes [**`SimpleImputer`**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) et [**`IterativeImputer`**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html) disposent toutes deux du paramètre booléen `add_indicator` (`False` par défaut) qui, lorsqu'il est défini sur `True`, permet de superposer facilement la sortie du transformateur [**`MissingIndicator`**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.MissingIndicator.html#sklearn.impute.MissingIndicator) avec la sortie de l'imputeur.\n",
    "\n",
    "`NaN` est généralement utilisé comme espace réservé pour les valeurs manquantes. Cependant, cela impose que le type de données soit flottant. Le paramètre `missing_values` permet de spécifier d'autres espaces réservés tels que les entiers. Dans l'exemple suivant, nous utiliserons `-1` comme valeurs manquantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True, False],\n",
       "       [False,  True,  True],\n",
       "       [False,  True, False]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import MissingIndicator\n",
    "X = np.array([[-1, -1, 1, 3],\n",
    "              [4, -1, 0, -1],\n",
    "              [8, -1, 1, 0]])\n",
    "indicator = MissingIndicator(missing_values=-1)\n",
    "mask_missing_values_only = indicator.fit_transform(X)\n",
    "mask_missing_values_only\n",
    "# array([[ True,  True, False],\n",
    "#        [False,  True,  True],\n",
    "#        [False,  True, False]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le paramètre `features` est utilisé pour choisir les caractéristiques pour lesquelles le masque est construit. Par défaut, il est défini sur `'missing-only'`, ce qui renvoie le masque de l'imputeur des caractéristiques contenant des valeurs manquantes au moment de l'ajustement (appel de `fit`) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 3], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicator.features_\n",
    "# array([0, 1, 3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le paramètre `features` peut être défini sur `'all'` pour renvoyer toutes les caractéristiques, qu'elles contiennent ou non des valeurs manquantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicator = MissingIndicator(missing_values=-1, features=\"all\")\n",
    "mask_all = indicator.fit_transform(X)\n",
    "mask_all\n",
    "# array([[ True,  True, False, False],\n",
    "#        [False,  True, False,  True],\n",
    "#        [False,  True, False, False]])\n",
    "indicator.features_\n",
    "# array([0, 1, 2, 3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lors de l'utilisation de [**`MissingIndicator`**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.MissingIndicator.html) dans un [**`Pipeline`**](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), assurez-vous d'utiliser [**`FeatureUnion`**](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html) ou [**`ColumnTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) pour ajouter les caractéristiques indicatrices aux caractéristiques régulières. Tout d'abord, nous obtenons l'ensemble de données `iris` et y ajoutons quelques valeurs manquantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.impute import SimpleImputer, MissingIndicator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import FeatureUnion, make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X, y = load_iris(return_X_y=True)\n",
    "mask = np.random.randint(0, 2, size=X.shape).astype(bool)\n",
    "X[mask] = np.nan\n",
    "X_train, X_test, y_train, _ = train_test_split(\n",
    "    X, y, test_size=100, random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, nous créons un [**`FeatureUnion`**](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html). Toutes les caractéristiques seront imputées à l'aide de [**`SimpleImputer`**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html), afin de permettre aux classificateurs de travailler avec ces données. De plus, il ajoute les variables indicatrices de [**`MissingIndicator`**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.MissingIndicator.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('features', SimpleImputer(strategy='mean')),\n",
    "        ('indicators', MissingIndicator())])\n",
    "transformer = transformer.fit(X_train, y_train)\n",
    "results = transformer.transform(X_test)\n",
    "results.shape\n",
    "# (100, 8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien sûr, nous ne pouvons pas utiliser le transformeur pour effectuer des prédictions. Nous devons l'envelopper dans un `Pipeline` avec un classifieur (par exemple, un `DecisionTreeClassifier`) pour pouvoir faire des prédictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(transformer, DecisionTreeClassifier())\n",
    "clf = clf.fit(X_train, y_train)\n",
    "results = clf.predict(X_test)\n",
    "results.shape\n",
    "# (100,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='estimators-that-handle-nan-values'></a> 6.4.7. Estimateurs qui gèrent les valeurs NaN\n",
    "\n",
    "Certains estimateurs sont conçus pour gérer les valeurs NaN sans prétraitement. Voici la liste de ces estimateurs, classés par type (regroupement, régresseur, classifieur, transformeur) :\n",
    "\n",
    "- **Les estimateurs qui autorisent les valeurs NaN pour le type de `regressor` :**\n",
    "    - [**`HistGradientBoostingRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html)\n",
    "- **Les estimateurs qui autorisent les valeurs NaN pour le type de `classifier` :**\n",
    "    - [**`HistGradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html)\n",
    "- **Les estimateurs qui autorisent les valeurs NaN pour le type de `transformer` :**\n",
    "    - [**`IterativeImputer`**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html)\n",
    "    - [**`KNNImputer`**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html)\n",
    "    - [**`MaxAbsScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html)\n",
    "    - [**`MinMaxScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n",
    "    - [**`MissingIndicator`**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.MissingIndicator.html)\n",
    "    - [**`PowerTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html)\n",
    "    - [**`QuantileTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html)\n",
    "    - [**`RobustScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)\n",
    "    - [**`SimpleImputer`**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)\n",
    "    - [**`StandardScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "    - [**`VarianceThreshold`**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
