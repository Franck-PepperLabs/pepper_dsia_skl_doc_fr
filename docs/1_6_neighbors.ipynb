{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='supervised-learning'></a> 1. [**Apprentissage supervisé**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_supervised_learning.ipynb#supervised-learning)</br>([*Supervised learning*](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning))\n",
    "\n",
    "# <a id='nearest-neighbors'></a> 1.6. [**Plus proches voisins ($k$-NN)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb)<br/>([_Nearest Neighbors_](https://scikit-learn.org/stable/modules/neighbors.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 16 pages, 9 exemples, 3 papiers\n",
    "- 1.6.1. [**Plus proches voisins non supervisé**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#unsupervised-nearest-neighbors)<br/>([_Unsupervised Nearest Neighbors_](https://scikit-learn.org/stable/modules/neighbors.html#unsupervised-nearest-neighbors))\n",
    "    - 1.6.1.1. [**Recherche des plus proches voisins**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#finding-the-nearest-neighbors)<br/>([_Finding the Nearest Neighbors_](https://scikit-learn.org/stable/modules/neighbors.html#finding-the-nearest-neighbors))\n",
    "    - 1.6.1.2. [**Classes `KDTree` et `BallTree`**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#kdtree-and-balltree-classes)<br/>([_`KDTree` and `BallTree` Classes_](https://scikit-learn.org/stable/modules/neighbors.html#kdtree-and-balltree-classes))\n",
    "- 1.6.2. [**Classification des plus proches voisins**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#nearest-neighbors-classification)<br/>([_Nearest Neighbors Classification_](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification))\n",
    "- 1.6.3. [**Régression des plus proches voisins**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#nearest-neighbors-regression)<br/>([_Nearest Neighbors Regression_](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-regression))\n",
    "- 1.6.4. [**Algorithmes des plus proches voisins**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#nearest-neighbor-algorithms)<br/>([_Nearest Neighbor Algorithms_](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbor-algorithms))\n",
    "    - 1.6.4.1. [**Force brute**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#brute-force)<br/>([_Brute Force_](https://scikit-learn.org/stable/modules/neighbors.html#brute-force))\n",
    "    - 1.6.4.2. [**Arbre KD**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#k-d-tree)<br/>([_K-D Tree_](https://scikit-learn.org/stable/modules/neighbors.html#k-d-tree))\n",
    "    - 1.6.4.3. [**Arbre de boules**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#ball-tree)<br/>([_Ball Tree_](https://scikit-learn.org/stable/modules/neighbors.html#ball-tree))\n",
    "    - 1.6.4.4. [**Choix de l'algorithme des plus proches voisins**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#choice-of-nearest-neighbors-algorithm)<br/>([_Choice of Nearest Neighbors Algorithm_](https://scikit-learn.org/stable/modules/neighbors.html#choice-of-nearest-neighbors-algorithm))\n",
    "    - 1.6.4.5. [**Impact de `leaf_size`**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#effect-of-leaf-size)<br/>([_Effect of `leaf_size`_](https://scikit-learn.org/stable/modules/neighbors.html#effect-of-leaf-size))\n",
    "    - 1.6.4.6. [**Métriques valides pour les algorithmes des plus proches voisins**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#valid-metrics-for-nearest-neighbor-algorithms)<br/>([_Valid Metrics for Nearest Neighbor Algorithms_](https://scikit-learn.org/stable/modules/neighbors.html#valid-metrics-for-nearest-neighbor-algorithms))\n",
    "- 1.6.5. [**Classifieur du plus proche centroïde**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#nearest-centroid-classifier)<br/>([_Nearest Centroid Classifier_](https://scikit-learn.org/stable/modules/neighbors.html#nearest-centroid-classifier))\n",
    "    - 1.6.5.1. [**Centroïde rétréci le plus proche**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#nearest-shrunken-centroid)<br/>([_Nearest Shrunken Centroid_](https://scikit-learn.org/stable/modules/neighbors.html#nearest-shrunken-centroid))\n",
    "- 1.6.6. [**Transformeur des plus proches voisins**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#nearest-neighbors-transformer)<br/>([_Nearest Neighbors Transformer_](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-transformer))\n",
    "- 1.6.7. [**Analyse des composantes du voisinage**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#neighborhood-components-analysis)<br/>([_Neighborhood Components Analysis_](https://scikit-learn.org/stable/modules/neighbors.html#neighborhood-components-analysis))\n",
    "    - 1.6.7.1. [**Classification**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#id4)<br/>([_Classification_](https://scikit-learn.org/stable/modules/neighbors.html#id4))\n",
    "    - 1.6.7.2. [**Réduction de dimension**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#dimensionality-reduction)<br/>([_Dimensionality reduction_](https://scikit-learn.org/stable/modules/neighbors.html#dimensionality-reduction))\n",
    "    - 1.6.7.3. [**Formulation mathématique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#mathematical-formulation)<br/>([_Mathematical formulation_](https://scikit-learn.org/stable/modules/neighbors.html#mathematical-formulation))\n",
    "    - 1.6.7.4. [**Implémentation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#implementation)<br/>([_Implementation_](https://scikit-learn.org/stable/modules/neighbors.html#implementation))\n",
    "    - 1.6.7.5. [**Complexité**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#complexity)<br/>([_Complexity_](https://scikit-learn.org/stable/modules/neighbors.html#complexity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='nearest-neighbors'></a> 1.6. **Plus proches voisins ($k$-NN)**<br/>([_Nearest Neighbors_](https://scikit-learn.org/stable/modules/neighbors.html))\n",
    "\n",
    "[**`sklearn.neighbors`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors) fournit des fonctionnalités pour les méthodes d'apprentissage non supervisées et supervisées basées sur les voisins. Les plus proches voisins non supervisés sont à la base de nombreuses autres méthodes d'apprentissage, notamment l'apprentissage multiple et le regroupement spectral. L'apprentissage supervisé basé sur les voisins se présente sous deux formes : la [**classification** (1.6.2)](#classification) pour les données avec des étiquettes discrètes et la [**régression** (1.6.3)](#regression) pour les données avec des étiquettes continues.\n",
    "\n",
    "Le principe derrière les méthodes du plus proche voisin est de trouver un nombre prédéfini d'échantillons d'apprentissage les plus proches en distance du nouveau point, et de prédire l'étiquette à partir de ceux-ci. Le nombre d'échantillons peut être une constante définie par l'utilisateur (apprentissage du $k$ plus proche voisin) ou varier en fonction de la densité locale de points (apprentissage du voisin basé sur le rayon). La distance peut, en général, être n'importe quelle mesure métrique : la distance euclidienne standard est le choix le plus courant. Les méthodes basées sur les voisins sont connues sous le nom de méthodes d'apprentissage automatique non généralisantes, car elles \"se souviennent\" simplement de toutes ses données d'entraînement (éventuellement transformées en une structure d'indexation rapide telle qu'un [**Ball Tree** (1.6.4.3)](https://scikit-learn.org/stable/modules/neighbors.html#ball-tree) ou un [**KD Tree** (1.6.4.2)](https://scikit-learn.org/stable/modules/neighbors.html#kd-tree)).\n",
    "\n",
    "Malgré sa simplicité, l'algorithme des plus proches voisins a réussi dans un grand nombre de problèmes de classification et de régression, y compris les chiffres manuscrits et les scènes d'images satellites. Comme méthode non paramétrique, elle réussit souvent dans des situations de classification où la frontière de décision est très irrégulière.\n",
    "\n",
    "Les classes de [**`sklearn.neighbors`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors) peuvent gérer des tableaux NumPy ou des matrices `scipy.sparse` en entrée. Pour les matrices denses, un grand nombre de mesures de distance possibles sont prises en charge. Pour les matrices creuses, les métriques de Minkowski arbitraires sont prises en charge pour les recherches.\n",
    "\n",
    "Il existe de nombreuses procédures d'apprentissage qui reposent sur les voisins les plus proches. Un exemple est l'[**estimation de la densité du noyau** (2.8.2)](https://scikit-learn.org/stable/modules/density.html#kernel-density), discutée dans la section sur l'[**estimation de la densité** (2.8)](https://scikit-learn.org/stable/modules/density.html#density-estimation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='unsupervised-nearest-neighbors'></a> 1.6.1. **Plus proches voisins non supervisé**<br/>([_Unsupervised Nearest Neighbors_](https://scikit-learn.org/stable/modules/neighbors.html#unsupervised-nearest-neighbors))\n",
    "\n",
    "[**`NearestNeighbors`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors) met en œuvre l'apprentissage non supervisé des plus proches voisins. Il agit comme une interface uniforme pour trois algorithmes différents des plus proches voisins : [**`BallTree`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree), [**`KDTree`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree), et un algorithme de force brute basé sur les routines de [**`sklearn.metrics.pairwise`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics.pairwise). Le choix de l'algorithme de recherche de voisins est contrôlé par le mot-clé `'algorithm'`, qui doit être l'un des suivants : `['auto', 'ball_tree', 'kd_tree', 'brute']`. Lorsque la valeur par défaut `'auto'` est utilisée, l'algorithme tente de déterminer la meilleure approche à partir des données d'entraînement. Pour une discussion des forces et des faiblesses de chaque option, consultez [**Algorithmes des plus proches voisins** (1.6.4)](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbor-algorithms).\n",
    "\n",
    "> **Avertissement :** En ce qui concerne les algorithmes des plus proches voisins, si deux voisins $k + 1$ et $k$ ont des distances identiques mais des étiquettes différentes, le résultat dépendra de l'ordre des données d'entraînement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='finding-the-nearest-neighbors'></a> 1.6.1.1. **Recherche des plus proches voisins**<br/>([_Finding the Nearest Neighbors_](https://scikit-learn.org/stable/modules/neighbors.html#finding-the-nearest-neighbors))\n",
    "\n",
    "Pour la tâche simple de trouver les plus proches voisins entre deux ensembles de données, les algorithmes non supervisés de [**`sklearn.neighbors`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors) peuvent être utilisés :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.41421356],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.41421356]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)\n",
    "distances, indices = nbrs.kneighbors(X)\n",
    "indices\n",
    "# array([[0, 1],\n",
    "#        [1, 0],\n",
    "#        [2, 1],\n",
    "#        [3, 4],\n",
    "#        [4, 3],\n",
    "#        [5, 4]]...)\n",
    "distances\n",
    "# array([[0.        , 1.        ],\n",
    "#        [0.        , 1.        ],\n",
    "#        [0.        , 1.41421356],\n",
    "#        [0.        , 1.        ],\n",
    "#        [0.        , 1.        ],\n",
    "#        [0.        , 1.41421356]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parce que l'ensemble de requête correspond à l'ensemble d'entraînement, le plus proche voisin de chaque point est le point lui-même, à une distance de zéro.\n",
    "\n",
    "Il est également possible de produire efficacement un graphe creux montrant les connexions entre les points voisins :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 1., 0.],\n",
       "       [0., 0., 0., 1., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbrs.kneighbors_graph(X).toarray()\n",
    "# array([[1., 1., 0., 0., 0., 0.],\n",
    "#        [1., 1., 0., 0., 0., 0.],\n",
    "#        [0., 1., 1., 0., 0., 0.],\n",
    "#        [0., 0., 0., 1., 1., 0.],\n",
    "#        [0., 0., 0., 1., 1., 0.],\n",
    "#        [0., 0., 0., 0., 1., 1.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'ensemble de données est structuré de telle sorte que les points proches dans l'ordre des indices sont proches dans l'espace des paramètres, conduisant à une matrice approximativement diagonale par bloc des $k$-plus proches voisins. Un tel graphe creux est utile dans diverses circonstances qui font usage des relations spatiales entre les points pour l'apprentissage non supervisé : en particulier, voir [**`Isomap`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html#sklearn.manifold.Isomap), [**`LocallyLinearEmbedding`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding), et [**`SpectralClustering`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='kdtree-and-balltree-classes'></a> 1.6.1.2. **Classes `KDTree` et `BallTree`**<br/>([_`KDTree` and `BallTree` Classes_](https://scikit-learn.org/stable/modules/neighbors.html#kdtree-and-balltree-classes))\n",
    "\n",
    "Alternativement, il est possible d'utiliser directement les classes [**`KDTree`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree) ou [**`BallTree`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree) pour trouver les voisins les plus proches. C'est la fonctionnalité encapsulée par la classe [**`NearestNeighbors`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors) utilisée précédemment. Le Ball Tree et le KD Tree partagent la même interface ; nous allons montrer un exemple d'utilisation du KD Tree ici :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "kdt = KDTree(X, leaf_size=30, metric='euclidean')\n",
    "kdt.query(X, k=2, return_distance=False)\n",
    "# array([[0, 1],\n",
    "#        [1, 0],\n",
    "#        [2, 1],\n",
    "#        [3, 4],\n",
    "#        [4, 3],\n",
    "#        [5, 4]]...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reportez-vous à la documentation des classes [**`KDTree`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree) et [**`BallTree`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree) pour plus d'informations sur les options disponibles pour les recherches de voisins les plus proches, y compris la spécification des stratégies de requête, des métriques de distance, etc. Pour obtenir une liste de métriques valides, utilisez `KDTree.valid_metrics` et `BallTree.valid_metrics` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['euclidean',\n",
       " 'l2',\n",
       " 'minkowski',\n",
       " 'p',\n",
       " 'manhattan',\n",
       " 'cityblock',\n",
       " 'l1',\n",
       " 'chebyshev',\n",
       " 'infinity',\n",
       " 'seuclidean',\n",
       " 'mahalanobis',\n",
       " 'hamming',\n",
       " 'canberra',\n",
       " 'braycurtis',\n",
       " 'jaccard',\n",
       " 'dice',\n",
       " 'rogerstanimoto',\n",
       " 'russellrao',\n",
       " 'sokalmichener',\n",
       " 'sokalsneath',\n",
       " 'haversine',\n",
       " 'pyfunc']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KDTree, BallTree\n",
    "KDTree.valid_metrics()\n",
    "# ['euclidean', 'l2', 'minkowski', 'p', 'manhattan', 'cityblock', 'l1', 'chebyshev', 'infinity']\n",
    "BallTree.valid_metrics()\n",
    "# ['euclidean', 'l2', 'minkowski', 'p', 'manhattan', 'cityblock', 'l1', 'chebyshev', 'infinity',\n",
    "# 'seuclidean', 'mahalanobis', 'hamming', 'canberra', 'braycurtis', 'jaccard', 'dice',\n",
    "# 'rogerstanimoto', 'russellrao', 'sokalmichener', 'sokalsneath', 'haversine', 'pyfunc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='nearest-neighbors-classification'></a> 1.6.2. **Classification des plus proches voisins**<br/>([_Nearest Neighbors Classification_](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification))\n",
    "\n",
    "Neighbors-based classification is a type of _instance-based learning_ or _non-generalizing learning_: it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.\n",
    "\n",
    "scikit-learn implements two different nearest neighbors classifiers: [**`KNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) implements learning based on the $k$ nearest neighbors of each query point, where $k$ is an integer value specified by the user. [**`RadiusNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier) implements learning based on the number of neighbors within a fixed radius $r$ of each training point, where $r$ is a floating-point value specified by the user.\n",
    "\n",
    "The $k$-neighbors classification in [**`KNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) is the most commonly used technique. The optimal choice of the value is highly data-dependent: in general a larger $k$ suppresses the effects of noise, but makes the classification boundaries less distinct.\n",
    "\n",
    "In cases where the data is not uniformly sampled, radius-based neighbors classification in [**`RadiusNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier) can be a better choice. The user specifies a fixed radius $r$, such that points in sparser neighborhoods use fewer nearest neighbors for the classification. For high-dimensional parameter spaces, this method becomes less effective due to the so-called “curse of dimensionality”.\n",
    "\n",
    "The basic nearest neighbors classification uses uniform weights: that is, the value assigned to a query point is computed from a simple majority vote of the nearest neighbors. Under some circumstances, it is better to weight the neighbors such that nearer neighbors contribute more to the fit. This can be accomplished through the `weights` keyword. The default value, `weights = 'uniform'`, assigns uniform weights to each neighbor. `weights = 'distance'` assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied to compute the weights.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_classification_001.png\"\n",
    "    alt=\"Nearest Neighbors Classification\"\n",
    "    style=\"max-width: 75%; height: auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='nearest-neighbors-classification'></a> 1.6.2. **Classification des plus proches voisins**<br/>([_Classification des plus proches voisins_](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification))\n",
    "\n",
    "La classification basée sur les voisins est un type d'_apprentissage basé sur les instances_ ou d'_apprentissage non généralisant_ : elle ne tente pas de construire un modèle interne général, mais se contente de stocker simplement des instances des données d'entraînement. La classification est calculée à partir d'un vote à la majorité simple des voisins les plus proches de chaque point : un point de requête se voit attribuer la classe de données qui a le plus de représentants parmi les voisins les plus proches du point.\n",
    "\n",
    "scikit-learn implémente deux classifieurs de voisins les plus proches différents : [**`KNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) implémente l'apprentissage basé sur les $k$ voisins les plus proches de chaque point de requête, où $k$ est une valeur entière spécifiée par l'utilisateur. [**`RadiusNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier) implémente l'apprentissage basé sur le nombre de voisins dans un rayon fixe $r$ de chaque point d'entraînement, où $r$ est une valeur en virgule flottante spécifiée par l'utilisateur.\n",
    "\n",
    "La classification basée sur les $k$ voisins dans [**`KNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) est la technique la plus couramment utilisée. Le choix optimal de la valeur $k$ est fortement dépendant des données : en général, une valeur de $k$ plus élevée supprime les effets du bruit, mais rend les limites de classification moins distinctes.\n",
    "\n",
    "Dans les cas où les données ne sont pas échantillonnées de manière uniforme, la classification basée sur les voisins dans [**`RadiusNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier) peut être un meilleur choix. L'utilisateur spécifie un rayon fixe $r$, de sorte que les points dans des voisinages moins denses utilisent moins de voisins les plus proches pour la classification. Pour les espaces de paramètres de grande dimension, cette méthode devient moins efficace en raison du \"fléau de la dimensionnalité\".\n",
    "\n",
    "La classification de base des voisins les plus proches utilise des poids uniformes : c'est-à-dire que la valeur attribuée à un point de requête est calculée à partir d'un vote à la majorité simple des voisins les plus proches. Dans certaines circonstances, il est préférable de pondérer les voisins de telle sorte que les voisins plus proches contribuent davantage à l'ajustement. Cela peut être accompli avec le mot-clé `weights`. La valeur par défaut, `weights = 'uniform'`, attribue des poids uniformes à chaque voisin. `weights = 'distance'` attribue des poids proportionnels à l'inverse de la distance par rapport au point de requête. Alternativement, une fonction définie par l'utilisateur de la distance peut être fournie pour calculer les poids.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_classification_001.png\"\n",
    "    alt=\"Classification des plus proches voisins\"\n",
    "    style=\"max-width: 75%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Classification des plus proches voisins**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_6_neighbors/plot_classification.ipynb)<br/>([_Nearest Neighbors Classification_](https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html))\n",
    "\n",
    "Un exemple de classification en utilisant les voisins les plus proches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='nearest-neighbors-regression'></a> 1.6.3. **Régression des plus proches voisins**<br/>([_Régression des plus proches voisins_](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-regression))\n",
    "\n",
    "La régression basée sur les voisins peut être utilisée dans les cas où les étiquettes des données sont des variables continues plutôt que des variables discrètes. L'étiquette attribuée à un point de requête est calculée en fonction de la moyenne des étiquettes de ses voisins les plus proches.\n",
    "\n",
    "scikit-learn met en œuvre deux régresseurs de voisins différents : [**`KNeighborsRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor) met en œuvre un apprentissage basé sur les $k$ voisins les plus proches de chaque point de requête, où $k$ est une valeur entière spécifiée par l'utilisateur. [**`RadiusNeighborsRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsRegressor.html#sklearn.neighbors.RadiusNeighborsRegressor) met en œuvre un apprentissage basé sur les voisins situés à l'intérieur d'un rayon fixe $r$ du point de requête, où $r$ est une valeur en virgule flottante spécifiée par l'utilisateur.\n",
    "\n",
    "La régression basée sur les voisins la plus élémentaire utilise des poids uniformes : c'est-à-dire que chaque point du voisinage local contribue uniformément à la régression d'un point de requête. Dans certaines circonstances, il peut être avantageux de pondérer les points de telle sorte que les points proches contribuent davantage à la régression que les points éloignés. Cela peut être réalisé grâce au mot-clé `weights`. La valeur par défaut, `weights = 'uniform'`, attribue des poids égaux à tous les points. `weights = 'distance'` attribue des poids proportionnels à l'inverse de la distance par rapport au point de requête. Alternativement, une fonction définie par l'utilisateur basée sur la distance peut être fournie, et elle sera utilisée pour calculer les poids.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_regression_001.png\"\n",
    "    alt=\"Régression des plus proches voisins\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "L'utilisation de voisins multiples en sortie pour la régression est démontrée dans **Complétion de visage avec des estimateurs à sortie multiple**. Dans cet exemple, les entrées X sont les pixels de la moitié supérieure des visages et les sorties Y sont les pixels de la moitié inférieure de ces visages.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_multioutput_face_completion_001.png\"\n",
    "    alt=\"Régression des plus proches voisins\"\n",
    "    style=\"max-width: 75%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Régression des plus proches voisins**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_6_neighbors/plot_regression.ipynb)<br/>([_Nearest Neighbors Regression_](https://scikit-learn.org/stable/auto_examples/neighbors/plot_regression.html))\n",
    "\n",
    "Un exemple de régression utilisant les plus proches voisins.\n",
    "\n",
    "#### [**Complétion de visage avec des estimateurs à sortie multiple**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_6_neighbors/plot_multioutput_face_completion.ipynb)<br/>([_Face completion with a multi-output estimators_](https://scikit-learn.org/stable/auto_examples/neighbors/plot_multioutput_face_completion.html))\n",
    "\n",
    "Un exemple de régression à sortie multiple en utilisant les plus proches voisins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='nearest-neighbor-algorithms'></a> 1.6.4. **Algorithmes des plus proches voisins**<br/>([_Nearest Neighbor Algorithms_](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbor-algorithms))\n",
    "\n",
    "### <a id='brute-force'></a> 1.6.4.1. **Force brute**<br/>([_Brute Force_](https://scikit-learn.org/stable/modules/neighbors.html#brute-force))\n",
    "\n",
    "La recherche rapide des plus proches voisins est un domaine actif de recherche en apprentissage automatique. L'implémentation la plus naïve de recherche de voisins implique le calcul en force brute des distances entre toutes les paires de points de l'ensemble de données : pour $N$ échantillons en $D$ dimensions, cette approche a une complexité en $\\mathcal{O}[D N^2]$. Les recherches de voisins en force brute efficaces peuvent être très compétitives pour de petits échantillons de données. Cependant, à mesure que le nombre d'échantillons $N$ augmente, l'approche en force brute devient rapidement inapplicable. Dans les classes de [**`sklearn.neighbors`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors), les recherches de voisins en force brute sont spécifiées à l'aide du mot-clé `algorithm = 'brute'` et sont calculées à l'aide des routines disponibles dans [**`sklearn.metrics.pairwise`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics.pairwise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='k-d-tree'></a> 1.6.4.2. **Arbre KD**<br/>([_K-D Tree_](https://scikit-learn.org/stable/modules/neighbors.html#k-d-tree))\n",
    "\n",
    "Pour remédier aux inefficacités computationnelles de l'approche en force brute, diverses structures de données basées sur des arbres ont été inventées. En général, ces structures tentent de réduire le nombre de calculs de distance nécessaires en encodant efficacement les informations de distance agrégée pour l'échantillon. L'idée de base est que si le point $A$ est très éloigné du point $B$, et que le point $B$ est très proche du point $C$, alors nous savons que les points $A$ et $C$ sont très éloignés, **sans avoir besoin de calculer explicitement leur distance**. De cette manière, le coût computationnel d'une recherche de voisins les plus proches peut être réduit à $\\mathcal{O}[D N \\log(N)]$ ou mieux. Il s'agit d'une amélioration significative par rapport à la méthode en force brute pour de grandes valeurs de $N$.\n",
    "\n",
    "Une première approche pour exploiter ces informations agrégées a été la structure de données **arbre KD** (abrégée pour **arbre K-dimensionnel**), qui généralise les arbres **Quad-trees** bidimensionnels et les arbres **Oct-trees** tridimensionnels à un nombre arbitraire de dimensions. L'arbre KD est une structure d'arbre binaire qui partitionne de manière récursive l'espace des paramètres le long des axes de données, le divisant en régions orthotropes imbriquées dans lesquelles les points de données sont classés. La construction d'un arbre KD est très rapide : étant donné que la partition est effectuée uniquement le long des axes de données, aucune distance en $D$ dimensions n'a besoin d'être calculée. Une fois construit, le voisin le plus proche d'un point de requête peut être déterminé avec seulement $\\mathcal{O}[\\log(N)]$ calculs de distance. Bien que l'approche de l'arbre KD soit très rapide pour les recherches de voisins en basse dimension ($D < 20$), elle devient inefficace à mesure que $D$ devient très grand : ceci est l'une des manifestations du \"fléau de la dimensionnalité\". Dans scikit-learn, les recherches de voisins avec des arbres KD sont spécifiées en utilisant le mot-clé `algorithm = 'kd_tree'` et sont calculées à l'aide de la classe [**`KDTree`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree).\n",
    "\n",
    "#### Références\n",
    "\n",
    "🔬 [**“Multidimensional binary search trees used for associative searching”**](https://dl.acm.org/doi/pdf/10.1145/361002.361007), Bentley, J.L., Communications of the ACM (1975)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='ball-tree'></a> 1.6.4.3. **Arbre de boules**<br/>([_Ball Tree_](https://scikit-learn.org/stable/modules/neighbors.html#ball-tree))\n",
    "\n",
    "Pour remédier à la mauvaise performance des arbres KD dans des dimensions plus élevées, la structure de données **arbre de boules** a été développée. Alors que les arbres KD partitionnent les données le long des axes cartésiens, les arbres de boules partitionnent les données dans une série d'hyper-sphères imbriquées. Cela rend la construction de l'arbre plus coûteuse que celle de l'arbre KD, mais aboutit à une structure de données qui peut être très efficace sur des données très structurées, même dans des dimensions très élevées.\n",
    "\n",
    "Un arbre de boules divise de manière récursive les données en nœuds définis par un centroïde $C$ et un rayon $r$, de telle sorte que chaque point du nœud se trouve à l'intérieur de l'hyper-sphère définie par $r$ et $C$. Le nombre de points candidats pour une recherche de voisins est réduit en utilisant l'**inégalité triangulaire** :\n",
    "\n",
    "$$\n",
    "|x+y| \\leq |x| + |y|\n",
    "$$\n",
    "\n",
    "Avec cette configuration, un seul calcul de distance entre un point de test et le centroïde est suffisant pour déterminer une limite inférieure et une limite supérieure de la distance à tous les points à l'intérieur du nœud. En raison de la géométrie sphérique des nœuds de l'arbre de boules, il peut surpasser un **arbre KD** dans des dimensions élevées, bien que les performances réelles dépendent fortement de la structure des données d'entraînement. Dans scikit-learn, les recherches de voisins basées sur l'arbre de boules sont spécifiées en utilisant le mot-clé `algorithm = 'ball_tree'` et sont calculées à l'aide de la classe [**`BallTree`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree). Alternativement, l'utilisateur peut travailler directement avec la classe [**`BallTree`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree).\n",
    "\n",
    "#### Références\n",
    "\n",
    "🔬 [**“Five Balltree Construction Algorithms”**](https://www1.icsi.berkeley.edu/ftp/pub/techreports/1989/tr-89-063.pdf), Omohundro, S.M., International Computer Science Institute Technical Report (1989)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='choice-of-nearest-neighbors-algorithm'></a> 1.6.4.4. **Choix de l'algorithme des plus proches voisins**<br/>([_Choice of Nearest Neighbors Algorithm_](https://scikit-learn.org/stable/modules/neighbors.html#choice-of-nearest-neighbors-algorithm))\n",
    "\n",
    "Le choix de l'algorithme optimal pour un jeu de données particulier est une décision compliquée et dépend de plusieurs facteurs :\n",
    "\n",
    "- nombre d'échantillons $N$ (c'est-à-dire `n_samples`) et dimensionnalité $D$ (c'est-à-dire `n_features`).\n",
    "    - Le temps de requête en _force brute_ croît en $\\mathcal{O}[D N]$.\n",
    "    - Le temps de requête en _arbre de boules_ croît approximativement en $\\mathcal{O}[D \\log(N)]$.\n",
    "    - Le temps de requête en _arbre KD_ change avec $D$ d'une manière difficile à caractériser précisément. Pour de faibles valeurs de $D$ (moins de 20 environ), le coût est approximativement de $\\mathcal{O}[D \\log(N)]$, et la requête de l'arbre KD peut être très efficace. Pour des valeurs de $D$ plus élevées, le coût augmente presque jusqu'à $\\mathcal{O}[D N]$, et le surcoût dû à la structure de l'arbre peuvent entraîner des requêtes plus lentes que la force brute.\n",
    "\n",
    "Pour de petits ensembles de données ($N$ inférieur à 30 environ), $\\log(N)$ est comparable à $N$, et les algorithmes de force brute peuvent être plus efficaces qu'une approche basée sur les arbres. [**`KDTree`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree) et [**`BallTree`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree) abordent cette situation en fournissant un paramètre _taille des feuilles_ (`leaf_size`) : cela contrôle le nombre d'échantillons à partir duquel une requête bascule en force brute. Cela permet aux deux algorithmes de se rapprocher de l'efficacité d'un calcul en force brute pour de petits $N$.\n",
    "\n",
    "- structure des données : **dimensionnalité intrinsèque** des données et/ou densité des données. La dimensionnalité intrinsèque fait référence à la dimension $d \\le D$ d'une variété sur laquelle les données reposent, qui peut être plongée de manière linéaire ou non linéaire dans l'espace des paramètres. La rareté fait référence au degré selon lequel les données remplissent l'espace des paramètres (ceci doit être distingué du concept tel qu'il est utilisé dans les matrices « creuses ». La matrice de données peut ne comporter aucune entrée nulle, mais la structure peut toujours être « rare » dans ce sens).\n",
    "\n",
    "    - Le temps de requête en _force brute_ n'est pas modifié par la structure des données.\n",
    "    - Les temps de requête en _arbre de boules_ et en _arbre KD_ peuvent être fortement influencés par la structure des données. En général, des données plus rares avec une dimensionnalité intrinsèque plus petite conduisent à des temps de requête plus rapides. Parce que la représentation interne de l'arbre KD est alignée avec les axes des paramètres, il ne montrera généralement pas autant d'amélioration que l'arbre de boules pour des données arbitrairement structurées.\n",
    "\n",
    "Les ensembles de données utilisés en apprentissage automatique ont tendance à être très structurés et conviennent très bien aux requêtes basées sur des arbres.\n",
    "\n",
    "- nombre de voisins $k$ demandés pour un point de requête.\n",
    "\n",
    "    - Le temps de requête en _force brute_ est très peu sensible à la valeur de $k$.\n",
    "    - Le temps de requête en _arbre de boules_ et en _arbre KD_ deviendra plus lent à mesure que $k$ augmentera. Cela est dû à deux effets : premièrement, un $k$ plus grand nécessite la recherche d'une plus grande partie de l'espace des paramètres. Deuxièmement, l'utilisation de $k > 1$ nécessite l'enfilage interne des résultats lorsque l'arbre est parcouru.\n",
    "\n",
    "Lorsque $k$ devient grand par rapport à $N$, la capacité à élaguer les branches dans une requête basée sur un arbre est réduite. Dans cette situation, les requêtes en force brute peuvent être plus efficaces.\n",
    "\n",
    "- nombre de points de requête. L'arbre de boules et l'arbre KD Tree nécessitent une phase de construction. Le coût de cette construction devient négligeable lorsqu'il est amorti sur de nombreuses requêtes. Cependant, si seulement un petit nombre de requêtes sont effectuées, la construction peut représenter une fraction significative du coût total. Si très peu de points de requête sont requis, la force brute est préférable à une méthode basée sur un arbre.\n",
    "\n",
    "Actuellement, `algorithm = 'auto'` sélectionne `'brute'` si l'une des conditions suivantes est vérifiée :\n",
    "- les données d'entrée sont rares\n",
    "- `metric = 'precomputed'`\n",
    "- $D > 15$\n",
    "- $k \\ge N/2$\n",
    "- `effective_metric_` n'appartient pas à la liste des `VALID_METRICS` pour `'kd_tree'` ou `'ball_tree'`\n",
    "\n",
    "Sinon, il sélectionne le premier de `'kd_tree'` et `'ball_tree'` qui a `effective_metric_` dans sa liste `VALID_METRICS`. Cette heuristique est basée sur les hypothèses suivantes :\n",
    "- le nombre de points de requête est au moins du même ordre que le nombre de points d'entraînement\n",
    "- `leaf_size` est proche de sa valeur par défaut de `30`\n",
    "- lorsque $D > 15$, la dimensionnalité intrinsèque des données est généralement trop élevée pour les méthodes basées sur des arbres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='effect-of-leaf-size'></a> 1.6.4.5. **Impact de `leaf_size`**<br/>([_Effect of `leaf_size`_](https://scikit-learn.org/stable/modules/neighbors.html#effect-of-leaf-size))\n",
    "\n",
    "Comme mentionné précédemment, pour de petites tailles d'échantillon, une recherche en force brute peut être plus efficace qu'une requête basée sur un arbre. Ce fait est pris en compte dans l'arbre de boules et l'arbre KD par un basculement interne vers des recherches en force brute au sein des nœuds feuilles. Le niveau de cette transition peut être spécifié avec le paramètre `leaf_size`. Ce choix de paramètre a de nombreux effets :\n",
    "\n",
    "**temps de construction**\n",
    "\n",
    "Un `leaf_size` plus grand conduit à un temps de construction de l'arbre plus rapide, car il y a moins de nœuds à créer.\n",
    "\n",
    "**temps de requête**\n",
    "\n",
    "Tant un `leaf_size` grand que petit peuvent entraîner un coût de requête sous-optimal. Pour un `leaf_size` approchant 1, le surcoût lié à la traversée des nœuds peut ralentir considérablement les temps de requête. Pour un `leaf_size` approchant la taille de l'ensemble d'entraînement, les requêtes deviennent essentiellement de la force brute. Un bon compromis entre ces deux extrêmes est `leaf_size = 30`, la valeur par défaut du paramètre.\n",
    "\n",
    "**mémoire**\n",
    "\n",
    "À mesure que `leaf_size` augmente, la mémoire requise pour stocker la structure de l'arbre diminue. Cela est particulièrement important dans le cas de l'arbre de boules, qui stocke un centroïde à $D$ dimensions pour chaque nœud. L'espace de stockage requis pour [**`BallTree`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree) est d'environ `1 / leaf_size` fois la taille de l'ensemble d'entraînement.\n",
    "\n",
    "`leaf_size` n'est pas pris en compte pour les requêtes en force brute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='valid-metrics-for-nearest-neighbor-algorithms'></a> 1.6.4.6. **Métriques valides pour les algorithmes des plus proches voisins**<br/>([_Valid Metrics for Nearest Neighbor Algorithms_](https://scikit-learn.org/stable/modules/neighbors.html#valid-metrics-for-nearest-neighbor-algorithms))\n",
    "\n",
    "Pour une liste des métriques disponibles, consultez la documentation de la classe [**`DistanceMetric`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DistanceMetric.html#sklearn.metrics.DistanceMetric) et les métriques répertoriées dans `sklearn.metrics.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`. Notez que la métrique \"cosine\" utilise [**`cosine_distances`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_distances.html#sklearn.metrics.pairwise.cosine_distances).\n",
    "\n",
    "Une liste des métriques valides pour l'un des algorithmes ci-dessus peut être obtenue en utilisant leur méthode `valid_metric()`. Par exemple, les métriques valides pour `KDTree` peuvent être générées avec :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chebyshev', 'cityblock', 'euclidean', 'infinity', 'l1', 'l2', 'manhattan', 'minkowski', 'p']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "print(sorted(KDTree.valid_metrics()))\n",
    "# ['chebyshev', 'cityblock', 'euclidean', 'infinity', 'l1', 'l2', 'manhattan', 'minkowski', 'p']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='nearest-centroid-classifier'></a> 1.6.5. **Classifieur du plus proche centroïde**<br/>([_Nearest Centroid Classifier_](https://scikit-learn.org/stable/modules/neighbors.html#nearest-centroid-classifier))\n",
    "\n",
    "The [**`NearestCentroid`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid) classifier is a simple algorithm that represents each class by the centroid of its members. In effect, this makes it similar to the label updating phase of the [**`KMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) algorithm. It also has no parameters to choose, making it a good baseline classifier. It does, however, suffer on non-convex classes, as well as when classes have drastically different variances, as equal variance in all dimensions is assumed. See Linear Discriminant Analysis ([**`LinearDiscriminantAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis)) and Quadratic Discriminant Analysis ([**`QuadraticDiscriminantAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis)) for more complex methods that do not make this assumption. Usage of the default [**`NearestCentroid`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid) is simple:\n",
    "\n",
    "\n",
    "Le classifieur [**`NearestCentroid`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid) est un algorithme simple qui représente chaque classe par le centroïde de ses membres. En effet, cela le rend similaire à la phase de mise à jour des étiquettes de l'algorithme [**`KMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans). Il n'a pas non plus de paramètres à choisir, ce qui en fait un bon classifieur de base. Cependant, il présente des limites avec des classes non convexes, ainsi que lorsque les classes ont des variances très différentes, car il suppose une variance égale dans toutes les dimensions. Consultez l'Analyse Discriminante Linéaire ([**`LinearDiscriminantAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis)) et l'Analyse Discriminante Quadratique ([**`QuadraticDiscriminantAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis)) pour des méthodes plus complexes qui ne font pas cette hypothèse. L'utilisation du [**`NearestCentroid`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid) par défaut est simple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestCentroid\n",
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "y = np.array([1, 1, 1, 2, 2, 2])\n",
    "clf = NearestCentroid()\n",
    "clf.fit(X, y)\n",
    "# NearestCentroid()\n",
    "print(clf.predict([[-0.8, -1]]))\n",
    "# [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='nearest-shrunken-centroid'></a> 1.6.5.1. **Centroïde rétréci le plus proche**<br/>([_Nearest Shrunken Centroid_](https://scikit-learn.org/stable/modules/neighbors.html#nearest-shrunken-centroid))\n",
    "\n",
    "Le classifieur [**`NearestCentroid`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid) possède un paramètre `shrink_threshold`, qui implémente le classifieur du centroïde rétréci le plus proche. En effet, la valeur de chaque caractéristique pour chaque centroïde est divisée par la variance intra-classe de cette caractéristique. Les valeurs des caractéristiques sont ensuite réduites par `shrink_threshold`. Notamment, si une valeur de caractéristique particulière traverse zéro, elle est fixée à zéro. En effet, cela supprime la caractéristique de l'influence sur la classification. Cela est utile, par exemple, pour éliminer les caractéristiques bruyantes.\n",
    "\n",
    "Dans l'exemple ci-dessous, l'utilisation d'un petit seuil de réduction augmente la précision du modèle de 0,81 à 0,82.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_nearest_centroid_001.png\"\n",
    "    alt=\"Classification par le Centroïde le Plus Proche, sans rétrécissement\"\n",
    "    style=\"max-width: 30%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_nearest_centroid_002.png\"\n",
    "    alt=\"Classification par le Centroïde le Plus Proche, avec rétrécissement\"\n",
    "    style=\"max-width: 30%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Classification par le Centroïde le Plus Proche**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_6_neighbors/plot_nearest_centroid.ipynb)<br/>([_Nearest Centroid Classification_](https://scikit-learn.org/stable/auto_examples/neighbors/plot_nearest_centroid.html))\n",
    "\n",
    "Un exemple de classification en utilisant le centroïde le plus proche avec différents seuils de réduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='nearest-neighbors-transformer'></a> 1.6.6. **Transformeur des plus proches voisins**<br/>([_Nearest Neighbors Transformer_](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-transformer))\n",
    "\n",
    "De nombreux estimateurs de scikit-learn reposent sur les plus proches voisins : plusieurs classificateurs et régresseurs tels que [**`KNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) et [**`KNeighborsRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor), mais aussi certaines méthodes de regroupement comme [**`DBSCAN`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN) et [**`SpectralClustering`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering), ainsi que certaines techniques de plongement de variété comme [**`TSNE`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE) et [**`Isomap`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html#sklearn.manifold.Isomap).\n",
    "\n",
    "Tous ces estimateurs peuvent calculer les plus proches voisins en interne, mais la plupart d'entre eux acceptent également les plus proches voisins précalculés, sous forme de [**graphe creux**](https://scikit-learn.org/stable/glossary.html#term-sparse-graph), comme le fait [**`kneighbors_graph`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.kneighbors_graph.html#sklearn.neighbors.kneighbors_graph) et [**`radius_neighbors_graph`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.radius_neighbors_graph.html#sklearn.neighbors.radius_neighbors_graph). Avec le mode `mode='connectivity'`, ces fonctions renvoient un graphe d'adjacence binaire creux, comme requis, par exemple, dans [**`SpectralClustering`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering). Alors qu'avec `mode='distance'`, elles renvoient un graphe de distances creux, comme requis, par exemple, dans [**`DBSCAN`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN). Pour inclure ces fonctions dans un pipeline scikit-learn, on peut également utiliser les classes correspondantes [**`KNeighborsTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html#sklearn.neighbors.KNeighborsTransformer) et [**`RadiusNeighborsTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsTransformer.html#sklearn.neighbors.RadiusNeighborsTransformer). Les avantages de cette API de graphe creux sont multiples.\n",
    "\n",
    "Tout d'abord, le graphe précalculé peut être réutilisé plusieurs fois, par exemple en faisant varier un paramètre de l'estimateur. Cela peut être fait manuellement par l'utilisateur, ou en utilisant les propriétés de mise en cache du pipeline scikit-learn :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tempfile\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.neighbors import KNeighborsTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import make_regression\n",
    "cache_path = tempfile.gettempdir()  # we use a temporary folder here\n",
    "X, _ = make_regression(n_samples=50, n_features=25, random_state=0)\n",
    "estimator = make_pipeline(\n",
    "    KNeighborsTransformer(mode='distance'),\n",
    "    Isomap(n_components=3, metric='precomputed'),\n",
    "    memory=cache_path)\n",
    "X_embedded = estimator.fit_transform(X)\n",
    "X_embedded.shape\n",
    "# (50, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De plus, le pré-calcul du graphe permet d'obtenir un meilleur contrôle sur l'estimation des plus proches voisins, par exemple en activant le traitement parallèle grâce au paramètre `n_jobs`, qui peut ne pas être disponible dans tous les estimateurs.\n",
    "\n",
    "Enfin, le pré-calcul peut être effectuée par des estimateurs personnalisés pour utiliser différentes implémentations, telles que des méthodes de plus proches voisins approximatives, ou des implémentations avec des types de données spéciaux. Le graphe précalculé de voisins [**sparse graph**](https://scikit-learn.org/stable/glossary.html#term-sparse-graph) doit être formaté comme dans la sortie de [**`radius_neighbors_graph`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.radius_neighbors_graph.html#sklearn.neighbors.radius_neighbors_graph) :\n",
    "\n",
    "- une matrice CSR (bien que COO, CSC ou LIL soient acceptées).\n",
    "- stocker explicitement uniquement les voisins les plus proches de chaque échantillon par rapport aux données d'entraînement. Cela doit inclure ceux à une distance de 0 d'un point de requête, y compris la diagonale de la matrice lors du calcul des voisins les plus proches entre les données d'entraînement et elles-mêmes.\n",
    "- les données de chaque ligne doivent stocker la distance dans l'ordre croissant (facultatif. Les données non triées seront triées de manière stable, ce qui entraînera une surcharge computationnelle).\n",
    "- toutes les valeurs des données doivent être non négatives.\n",
    "- il ne doit y avoir aucune duplication d'`indices` dans aucune ligne (voir https://github.com/scipy/scipy/issues/5807).\n",
    "- si l'algorithme auquel est passée la matrice précalculée utilise $k$ plus proches voisins (par opposition à une distance par rapport au voisinage de rayon), au moins $k$ voisins doivent être stockés dans chaque ligne (ou $k + 1$, comme expliqué dans la note suivante).\n",
    "\n",
    "> **Remarque :** Lorsqu'un nombre spécifique de voisins est interrogé (en utilisant [**`KNeighborsTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html#sklearn.neighbors.KNeighborsTransformer)), la définition de `n_neighbors` est ambiguë car elle peut inclure chaque point d'entraînement comme son propre voisin ou les exclure. Aucun des choix n'est parfait, car les inclure conduit à un nombre différent de voisins non propres pendant l'entraînement et les tests, tandis que les exclure entraîne une différence entre `fit(X).transform(X)` et `fit_transform(X)`, ce qui va à l'encontre de l'API de scikit-learn. Dans [**`KNeighborsTransformer`**](https://scikit-learn.org/stable/modules/generated-skin_neighbors.KNeighborsTransformer.html#sklearn.neighbors.KNeighborsTransformer), nous utilisons la définition qui inclut chaque point d'entraînement comme son propre voisin dans le décompte de `n_neighbors`. Cependant, pour des raisons de compatibilité avec d'autres estimateurs utilisant l'autre définition, un voisin supplémentaire sera calculé lorsque `mode == 'distance'`. Pour maximiser la compatibilité avec tous les estimateurs, un choix sûr consiste à toujours inclure un voisin supplémentaire dans un estimateur de plus proches voisins personnalisé, car les voisins inutiles seront filtrés par les estimateurs ultérieurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**Plus proches voisins approchés dans TSNE**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_6_neighbors/approximate_nearest_neighbors.ipynb)<br/>([_Approximate nearest neighbors in TSNE_](https://scikit-learn.org/stable/auto_examples/neighbors/approximate_nearest_neighbors.html))\n",
    "\n",
    "Un exemple de création d'un pipeline avec [**`KNeighborsTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html#sklearn.neighbors.KNeighborsTransformer) et [**`TSNE`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE). Propose également deux estimateurs personnalisés de plus proches voisins basés sur des packages externes.\n",
    "\n",
    "#### [**Mise en cache des plus proches voisins**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_6_neighbors/plot_caching_nearest_neighbors.ipynb)<br/>([_Caching nearest neighbors_](https://scikit-learn.org/stable/auto_examples/neighbors/plot_caching_nearest_neighbors.html))\n",
    "\n",
    "Un exemple de création d'un pipeline avec [**`KNeighborsTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html#sklearn.neighbors.KNeighborsTransformer) et [**`KNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) pour permettre la mise en cache du graphe des voisins lors d'une recherche de grille d'hyperparamètres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='neighborhood-components-analysis'></a> 1.6.7. **Analyse des composantes du voisinage**<br/>([_Neighborhood Components Analysis_](https://scikit-learn.org/stable/modules/neighbors.html#neighborhood-components-analysis))\n",
    "\n",
    "L'Analyse des composantes du voisinage (NCA, [**`NeighborhoodComponentsAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NeighborhoodComponentsAnalysis.html#sklearn.neighbors.NeighborhoodComponentsAnalysis)) est un algorithme d'apprentissage de métriques de distance qui vise à améliorer la précision de la classification par plus proches voisins par rapport à la distance euclidienne standard. L'algorithme maximise directement une variante stochastique du score des $k$ plus proches voisins ($k$-NN) _leave-one-out_ sur l'ensemble d'entraînement. Il peut également apprendre une projection linéaire de faible dimension des données qui peut être utilisée pour la visualisation des données et la classification rapide.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_illustration_001.png\"\n",
    "    alt=\"Analyse des composantes du voisinage, Points d'origine\"\n",
    "    style=\"max-width: 30%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_illustration_002.png\"\n",
    "    alt=\"Analyse des composantes du voisinage, Intégration de l'ACN\"\n",
    "    style=\"max-width: 30%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Dans la figure illustrée ci-dessus, nous considérons certains points d'un ensemble de données généré de manière aléatoire. Nous nous concentrons sur la classification $k$-NN stochastique du point n° 3. L'épaisseur d'un lien entre l'échantillon 3 et un autre point est proportionnelle à leur distance, et peut être vue comme le poids relatif (ou la probabilité) qu'une règle de prédiction stochastique de plus proche voisin attribuerait à ce point. Dans l'espace d'origine, l'échantillon 3 a de nombreux voisins stochastiques de diverses classes, de sorte que la bonne classe n'est pas très probable. Cependant, dans l'espace projeté appris par NCA, les seuls voisins stochastiques ayant un poids non négligeable sont de la même classe que l'échantillon 3, garantissant que ce dernier sera bien classé. Consultez la [**formulation mathématique** (1.6.7.3)](#nca-mathematical-formulation) pour plus de détails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='id4'></a> 1.6.7.1. **Classification**<br/>([_Classification_](https://scikit-learn.org/stable/modules/neighbors.html#id4))\n",
    "\n",
    "Combined with a nearest neighbors classifier ([**`KNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)), NCA is attractive for classification because it can naturally handle multi-class problems without any increase in the model size, and does not introduce additional parameters that require fine-tuning by the user.\n",
    "\n",
    "NCA classification has been shown to work well in practice for data sets of varying size and difficulty. In contrast to related methods such as Linear Discriminant Analysis, NCA does not make any assumptions about the class distributions. The nearest neighbor classification can naturally produce highly irregular decision boundaries.\n",
    "\n",
    "To use this model for classification, one needs to combine a [**`NeighborhoodComponentsAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NeighborhoodComponentsAnalysis.html#sklearn.neighbors.NeighborhoodComponentsAnalysis) instance that learns the optimal transformation with a [**`KNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) instance that performs the classification in the projected space. Here is an example using the two classes:\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_classification_001.png\"\n",
    "    alt=\"KNN\"\n",
    "    style=\"max-width: 30%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_classification_002.png\"\n",
    "    alt=\"NCA, KNN\"\n",
    "    style=\"max-width: 30%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "The plot shows decision boundaries for Nearest Neighbor Classification and Neighborhood Components Analysis classification on the iris dataset, when training and scoring on only two features, for visualisation purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='id4'></a> 1.6.7.1. **Classification**<br/>([_Classification_](https://scikit-learn.org/stable/modules/neighbors.html#id4))\n",
    "\n",
    "En combinaison avec un classificateur des plus proches voisins ([**`KNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)), l'Analyse des composantes du voisinage (NCA) est attrayante pour la classification car elle peut naturellement gérer des problèmes multi-classes sans augmenter la taille du modèle, et elle n'introduit pas de paramètres supplémentaires qui nécessitent un réglage fin de la part de l'utilisateur.\n",
    "\n",
    "La classification par ACN a montré son efficacité en pratique pour des ensembles de données de tailles et de difficultés variées. Contrairement à des méthodes connexes telles que l'Analyse Discriminante Linéaire, le NCA ne fait aucune hypothèse sur les distributions des classes. La classification par plus proches voisins peut naturellement produire des frontières de décision très irrégulières.\n",
    "\n",
    "Pour utiliser ce modèle pour la classification, il faut combiner une instance d'[**`NeighborhoodComponentsAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NeighborhoodComponentsAnalysis.html#sklearn.neighbors.NeighborhoodComponentsAnalysis) qui apprend la transformation optimale avec une instance de [**`KNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) qui effectue la classification dans l'espace projeté. Voici un exemple utilisant deux classes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9619047619047619\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import (\n",
    "    NeighborhoodComponentsAnalysis,\n",
    "    KNeighborsClassifier\n",
    ")\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.7, random_state=42\n",
    ")\n",
    "nca = NeighborhoodComponentsAnalysis(random_state=42)\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "nca_pipe = Pipeline([('nca', nca), ('knn', knn)])\n",
    "nca_pipe.fit(X_train, y_train)\n",
    "# Pipeline(...)\n",
    "print(nca_pipe.score(X_test, y_test))\n",
    "# 0.96190476..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_classification_001.png\"\n",
    "    alt=\"KNN\"\n",
    "    style=\"max-width: 30%; height; auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_classification_002.png\"\n",
    "    alt=\"NCA, KNN\"\n",
    "    style=\"max-width: 30%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Le graphique montre les frontières de décision pour la Classification par les Plus Proches Voisins et la Classification par Analyse des Composantes du Voisinage sur l'ensemble de données iris, lors de l'entraînement et de l'évaluation sur seulement deux caractéristiques, à des fins de visualisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='dimensionality-reduction'></a> 1.6.7.2. **Réduction de dimension**<br/>([_Dimensionality reduction_](https://scikit-learn.org/stable/modules/neighbors.html#dimensionality-reduction))\n",
    "\n",
    "Le NCA peut être utilisée pour effectuer une réduction de dimensionnalité supervisée. Les données d'entrée sont projetées sur un sous-espace linéaire composé des directions qui minimisent l'objectif du NCA. La dimensionnalité souhaitée peut être définie à l'aide du paramètre `n_components`. Par exemple, la figure suivante montre une comparaison de la réduction de dimensionnalité avec l'Analyse en Composantes Principales ([**`PCA`**](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA)), l'Analyse Discriminante Linéaire ([**`LinearDiscriminantAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis)) et l'Analyse des Composantes du Voisinage ([**`NeighborhoodComponentsAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NeighborhoodComponentsAnalysis.html#sklearn.neighbors.NeighborhoodComponentsAnalysis)) sur l'ensemble de données Digits, un ensemble de données de taille $n_{samples} = 1797$ et $n_{features} = 64$. L'ensemble de données est divisé en un ensemble d'entraînement et un ensemble de test de taille égale, puis standardisé. Pour l'évaluation, l'exactitude de la classification des 3 plus proches voisins est calculée sur les points projetés en 2 dimensions trouvés par chaque méthode. Chaque échantillon de données appartient à l'une des 10 classes.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_dim_reduction_001.png\"\n",
    "    alt=\"PCA, KNN\"\n",
    "    style=\"max-width: 30%; height; auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_dim_reduction_002.png\"\n",
    "    alt=\"LDA, KNN\"\n",
    "    style=\"max-width: 30%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_dim_reduction_003.png\"\n",
    "    alt=\"NCA, KNN\"\n",
    "    style=\"max-width: 30%; height: auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**Illustration de l'Analyse des Composantes du Voisinage**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_6_neighbors/plot_nca_illustration.ipynb)<br/>([_Neighborhood Components Analysis Illustration_](https://scikit-learn.org/stable/auto_examples/neighbors/plot_nca_illustration.html))\n",
    "\n",
    "#### [**Comparaison entre les plus proches voisins avec et sans l'Analyse des Composantes du Voisinage**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_6_neighbors/plot_nca_classification.ipynb)<br/>([_Comparing Nearest Neighbors with and without Neighborhood Components Analysis_](https://scikit-learn.org/stable/auto_examples/neighbors/plot_nca_classification.html))\n",
    "\n",
    "#### [**Réduction de la dimensionnalité avec l'analyse des composantes du voisinage**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_6_neighbors/plot_nca_dim_reduction.ipynb)<br/>([_Dimensionality Reduction with Neighborhood Components Analysis_](https://scikit-learn.org/stable/auto_examples/neighbors/plot_nca_dim_reduction.html))\n",
    "\n",
    "#### [**Apprentissage des variétés sur des chiffres manuscrits : Plongement localement linéaire, Isomap…**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_2_manifold/plot_lle_digits.ipynb)<br/>([_Manifold learning on handwritten digits: Locally Linear Embedding, Isomap…_](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='mathematical-formulation'></a> 1.6.7.3. **Formulation mathématique**<br/>([_Mathematical formulation_](https://scikit-learn.org/stable/modules/neighbors.html#mathematical-formulation))\n",
    "\n",
    "Le but de l'analyse des composantes du voisinage (NCA) est d'apprendre une matrice de transformation linéaire optimale de taille `(n_components, n_features)`, qui maximise la somme de toutes les probabilités $p_i$ que $i$ soit correctement classifié, c'est-à-dire :\n",
    "\n",
    "$$\n",
    "\\underset{L}{\\arg\\max} \\sum\\limits_{i=0}^{N - 1} p_{i}\n",
    "$$\n",
    "\n",
    "avec $N$ = `n_samples` et $p_i$ la probabilité que l'échantillon $i$ soit correctement classifié selon une règle stochastique des plus proches voisins dans l'espace embarqué appris :\n",
    "\n",
    "$$\n",
    "p_{i}=\\sum\\limits_{j \\in C_i}{p_{i j}}\n",
    "$$\n",
    "\n",
    "où $C_i$ est l'ensemble de points de la même classe que l'échantillon $i$, et $p_{i j}$ est le softmax des distances euclidiennes dans l'espace embarqué :\n",
    "\n",
    "$$\n",
    "p_{i j} = \\frac{\\exp(-||L x_i - L x_j||^2)}{\\sum\\limits_{k \\ne i} {\\exp{-(||L x_i - L x_k||^2)}}} , \\quad p_{i i} = 0\n",
    "$$\n",
    "\n",
    "#### Distance de Mahalanobis\n",
    "\n",
    "NCA peut être considérée comme l'apprentissage d'une distance de Mahalanobis (au carré) :\n",
    "\n",
    "$$\n",
    "|| L(x_i - x_j)||^2 = (x_i - x_j)^TM(x_i - x_j),\n",
    "$$\n",
    "\n",
    "où $M$ est une matrice symétrique semi-définie positive de taille `(n_features, n_features)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='implementation'></a> 1.6.7.4. **Implémentation**<br/>([_Implementation_](https://scikit-learn.org/stable/modules/neighbors.html#implementation))\n",
    "\n",
    "Cette implémentation suit ce qui est expliqué dans l'article original [1]. Pour la méthode d'optimisation, elle utilise actuellement la méthode L-BFGS-B de scipy avec un calcul de gradient complet à chaque itération, afin d'éviter d'ajuster le taux d'apprentissage et de fournir un apprentissage stable.\n",
    "\n",
    "Veuillez consulter les exemples ci-dessous et la documentation de la méthode [**`NeighborhoodComponentsAnalysis.fit`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NeighborhoodComponentsAnalysis.html#sklearn.neighbors.NeighborhoodComponentsAnalysis.fit) pour plus d'informations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='complexity'></a> 1.6.7.5. **Complexité**<br/>([_Complexity_](https://scikit-learn.org/stable/modules/neighbors.html#complexity))\n",
    "\n",
    "Posons $N$ = `n_samples`, $D$ = `n_features`, $C$ = `n_components`.\n",
    "\n",
    "#### Entraînement\n",
    "\n",
    "NCA stocke une matrice de distances par paires, occupant $N^2$ de mémoire. La complexité temporelle dépend du nombre d'itérations effectuées par l'algorithme d'optimisation. Cependant, on peut définir le nombre maximal d'itérations avec l'argument `max_iter`. Pour chaque itération, la complexité temporelle est de $\\mathcal{O}(C N \\min(N, D))$.\n",
    "\n",
    "#### Transformation\n",
    "\n",
    "Ici, l'opération `transform` renvoie $LX^\\top$, donc sa complexité temporelle est de $\\mathcal{O}(C D N_{test})$. Il n'y a pas de complexité spatiale supplémentaire dans l'opération."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Références\n",
    "\n",
    "🔬 [1] [**“Neighbourhood Components Analysis”**](https://cs.nyu.edu/~roweis/papers/ncanips.pdf), J. Goldberger, S. Roweis, G. Hinton, R. Salakhutdinov, Advances in Neural Information Processing Systems, Vol. 17, May 2005, pp. 513-520.\n",
    "\n",
    "🌐 [**Wikipedia entry on _Neighborhood Components Analysis_**](https://en.wikipedia.org/wiki/Neighbourhood_components_analysis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
