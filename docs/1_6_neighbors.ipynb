{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='supervised-learning'></a> 1. [**Apprentissage supervis√©**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_supervised_learning.ipynb#supervised-learning)</br>([*Supervised learning*](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning))\n",
    "\n",
    "# <a id='nearest-neighbors'></a> 1.6. [**Plus proches voisins ($k$-NN)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb)<br/>([_Nearest Neighbors_](https://scikit-learn.org/stable/modules/neighbors.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 16 pages, 9 exemples, 3 papiers\n",
    "- 1.6.1. [**Plus proches voisins non supervis√©**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#unsupervised-nearest-neighbors)<br/>([_Unsupervised Nearest Neighbors_](https://scikit-learn.org/stable/modules/neighbors.html#unsupervised-nearest-neighbors))\n",
    "    - 1.6.1.1. [**Recherche des plus proches voisins**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#finding-the-nearest-neighbors)<br/>([_Finding the Nearest Neighbors_](https://scikit-learn.org/stable/modules/neighbors.html#finding-the-nearest-neighbors))\n",
    "    - 1.6.1.2. [**Classes `KDTree` et `BallTree`**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#kdtree-and-balltree-classes)<br/>([_`KDTree` and `BallTree` Classes_](https://scikit-learn.org/stable/modules/neighbors.html#kdtree-and-balltree-classes))\n",
    "- 1.6.2. [**Classification des plus proches voisins**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#nearest-neighbors-classification)<br/>([_Nearest Neighbors Classification_](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification))\n",
    "- 1.6.3. [**R√©gression des plus proches voisins**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#nearest-neighbors-regression)<br/>([_Nearest Neighbors Regression_](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-regression))\n",
    "- 1.6.4. [**Algorithmes des plus proches voisins**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#nearest-neighbor-algorithms)<br/>([_Nearest Neighbor Algorithms_](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbor-algorithms))\n",
    "    - 1.6.4.1. [**Force brute**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#brute-force)<br/>([_Brute Force_](https://scikit-learn.org/stable/modules/neighbors.html#brute-force))\n",
    "    - 1.6.4.2. [**Arbre KD**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#k-d-tree)<br/>([_K-D Tree_](https://scikit-learn.org/stable/modules/neighbors.html#k-d-tree))\n",
    "    - 1.6.4.3. [**Arbre de boules**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#ball-tree)<br/>([_Ball Tree_](https://scikit-learn.org/stable/modules/neighbors.html#ball-tree))\n",
    "    - 1.6.4.4. [**Choix de l'algorithme des plus proches voisins**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#choice-of-nearest-neighbors-algorithm)<br/>([_Choice of Nearest Neighbors Algorithm_](https://scikit-learn.org/stable/modules/neighbors.html#choice-of-nearest-neighbors-algorithm))\n",
    "    - 1.6.4.5. [**Impact de `leaf_size`**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#effect-of-leaf-size)<br/>([_Effect of `leaf_size`_](https://scikit-learn.org/stable/modules/neighbors.html#effect-of-leaf-size))\n",
    "    - 1.6.4.6. [**M√©triques valides pour les algorithmes des plus proches voisins**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#valid-metrics-for-nearest-neighbor-algorithms)<br/>([_Valid Metrics for Nearest Neighbor Algorithms_](https://scikit-learn.org/stable/modules/neighbors.html#valid-metrics-for-nearest-neighbor-algorithms))\n",
    "- 1.6.5. [**Classifieur du plus proche centro√Øde**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#nearest-centroid-classifier)<br/>([_Nearest Centroid Classifier_](https://scikit-learn.org/stable/modules/neighbors.html#nearest-centroid-classifier))\n",
    "    - 1.6.5.1. [**Centro√Øde r√©tr√©ci le plus proche**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#nearest-shrunken-centroid)<br/>([_Nearest Shrunken Centroid_](https://scikit-learn.org/stable/modules/neighbors.html#nearest-shrunken-centroid))\n",
    "- 1.6.6. [**Transformeur des plus proches voisins**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#nearest-neighbors-transformer)<br/>([_Nearest Neighbors Transformer_](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-transformer))\n",
    "- 1.6.7. [**Analyse des composantes du voisinage**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#neighborhood-components-analysis)<br/>([_Neighborhood Components Analysis_](https://scikit-learn.org/stable/modules/neighbors.html#neighborhood-components-analysis))\n",
    "    - 1.6.7.1. [**Classification**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#id4)<br/>([_Classification_](https://scikit-learn.org/stable/modules/neighbors.html#id4))\n",
    "    - 1.6.7.2. [**R√©duction de dimension**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#dimensionality-reduction)<br/>([_Dimensionality reduction_](https://scikit-learn.org/stable/modules/neighbors.html#dimensionality-reduction))\n",
    "    - 1.6.7.3. [**Formulation math√©matique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#mathematical-formulation)<br/>([_Mathematical formulation_](https://scikit-learn.org/stable/modules/neighbors.html#mathematical-formulation))\n",
    "    - 1.6.7.4. [**Impl√©mentation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#implementation)<br/>([_Implementation_](https://scikit-learn.org/stable/modules/neighbors.html#implementation))\n",
    "    - 1.6.7.5. [**Complexit√©**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/1_6_neighbors.ipynb#complexity)<br/>([_Complexity_](https://scikit-learn.org/stable/modules/neighbors.html#complexity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='nearest-neighbors'></a> 1.6. **Plus proches voisins ($k$-NN)**<br/>([_Nearest Neighbors_](https://scikit-learn.org/stable/modules/neighbors.html))\n",
    "\n",
    "[**`sklearn.neighbors`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors) fournit des fonctionnalit√©s pour les m√©thodes d'apprentissage non supervis√©es et supervis√©es bas√©es sur les voisins. Les plus proches voisins non supervis√©s sont √† la base de nombreuses autres m√©thodes d'apprentissage, notamment l'apprentissage multiple et le regroupement spectral. L'apprentissage supervis√© bas√© sur les voisins se pr√©sente sous deux formes¬†: la [**classification** (1.6.2)](#classification) pour les donn√©es avec des √©tiquettes discr√®tes et la [**r√©gression** (1.6.3)](#regression) pour les donn√©es avec des √©tiquettes continues.\n",
    "\n",
    "Le principe derri√®re les m√©thodes du plus proche voisin est de trouver un nombre pr√©d√©fini d'√©chantillons d'apprentissage les plus proches en distance du nouveau point, et de pr√©dire l'√©tiquette √† partir de ceux-ci. Le nombre d'√©chantillons peut √™tre une constante d√©finie par l'utilisateur (apprentissage du $k$ plus proche voisin) ou varier en fonction de la densit√© locale de points (apprentissage du voisin bas√© sur le rayon). La distance peut, en g√©n√©ral, √™tre n'importe quelle mesure m√©trique : la distance euclidienne standard est le choix le plus courant. Les m√©thodes bas√©es sur les voisins sont connues sous le nom de m√©thodes d'apprentissage automatique non g√©n√©ralisantes, car elles \"se souviennent\" simplement de toutes ses donn√©es d'entra√Ænement (√©ventuellement transform√©es en une structure d'indexation rapide telle qu'un [**Ball Tree** (1.6.4.3)](https://scikit-learn.org/stable/modules/neighbors.html#ball-tree) ou un [**KD Tree** (1.6.4.2)](https://scikit-learn.org/stable/modules/neighbors.html#kd-tree)).\n",
    "\n",
    "Malgr√© sa simplicit√©, l'algorithme des plus proches voisins a r√©ussi dans un grand nombre de probl√®mes de classification et de r√©gression, y compris les chiffres manuscrits et les sc√®nes d'images satellites. Comme m√©thode non param√©trique, elle r√©ussit souvent dans des situations de classification o√π la fronti√®re de d√©cision est tr√®s irr√©guli√®re.\n",
    "\n",
    "Les classes de [**`sklearn.neighbors`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors) peuvent g√©rer des tableaux NumPy ou des matrices `scipy.sparse` en entr√©e. Pour les matrices denses, un grand nombre de mesures de distance possibles sont prises en charge. Pour les matrices creuses, les m√©triques de Minkowski arbitraires sont prises en charge pour les recherches.\n",
    "\n",
    "Il existe de nombreuses proc√©dures d'apprentissage qui reposent sur les voisins les plus proches. Un exemple est l'[**estimation de la densit√© du noyau** (2.8.2)](https://scikit-learn.org/stable/modules/density.html#kernel-density), discut√©e dans la section sur l'[**estimation de la densit√©** (2.8)](https://scikit-learn.org/stable/modules/density.html#density-estimation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='unsupervised-nearest-neighbors'></a> 1.6.1. **Plus proches voisins non supervis√©**<br/>([_Unsupervised Nearest Neighbors_](https://scikit-learn.org/stable/modules/neighbors.html#unsupervised-nearest-neighbors))\n",
    "\n",
    "[**`NearestNeighbors`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors) met en ≈ìuvre l'apprentissage non supervis√© des plus proches voisins. Il agit comme une interface uniforme pour trois algorithmes diff√©rents des plus proches voisins : [**`BallTree`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree), [**`KDTree`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree), et un algorithme de force brute bas√© sur les routines de [**`sklearn.metrics.pairwise`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics.pairwise). Le choix de l'algorithme de recherche de voisins est contr√¥l√© par le mot-cl√© `'algorithm'`, qui doit √™tre l'un des suivants : `['auto', 'ball_tree', 'kd_tree', 'brute']`. Lorsque la valeur par d√©faut `'auto'` est utilis√©e, l'algorithme tente de d√©terminer la meilleure approche √† partir des donn√©es d'entra√Ænement. Pour une discussion des forces et des faiblesses de chaque option, consultez [**Algorithmes des plus proches voisins** (1.6.4)](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbor-algorithms).\n",
    "\n",
    "> **Avertissement :** En ce qui concerne les algorithmes des plus proches voisins, si deux voisins $k + 1$ et $k$ ont des distances identiques mais des √©tiquettes diff√©rentes, le r√©sultat d√©pendra de l'ordre des donn√©es d'entra√Ænement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='finding-the-nearest-neighbors'></a> 1.6.1.1. **Recherche des plus proches voisins**<br/>([_Finding the Nearest Neighbors_](https://scikit-learn.org/stable/modules/neighbors.html#finding-the-nearest-neighbors))\n",
    "\n",
    "Pour la t√¢che simple de trouver les plus proches voisins entre deux ensembles de donn√©es, les algorithmes non supervis√©s de [**`sklearn.neighbors`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors) peuvent √™tre utilis√©s :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.41421356],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.41421356]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)\n",
    "distances, indices = nbrs.kneighbors(X)\n",
    "indices\n",
    "# array([[0, 1],\n",
    "#        [1, 0],\n",
    "#        [2, 1],\n",
    "#        [3, 4],\n",
    "#        [4, 3],\n",
    "#        [5, 4]]...)\n",
    "distances\n",
    "# array([[0.        , 1.        ],\n",
    "#        [0.        , 1.        ],\n",
    "#        [0.        , 1.41421356],\n",
    "#        [0.        , 1.        ],\n",
    "#        [0.        , 1.        ],\n",
    "#        [0.        , 1.41421356]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parce que l'ensemble de requ√™te correspond √† l'ensemble d'entra√Ænement, le plus proche voisin de chaque point est le point lui-m√™me, √† une distance de z√©ro.\n",
    "\n",
    "Il est √©galement possible de produire efficacement un graphe creux montrant les connexions entre les points voisins :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 1., 0.],\n",
       "       [0., 0., 0., 1., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbrs.kneighbors_graph(X).toarray()\n",
    "# array([[1., 1., 0., 0., 0., 0.],\n",
    "#        [1., 1., 0., 0., 0., 0.],\n",
    "#        [0., 1., 1., 0., 0., 0.],\n",
    "#        [0., 0., 0., 1., 1., 0.],\n",
    "#        [0., 0., 0., 1., 1., 0.],\n",
    "#        [0., 0., 0., 0., 1., 1.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'ensemble de donn√©es est structur√© de telle sorte que les points proches dans l'ordre des indices sont proches dans l'espace des param√®tres, conduisant √† une matrice approximativement diagonale par bloc des $k$-plus proches voisins. Un tel graphe creux est utile dans diverses circonstances qui font usage des relations spatiales entre les points pour l'apprentissage non supervis√© : en particulier, voir [**`Isomap`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html#sklearn.manifold.Isomap), [**`LocallyLinearEmbedding`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding), et [**`SpectralClustering`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='kdtree-and-balltree-classes'></a> 1.6.1.2. **Classes `KDTree` et `BallTree`**<br/>([_`KDTree` and `BallTree` Classes_](https://scikit-learn.org/stable/modules/neighbors.html#kdtree-and-balltree-classes))\n",
    "\n",
    "Alternativement, il est possible d'utiliser directement les classes [**`KDTree`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree) ou [**`BallTree`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree) pour trouver les voisins les plus proches. C'est la fonctionnalit√© encapsul√©e par la classe [**`NearestNeighbors`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors) utilis√©e pr√©c√©demment. Le Ball Tree et le KD Tree partagent la m√™me interface ; nous allons montrer un exemple d'utilisation du KD Tree ici :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "kdt = KDTree(X, leaf_size=30, metric='euclidean')\n",
    "kdt.query(X, k=2, return_distance=False)\n",
    "# array([[0, 1],\n",
    "#        [1, 0],\n",
    "#        [2, 1],\n",
    "#        [3, 4],\n",
    "#        [4, 3],\n",
    "#        [5, 4]]...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reportez-vous √† la documentation des classes [**`KDTree`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree) et [**`BallTree`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree) pour plus d'informations sur les options disponibles pour les recherches de voisins les plus proches, y compris la sp√©cification des strat√©gies de requ√™te, des m√©triques de distance, etc. Pour obtenir une liste de m√©triques valides, utilisez `KDTree.valid_metrics` et `BallTree.valid_metrics` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['euclidean',\n",
       " 'l2',\n",
       " 'minkowski',\n",
       " 'p',\n",
       " 'manhattan',\n",
       " 'cityblock',\n",
       " 'l1',\n",
       " 'chebyshev',\n",
       " 'infinity',\n",
       " 'seuclidean',\n",
       " 'mahalanobis',\n",
       " 'hamming',\n",
       " 'canberra',\n",
       " 'braycurtis',\n",
       " 'jaccard',\n",
       " 'dice',\n",
       " 'rogerstanimoto',\n",
       " 'russellrao',\n",
       " 'sokalmichener',\n",
       " 'sokalsneath',\n",
       " 'haversine',\n",
       " 'pyfunc']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KDTree, BallTree\n",
    "KDTree.valid_metrics()\n",
    "# ['euclidean', 'l2', 'minkowski', 'p', 'manhattan', 'cityblock', 'l1', 'chebyshev', 'infinity']\n",
    "BallTree.valid_metrics()\n",
    "# ['euclidean', 'l2', 'minkowski', 'p', 'manhattan', 'cityblock', 'l1', 'chebyshev', 'infinity',\n",
    "# 'seuclidean', 'mahalanobis', 'hamming', 'canberra', 'braycurtis', 'jaccard', 'dice',\n",
    "# 'rogerstanimoto', 'russellrao', 'sokalmichener', 'sokalsneath', 'haversine', 'pyfunc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='nearest-neighbors-classification'></a> 1.6.2. **Classification des plus proches voisins**<br/>([_Nearest Neighbors Classification_](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification))\n",
    "\n",
    "Neighbors-based classification is a type of _instance-based learning_ or _non-generalizing learning_: it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.\n",
    "\n",
    "scikit-learn implements two different nearest neighbors classifiers: [**`KNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) implements learning based on the $k$ nearest neighbors of each query point, where $k$ is an integer value specified by the user. [**`RadiusNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier) implements learning based on the number of neighbors within a fixed radius $r$ of each training point, where $r$ is a floating-point value specified by the user.\n",
    "\n",
    "The $k$-neighbors classification in [**`KNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) is the most commonly used technique. The optimal choice of the value is highly data-dependent: in general a larger $k$ suppresses the effects of noise, but makes the classification boundaries less distinct.\n",
    "\n",
    "In cases where the data is not uniformly sampled, radius-based neighbors classification in [**`RadiusNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier) can be a better choice. The user specifies a fixed radius $r$, such that points in sparser neighborhoods use fewer nearest neighbors for the classification. For high-dimensional parameter spaces, this method becomes less effective due to the so-called ‚Äúcurse of dimensionality‚Äù.\n",
    "\n",
    "The basic nearest neighbors classification uses uniform weights: that is, the value assigned to a query point is computed from a simple majority vote of the nearest neighbors. Under some circumstances, it is better to weight the neighbors such that nearer neighbors contribute more to the fit. This can be accomplished through the `weights` keyword. The default value, `weights = 'uniform'`, assigns uniform weights to each neighbor. `weights = 'distance'` assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied to compute the weights.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_classification_001.png\"\n",
    "    alt=\"Nearest Neighbors Classification\"\n",
    "    style=\"max-width: 75%; height: auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='nearest-neighbors-classification'></a> 1.6.2. **Classification des plus proches voisins**<br/>([_Classification des plus proches voisins_](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification))\n",
    "\n",
    "La classification bas√©e sur les voisins est un type d'_apprentissage bas√© sur les instances_ ou d'_apprentissage non g√©n√©ralisant_ : elle ne tente pas de construire un mod√®le interne g√©n√©ral, mais se contente de stocker simplement des instances des donn√©es d'entra√Ænement. La classification est calcul√©e √† partir d'un vote √† la majorit√© simple des voisins les plus proches de chaque point : un point de requ√™te se voit attribuer la classe de donn√©es qui a le plus de repr√©sentants parmi les voisins les plus proches du point.\n",
    "\n",
    "scikit-learn impl√©mente deux classifieurs de voisins les plus proches diff√©rents : [**`KNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) impl√©mente l'apprentissage bas√© sur les $k$ voisins les plus proches de chaque point de requ√™te, o√π $k$ est une valeur enti√®re sp√©cifi√©e par l'utilisateur. [**`RadiusNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier) impl√©mente l'apprentissage bas√© sur le nombre de voisins dans un rayon fixe $r$ de chaque point d'entra√Ænement, o√π $r$ est une valeur en virgule flottante sp√©cifi√©e par l'utilisateur.\n",
    "\n",
    "La classification bas√©e sur les $k$ voisins dans [**`KNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) est la technique la plus couramment utilis√©e. Le choix optimal de la valeur $k$ est fortement d√©pendant des donn√©es : en g√©n√©ral, une valeur de $k$ plus √©lev√©e supprime les effets du bruit, mais rend les limites de classification moins distinctes.\n",
    "\n",
    "Dans les cas o√π les donn√©es ne sont pas √©chantillonn√©es de mani√®re uniforme, la classification bas√©e sur les voisins dans [**`RadiusNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier) peut √™tre un meilleur choix. L'utilisateur sp√©cifie un rayon fixe $r$, de sorte que les points dans des voisinages moins denses utilisent moins de voisins les plus proches pour la classification. Pour les espaces de param√®tres de grande dimension, cette m√©thode devient moins efficace en raison du \"fl√©au de la dimensionnalit√©\".\n",
    "\n",
    "La classification de base des voisins les plus proches utilise des poids uniformes : c'est-√†-dire que la valeur attribu√©e √† un point de requ√™te est calcul√©e √† partir d'un vote √† la majorit√© simple des voisins les plus proches. Dans certaines circonstances, il est pr√©f√©rable de pond√©rer les voisins de telle sorte que les voisins plus proches contribuent davantage √† l'ajustement. Cela peut √™tre accompli avec le mot-cl√© `weights`. La valeur par d√©faut, `weights = 'uniform'`, attribue des poids uniformes √† chaque voisin. `weights = 'distance'` attribue des poids proportionnels √† l'inverse de la distance par rapport au point de requ√™te. Alternativement, une fonction d√©finie par l'utilisateur de la distance peut √™tre fournie pour calculer les poids.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_classification_001.png\"\n",
    "    alt=\"Classification des plus proches voisins\"\n",
    "    style=\"max-width: 75%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Classification des plus proches voisins**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_6_neighbors/plot_classification.ipynb)<br/>([_Nearest Neighbors Classification_](https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html))\n",
    "\n",
    "Un exemple de classification en utilisant les voisins les plus proches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='nearest-neighbors-regression'></a> 1.6.3. **R√©gression des plus proches voisins**<br/>([_R√©gression des plus proches voisins_](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-regression))\n",
    "\n",
    "La r√©gression bas√©e sur les voisins peut √™tre utilis√©e dans les cas o√π les √©tiquettes des donn√©es sont des variables continues plut√¥t que des variables discr√®tes. L'√©tiquette attribu√©e √† un point de requ√™te est calcul√©e en fonction de la moyenne des √©tiquettes de ses voisins les plus proches.\n",
    "\n",
    "scikit-learn met en ≈ìuvre deux r√©gresseurs de voisins diff√©rents¬†: [**`KNeighborsRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor) met en ≈ìuvre un apprentissage bas√© sur les $k$ voisins les plus proches de chaque point de requ√™te, o√π $k$ est une valeur enti√®re sp√©cifi√©e par l'utilisateur. [**`RadiusNeighborsRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsRegressor.html#sklearn.neighbors.RadiusNeighborsRegressor) met en ≈ìuvre un apprentissage bas√© sur les voisins situ√©s √† l'int√©rieur d'un rayon fixe $r$ du point de requ√™te, o√π $r$ est une valeur en virgule flottante sp√©cifi√©e par l'utilisateur.\n",
    "\n",
    "La r√©gression bas√©e sur les voisins la plus √©l√©mentaire utilise des poids uniformes¬†: c'est-√†-dire que chaque point du voisinage local contribue uniform√©ment √† la r√©gression d'un point de requ√™te. Dans certaines circonstances, il peut √™tre avantageux de pond√©rer les points de telle sorte que les points proches contribuent davantage √† la r√©gression que les points √©loign√©s. Cela peut √™tre r√©alis√© gr√¢ce au mot-cl√© `weights`. La valeur par d√©faut, `weights = 'uniform'`, attribue des poids √©gaux √† tous les points. `weights = 'distance'` attribue des poids proportionnels √† l'inverse de la distance par rapport au point de requ√™te. Alternativement, une fonction d√©finie par l'utilisateur bas√©e sur la distance peut √™tre fournie, et elle sera utilis√©e pour calculer les poids.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_regression_001.png\"\n",
    "    alt=\"R√©gression des plus proches voisins\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "L'utilisation de voisins multiples en sortie pour la r√©gression est d√©montr√©e dans **Compl√©tion de visage avec des estimateurs √† sortie multiple**. Dans cet exemple, les entr√©es X sont les pixels de la moiti√© sup√©rieure des visages et les sorties Y sont les pixels de la moiti√© inf√©rieure de ces visages.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_multioutput_face_completion_001.png\"\n",
    "    alt=\"R√©gression des plus proches voisins\"\n",
    "    style=\"max-width: 75%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**R√©gression des plus proches voisins**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_6_neighbors/plot_regression.ipynb)<br/>([_Nearest Neighbors Regression_](https://scikit-learn.org/stable/auto_examples/neighbors/plot_regression.html))\n",
    "\n",
    "Un exemple de r√©gression utilisant les plus proches voisins.\n",
    "\n",
    "#### [**Compl√©tion de visage avec des estimateurs √† sortie multiple**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_6_neighbors/plot_multioutput_face_completion.ipynb)<br/>([_Face completion with a multi-output estimators_](https://scikit-learn.org/stable/auto_examples/neighbors/plot_multioutput_face_completion.html))\n",
    "\n",
    "Un exemple de r√©gression √† sortie multiple en utilisant les plus proches voisins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='nearest-neighbor-algorithms'></a> 1.6.4. **Algorithmes des plus proches voisins**<br/>([_Nearest Neighbor Algorithms_](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbor-algorithms))\n",
    "\n",
    "### <a id='brute-force'></a> 1.6.4.1. **Force brute**<br/>([_Brute Force_](https://scikit-learn.org/stable/modules/neighbors.html#brute-force))\n",
    "\n",
    "La recherche rapide des plus proches voisins est un domaine actif de recherche en apprentissage automatique. L'impl√©mentation la plus na√Øve de recherche de voisins implique le calcul en force brute des distances entre toutes les paires de points de l'ensemble de donn√©es : pour $N$ √©chantillons en $D$ dimensions, cette approche a une complexit√© en $\\mathcal{O}[D N^2]$. Les recherches de voisins en force brute efficaces peuvent √™tre tr√®s comp√©titives pour de petits √©chantillons de donn√©es. Cependant, √† mesure que le nombre d'√©chantillons $N$ augmente, l'approche en force brute devient rapidement inapplicable. Dans les classes de [**`sklearn.neighbors`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors), les recherches de voisins en force brute sont sp√©cifi√©es √† l'aide du mot-cl√© `algorithm = 'brute'` et sont calcul√©es √† l'aide des routines disponibles dans [**`sklearn.metrics.pairwise`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics.pairwise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='k-d-tree'></a> 1.6.4.2. **Arbre KD**<br/>([_K-D Tree_](https://scikit-learn.org/stable/modules/neighbors.html#k-d-tree))\n",
    "\n",
    "Pour rem√©dier aux inefficacit√©s computationnelles de l'approche en force brute, diverses structures de donn√©es bas√©es sur des arbres ont √©t√© invent√©es. En g√©n√©ral, ces structures tentent de r√©duire le nombre de calculs de distance n√©cessaires en encodant efficacement les informations de distance agr√©g√©e pour l'√©chantillon. L'id√©e de base est que si le point $A$ est tr√®s √©loign√© du point $B$, et que le point $B$ est tr√®s proche du point $C$, alors nous savons que les points $A$ et $C$ sont tr√®s √©loign√©s, **sans avoir besoin de calculer explicitement leur distance**. De cette mani√®re, le co√ªt computationnel d'une recherche de voisins les plus proches peut √™tre r√©duit √† $\\mathcal{O}[D N \\log(N)]$ ou mieux. Il s'agit d'une am√©lioration significative par rapport √† la m√©thode en force brute pour de grandes valeurs de $N$.\n",
    "\n",
    "Une premi√®re approche pour exploiter ces informations agr√©g√©es a √©t√© la structure de donn√©es **arbre KD** (abr√©g√©e pour **arbre K-dimensionnel**), qui g√©n√©ralise les arbres **Quad-trees** bidimensionnels et les arbres **Oct-trees** tridimensionnels √† un nombre arbitraire de dimensions. L'arbre KD est une structure d'arbre binaire qui partitionne de mani√®re r√©cursive l'espace des param√®tres le long des axes de donn√©es, le divisant en r√©gions orthotropes imbriqu√©es dans lesquelles les points de donn√©es sont class√©s. La construction d'un arbre KD est tr√®s rapide : √©tant donn√© que la partition est effectu√©e uniquement le long des axes de donn√©es, aucune distance en $D$ dimensions n'a besoin d'√™tre calcul√©e. Une fois construit, le voisin le plus proche d'un point de requ√™te peut √™tre d√©termin√© avec seulement $\\mathcal{O}[\\log(N)]$ calculs de distance. Bien que l'approche de l'arbre KD soit tr√®s rapide pour les recherches de voisins en basse dimension ($D < 20$), elle devient inefficace √† mesure que $D$ devient tr√®s grand : ceci est l'une des manifestations du \"fl√©au de la dimensionnalit√©\". Dans scikit-learn, les recherches de voisins avec des arbres KD sont sp√©cifi√©es en utilisant le mot-cl√© `algorithm = 'kd_tree'` et sont calcul√©es √† l'aide de la classe [**`KDTree`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree).\n",
    "\n",
    "#### R√©f√©rences\n",
    "\n",
    "üî¨ [**‚ÄúMultidimensional binary search trees used for associative searching‚Äù**](https://dl.acm.org/doi/pdf/10.1145/361002.361007), Bentley, J.L., Communications of the ACM (1975)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='ball-tree'></a> 1.6.4.3. **Arbre de boules**<br/>([_Ball Tree_](https://scikit-learn.org/stable/modules/neighbors.html#ball-tree))\n",
    "\n",
    "Pour rem√©dier √† la mauvaise performance des arbres KD dans des dimensions plus √©lev√©es, la structure de donn√©es **arbre de boules** a √©t√© d√©velopp√©e. Alors que les arbres KD partitionnent les donn√©es le long des axes cart√©siens, les arbres de boules partitionnent les donn√©es dans une s√©rie d'hyper-sph√®res imbriqu√©es. Cela rend la construction de l'arbre plus co√ªteuse que celle de l'arbre KD, mais aboutit √† une structure de donn√©es qui peut √™tre tr√®s efficace sur des donn√©es tr√®s structur√©es, m√™me dans des dimensions tr√®s √©lev√©es.\n",
    "\n",
    "Un arbre de boules divise de mani√®re r√©cursive les donn√©es en n≈ìuds d√©finis par un centro√Øde $C$ et un rayon $r$, de telle sorte que chaque point du n≈ìud se trouve √† l'int√©rieur de l'hyper-sph√®re d√©finie par $r$ et $C$. Le nombre de points candidats pour une recherche de voisins est r√©duit en utilisant l'**in√©galit√© triangulaire** :\n",
    "\n",
    "$$\n",
    "|x+y| \\leq |x| + |y|\n",
    "$$\n",
    "\n",
    "Avec cette configuration, un seul calcul de distance entre un point de test et le centro√Øde est suffisant pour d√©terminer une limite inf√©rieure et une limite sup√©rieure de la distance √† tous les points √† l'int√©rieur du n≈ìud. En raison de la g√©om√©trie sph√©rique des n≈ìuds de l'arbre de boules, il peut surpasser un **arbre KD** dans des dimensions √©lev√©es, bien que les performances r√©elles d√©pendent fortement de la structure des donn√©es d'entra√Ænement. Dans scikit-learn, les recherches de voisins bas√©es sur l'arbre de boules sont sp√©cifi√©es en utilisant le mot-cl√© `algorithm = 'ball_tree'` et sont calcul√©es √† l'aide de la classe [**`BallTree`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree). Alternativement, l'utilisateur peut travailler directement avec la classe [**`BallTree`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree).\n",
    "\n",
    "#### R√©f√©rences\n",
    "\n",
    "üî¨ [**‚ÄúFive Balltree Construction Algorithms‚Äù**](https://www1.icsi.berkeley.edu/ftp/pub/techreports/1989/tr-89-063.pdf), Omohundro, S.M., International Computer Science Institute Technical Report (1989)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='choice-of-nearest-neighbors-algorithm'></a> 1.6.4.4. **Choix de l'algorithme des plus proches voisins**<br/>([_Choice of Nearest Neighbors Algorithm_](https://scikit-learn.org/stable/modules/neighbors.html#choice-of-nearest-neighbors-algorithm))\n",
    "\n",
    "Le choix de l'algorithme optimal pour un jeu de donn√©es particulier est une d√©cision compliqu√©e et d√©pend de plusieurs facteurs :\n",
    "\n",
    "- nombre d'√©chantillons $N$ (c'est-√†-dire `n_samples`) et dimensionnalit√© $D$ (c'est-√†-dire `n_features`).\n",
    "    - Le temps de requ√™te en _force brute_ cro√Æt en $\\mathcal{O}[D N]$.\n",
    "    - Le temps de requ√™te en _arbre de boules_ cro√Æt approximativement en $\\mathcal{O}[D \\log(N)]$.\n",
    "    - Le temps de requ√™te en _arbre KD_ change avec $D$ d'une mani√®re difficile √† caract√©riser pr√©cis√©ment. Pour de faibles valeurs de $D$ (moins de 20 environ), le co√ªt est approximativement de $\\mathcal{O}[D \\log(N)]$, et la requ√™te de l'arbre KD peut √™tre tr√®s efficace. Pour des valeurs de $D$ plus √©lev√©es, le co√ªt augmente presque jusqu'√† $\\mathcal{O}[D N]$, et le surco√ªt d√ª √† la structure de l'arbre peuvent entra√Æner des requ√™tes plus lentes que la force brute.\n",
    "\n",
    "Pour de petits ensembles de donn√©es ($N$ inf√©rieur √† 30 environ), $\\log(N)$ est comparable √† $N$, et les algorithmes de force brute peuvent √™tre plus efficaces qu'une approche bas√©e sur les arbres. [**`KDTree`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree) et [**`BallTree`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree) abordent cette situation en fournissant un param√®tre _taille des feuilles_ (`leaf_size`) : cela contr√¥le le nombre d'√©chantillons √† partir duquel une requ√™te bascule en force brute. Cela permet aux deux algorithmes de se rapprocher de l'efficacit√© d'un calcul en force brute pour de petits $N$.\n",
    "\n",
    "- structure des donn√©es : **dimensionnalit√© intrins√®que** des donn√©es et/ou densit√© des donn√©es. La dimensionnalit√© intrins√®que fait r√©f√©rence √† la dimension $d \\le D$ d'une vari√©t√© sur laquelle les donn√©es reposent, qui peut √™tre plong√©e de mani√®re lin√©aire ou non lin√©aire dans l'espace des param√®tres. La raret√© fait r√©f√©rence au degr√© selon lequel les donn√©es remplissent l'espace des param√®tres (ceci doit √™tre distingu√© du concept tel qu'il est utilis√© dans les matrices ¬´ creuses ¬ª. La matrice de donn√©es peut ne comporter aucune entr√©e nulle, mais la structure peut toujours √™tre ¬´ rare ¬ª dans ce sens).\n",
    "\n",
    "    - Le temps de requ√™te en _force brute_ n'est pas modifi√© par la structure des donn√©es.\n",
    "    - Les temps de requ√™te en _arbre de boules_ et en _arbre KD_ peuvent √™tre fortement influenc√©s par la structure des donn√©es. En g√©n√©ral, des donn√©es plus rares avec une dimensionnalit√© intrins√®que plus petite conduisent √† des temps de requ√™te plus rapides. Parce que la repr√©sentation interne de l'arbre KD est align√©e avec les axes des param√®tres, il ne montrera g√©n√©ralement pas autant d'am√©lioration que l'arbre de boules pour des donn√©es arbitrairement structur√©es.\n",
    "\n",
    "Les ensembles de donn√©es utilis√©s en apprentissage automatique ont tendance √† √™tre tr√®s structur√©s et conviennent tr√®s bien aux requ√™tes bas√©es sur des arbres.\n",
    "\n",
    "- nombre de voisins $k$ demand√©s pour un point de requ√™te.\n",
    "\n",
    "    - Le temps de requ√™te en _force brute_ est tr√®s peu sensible √† la valeur de $k$.\n",
    "    - Le temps de requ√™te en _arbre de boules_ et en _arbre KD_ deviendra plus lent √† mesure que $k$ augmentera. Cela est d√ª √† deux effets : premi√®rement, un $k$ plus grand n√©cessite la recherche d'une plus grande partie de l'espace des param√®tres. Deuxi√®mement, l'utilisation de $k > 1$ n√©cessite l'enfilage interne des r√©sultats lorsque l'arbre est parcouru.\n",
    "\n",
    "Lorsque $k$ devient grand par rapport √† $N$, la capacit√© √† √©laguer les branches dans une requ√™te bas√©e sur un arbre est r√©duite. Dans cette situation, les requ√™tes en force brute peuvent √™tre plus efficaces.\n",
    "\n",
    "- nombre de points de requ√™te. L'arbre de boules et l'arbre KD Tree n√©cessitent une phase de construction. Le co√ªt de cette construction devient n√©gligeable lorsqu'il est amorti sur de nombreuses requ√™tes. Cependant, si seulement un petit nombre de requ√™tes sont effectu√©es, la construction peut repr√©senter une fraction significative du co√ªt total. Si tr√®s peu de points de requ√™te sont requis, la force brute est pr√©f√©rable √† une m√©thode bas√©e sur un arbre.\n",
    "\n",
    "Actuellement, `algorithm = 'auto'` s√©lectionne `'brute'` si l'une des conditions suivantes est v√©rifi√©e :\n",
    "- les donn√©es d'entr√©e sont rares\n",
    "- `metric = 'precomputed'`\n",
    "- $D > 15$\n",
    "- $k \\ge N/2$\n",
    "- `effective_metric_` n'appartient pas √† la liste des `VALID_METRICS` pour `'kd_tree'` ou `'ball_tree'`\n",
    "\n",
    "Sinon, il s√©lectionne le premier de `'kd_tree'` et `'ball_tree'` qui a `effective_metric_` dans sa liste `VALID_METRICS`. Cette heuristique est bas√©e sur les hypoth√®ses suivantes :\n",
    "- le nombre de points de requ√™te est au moins du m√™me ordre que le nombre de points d'entra√Ænement\n",
    "- `leaf_size` est proche de sa valeur par d√©faut de `30`\n",
    "- lorsque $D > 15$, la dimensionnalit√© intrins√®que des donn√©es est g√©n√©ralement trop √©lev√©e pour les m√©thodes bas√©es sur des arbres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='effect-of-leaf-size'></a> 1.6.4.5. **Impact de `leaf_size`**<br/>([_Effect of `leaf_size`_](https://scikit-learn.org/stable/modules/neighbors.html#effect-of-leaf-size))\n",
    "\n",
    "Comme mentionn√© pr√©c√©demment, pour de petites tailles d'√©chantillon, une recherche en force brute peut √™tre plus efficace qu'une requ√™te bas√©e sur un arbre. Ce fait est pris en compte dans l'arbre de boules et l'arbre KD par un basculement interne vers des recherches en force brute au sein des n≈ìuds feuilles. Le niveau de cette transition peut √™tre sp√©cifi√© avec le param√®tre `leaf_size`. Ce choix de param√®tre a de nombreux effets :\n",
    "\n",
    "**temps de construction**\n",
    "\n",
    "Un `leaf_size` plus grand conduit √† un temps de construction de l'arbre plus rapide, car il y a moins de n≈ìuds √† cr√©er.\n",
    "\n",
    "**temps de requ√™te**\n",
    "\n",
    "Tant un `leaf_size` grand que petit peuvent entra√Æner un co√ªt de requ√™te sous-optimal. Pour un `leaf_size` approchant 1, le surco√ªt li√© √† la travers√©e des n≈ìuds peut ralentir consid√©rablement les temps de requ√™te. Pour un `leaf_size` approchant la taille de l'ensemble d'entra√Ænement, les requ√™tes deviennent essentiellement de la force brute. Un bon compromis entre ces deux extr√™mes est `leaf_size = 30`, la valeur par d√©faut du param√®tre.\n",
    "\n",
    "**m√©moire**\n",
    "\n",
    "√Ä mesure que `leaf_size` augmente, la m√©moire requise pour stocker la structure de l'arbre diminue. Cela est particuli√®rement important dans le cas de l'arbre de boules, qui stocke un centro√Øde √† $D$ dimensions pour chaque n≈ìud. L'espace de stockage requis pour [**`BallTree`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree) est d'environ `1 / leaf_size` fois la taille de l'ensemble d'entra√Ænement.\n",
    "\n",
    "`leaf_size` n'est pas pris en compte pour les requ√™tes en force brute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='valid-metrics-for-nearest-neighbor-algorithms'></a> 1.6.4.6. **M√©triques valides pour les algorithmes des plus proches voisins**<br/>([_Valid Metrics for Nearest Neighbor Algorithms_](https://scikit-learn.org/stable/modules/neighbors.html#valid-metrics-for-nearest-neighbor-algorithms))\n",
    "\n",
    "Pour une liste des m√©triques disponibles, consultez la documentation de la classe [**`DistanceMetric`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DistanceMetric.html#sklearn.metrics.DistanceMetric) et les m√©triques r√©pertori√©es dans `sklearn.metrics.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`. Notez que la m√©trique \"cosine\" utilise [**`cosine_distances`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_distances.html#sklearn.metrics.pairwise.cosine_distances).\n",
    "\n",
    "Une liste des m√©triques valides pour l'un des algorithmes ci-dessus peut √™tre obtenue en utilisant leur m√©thode `valid_metric()`. Par exemple, les m√©triques valides pour `KDTree` peuvent √™tre g√©n√©r√©es avec :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chebyshev', 'cityblock', 'euclidean', 'infinity', 'l1', 'l2', 'manhattan', 'minkowski', 'p']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "print(sorted(KDTree.valid_metrics()))\n",
    "# ['chebyshev', 'cityblock', 'euclidean', 'infinity', 'l1', 'l2', 'manhattan', 'minkowski', 'p']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='nearest-centroid-classifier'></a> 1.6.5. **Classifieur du plus proche centro√Øde**<br/>([_Nearest Centroid Classifier_](https://scikit-learn.org/stable/modules/neighbors.html#nearest-centroid-classifier))\n",
    "\n",
    "The [**`NearestCentroid`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid) classifier is a simple algorithm that represents each class by the centroid of its members. In effect, this makes it similar to the label updating phase of the [**`KMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) algorithm. It also has no parameters to choose, making it a good baseline classifier. It does, however, suffer on non-convex classes, as well as when classes have drastically different variances, as equal variance in all dimensions is assumed. See Linear Discriminant Analysis ([**`LinearDiscriminantAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis)) and Quadratic Discriminant Analysis ([**`QuadraticDiscriminantAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis)) for more complex methods that do not make this assumption. Usage of the default [**`NearestCentroid`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid) is simple:\n",
    "\n",
    "\n",
    "Le classifieur [**`NearestCentroid`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid) est un algorithme simple qui repr√©sente chaque classe par le centro√Øde de ses membres. En effet, cela le rend similaire √† la phase de mise √† jour des √©tiquettes de l'algorithme [**`KMeans`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans). Il n'a pas non plus de param√®tres √† choisir, ce qui en fait un bon classifieur de base. Cependant, il pr√©sente des limites avec des classes non convexes, ainsi que lorsque les classes ont des variances tr√®s diff√©rentes, car il suppose une variance √©gale dans toutes les dimensions. Consultez l'Analyse Discriminante Lin√©aire ([**`LinearDiscriminantAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis)) et l'Analyse Discriminante Quadratique ([**`QuadraticDiscriminantAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis)) pour des m√©thodes plus complexes qui ne font pas cette hypoth√®se. L'utilisation du [**`NearestCentroid`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid) par d√©faut est simple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestCentroid\n",
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "y = np.array([1, 1, 1, 2, 2, 2])\n",
    "clf = NearestCentroid()\n",
    "clf.fit(X, y)\n",
    "# NearestCentroid()\n",
    "print(clf.predict([[-0.8, -1]]))\n",
    "# [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='nearest-shrunken-centroid'></a> 1.6.5.1. **Centro√Øde r√©tr√©ci le plus proche**<br/>([_Nearest Shrunken Centroid_](https://scikit-learn.org/stable/modules/neighbors.html#nearest-shrunken-centroid))\n",
    "\n",
    "Le classifieur [**`NearestCentroid`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid) poss√®de un param√®tre `shrink_threshold`, qui impl√©mente le classifieur du centro√Øde r√©tr√©ci le plus proche. En effet, la valeur de chaque caract√©ristique pour chaque centro√Øde est divis√©e par la variance intra-classe de cette caract√©ristique. Les valeurs des caract√©ristiques sont ensuite r√©duites par `shrink_threshold`. Notamment, si une valeur de caract√©ristique particuli√®re traverse z√©ro, elle est fix√©e √† z√©ro. En effet, cela supprime la caract√©ristique de l'influence sur la classification. Cela est utile, par exemple, pour √©liminer les caract√©ristiques bruyantes.\n",
    "\n",
    "Dans l'exemple ci-dessous, l'utilisation d'un petit seuil de r√©duction augmente la pr√©cision du mod√®le de 0,81 √† 0,82.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_nearest_centroid_001.png\"\n",
    "    alt=\"Classification par le Centro√Øde le Plus Proche, sans r√©tr√©cissement\"\n",
    "    style=\"max-width: 30%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_nearest_centroid_002.png\"\n",
    "    alt=\"Classification par le Centro√Øde le Plus Proche, avec r√©tr√©cissement\"\n",
    "    style=\"max-width: 30%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Classification par le Centro√Øde le Plus Proche**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_6_neighbors/plot_nearest_centroid.ipynb)<br/>([_Nearest Centroid Classification_](https://scikit-learn.org/stable/auto_examples/neighbors/plot_nearest_centroid.html))\n",
    "\n",
    "Un exemple de classification en utilisant le centro√Øde le plus proche avec diff√©rents seuils de r√©duction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='nearest-neighbors-transformer'></a> 1.6.6. **Transformeur des plus proches voisins**<br/>([_Nearest Neighbors Transformer_](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-transformer))\n",
    "\n",
    "De nombreux estimateurs de scikit-learn reposent sur les plus proches voisins : plusieurs classificateurs et r√©gresseurs tels que [**`KNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) et [**`KNeighborsRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor), mais aussi certaines m√©thodes de regroupement comme [**`DBSCAN`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN) et [**`SpectralClustering`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering), ainsi que certaines techniques de plongement de vari√©t√© comme [**`TSNE`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE) et [**`Isomap`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html#sklearn.manifold.Isomap).\n",
    "\n",
    "Tous ces estimateurs peuvent calculer les plus proches voisins en interne, mais la plupart d'entre eux acceptent √©galement les plus proches voisins pr√©calcul√©s, sous forme de [**graphe creux**](https://scikit-learn.org/stable/glossary.html#term-sparse-graph), comme le fait [**`kneighbors_graph`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.kneighbors_graph.html#sklearn.neighbors.kneighbors_graph) et [**`radius_neighbors_graph`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.radius_neighbors_graph.html#sklearn.neighbors.radius_neighbors_graph). Avec le mode `mode='connectivity'`, ces fonctions renvoient un graphe d'adjacence binaire creux, comme requis, par exemple, dans [**`SpectralClustering`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering). Alors qu'avec `mode='distance'`, elles renvoient un graphe de distances creux, comme requis, par exemple, dans [**`DBSCAN`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN). Pour inclure ces fonctions dans un pipeline scikit-learn, on peut √©galement utiliser les classes correspondantes [**`KNeighborsTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html#sklearn.neighbors.KNeighborsTransformer) et [**`RadiusNeighborsTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsTransformer.html#sklearn.neighbors.RadiusNeighborsTransformer). Les avantages de cette API de graphe creux sont multiples.\n",
    "\n",
    "Tout d'abord, le graphe pr√©calcul√© peut √™tre r√©utilis√© plusieurs fois, par exemple en faisant varier un param√®tre de l'estimateur. Cela peut √™tre fait manuellement par l'utilisateur, ou en utilisant les propri√©t√©s de mise en cache du pipeline scikit-learn :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tempfile\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.neighbors import KNeighborsTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import make_regression\n",
    "cache_path = tempfile.gettempdir()  # we use a temporary folder here\n",
    "X, _ = make_regression(n_samples=50, n_features=25, random_state=0)\n",
    "estimator = make_pipeline(\n",
    "    KNeighborsTransformer(mode='distance'),\n",
    "    Isomap(n_components=3, metric='precomputed'),\n",
    "    memory=cache_path)\n",
    "X_embedded = estimator.fit_transform(X)\n",
    "X_embedded.shape\n",
    "# (50, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De plus, le pr√©-calcul du graphe permet d'obtenir un meilleur contr√¥le sur l'estimation des plus proches voisins, par exemple en activant le traitement parall√®le gr√¢ce au param√®tre `n_jobs`, qui peut ne pas √™tre disponible dans tous les estimateurs.\n",
    "\n",
    "Enfin, le pr√©-calcul peut √™tre effectu√©e par des estimateurs personnalis√©s pour utiliser diff√©rentes impl√©mentations, telles que des m√©thodes de plus proches voisins approximatives, ou des impl√©mentations avec des types de donn√©es sp√©ciaux. Le graphe pr√©calcul√© de voisins [**sparse graph**](https://scikit-learn.org/stable/glossary.html#term-sparse-graph) doit √™tre format√© comme dans la sortie de [**`radius_neighbors_graph`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.radius_neighbors_graph.html#sklearn.neighbors.radius_neighbors_graph) :\n",
    "\n",
    "- une matrice CSR (bien que COO, CSC ou LIL soient accept√©es).\n",
    "- stocker explicitement uniquement les voisins les plus proches de chaque √©chantillon par rapport aux donn√©es d'entra√Ænement. Cela doit inclure ceux √† une distance de 0 d'un point de requ√™te, y compris la diagonale de la matrice lors du calcul des voisins les plus proches entre les donn√©es d'entra√Ænement et elles-m√™mes.\n",
    "- les donn√©es de chaque ligne doivent stocker la distance dans l'ordre croissant (facultatif. Les donn√©es non tri√©es seront tri√©es de mani√®re stable, ce qui entra√Ænera une surcharge computationnelle).\n",
    "- toutes les valeurs des donn√©es doivent √™tre non n√©gatives.\n",
    "- il ne doit y avoir aucune duplication d'`indices` dans aucune ligne (voir https://github.com/scipy/scipy/issues/5807).\n",
    "- si l'algorithme auquel est pass√©e la matrice pr√©calcul√©e utilise $k$ plus proches voisins (par opposition √† une distance par rapport au voisinage de rayon), au moins $k$ voisins doivent √™tre stock√©s dans chaque ligne (ou $k + 1$, comme expliqu√© dans la note suivante).\n",
    "\n",
    "> **Remarque :** Lorsqu'un nombre sp√©cifique de voisins est interrog√© (en utilisant [**`KNeighborsTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html#sklearn.neighbors.KNeighborsTransformer)), la d√©finition de `n_neighbors` est ambigu√´ car elle peut inclure chaque point d'entra√Ænement comme son propre voisin ou les exclure. Aucun des choix n'est parfait, car les inclure conduit √† un nombre diff√©rent de voisins non propres pendant l'entra√Ænement et les tests, tandis que les exclure entra√Æne une diff√©rence entre `fit(X).transform(X)` et `fit_transform(X)`, ce qui va √† l'encontre de l'API de scikit-learn. Dans [**`KNeighborsTransformer`**](https://scikit-learn.org/stable/modules/generated-skin_neighbors.KNeighborsTransformer.html#sklearn.neighbors.KNeighborsTransformer), nous utilisons la d√©finition qui inclut chaque point d'entra√Ænement comme son propre voisin dans le d√©compte de `n_neighbors`. Cependant, pour des raisons de compatibilit√© avec d'autres estimateurs utilisant l'autre d√©finition, un voisin suppl√©mentaire sera calcul√© lorsque `mode == 'distance'`. Pour maximiser la compatibilit√© avec tous les estimateurs, un choix s√ªr consiste √† toujours inclure un voisin suppl√©mentaire dans un estimateur de plus proches voisins personnalis√©, car les voisins inutiles seront filtr√©s par les estimateurs ult√©rieurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**Plus proches voisins approch√©s dans TSNE**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_6_neighbors/approximate_nearest_neighbors.ipynb)<br/>([_Approximate nearest neighbors in TSNE_](https://scikit-learn.org/stable/auto_examples/neighbors/approximate_nearest_neighbors.html))\n",
    "\n",
    "Un exemple de cr√©ation d'un pipeline avec [**`KNeighborsTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html#sklearn.neighbors.KNeighborsTransformer) et [**`TSNE`**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE). Propose √©galement deux estimateurs personnalis√©s de plus proches voisins bas√©s sur des packages externes.\n",
    "\n",
    "#### [**Mise en cache des plus proches voisins**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_6_neighbors/plot_caching_nearest_neighbors.ipynb)<br/>([_Caching nearest neighbors_](https://scikit-learn.org/stable/auto_examples/neighbors/plot_caching_nearest_neighbors.html))\n",
    "\n",
    "Un exemple de cr√©ation d'un pipeline avec [**`KNeighborsTransformer`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html#sklearn.neighbors.KNeighborsTransformer) et [**`KNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) pour permettre la mise en cache du graphe des voisins lors d'une recherche de grille d'hyperparam√®tres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='neighborhood-components-analysis'></a> 1.6.7. **Analyse des composantes du voisinage**<br/>([_Neighborhood Components Analysis_](https://scikit-learn.org/stable/modules/neighbors.html#neighborhood-components-analysis))\n",
    "\n",
    "L'Analyse des composantes du voisinage (NCA, [**`NeighborhoodComponentsAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NeighborhoodComponentsAnalysis.html#sklearn.neighbors.NeighborhoodComponentsAnalysis)) est un algorithme d'apprentissage de m√©triques de distance qui vise √† am√©liorer la pr√©cision de la classification par plus proches voisins par rapport √† la distance euclidienne standard. L'algorithme maximise directement une variante stochastique du score des $k$ plus proches voisins ($k$-NN) _leave-one-out_ sur l'ensemble d'entra√Ænement. Il peut √©galement apprendre une projection lin√©aire de faible dimension des donn√©es qui peut √™tre utilis√©e pour la visualisation des donn√©es et la classification rapide.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_illustration_001.png\"\n",
    "    alt=\"Analyse des composantes du voisinage, Points d'origine\"\n",
    "    style=\"max-width: 30%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_illustration_002.png\"\n",
    "    alt=\"Analyse des composantes du voisinage, Int√©gration de l'ACN\"\n",
    "    style=\"max-width: 30%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Dans la figure illustr√©e ci-dessus, nous consid√©rons certains points d'un ensemble de donn√©es g√©n√©r√© de mani√®re al√©atoire. Nous nous concentrons sur la classification $k$-NN stochastique du point n¬∞ 3. L'√©paisseur d'un lien entre l'√©chantillon 3 et un autre point est proportionnelle √† leur distance, et peut √™tre vue comme le poids relatif (ou la probabilit√©) qu'une r√®gle de pr√©diction stochastique de plus proche voisin attribuerait √† ce point. Dans l'espace d'origine, l'√©chantillon 3 a de nombreux voisins stochastiques de diverses classes, de sorte que la bonne classe n'est pas tr√®s probable. Cependant, dans l'espace projet√© appris par NCA, les seuls voisins stochastiques ayant un poids non n√©gligeable sont de la m√™me classe que l'√©chantillon 3, garantissant que ce dernier sera bien class√©. Consultez la [**formulation math√©matique** (1.6.7.3)](#nca-mathematical-formulation) pour plus de d√©tails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='id4'></a> 1.6.7.1. **Classification**<br/>([_Classification_](https://scikit-learn.org/stable/modules/neighbors.html#id4))\n",
    "\n",
    "Combined with a nearest neighbors classifier ([**`KNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)), NCA is attractive for classification because it can naturally handle multi-class problems without any increase in the model size, and does not introduce additional parameters that require fine-tuning by the user.\n",
    "\n",
    "NCA classification has been shown to work well in practice for data sets of varying size and difficulty. In contrast to related methods such as Linear Discriminant Analysis, NCA does not make any assumptions about the class distributions. The nearest neighbor classification can naturally produce highly irregular decision boundaries.\n",
    "\n",
    "To use this model for classification, one needs to combine a [**`NeighborhoodComponentsAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NeighborhoodComponentsAnalysis.html#sklearn.neighbors.NeighborhoodComponentsAnalysis) instance that learns the optimal transformation with a [**`KNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) instance that performs the classification in the projected space. Here is an example using the two classes:\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_classification_001.png\"\n",
    "    alt=\"KNN\"\n",
    "    style=\"max-width: 30%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_classification_002.png\"\n",
    "    alt=\"NCA, KNN\"\n",
    "    style=\"max-width: 30%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "The plot shows decision boundaries for Nearest Neighbor Classification and Neighborhood Components Analysis classification on the iris dataset, when training and scoring on only two features, for visualisation purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='id4'></a> 1.6.7.1. **Classification**<br/>([_Classification_](https://scikit-learn.org/stable/modules/neighbors.html#id4))\n",
    "\n",
    "En combinaison avec un classificateur des plus proches voisins ([**`KNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)), l'Analyse des composantes du voisinage (NCA) est attrayante pour la classification car elle peut naturellement g√©rer des probl√®mes multi-classes sans augmenter la taille du mod√®le, et elle n'introduit pas de param√®tres suppl√©mentaires qui n√©cessitent un r√©glage fin de la part de l'utilisateur.\n",
    "\n",
    "La classification par ACN a montr√© son efficacit√© en pratique pour des ensembles de donn√©es de tailles et de difficult√©s vari√©es. Contrairement √† des m√©thodes connexes telles que l'Analyse Discriminante Lin√©aire, le NCA ne fait aucune hypoth√®se sur les distributions des classes. La classification par plus proches voisins peut naturellement produire des fronti√®res de d√©cision tr√®s irr√©guli√®res.\n",
    "\n",
    "Pour utiliser ce mod√®le pour la classification, il faut combiner une instance d'[**`NeighborhoodComponentsAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NeighborhoodComponentsAnalysis.html#sklearn.neighbors.NeighborhoodComponentsAnalysis) qui apprend la transformation optimale avec une instance de [**`KNeighborsClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) qui effectue la classification dans l'espace projet√©. Voici un exemple utilisant deux classes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9619047619047619\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import (\n",
    "    NeighborhoodComponentsAnalysis,\n",
    "    KNeighborsClassifier\n",
    ")\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.7, random_state=42\n",
    ")\n",
    "nca = NeighborhoodComponentsAnalysis(random_state=42)\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "nca_pipe = Pipeline([('nca', nca), ('knn', knn)])\n",
    "nca_pipe.fit(X_train, y_train)\n",
    "# Pipeline(...)\n",
    "print(nca_pipe.score(X_test, y_test))\n",
    "# 0.96190476..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_classification_001.png\"\n",
    "    alt=\"KNN\"\n",
    "    style=\"max-width: 30%; height; auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_classification_002.png\"\n",
    "    alt=\"NCA, KNN\"\n",
    "    style=\"max-width: 30%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Le graphique montre les fronti√®res de d√©cision pour la Classification par les Plus Proches Voisins et la Classification par Analyse des Composantes du Voisinage sur l'ensemble de donn√©es iris, lors de l'entra√Ænement et de l'√©valuation sur seulement deux caract√©ristiques, √† des fins de visualisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='dimensionality-reduction'></a> 1.6.7.2. **R√©duction de dimension**<br/>([_Dimensionality reduction_](https://scikit-learn.org/stable/modules/neighbors.html#dimensionality-reduction))\n",
    "\n",
    "Le NCA peut √™tre utilis√©e pour effectuer une r√©duction de dimensionnalit√© supervis√©e. Les donn√©es d'entr√©e sont projet√©es sur un sous-espace lin√©aire compos√© des directions qui minimisent l'objectif du NCA. La dimensionnalit√© souhait√©e peut √™tre d√©finie √† l'aide du param√®tre `n_components`. Par exemple, la figure suivante montre une comparaison de la r√©duction de dimensionnalit√© avec l'Analyse en Composantes Principales ([**`PCA`**](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA)), l'Analyse Discriminante Lin√©aire ([**`LinearDiscriminantAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis)) et l'Analyse des Composantes du Voisinage ([**`NeighborhoodComponentsAnalysis`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NeighborhoodComponentsAnalysis.html#sklearn.neighbors.NeighborhoodComponentsAnalysis)) sur l'ensemble de donn√©es Digits, un ensemble de donn√©es de taille $n_{samples} = 1797$ et $n_{features} = 64$. L'ensemble de donn√©es est divis√© en un ensemble d'entra√Ænement et un ensemble de test de taille √©gale, puis standardis√©. Pour l'√©valuation, l'exactitude de la classification des 3 plus proches voisins est calcul√©e sur les points projet√©s en 2 dimensions trouv√©s par chaque m√©thode. Chaque √©chantillon de donn√©es appartient √† l'une des 10 classes.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_dim_reduction_001.png\"\n",
    "    alt=\"PCA, KNN\"\n",
    "    style=\"max-width: 30%; height; auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_dim_reduction_002.png\"\n",
    "    alt=\"LDA, KNN\"\n",
    "    style=\"max-width: 30%; height: auto;\"/>\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_dim_reduction_003.png\"\n",
    "    alt=\"NCA, KNN\"\n",
    "    style=\"max-width: 30%; height: auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**Illustration de l'Analyse des Composantes du Voisinage**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_6_neighbors/plot_nca_illustration.ipynb)<br/>([_Neighborhood Components Analysis Illustration_](https://scikit-learn.org/stable/auto_examples/neighbors/plot_nca_illustration.html))\n",
    "\n",
    "#### [**Comparaison entre les plus proches voisins avec et sans l'Analyse des Composantes du Voisinage**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_6_neighbors/plot_nca_classification.ipynb)<br/>([_Comparing Nearest Neighbors with and without Neighborhood Components Analysis_](https://scikit-learn.org/stable/auto_examples/neighbors/plot_nca_classification.html))\n",
    "\n",
    "#### [**R√©duction de la dimensionnalit√© avec l'analyse des composantes du voisinage**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/1_6_neighbors/plot_nca_dim_reduction.ipynb)<br/>([_Dimensionality Reduction with Neighborhood Components Analysis_](https://scikit-learn.org/stable/auto_examples/neighbors/plot_nca_dim_reduction.html))\n",
    "\n",
    "#### [**Apprentissage des vari√©t√©s sur des chiffres manuscrits : Plongement localement lin√©aire, Isomap‚Ä¶**](https://nbviewer.org/github/Franck-PepperLabs/pepper_dsia_skl_doc_fr/blob/main/docs/examples/2_2_manifold/plot_lle_digits.ipynb)<br/>([_Manifold learning on handwritten digits: Locally Linear Embedding, Isomap‚Ä¶_](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='mathematical-formulation'></a> 1.6.7.3. **Formulation math√©matique**<br/>([_Mathematical formulation_](https://scikit-learn.org/stable/modules/neighbors.html#mathematical-formulation))\n",
    "\n",
    "Le but de l'analyse des composantes du voisinage (NCA) est d'apprendre une matrice de transformation lin√©aire optimale de taille `(n_components, n_features)`, qui maximise la somme de toutes les probabilit√©s $p_i$ que $i$ soit correctement classifi√©, c'est-√†-dire :\n",
    "\n",
    "$$\n",
    "\\underset{L}{\\arg\\max} \\sum\\limits_{i=0}^{N - 1} p_{i}\n",
    "$$\n",
    "\n",
    "avec $N$ = `n_samples` et $p_i$ la probabilit√© que l'√©chantillon $i$ soit correctement classifi√© selon une r√®gle stochastique des plus proches voisins dans l'espace embarqu√© appris :\n",
    "\n",
    "$$\n",
    "p_{i}=\\sum\\limits_{j \\in C_i}{p_{i j}}\n",
    "$$\n",
    "\n",
    "o√π $C_i$ est l'ensemble de points de la m√™me classe que l'√©chantillon $i$, et $p_{i j}$ est le softmax des distances euclidiennes dans l'espace embarqu√© :\n",
    "\n",
    "$$\n",
    "p_{i j} = \\frac{\\exp(-||L x_i - L x_j||^2)}{\\sum\\limits_{k \\ne i} {\\exp{-(||L x_i - L x_k||^2)}}} , \\quad p_{i i} = 0\n",
    "$$\n",
    "\n",
    "#### Distance de Mahalanobis\n",
    "\n",
    "NCA peut √™tre consid√©r√©e comme l'apprentissage d'une distance de Mahalanobis (au carr√©) :\n",
    "\n",
    "$$\n",
    "|| L(x_i - x_j)||^2 = (x_i - x_j)^TM(x_i - x_j),\n",
    "$$\n",
    "\n",
    "o√π $M$ est une matrice sym√©trique semi-d√©finie positive de taille `(n_features, n_features)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='implementation'></a> 1.6.7.4. **Impl√©mentation**<br/>([_Implementation_](https://scikit-learn.org/stable/modules/neighbors.html#implementation))\n",
    "\n",
    "Cette impl√©mentation suit ce qui est expliqu√© dans l'article original [1]. Pour la m√©thode d'optimisation, elle utilise actuellement la m√©thode L-BFGS-B de scipy avec un calcul de gradient complet √† chaque it√©ration, afin d'√©viter d'ajuster le taux d'apprentissage et de fournir un apprentissage stable.\n",
    "\n",
    "Veuillez consulter les exemples ci-dessous et la documentation de la m√©thode [**`NeighborhoodComponentsAnalysis.fit`**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NeighborhoodComponentsAnalysis.html#sklearn.neighbors.NeighborhoodComponentsAnalysis.fit) pour plus d'informations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='complexity'></a> 1.6.7.5. **Complexit√©**<br/>([_Complexity_](https://scikit-learn.org/stable/modules/neighbors.html#complexity))\n",
    "\n",
    "Posons $N$ = `n_samples`, $D$ = `n_features`, $C$ = `n_components`.\n",
    "\n",
    "#### Entra√Ænement\n",
    "\n",
    "NCA stocke une matrice de distances par paires, occupant $N^2$ de m√©moire. La complexit√© temporelle d√©pend du nombre d'it√©rations effectu√©es par l'algorithme d'optimisation. Cependant, on peut d√©finir le nombre maximal d'it√©rations avec l'argument `max_iter`. Pour chaque it√©ration, la complexit√© temporelle est de $\\mathcal{O}(C N \\min(N, D))$.\n",
    "\n",
    "#### Transformation\n",
    "\n",
    "Ici, l'op√©ration `transform` renvoie $LX^\\top$, donc sa complexit√© temporelle est de $\\mathcal{O}(C D N_{test})$. Il n'y a pas de complexit√© spatiale suppl√©mentaire dans l'op√©ration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R√©f√©rences\n",
    "\n",
    "üî¨ [1] [**‚ÄúNeighbourhood Components Analysis‚Äù**](https://cs.nyu.edu/~roweis/papers/ncanips.pdf), J. Goldberger, S. Roweis, G. Hinton, R. Salakhutdinov, Advances in Neural Information Processing Systems, Vol. 17, May 2005, pp. 513-520.\n",
    "\n",
    "üåê [**Wikipedia entry on _Neighborhood Components Analysis_**](https://en.wikipedia.org/wiki/Neighbourhood_components_analysis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
