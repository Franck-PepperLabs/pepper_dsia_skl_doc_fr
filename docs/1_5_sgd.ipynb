{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='supervised-learning'></a> 1. [**Apprentissage supervis√©**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#supervised-learning)</br>([*Supervised learning*](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning))\n",
    "\n",
    "# <a id='stochastic-gradient-descent'></a> 1.5. [**Descente de gradient stochastique (SGD)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_5_sgd.ipynb)<br/>([_Stochastic Gradient Descent_](https://scikit-learn.org/stable/modules/sgd.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 11 pages, 6 exemples, 7 papiers\n",
    "- 1.5.1. [**Classification**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_5_sgd.ipynb#classification)<br/>([_Classification_](https://scikit-learn.org/stable/modules/sgd.html#classification))\n",
    "- 1.5.2. [**R√©gression**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_5_sgd.ipynb#regression)<br/>([_Regression_](https://scikit-learn.org/stable/modules/sgd.html#regression))\n",
    "- 1.5.3. [**SVM √† une classe en ligne**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_5_sgd.ipynb#online-one-class-svm)<br/>([_Online One-Class SVM_](https://scikit-learn.org/stable/modules/sgd.html#online-one-class-svm))\n",
    "- 1.5.4. [**Descente de gradient stochastique pour donn√©es creuses**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_5_sgd.ipynb#stochastic-gradient-descent-for-sparse-data)<br/>([_Stochastic Gradient Descent for sparse data_](https://scikit-learn.org/stable/modules/sgd.html#stochastic-gradient-descent-for-sparse-data))\n",
    "- 1.5.5. [**Complexit√©**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_5_sgd.ipynb#complexity)<br/>([_Complexity_](https://scikit-learn.org/stable/modules/sgd.html#complexity))\n",
    "- 1.5.6. [**Crit√®re d'arr√™t**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_5_sgd.ipynb#stopping-criterion)<br/>([_Stopping criterion_](https://scikit-learn.org/stable/modules/sgd.html#stopping-criterion))\n",
    "- 1.5.7. [**Conseils d'utilisation pratique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_5_sgd.ipynb#tips-on-practical-use)<br/>([_Tips on Practical Use_](https://scikit-learn.org/stable/modules/sgd.html#tips-on-practical-use))\n",
    "- 1.5.8. [**Formulation math√©matique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_5_sgd.ipynb#mathematical-formulation)<br/>([_Mathematical formulation_](https://scikit-learn.org/stable/modules/sgd.html#mathematical-formulation))\n",
    "- 1.5.9. [**D√©tails d'impl√©mentation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_5_sgd.ipynb#implementation-details)<br/>([_Implementation details_](https://scikit-learn.org/stable/modules/sgd.html#implementation-details))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='stochastic-gradient-descent'></a> 1.5. **Descente de gradient stochastique (SGD)**<br/>([_Stochastic Gradient Descent_](https://scikit-learn.org/stable/modules/sgd.html))\n",
    "\n",
    "La [**descente de gradient stochastique (SGD)**](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) est une approche simple mais tr√®s efficace pour ajuster des classifieurs lin√©aires et des r√©gresseurs sous des fonctions de perte convexes telles que les [**Machines √† vecteurs de support**](https://en.wikipedia.org/wiki/Support_vector_machine) (lin√©aires) et la [**R√©gression logistique**](https://en.wikipedia.org/wiki/Logistic_regression). Bien que la SGD existe depuis longtemps dans la communaut√© de l'apprentissage automatique, elle a r√©cemment re√ßu une attention consid√©rable dans le contexte de l'apprentissage √† grande √©chelle.\n",
    "\n",
    "La SGD a √©t√© appliqu√©e avec succ√®s √† des probl√®mes d'apprentissage automatique de grande taille et √† donn√©es creuses, souvent rencontr√©s dans la classification de texte et le traitement du langage naturel. √âtant donn√© que les donn√©es sont creuses, les classifieurs de ce module s'adaptent facilement √† des probl√®mes comportant plus de 10^5 exemples d'entra√Ænement et plus de 10^5 caract√©ristiques.\n",
    "\n",
    "Strictement parlant, la SGD est simplement une technique d'optimisation et ne correspond pas √† une famille sp√©cifique de mod√®les d'apprentissage automatique. Il s'agit simplement d'une mani√®re de former un mod√®le. Souvent, une instance de [**`SGDClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier) ou [**`SGDRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor) aura un estimateur √©quivalent dans l'API scikit-learn, utilisant √©ventuellement une technique d'optimisation diff√©rente. Par exemple, l'utilisation de `SGDClassifier(loss='log_loss')` donne lieu √† une r√©gression logistique, c'est-√†-dire un mod√®le √©quivalent √† [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) qui est ajust√© via SGD au lieu d'√™tre ajust√© par l'un des autres solveurs de [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression). De m√™me, `SGDRegressor(loss='squared_error', penalty='l2')` et [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) r√©solvent le m√™me probl√®me d'optimisation, par des moyens diff√©rents.\n",
    "\n",
    "Les avantages de la descente de gradient stochastique sont :\n",
    "- Efficacit√©.\n",
    "- Facilit√© d'impl√©mentation (de nombreuses opportunit√©s pour l'optimisation du code).\n",
    "\n",
    "Les inconv√©nients de la descente de gradient stochastique comprennent :\n",
    "- La n√©cessit√© de d√©finir plusieurs hyperparam√®tres tels que le param√®tre de r√©gularisation et le nombre d'it√©rations.\n",
    "- La sensibilit√© de la SGD √† la mise √† l'√©chelle des caract√©ristiques.\n",
    "\n",
    "> **Avertissement :** Assurez-vous de permuter (m√©langer) vos donn√©es d'entra√Ænement avant d'ajuster le mod√®le ou utilisez `shuffle=True` pour m√©langer apr√®s chaque it√©ration (utilis√© par d√©faut). De plus, de pr√©f√©rence, les caract√©ristiques doivent √™tre standardis√©es √† l'aide de, par exemple, `make_pipeline(StandardScaler(), SGDClassifier())` (voir [**Pipelines** (6.1)](https://scikit-learn.org/stable/modules/compose.html#combining-estimators))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='classification'></a> 1.5.1. **Classification**<br/>([_Classification_](https://scikit-learn.org/stable/modules/sgd.html#classification))\n",
    "\n",
    "La classe [**`SGDClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier) met en ≈ìuvre une simple routine d'apprentissage par descente de gradient stochastique qui prend en charge diff√©rentes fonctions de perte et p√©nalit√©s pour la classification. Ci-dessous se trouve la fronti√®re de d√©cision d'un [**`SGDClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier) entra√Æn√© avec la perte de charni√®re, √©quivalente √† une SVM lin√©aire.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_sgd_separating_hyperplane_001.png\"\n",
    "    alt=\"SGD : Hyperplan de s√©paration √† marge maximale\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Comme d'autres classifieurs, SGD doit √™tre ajust√© avec deux tableaux : un tableau `X` de forme `(n_samples, n_features)` contenant les √©chantillons d'entra√Ænement, et un tableau `y` de forme `(n_samples,)` contenant les valeurs cibles (√©tiquettes de classe) pour les √©chantillons d'entra√Ænement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(max_iter=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(max_iter=5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(max_iter=5)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "X = [[0., 0.], [1., 1.]]\n",
    "y = [0, 1]\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n",
    "clf.fit(X, y)\n",
    "# SGDClassifier(max_iter=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois ajust√©, le mod√®le peut √™tre utilis√© pour pr√©dire de nouvelles valeurs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[2., 2.]])\n",
    "# array([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD ajuste un mod√®le lin√©aire aux donn√©es d'entra√Ænement. L'attribut `coef_` contient les param√®tres du mod√®le :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.91080278, 9.91080278]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_\n",
    "# array([[9.9..., 9.9...]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'attribut `intercept_` contient l'interception (√©galement appel√©e _d√©calage_ ou _biais_) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.99002993])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.intercept_\n",
    "# array([-9.9...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que le mod√®le utilise ou non une interception, c'est-√†-dire un hyperplan biais√©, est contr√¥l√© par le param√®tre `fit_intercept`.\n",
    "\n",
    "La distance sign√©e √† l'hyperplan (calcul√©e comme le produit scalaire entre les coefficients et l'√©chantillon d'entr√©e, plus l'interception) est donn√©e par [**`SGDClassifier.decision_function`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.decision_function) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29.65318117])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.decision_function([[2., 2.]])\n",
    "# array([29.6...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction de perte concr√®te peut √™tre d√©finie via le param√®tre `loss`. [**`SGDClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier) prend en charge les fonctions de perte suivantes :\n",
    "\n",
    "- `loss=\"hinge\"` : (marge douce) Machine √† vecteurs de support lin√©aire,\n",
    "- `loss=\"modified_huber\"` : perte de charni√®re liss√©e,\n",
    "- `loss=\"log_loss\"` : r√©gression logistique,\n",
    "- et toutes les pertes de r√©gression ci-dessous. Dans ce cas, la cible est encod√©e en tant que -1 ou 1, et le probl√®me est trait√© comme un probl√®me de r√©gression. La classe pr√©dite correspond alors au signe de la cible pr√©dite.\n",
    "\n",
    "Veuillez vous r√©f√©rer √† la [**section math√©matique ci-dessous** (1.5.8)](#sgd-mathematical-formulation) pour les formules. Les deux premi√®res fonctions de perte sont \"paresseuses\" ; elles ne mettent √† jour les param√®tres du mod√®le que si un exemple viole la contrainte de marge, ce qui rend l'apprentissage tr√®s efficace et peut entra√Æner des mod√®les plus parcimonieux (c'est-√†-dire avec plus de coefficients nuls), m√™me lorsque la p√©nalit√© $\\ell_2$ est utilis√©e.\n",
    "\n",
    "L'utilisation de `loss=\"log_loss\"` ou de `loss=\"modified_huber\"` active la m√©thode `predict_proba`, qui renvoie un vecteur d'estimations de probabilit√© $P(y|x)$ par √©chantillon $x$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00459185, 0.99540815]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SGDClassifier(loss=\"log_loss\", max_iter=5).fit(X, y)\n",
    "clf.predict_proba([[1., 1.]]) \n",
    "# array([[0.00..., 0.99...]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La p√©nalit√© concr√®te peut √™tre d√©finie via le param√®tre `penalty`. La SGD prend en charge les p√©nalit√©s suivantes :\n",
    "- `penalty=\"l2\"` : P√©nalit√© de la norme $\\ell_2$ sur `coef_`.\n",
    "- `penalty=\"l1\"` : P√©nalit√© de la norme $\\ell_1$ sur `coef_`.\n",
    "- `penalty=\"elasticnet\"` : Combinaison convexe de $\\ell_2$ et $\\ell_1$ ; `(1 - l1_ratio) * L2 + l1_ratio * L1`.\n",
    "\n",
    "Le param√®tre par d√©faut est `penalty=\"l2\"`. La p√©nalit√© $\\ell_1$ conduit √† des solutions parcimonieuses, poussant la plupart des coefficients vers z√©ro. L'Elastic Net [11] r√©sout certaines lacunes de la p√©nalit√© $\\ell_1$ en pr√©sence d'attributs fortement corr√©l√©s. Le param√®tre `l1_ratio` contr√¥le la combinaison convexe des p√©nalit√©s $\\ell_1$ et $\\ell_2$.\n",
    "\n",
    "[**`SGDClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier) prend en charge la classification multiclasse en combinant plusieurs classifieurs binaires selon un sch√©ma \"un contre tous\" (OVA). Pour chacune des $K$ classes, un classifieur binaire est entra√Æn√© √† distinguer cette classe de toutes les autres $K-1$ classes. Au moment du test, nous calculons le score de confiance (c'est-√†-dire les distances sign√©es par rapport au plan) pour chaque classifieur et choisissons la classe avec la plus grande confiance. La figure ci-dessous illustre l'approche OVA sur l'ensemble de donn√©es iris. Les lignes en pointill√©s repr√©sentent les trois classifieurs OVA ; les couleurs de fond montrent la surface de d√©cision induite par les trois classifieurs.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_sgd_iris_001.png\"\n",
    "    alt=\"Classification SGD multiclasse sur l'ensemble de donn√©es iris\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Dans le cas de la classification multiclasse, `coef_` est un tableau bidimensionnel de forme `(n_classes, n_features)` et `intercept_` est un tableau unidimensionnel de forme `(n_classes,)`. La $i$-√®me ligne de `coef_` contient le vecteur de poids du classifieur OVA pour la $i$-√®me classe ; les classes sont index√©es dans l'ordre croissant (voir l'attribut `classes_`). √Ä noter que, en principe, puisqu'elles permettent de cr√©er un mod√®le de probabilit√©, `loss=\"log_loss\"` et `loss=\"modified_huber\"` sont plus adapt√©es √† la classification un contre tous.\n",
    "\n",
    "[**`SGDClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier) prend en charge √† la fois les classes pond√©r√©es et les instances pond√©r√©es via les param√®tres de r√©glage `class_weight` et `sample_weight`. Consultez les exemples ci-dessous et la docstring de [**`SGDClassifier.fit`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.fit) pour plus d'informations.\n",
    "\n",
    "[**`SGDClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier) prend en charge la descente de gradient stochastique moyenn√©e (ASGD) [10]. La moyenne peut √™tre activ√©e en d√©finissant `average=True`. ASGD effectue les m√™mes mises √† jour que la SGD classique (voir [**Formulation math√©matique** (1.5.8)](#mathematical-formulation)), mais au lieu d'utiliser la derni√®re valeur des coefficients comme attribut `coef_` (c'est-√†-dire les valeurs de la derni√®re mise √† jour), `coef_` est plut√¥t d√©fini comme la valeur **moyenne** des coefficients sur l'ensemble des mises √† jour. Il en va de m√™me pour l'attribut `intercept_`. Lors de l'utilisation d'ASGD, le taux d'apprentissage peut √™tre plus √©lev√© et m√™me constant, ce qui peut acc√©l√©rer le temps d'entra√Ænement sur certains ensembles de donn√©es.\n",
    "\n",
    "Pour la classification avec une perte logistique, une autre variante de la SGD avec une strat√©gie de moyenne est disponible avec l'algorithme Stochastic Average Gradient (SAG), disponible en tant que solveur dans [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples\n",
    "\n",
    "#### [**SGD : Hyperplan de s√©paration √† marge maximale**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_sgd_separating_hyperplane.ipynb)<br/>([_SGD: Maximum margin separating hyperplane_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_separating_hyperplane.html))\n",
    "\n",
    "#### [**Trac√© de la classification multiclasse SGD sur le jeu de donn√©es Iris**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_sgd_iris.ipynb)<br/>([_Plot multi-class SGD on the iris dataset_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_iris.html))\n",
    "\n",
    "#### [**SGD : √âchantillons pond√©r√©s**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_sgd_weighted_samples.ipynb)<br/>([_SGD: Weighted samples_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_weighted_samples.html))\n",
    "\n",
    "#### [**Comparaison de divers solveurs en ligne**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_1_linear_model/plot_sgd_comparison.ipynb)<br/>([_Comparing various online solvers_](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_comparison.html))\n",
    "\n",
    "#### [**SVM¬†: hyperplan s√©parateur pour les classes d√©s√©quilibr√©es**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_4_svm/plot_separating_hyperplane_unbalanced.ipynb)<br/>([*SVM: Separating hyperplane for unbalanced classes*](https://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html))\n",
    "\n",
    "(voir la note dans l'exemple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='regression'></a> 1.5.2. **R√©gression**<br/>([_Regression_](https://scikit-learn.org/stable/modules/sgd.html#regression))\n",
    "\n",
    "La classe [**`SGDRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor) impl√©mente une proc√©dure d'apprentissage simple de descente de gradient stochastique qui prend en charge diff√©rentes fonctions de perte et r√©gularisations pour ajuster des mod√®les de r√©gression lin√©aire. [**`SGDRegressor`**](https://scikit-learn.org/stable/modules/sgd.html#regression) est bien adapt√© √† des probl√®mes de r√©gression avec un grand nombre d'√©chantillons d'entra√Ænement (> 10 000), pour d'autres probl√®mes, nous recommandons d'utiliser [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge), [**`Lasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso) ou [**`ElasticNet`**](https://scikit-learn.org/stable/modules/sgd.html#regression).\n",
    "\n",
    "La fonction de perte concr√®te peut √™tre d√©finie via le param√®tre `loss`. [**`SGDRegressor`**](https://scikit-learn.org/stable/modules/sgd.html#regression) prend en charge les fonctions de perte suivantes :\n",
    "- `loss=\"squared_error\"` : Moindres carr√©s ordinaires,\n",
    "- `loss=\"huber\"` : Perte de Huber pour la r√©gression robuste,\n",
    "- `loss=\"epsilon_insensitive\"` : R√©gression vectorielle de support lin√©aire.\n",
    "\n",
    "Veuillez vous r√©f√©rer √† la [**section math√©matique ci-dessous** (1.5.8)](sgd-mathematical-formulation) pour les formules. Les fonctions de perte de Huber et epsilon-insensible peuvent √™tre utilis√©es pour la r√©gression robuste. La largeur de la r√©gion insensible doit √™tre sp√©cifi√©e via le param√®tre `epsilon`. Ce param√®tre d√©pend de l'√©chelle des variables cibles.\n",
    "\n",
    "Le param√®tre `penalty` d√©termine la r√©gularisation √† utiliser (voir la description ci-dessus dans la section classification).\n",
    "\n",
    "[**`SGDRegressor`**](https://scikit-learn.org/stable/modules/sgd.html#regression) prend √©galement en charge la descente de gradient stochastique moyenn√©e [10] (ici encore, voir la description ci-dessus dans la section classification).\n",
    "\n",
    "Pour la r√©gression avec une perte quadratique et une r√©gularisation $\\ell_2$, une autre variante de la descente de gradient stochastique avec une strat√©gie de moyenne est disponible avec l'algorithme de gradient stochastique moyen (SAG), disponible en tant que solveur dans [**`Ridge`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='online-one-class-svm'></a> 1.5.3. **SVM √† une classe en ligne**<br/>([_Online One-Class SVM_](https://scikit-learn.org/stable/modules/sgd.html#online-one-class-svm))\n",
    "\n",
    "La classe [**`sklearn.linear_model.SGDOneClassSVM`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDOneClassSVM.html#sklearn.linear_model.SGDOneClassSVM) impl√©mente une version lin√©aire en ligne de la One-Class SVM √† l'aide de la descente de gradient stochastique. Combin√©e avec des techniques d'approximation de noyau, [**`sklearn.linear_model.SGDOneClassSVM`**](https://scikit-learn.org/stable/modules/sgd.html#online-one-class-svm) peut √™tre utilis√©e pour approximer la solution d'une One-Class SVM √† noyau, mise en ≈ìuvre dans [**`sklearn.svm.OneClassSVM`**](https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM), avec une complexit√© lin√©aire dans le nombre d'√©chantillons. Notez que la complexit√© d'une One-Class SVM √† noyau est au mieux quadratique dans le nombre d'√©chantillons. [**`sklearn.linear_model.SGDOneClassSVM`**](https://scikit-learn.org/stable/modules/sgd.html#online-one-class-svm) convient donc bien aux ensembles de donn√©es avec un grand nombre d'√©chantillons d'entra√Ænement (> 10 000), pour lesquels la variante SGD peut √™tre plusieurs ordres de grandeur plus rapide.\n",
    "\n",
    "Comme [**`SGDClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier) et [**`SGDRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor), [**`SGDOneClassSVM`**](https://scikit-learn.org/stable/modules/sgd.html#online-one-class-svm) prend en charge la descente de gradient stochastique moyenn√©e. L'option de moyennage peut √™tre activ√©e en d√©finissant `average=True`.\n",
    "\n",
    "### D√©tails math√©matiques\n",
    "\n",
    "Son impl√©mentation est bas√©e sur l'impl√©mentation de la descente de gradient stochastique. En effet, le probl√®me d'optimisation original de la One-Class SVM est donn√© par\n",
    "\n",
    "$$\n",
    "\\begin{split}\\begin{aligned}\n",
    "\\min_{w, \\rho, \\xi} & \\quad \\frac{1}{2}\\Vert w \\Vert^2 - \\rho + \\frac{1}{\\nu n} \\sum_{i=1}^n \\xi_i \\\\\n",
    "\\text{s.t.} & \\quad \\langle w, x_i \\rangle \\geq \\rho - \\xi_i \\quad 1 \\leq i \\leq n \\\\\n",
    "& \\quad \\xi_i \\geq 0 \\quad 1 \\leq i \\leq n\n",
    "\\end{aligned}\\end{split}\n",
    "$$\n",
    "\n",
    "o√π $\\nu \\in (0, 1]$ est le param√®tre sp√©cifi√© par l'utilisateur contr√¥lant la proportion d'outliers et la proportion de vecteurs de support. En se d√©barrassant des variables de rel√¢chement $\\xi_i$, ce probl√®me est √©quivalent √†\n",
    "\n",
    "$$\n",
    "\\min_{w, \\rho} \\frac{1}{2}\\Vert w \\Vert^2 - \\rho + \\frac{1}{\\nu n} \\sum_{i=1}^n \\max(0, \\rho - \\langle w, x_i \\rangle) \\, .\n",
    "$$\n",
    "\n",
    "En multipliant par la constante $\\nu$ et en introduisant l'interception $b = 1 - \\rho$, nous obtenons le probl√®me d'optimisation √©quivalent suivant\n",
    "\n",
    "$$\n",
    "\\min_{w, b} \\frac{\\nu}{2}\\Vert w \\Vert^2 + b\\nu + \\frac{1}{n} \\sum_{i=1}^n \\max(0, 1 - (\\langle w, x_i \\rangle + b)) \\, .\n",
    "$$\n",
    "\n",
    "Cela ressemble aux probl√®mes d'optimisation √©tudi√©s dans la section [**Formulation math√©matique** (1.5.8)](#sgd-mathematical-formulation) avec $y_i = 1, 1 \\leq i \\leq n$ et $\\alpha = \\nu/2$, √©tant la fonction de perte charni√®re et $R$ √©tant la norme $\\ell_2$. Nous devons simplement ajouter le terme $b\\nu$ dans la boucle d'optimisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='stochastic-gradient-descent-for-sparse-data'></a> 1.5.4. **Descente de gradient stochastique pour donn√©es creuses**<br/>([_Stochastic Gradient Descent for sparse data_](https://scikit-learn.org/stable/modules/sgd.html#stochastic-gradient-descent-for-sparse-data))\n",
    "\n",
    "**Note:** L'impl√©mentation pour donn√©es creuses produit des r√©sultats l√©g√®rement diff√©rents de l'impl√©mentation dense, en raison d'un taux d'apprentissage r√©duit pour l'interception. Consultez [**D√©tails de l'impl√©mentation** (1.5.9)](#implementation-details).\n",
    "\n",
    "Il existe une prise en charge int√©gr√©e pour les donn√©es creuses fournies dans n'importe quelle matrice dans un format pris en charge par [**`scipy.sparse`**](https://docs.scipy.org/doc/scipy/reference/sparse.html). Cependant, pour une efficacit√© maximale, utilisez le format CSR (Compressed Sparse Row) tel que d√©fini dans [**`scipy.sparse.csr_matrix`**](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html).\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Classification de documents textuels √† l'aide de caract√©ristiques creuses**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/text/plot_document_classification_20newsgroups.ipynb)<br/>([_Classification of text documents using sparse features_](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='complexity'></a> 1.5.5. **Complexit√©**<br/>([_Complexity_](https://scikit-learn.org/stable/modules/sgd.html#complexity))\n",
    "\n",
    "Le principal avantage de la SGD r√©side dans son efficacit√©, qui est essentiellement lin√©aire par rapport au nombre d'exemples d'entra√Ænement. Si `X` est une matrice de taille `(n, p)`, l'entra√Ænement a un co√ªt de $\\mathcal{O}(kn\\bar{p})$, o√π $k$ est le nombre d'it√©rations (epochs) et $\\bar p$ est le nombre moyen d'attributs non nuls par √©chantillon.\n",
    "\n",
    "Cependant, des r√©sultats th√©oriques r√©cents montrent que le temps n√©cessaire pour obtenir une pr√©cision d'optimisation souhait√©e n'augmente pas √† mesure que la taille de l'ensemble d'entra√Ænement augmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='stopping-criterion'></a> 1.5.6. **Crit√®re d'arr√™t**<br/>([_Stopping criterion_](https://scikit-learn.org/stable/modules/sgd.html#stopping-criterion))\n",
    "\n",
    "Les classes [**`SGDClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier) et [**`SGDRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor) proposent deux crit√®res pour arr√™ter l'algorithme lorsqu'un certain niveau de convergence est atteint :\n",
    "\n",
    "- Avec `early_stopping=True`, les donn√©es d'entr√©e sont divis√©es en un ensemble d'entra√Ænement et un ensemble de validation. Le mod√®le est ensuite ajust√© sur l'ensemble d'entra√Ænement, et le crit√®re d'arr√™t est bas√© sur la pr√©diction de `score` (en utilisant la m√©thode de score) calcul√©e sur l'ensemble de validation. La taille de l'ensemble de validation peut √™tre modifi√©e avec le param√®tre `validation_fraction`.\n",
    "- Avec `early_stopping=False`, le mod√®le est ajust√© sur l'ensemble des donn√©es d'entr√©e, et le crit√®re d'arr√™t est bas√© sur la fonction objectif calcul√©e sur les donn√©es d'entra√Ænement.\n",
    "\n",
    "Dans les deux cas, le crit√®re est √©valu√© une fois par √©poque, et l'algorithme s'arr√™te lorsque le crit√®re n'am√©liore pas `n_iter_no_change` fois d'affil√©e. L'am√©lioration est √©valu√©e avec une tol√©rance absolue `tol`, et l'algorithme s'arr√™te de toute fa√ßon apr√®s un nombre maximal d'it√©rations `max_iter`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='tips-on-practical-use'></a> 1.5.7. **Conseils d'utilisation pratique**<br/>([_Tips on Practical Use_](https://scikit-learn.org/stable/modules/sgd.html#tips-on-practical-use))\n",
    "\n",
    "### Mise √† l'√©chelle des caract√©ristiques\n",
    "\n",
    "La descente de gradient stochastique est sensible √† la mise √† l'√©chelle des caract√©ristiques, il est donc fortement recommand√© de mettre √† l'√©chelle vos donn√©es. Par exemple, mettez √† l'√©chelle chaque attribut sur le vecteur d'entr√©e `X` dans l'intervalle $[0, 1]$ ou $[-1, +1]$, ou standardisez-le pour avoir une moyenne de $0$ et une variance de $1$. Notez que la **m√™me** mise √† l'√©chelle doit √™tre appliqu√©e au vecteur de test pour obtenir des r√©sultats significatifs. Cela peut √™tre facilement fait en utilisant [**`StandardScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) :\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)  # Ne trichez pas - ajustez uniquement sur les donn√©es d'entra√Ænement\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)  # appliquer la m√™me transformation aux donn√©es de test\n",
    "\n",
    "# Ou mieux encore : utilisez un pipeline !\n",
    "from sklearn.pipeline import make_pipeline\n",
    "est = make_pipeline(StandardScaler(), SGDClassifier())\n",
    "est.fit(X_train)\n",
    "est.predict(X_test)\n",
    "```\n",
    "\n",
    "Si vos attributs ont une √©chelle intrins√®que (par exemple, les fr√©quences de mots ou les caract√©ristiques indicatrices), la mise √† l'√©chelle n'est pas n√©cessaire.\n",
    "\n",
    "### R√©gularisation\n",
    "\n",
    "Trouver un terme de r√©gularisation raisonnable $\\alpha$ est mieux fait en utilisant une recherche automatique des hyperparam√®tres, par exemple [**`GridSearchCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) ou [**`RandomizedSearchCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV), g√©n√©ralement dans la plage `10.0**-np.arange(1,7)`.\n",
    "\n",
    "### Nombre d'it√©rations\n",
    "\n",
    "Empiriquement, nous avons constat√© que la SGD converge apr√®s avoir observ√© environ 10^6 exemples d'entra√Ænement. Ainsi, une premi√®re estimation raisonnable pour le nombre d'it√©rations est `max_iter = np.ceil(10**6 / n)`, o√π `n` est la taille de l'ensemble d'entra√Ænement.\n",
    "\n",
    "### Mise √† l'√©chelle des caract√©ristiques extraites par PCA\n",
    "\n",
    "Si vous appliquez la SGD √† des caract√©ristiques extraites √† l'aide de l'ACP, nous avons constat√© qu'il est souvent judicieux de mettre √† l'√©chelle les valeurs des caract√©ristiques par une constante $c$ de telle sorte que la norme L2 moyenne des donn√©es d'entra√Ænement soit √©gale √† un.\n",
    "\n",
    "### Meilleure performance de la SGD moyenn√©e\n",
    "\n",
    "Nous avons constat√© que la SGD moyenn√©e fonctionne mieux avec un plus grand nombre de caract√©ristiques et une valeur de $\\eta_0$ plus √©lev√©e.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='mathematical-formulation'></a> 1.5.8. **Formulation math√©matique**<br/>([_Mathematical formulation_](https://scikit-learn.org/stable/modules/sgd.html#mathematical-formulation))\n",
    "\n",
    "Nous d√©crivons ici les d√©tails math√©matiques de la proc√©dure SGD. Vous pouvez trouver un bon aper√ßu des taux de convergence dans [12].\n",
    "\n",
    "√âtant donn√© un ensemble d'exemples d'entra√Ænement $(x_1, y_1), \\ldots, (x_n, y_n)$ o√π $x_i \\in \\mathbf{R}^m$ et $y_i \\in \\mathbf{R}$ ($y_i \\in \\{-1, 1\\}$ pour la classification), notre objectif est d'apprendre une fonction de score lin√©aire $f(x) = w^T x + b$ avec les param√®tres de mod√®le $w \\in \\mathbf{R}^m$ et l'interception $b \\in \\mathbf{R}$. Pour effectuer des pr√©dictions pour la classification binaire, nous regardons simplement le signe de $f(x)$. Pour trouver les param√®tres du mod√®le, nous minimisons l'erreur d'entra√Ænement r√©gularis√©e donn√©e par\n",
    "\n",
    "$$\n",
    "E(w,b) = \\frac{1}{n}\\sum_{i=1}^{n} L(y_i, f(x_i)) + \\alpha R(w)\n",
    "$$\n",
    "\n",
    "o√π $L$ est une fonction de perte qui mesure l'ad√©quation du mod√®le et $R$ est un terme de r√©gularisation (alias _p√©nalit√©_) qui p√©nalise la complexit√© du mod√®le ; $\\alpha > 0$ est un hyperparam√®tre non n√©gatif qui contr√¥le la force de r√©gularisation.\n",
    "\n",
    "> **D√©tails des fonctions de perte :** Diff√©rents choix pour $L$ entra√Ænent diff√©rents classifieurs ou r√©gresseurs :\n",
    "> - Charni√®re (marge souple) : √©quivalent √† la Classification par Machine √† Vecteurs de Support. $L(y_i, f(x_i)) = \\max(0, 1 - y_i f(x_i))$.\n",
    "> - Perceptron : $L(y_i, f(x_i)) = \\max(0, - y_i f(x_i))$.\n",
    "> - Huber modifi√© : $L(y_i, f(x_i)) = \\max(0, 1 - y_i f(x_i))^2$ si $y_i f(x_i) > 1$, et $L(y_i, f(x_i)) = -4 y_i f(x_i)$ sinon.\n",
    "> - Perte logarithmique : √©quivalent √† la R√©gression Logistique. $L(y_i, f(x_i)) = \\log(1 + \\exp (-y_i f(x_i)))$.\n",
    "> - Erreur quadratique : R√©gression lin√©aire (Ridge ou Lasso en fonction de $R$). $L(y_i, f(x_i)) = \\frac{1}{2}(y_i - f(x_i))^2$.\n",
    "> - Huber : moins sensible aux valeurs aberrantes que les moindres carr√©s. C'est √©quivalent aux moindres carr√©s lorsque $|y_i - f(x_i)| \\leq \\varepsilon$, et $L(y_i, f(x_i)) = \\varepsilon |y_i - f(x_i)| - \\frac{1}{2} \\varepsilon^2$ sinon.\n",
    "> - Epsilon-Insensitive (marge souple) : √©quivalent √† la R√©gression par Machine √† Vecteurs de Support. $L(y_i, f(x_i)) = \\max(0, |y_i - f(x_i)| - \\varepsilon)$.\n",
    "\n",
    "Toutes les fonctions de perte ci-dessus peuvent √™tre consid√©r√©es comme une borne sup√©rieure sur l'erreur de classification (perte z√©ro-un), comme le montre la figure ci-dessous.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_sgd_loss_functions_001.png\"\n",
    "    alt=\"Fonctions de perte convexes SGD\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Les choix populaires pour le terme de r√©gularisation $R$ (le param√®tre `penalty`) comprennent :\n",
    "- Norme $\\ell_2$ : $R(w) := \\frac{1}{2} \\sum_{j=1}^{m} w_j^2 = ||w||_2^2$,\n",
    "- Norme $\\ell_1$ : $R(w) := \\sum_{j=1}^{m} |w_j|$, ce qui conduit √† des solutions clairsem√©es.\n",
    "- Elastic Net : $R(w) := \\frac{\\rho}{2} \\sum_{j=1}^{n} w_j^2 + (1-\\rho) \\sum_{j=1}^{m} |w_j|$, une combinaison convexe de $\\ell_2$ et $\\ell_1$, o√π $\\rho$ est donn√© par `1 - l1_ratio`.\n",
    "\n",
    "La figure ci-dessous (voir l'exemple [**P√©nalit√© SGD**](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_penalties.html)) montre les contours des diff√©rents termes de r√©gularisation dans un espace de param√®tres √† deux dimensions $(m = 2)$ lorsque $R(w) = 1$.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_sgd_penalties_001.png\"\n",
    "    alt=\"P√©nalit√©s SGD\"\n",
    "    style=\"max-width: 50%; height: auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.8.1 **SGD**<br/>([_SGD_](https://scikit-learn.org/stable/modules/sgd.html#id5))\n",
    "\n",
    "La descente de gradient stochastique est une m√©thode d'optimisation pour les probl√®mes d'optimisation non contraints. Contrairement √† la descente de gradient (par lots), la SGD approxime le vrai gradient de $E(w, b)$ en consid√©rant un seul exemple d'entra√Ænement √† la fois.\n",
    "\n",
    "La classe [**`SGDClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier) met en ≈ìuvre une routine d'apprentissage de type SGD de premier ordre. L'algorithme it√®re sur les exemples d'entra√Ænement et, pour chaque exemple, met √† jour les param√®tres du mod√®le en fonction de la r√®gle de mise √† jour donn√©e par\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta \\left[\\alpha \\frac{\\partial R(w)}{\\partial w}\n",
    "+ \\frac{\\partial L(w^T x_i + b, y_i)}{\\partial w}\\right]\n",
    "$$\n",
    "\n",
    "o√π $\\eta$ est le taux d'apprentissage qui contr√¥le la taille du pas dans l'espace des param√®tres. L'interception $b$ est mise √† jour de mani√®re similaire, mais sans r√©gularisation (et avec une d√©croissance suppl√©mentaire pour les matrices creuses, comme d√©taill√© dans [**D√©tails de l'impl√©mentation** (1.5.9)](#implementation-details)).\n",
    "\n",
    "Le taux d'apprentissage $\\eta$ peut √™tre soit constant, soit en d√©croissance progressive. Pour la classification, l'ordonnancement de taux d'apprentissage par d√©faut (`learning_rate='optimal'`) est donn√© par\n",
    "\n",
    "$$\n",
    "\\eta^{(t)} = \\frac {1}{\\alpha  (t_0 + t)}\n",
    "$$\n",
    "\n",
    "o√π $t$ est l'√©tape de temps (il y a un total de `n_samples * n_iter` √©tapes de temps), $t_0$ est d√©termin√© sur la base d'une heuristique propos√©e par L√©on Bottou de telle sorte que les mises √† jour initiales attendues soient comparables √† la taille attendue des poids (en supposant que la norme des √©chantillons d'entra√Ænement est d'environ 1). La d√©finition exacte peut √™tre trouv√©e dans `_init_t` dans `BaseSGD`.\n",
    "\n",
    "Pour la r√©gression, l'ordonnancement de taux d'apprentissage par d√©faut est l'inversion de l'√©chelle (`learning_rate='invscaling'`), donn√©e par\n",
    "\n",
    "$$\n",
    "\\eta^{(t)} = \\frac{\\eta_0}{t^{e_t}}\n",
    "$$\n",
    "\n",
    "o√π $\\eta_0$ et $e_t$ sont des hyperparam√®tres choisis par l'utilisateur via `eta0` et `power_t`, respectivement.\n",
    "\n",
    "Pour un taux d'apprentissage constant, utilisez `learning_rate='constant'` et utilisez `eta0` pour sp√©cifier le taux d'apprentissage.\n",
    "\n",
    "Pour un taux d'apprentissage d√©croissant de mani√®re adaptative, utilisez `learning_rate='adaptive'` et utilisez `eta0` pour sp√©cifier le taux d'apprentissage initial. Lorsque le crit√®re d'arr√™t est atteint, le taux d'apprentissage est divis√© par 5, et l'algorithme ne s'arr√™te pas. L'algorithme s'arr√™te lorsque le taux d'apprentissage passe en dessous de 1e-6.\n",
    "\n",
    "Les param√®tres du mod√®le peuvent √™tre accessibles via les attributs `coef_` et `intercept_` : `coef_` contient les poids $w$ et `intercept_` contient $b$.\n",
    "\n",
    "Lors de l'utilisation de la SGD moyenn√©e (avec le param√®tre `average`), `coef_` est d√©fini comme la moyenne des poids sur toutes les mises √† jour : `coef_`\n",
    "$= \\frac{1}{T} \\sum_{t=0}^{T-1} w^{(t)}$, o√π $T$ est le nombre total de mises √† jour, trouv√© dans l'attribut `t_`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='implementation-details'></a> 1.5.9. **D√©tails d'impl√©mentation**<br/>([_Implementation details_](https://scikit-learn.org/stable/modules/sgd.html#implementation-details))\n",
    "\n",
    "L'impl√©mentation de la SGD est influenc√©e par la ¬´ Stochastic Gradient SVM ¬ª de [7]. Tout comme SvmSGD, le vecteur de poids est repr√©sent√© comme le produit d'un scalaire et d'un vecteur, ce qui permet une mise √† jour efficace des poids dans le cas de la r√©gularisation $\\ell_2$. Dans le cas d'une entr√©e creuse `X`, l'interception est mise √† jour avec un taux d'apprentissage plus faible (multipli√© par 0,01) pour tenir compte du fait qu'elle est mise √† jour plus fr√©quemment. Les exemples d'entra√Ænement sont s√©lectionn√©s s√©quentiellement et le taux d'apprentissage est r√©duit apr√®s chaque exemple observ√©. Nous avons adopt√© l'ordonnancement de taux d'apprentissage de [8]. Pour la classification multiclasse, on utilise une approche ¬´ un contre tous ¬ª. Nous utilisons l'algorithme de gradient tronqu√© propos√© dans [9] pour la r√©gularisation $\\ell_1$ (et l'Elastic Net). Le code est √©crit en Cython.\n",
    "\n",
    "### R√©f√©rences\n",
    "\n",
    "üåê [7] [**‚ÄúStochastic Gradient Descent‚Äù**](https://leon.bottou.org/projects/sgd) L. Bottou - Website, 2010.\n",
    "\n",
    "üî¨ [8] [**‚ÄúPegasos: Primal Estimated sub-GrAdient SOlver for SVM‚Äù**](https://home.ttic.edu/~nati/Publications/PegasosMPB.pdf) S. Shalev-Shwartz, Y. Singer, N. Srebro - In Proceedings of ICML 2007.\n",
    "\n",
    "üî¨ [9] [**‚ÄúStochastic Gradient Descent Training for L1-regularized Log-linear Models with Cumulative Penalty‚Äù**](https://aclanthology.org/P09-1054.pdf) Y. Tsuruoka, J. Tsujii, S. Ananiadou - In Proceedings of the AFNLP/ACL 2009.\n",
    "\n",
    "üî¨ [10] (1,2) [**‚ÄúTowards Optimal One Pass Large Scale Learning with Averaged Stochastic Gradient Descent‚Äù**](https://www.semanticscholar.org/reader/a7e2d53c47bdef073add879557270d915d80a098) Xu, Wei (2011)\n",
    "\n",
    "üî¨ [11] [**‚ÄúRegularization and variable selection via the elastic net‚Äù**](http://web.stanford.edu/%7Ehastie/Papers/elasticnet.pdf) H. Zou, T. Hastie - Journal of the Royal Statistical Society Series B, 67 (2), 301-320.\n",
    "\n",
    "üî¨ [12] [**‚ÄúSolving large scale linear prediction problems using stochastic gradient descent algorithms‚Äù**](https://dl.acm.org/doi/10.1145/1015330.1015332) T. Zhang - In Proceedings of ICML 2004."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
