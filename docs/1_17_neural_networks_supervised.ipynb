{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='supervised-learning'></a> 1. [**Apprentissage supervisé**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#supervised-learning)</br>([*Supervised learning*](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning))\n",
    "\n",
    "# <a id='neural-network-models-supervised'></a> 1.17. [**Modèles de réseaux neuronaux (supervisés)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_17_neural_networks_supervised.ipynb)<br/>([_Neural network models (supervised)_](https://scikit-learn.org/stable/modules/neural_networks_supervised.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 7 pages, 3 exemples, 5 papiers\n",
    "- 1.17.1. [**Perceptron multi-couches**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_17_neural_networks_supervised.ipynb#multi-layer-perceptron)<br/>([_Multi-layer Perceptron_](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#multi-layer-perceptron))\n",
    "- 1.17.2. [**Classification**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_17_neural_networks_supervised.ipynb#classification)<br/>([_Classification_](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#classification))\n",
    "- 1.17.3. [**Régression**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_17_neural_networks_supervised.ipynb#regression)<br/>([_Regression_](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#regression))\n",
    "- 1.17.4. [**Régularisation**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_17_neural_networks_supervised.ipynb#regularization)<br/>([_Regularization_](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#regularization))\n",
    "- 1.17.5. [**Algorithmes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_17_neural_networks_supervised.ipynb#algorithms)<br/>([_Algorithms_](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#algorithms))\n",
    "- 1.17.6. [**Complexité**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_17_neural_networks_supervised.ipynb#complexity)<br/>([_Complexity_](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#complexity))\n",
    "- 1.17.7. [**Formulation mathématique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_17_neural_networks_supervised.ipynb#mathematical-formulation)<br/>([_Mathematical formulation_](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#mathematical-formulation))\n",
    "- 1.17.8. [**Conseils d'utilisation pratique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_17_neural_networks_supervised.ipynb#tips-on-practical-use)<br/>([_Tips on Practical Use_](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#tips-on-practical-use))\n",
    "- 1.17.9. [**Plus de contrôle avec `warm_start`**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_17_neural_networks_supervised.ipynb#more-control-with-warm-start)<br/>([_More control with `warm_start`_](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#more-control-with-warm-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='neural-network-models-supervised'></a> 1.17. **Modèles de réseaux neuronaux (supervisés)**<br/>([_Neural network models (supervised)_](https://scikit-learn.org/stable/modules/neural_networks_supervised.html))\n",
    "\n",
    "> **Attention :** Cette implémentation n'est pas destinée aux applications à grande échelle. En particulier, scikit-learn ne prend pas en charge les GPU. Pour des implémentations beaucoup plus rapides basées sur GPU, ainsi que des cadres offrant beaucoup plus de flexibilité pour construire des architectures d'apprentissage profond, consultez les [**Projets Connexes**](https://scikit-learn.org/stable/related_projects.html#related-projects).\n",
    "\n",
    "## <a id='multi-layer-perceptron'></a> 1.17.1. **Perceptron multi-couches**<br/>([_Multi-layer Perceptron_](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#multi-layer-perceptron))\n",
    "\n",
    "Le Perceptron multi-couches (MLP) est un algorithme d'apprentissage supervisé qui apprend une fonction $f(\\cdot) : R^m \\rightarrow R^o$ en s'entraînant sur un ensemble de données, où $m$ est le nombre de dimensions en entrée et $o$ est le nombre de dimensions en sortie. Étant donné un ensemble de caractéristiques $X = \\{x_1, x_2, \\ldots, x_m\\}$ et une cible $y$, il peut apprendre un approximateur de fonction non linéaire pour la classification ou la régression. Il est différent de la régression logistique en ce sens qu'entre la couche d'entrée et la couche de sortie, il peut y avoir une ou plusieurs couches non linéaires, appelées couches cachées. La Figure 1 montre un MLP à une seule couche cachée avec une sortie scalaire.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/multilayerperceptron_network.png\"\n",
    "    alt=\"Perceptron multi-couches\"\n",
    "    style=\"max-width: 40%; height; auto;\"/>\n",
    "</div>\n",
    "\n",
    "**Figure 1 : Perceptron multi-couches à une couche cachée.**\n",
    "\n",
    "La couche la plus à gauche, appelée la couche d'entrée, est composée d'un ensemble de neurones $\\{x_i | x_1, x_2, \\ldots, x_m\\}$ représentant les caractéristiques d'entrée. Chaque neurone de la couche cachée transforme les valeurs de la couche précédente avec une sommation linéaire pondérée $w_1x_1 + w_2x_2 + \\ldots + w_mx_m$, suivie d'une fonction d'activation non linéaire $g(\\cdot) : R \\rightarrow R$ - comme la fonction tangente hyperbolique. La couche de sortie reçoit les valeurs de la dernière couche cachée et les transforme en valeurs de sortie.\n",
    "\n",
    "Le module contient les attributs publics `coefs_` et `intercepts_`. `coefs_` est une liste de matrices de poids, où la matrice de poids d'indice $i$ représente les poids entre la couche $i$ et la couche $i + 1$. `intercepts_` est une liste de vecteurs de biais, où le vecteur d'indice $i$ représente les valeurs de biais ajoutées à la couche $i + 1$.\n",
    "\n",
    "Les avantages du Perceptron multi-couches sont les suivants :\n",
    "- Capacité à apprendre des modèles non linéaires.\n",
    "- Capacité à apprendre des modèles en temps réel (apprentissage en ligne) en utilisant `partial_fit`.\n",
    "\n",
    "Les inconvénients du Perceptron multi-couches (MLP) comprennent :\n",
    "- Les MLP avec des couches cachées ont une fonction de perte non convexe où il existe plus d'un minimum local. Par conséquent, différentes initialisations aléatoires des poids peuvent conduire à des précisions de validation différentes.\n",
    "- Le MLP nécessite le réglage de plusieurs hyperparamètres tels que le nombre de neurones cachés, les couches et les itérations.\n",
    "- Le MLP est sensible à l'échelle des caractéristiques.\n",
    "\n",
    "Veuillez consulter la section [**Conseils sur l'utilisation pratique** (1.17.8)](#tips-on-practical-use) qui aborde certains de ces inconvénients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='classification'></a> 1.17.2. **Classification**<br/>([_Classification_](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#classification))\n",
    "\n",
    "La classe [**`MLPClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier) implémente un algorithme de perceptron multi-couches (MLP) qui s'entraîne à l'aide de la [**rétropropagation**](https://en.wikipedia.org/wiki/Backpropagation).\n",
    "\n",
    "MLP s'entraîne sur deux tableaux : le tableau `X` de taille `(n_samples, n_features)`, qui contient les échantillons d'entraînement représentés sous forme de vecteurs de caractéristiques à virgule flottante ; et le tableau `y` de taille `(n_samples,)`, qui contient les valeurs cibles (étiquettes de classe) pour les échantillons d'entraînement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,\n",
       "              solver=&#x27;lbfgs&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,\n",
       "              solver=&#x27;lbfgs&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,\n",
       "              solver='lbfgs')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "X = [[0., 0.], [1., 1.]]\n",
    "y = [0, 1]\n",
    "clf = MLPClassifier(\n",
    "    solver=\"lbfgs\", alpha=1e-5,\n",
    "    hidden_layer_sizes=(5, 2), random_state=1\n",
    ")\n",
    "clf.fit(X, y)\n",
    "# MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,\n",
    "#               solver='lbfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après l'ajustement (l'entraînement), le modèle peut prédire des étiquettes pour de nouveaux échantillons :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[2., 2.], [-1., -2.]])\n",
    "# array([1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP peut ajuster un modèle non linéaire aux données d'entraînement. `clf.coefs_` contient les matrices de poids qui constituent les paramètres du modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 5), (5, 2), (2, 1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[coef.shape for coef in clf.coefs_]\n",
    "# [(2, 5), (5, 2), (2, 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actuellement, [**`MLPClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier) ne prend en charge que la fonction de perte d'entropie croisée, qui permet d'estimer les probabilités en exécutant la méthode `predict_proba`.\n",
    "\n",
    "MLP s'entraîne en utilisant la rétropropagation. Plus précisément, il s'entraîne à l'aide d'une forme de descente de gradient et les gradients sont calculés à l'aide de la rétropropagation. Pour la classification, il minimise la fonction de perte d'entropie croisée, donnant un vecteur d'estimations de probabilité $P(y|x)$ par échantillon $x$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.96718015e-04, 9.99803282e-01],\n",
       "       [1.96718015e-04, 9.99803282e-01]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba([[2., 2.], [1., 2.]])\n",
    "# array([[1.967...e-04, 9.998...-01],\n",
    "#        [1.967...e-04, 9.998...-01]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`MLPClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier) prend en charge la classification multi-classe en appliquant le [**Softmax**](https://en.wikipedia.org/wiki/Softmax_activation_function) comme fonction de sortie.\n",
    "\n",
    "De plus, le modèle prend en charge la [**classification multi-étiquette** (1.12)](https://scikit-learn.org/stable/modules/multiclass.html#multiclass) dans laquelle un échantillon peut appartenir à plus d'une classe. Pour chaque classe, la sortie brute passe par la fonction logistique. Les valeurs supérieures ou égales à `0.5` sont arrondies à `1`, sinon à `0`. Pour une sortie prédite d'un échantillon, les indices où la valeur est `1` représentent les classes assignées à cet échantillon :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[0., 0.], [1., 1.]]\n",
    "y = [[0, 1], [1, 1]]\n",
    "clf = MLPClassifier(\n",
    "    solver=\"lbfgs\", alpha=1e-5,\n",
    "    hidden_layer_sizes=(15,), random_state=1\n",
    ")\n",
    "clf.fit(X, y)\n",
    "# MLPClassifier(alpha=1e-05, hidden_layer_sizes=(15,), random_state=1,\n",
    "#               solver='lbfgs')\n",
    "clf.predict([[1., 2.]])\n",
    "# array([[1, 1]])\n",
    "clf.predict([[0., 0.]])\n",
    "# array([[0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consultez les exemples ci-dessous et la docstring de [**`MLPClassifier.fit`**](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier.fit) pour plus d'informations.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Comparaison des stratégies d'apprentissage stochastique pour `MLPClassifier`**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_17_neural_networks/plot_mlp_training_curves.ipynb)<br/>([_Compare Stochastic learning strategies for `MLPClassifier`_](https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_training_curves.html))\n",
    "\n",
    "##### [**Visualisation des poids MLP sur MNIST**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_17_neural_networks/plot_mnist_filters.ipynb)<br/>([_Visualization of MLP weights on MNIST_](https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mnist_filters.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='regression'></a> 1.17.3. **Régression**<br/>([_Regression_](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#regression))\n",
    "\n",
    "La classe [**`MLPRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor) implémente un perceptron multicouche (MLP) qui s'entraîne en utilisant la rétropropagation sans fonction d'activation dans la couche de sortie, ce qui peut également être vu comme l'utilisation de la fonction d'activation identité. Par conséquent, il utilise l'erreur quadratique comme fonction de perte, et la sortie est un ensemble de valeurs continues.\n",
    "\n",
    "[**`MLPRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor) prend également en charge la régression multi-sortie, dans laquelle un échantillon peut avoir plus d'une cible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='regularization'></a> 1.17.4. **Régularisation**<br/>([_Regularization_](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#regularization))\n",
    "\n",
    "[**`MLPRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor) et [**`MLPClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier) utilisent le paramètre `alpha` pour la régularisation (régularisation $\\ell_2$), ce qui aide à éviter le surajustement en pénalisant les poids de grande magnitude. Le graphique ci-dessous affiche la fonction de décision variable en fonction de la valeur de `alpha`.\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_mlp_alpha_001.png\"\n",
    "    alt=\"Régularisation variable dans le Perceptron multicouche\"\n",
    "    style=\"max-width: 100%; height; auto;\"/>\n",
    "</div>\n",
    "\n",
    "Consultez les exemples ci-dessous pour plus d'informations.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Régularisation variable dans le Perceptron multicouche**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_17_neural_networks/plot_mlp_alpha.ipynb)<br/>([_Varying regularization in Multi-layer Perceptron_](https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_alpha.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='algorithms'></a> 1.17.5. **Algorithmes**<br/>([_Algorithms_](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#algorithms))\n",
    "\n",
    "Le MLP s'entraîne en utilisant la [**Descente de Gradient Stochastique*$ (SGD)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent), [**Adam**](https://arxiv.org/abs/1412.6980), ou [**L-BFGS**](https://en.wikipedia.org/wiki/Limited-memory_BFGS). La Descente de Gradient Stochastique (SGD) met à jour les paramètres en utilisant le gradient de la fonction de perte par rapport à un paramètre qui nécessite une adaptation, c'est-à-dire :\n",
    "\n",
    "$$w \\leftarrow w - \\eta (\\alpha \\frac{\\partial R(w)}{\\partial w} + \\frac{\\partial Loss}{\\partial w})$$\n",
    "\n",
    "où $\\eta$ est le taux d'apprentissage qui contrôle la taille du pas dans l'espace des paramètres. $Loss$ est la fonction de perte utilisée pour le réseau.\n",
    "\n",
    "Vous pouvez trouver plus de détails dans la documentation de [**SGD** (1.5)](https://scikit-learn.org/stable/modules/sgd.html).\n",
    "\n",
    "Adam est similaire à SGD dans le sens où c'est un optimiseur stochastique, mais il peut ajuster automatiquement la quantité pour mettre à jour les paramètres en se basant sur des estimations adaptatives des moments d'ordre inférieur.\n",
    "\n",
    "Avec SGD ou Adam, l'entraînement prend en charge l'apprentissage en ligne et par mini-lots.\n",
    "\n",
    "L-BFGS est un solveur qui approxime la matrice Hessienne, qui représente la dérivée partielle d'ordre deux d'une fonction. De plus, il approxime l'inverse de la matrice Hessienne pour effectuer les mises à jour des paramètres. La mise en œuvre utilise la version Scipy de [**L-BFGS**](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html).\n",
    "\n",
    "Si le solveur sélectionné est 'L-BFGS', l'entraînement ne prend pas en charge l'apprentissage en ligne ni par mini-lots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='complexity'></a> 1.17.6. **Complexité**<br/>([_Complexity_](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#complexity))\n",
    "\n",
    "Supposons qu'il y ait $n$ échantillons d'entraînement, $m$ caractéristiques, $k$ couches cachées, chacune contenant $h$ neurones (pour simplifier), et $o$ neurones de sortie. La complexité temporelle de la rétropropagation est $\\mathcal{O}(n\\cdot m \\cdot h^k \\cdot o \\cdot i)$, où $i$ est le nombre d'itérations. Comme la rétropropagation a une complexité temporelle élevée, il est conseillé de commencer avec un plus petit nombre de neurones cachés et quelques couches cachées lors de l'entraînement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='mathematical-formulation'></a> 1.17.7. **Formulation mathématique**<br/>([_Mathematical formulation_](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#mathematical-formulation))\n",
    "\n",
    "Étant donné un ensemble d'exemples d'entraînement $(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)$ où $x_i \\in \\mathbf{R}^n$ et $y_i \\in \\{0, 1\\}$, un MLP à une couche cachée et un neurone apprend la fonction $f(x) = W_2 g(W_1^T x + b_1) + b_2$ où $W_1 \\in \\mathbf{R}^m$ sont les paramètres du modèle. $W_1, W_2$ représentent les poids de la couche d'entrée et de la couche cachée, respectivement ; et $b_1, b_2$ représentent les biais ajoutés à la couche cachée et à la couche de sortie, respectivement. $g(\\cdot) : R \\rightarrow R$ est la fonction d'activation, définie par défaut comme la tangente hyperbolique. Elle est donnée par,\n",
    "\n",
    "$$g(z)= \\frac{e^z-e^{-z}}{e^z+e^{-z}}$$\n",
    "\n",
    "Pour la classification binaire, $f(x)$ passe par la fonction logistique $g(z)=1/(1+e^{-z})$ pour obtenir des valeurs de sortie entre zéro et un. Un seuil, fixé à 0,5, attribuerait les échantillons de sortie supérieurs ou égaux à 0,5 à la classe positive, et les autres à la classe négative.\n",
    "\n",
    "S'il y a plus de deux classes, $f(x)$ serait lui-même un vecteur de taille `(n_classes,)`. Au lieu de passer par la fonction logistique, il passe par la fonction softmax, qui s'écrit comme suit,\n",
    "\n",
    "$$\\text{softmax}(z)_i = \\frac{\\exp(z_i)}{\\sum_{l=1}^k\\exp(z_l)}$$\n",
    "\n",
    "où $z_i$ représente le $i$-ème élément de l'entrée de softmax, qui correspond à la classe $i$, et $k$ est le nombre de classes. Le résultat est un vecteur contenant les probabilités que l'échantillon $x$ appartienne à chaque classe. La classe ayant la probabilité la plus élevée est la sortie.\n",
    "\n",
    "En régression, la sortie reste telle que $f(x)$ ; par conséquent, la fonction d'activation de sortie est simplement la fonction identité.\n",
    "\n",
    "MLP utilise différentes fonctions de perte suivant le type de problème. La fonction de perte pour la classification est l'entropie croisée moyenne (ACE), qui dans le cas binaire est donnée par,\n",
    "\n",
    "$$Loss(\\hat{y},y,W) = -\\dfrac{1}{n}\\sum_{i=0}^n(y_i \\ln {\\hat{y_i}} + (1-y_i) \\ln{(1-\\hat{y_i})}) + \\dfrac{\\alpha}{2n} ||W||_2^2$$\n",
    "\n",
    "où $\\alpha ||W||_2^2$ est un terme de régularisation $\\ell_2$ (également appelé pénalité) qui pénalise les modèles complexes ; et $\\alpha > 0$ est un hyperparamètre non négatif qui contrôle l'amplitude de la pénalité.\n",
    "\n",
    "Pour la régression, MLP utilise la fonction de perte de l'erreur quadratique moyenne (MSE) ; écrite comme suit,\n",
    "\n",
    "$$Loss(\\hat{y},y,W) = \\frac{1}{2n}\\sum_{i=0}^n||\\hat{y}_i - y_i ||_2^2 + \\frac{\\alpha}{2n} ||W||_2^2$$\n",
    "\n",
    "À partir de poids initiaux aléatoires, le perceptron multicouche (MLP) minimise la fonction de perte en mettant à jour ces poids de manière répétée. Après le calcul de la perte, une passe en arrière la propage de la couche de sortie aux couches précédentes, fournissant à chaque paramètre de poids une valeur de mise à jour destinée à réduire la perte.\n",
    "\n",
    "Dans la descente de gradient, le gradient $\\nabla Loss_{W}$ de la perte par rapport aux poids est calculé et soustrait à $W$. Plus formellement, cela s'exprime comme suit,\n",
    "\n",
    "$$W^{i+1} = W^i - \\epsilon \\nabla {Loss}_{W}^{i}$$\n",
    "\n",
    "où $i$ est l'étape d'itération, et $\\epsilon$ est le taux d'apprentissage avec une valeur supérieure à 0.\n",
    "\n",
    "L'algorithme s'arrête lorsqu'il atteint un nombre maximum d'itérations prédéfini ; ou lorsque l'amélioration de la perte est inférieure à un certain petit seuil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='mathematical-formulation'></a> 1.17.8. **Conseils d'utilisation pratique**<br/>([_Tips on Practical Use_](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#tips-on-practical-use))\n",
    "\n",
    "- Le perceptron multicouche est sensible à l'échelle des caractéristiques, il est donc fortement recommandé de mettre à l'échelle vos données. Par exemple, mettez à l'échelle chaque attribut du vecteur d'entrée $X$ dans l'intervalle $[0, 1]$ ou $[-1, +1]$, ou standardisez-le pour avoir une moyenne de 0 et une variance de 1. Notez que vous devez appliquer la même mise à l'échelle à l'ensemble de test pour obtenir des résultats significatifs. Vous pouvez utiliser le [**`StandardScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) pour la standardisation.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()  \n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(X_train)  \n",
    "X_train = scaler.transform(X_train)  \n",
    "# apply same transformation to test data\n",
    "X_test = scaler.transform(X_test) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une approche alternative et recommandée est d'utiliser le [**`StandardScaler`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) dans un [**`Pipeline`**](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline).\n",
    "\n",
    "- Trouver un paramètre de régularisation $\\alpha$ raisonnable se fait généralement en utilisant [**`GridSearchCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV), généralement dans la plage `10.0 ** -np.arange(1, 7)`.\n",
    "\n",
    "- De manière empirique, nous avons observé que `L-BFGS` converge plus rapidement et donne de meilleures solutions sur de petits ensembles de données. Cependant, pour des ensembles de données relativement importants, `Adam` est très robuste. Il converge généralement rapidement et offre de très bonnes performances. `SGD` avec moment ou moment de Nesterov, d'autre part, peut mieux performer que ces deux algorithmes si le taux d'apprentissage est correctement réglé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='more-control-with-warm-start'></a> 1.17.9. **Plus de contrôle avec `warm_start`**<br/>([_More control with `warm_start`_](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#more-control-with-warm-start))\n",
    "\n",
    "Si vous souhaitez avoir plus de contrôle sur les critères d'arrêt ou le taux d'apprentissage dans SGD, ou si vous souhaitez effectuer une surveillance supplémentaire, l'utilisation de `warm_start=True` et `max_iter=1` et itérer vous-même peut être utile :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X = [[0., 0.], [1., 1.]]\n",
    "y = [0, 1]\n",
    "clf = MLPClassifier(hidden_layer_sizes=(15,), random_state=1, max_iter=1, warm_start=True)\n",
    "for _ in range(10):\n",
    "    clf.fit(X, y)\n",
    "    # additional monitoring / inspection\n",
    "# MLPClassifier(..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Références\n",
    "\n",
    "🔬 [**“Learning representations by back-propagating errors”**](https://www.iro.umontreal.ca/~pift6266/A06/refs/backprop_old.pdf). Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams.\n",
    "\n",
    "🌐 [**“Stochastic Gradient Descent”**](https://leon.bottou.org/projects/sgd) L. Bottou - Website, 2010.\n",
    "\n",
    "X 🌐 [**“Backpropagation”**](http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm) Andrew Ng, Jiquan Ngiam, Chuan Yu Foo, Yifan Mai, Caroline Suen - Website, 2011.\n",
    "\n",
    "🔬 [**“Efficient BackProp”**](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf) Y. LeCun, L. Bottou, G. Orr, K. Müller - In Neural Networks: Tricks of the Trade 1998.\n",
    "\n",
    "🔬 [**“Adam: A method for stochastic optimization”**](https://arxiv.org/pdf/1412.6980.pdf). Kingma, Diederik, and Jimmy Ba (2014)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
