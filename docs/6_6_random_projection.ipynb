{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='random-projection'></a> 6.6. [**Projection aléatoire**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_6_random_projection.ipynb#random-projection)</br>([*Random Projection*](https://scikit-learn.org/stable/modules/random_projection.html#random-projection))\n",
    "\n",
    "Le module [**`sklearn.random_projection`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.random_projection) implémente une méthode simple et efficace en termes de calcul pour réduire la dimensionnalité des données en échangeant une certaine quantité de précision (sous la forme d'une variance supplémentaire) contre des temps de traitement plus rapides et des tailles de modèle plus petites. Ce module met en œuvre deux types de matrices aléatoires non structurées : une [**Matrice aléatoire gaussienne** (6.6.2.)](https://scikit-learn.org/stable/modules/random_projection.html#gaussian-random-matrix) et une [**Matrice aléatoire creuse** (6.6.3.)](https://scikit-learn.org/stable/modules/random_projection.html#sparse-random-matrix).\n",
    "\n",
    "Les dimensions et la distribution des matrices de projection aléatoire sont contrôlées de manière à préserver les distances mutuelles entre deux échantillons de l'ensemble de données. Ainsi, la projection aléatoire est une technique d'approximation appropriée pour les méthodes basées sur les distances.\n",
    "\n",
    "#### Références\n",
    "\n",
    "- Sanjoy Dasgupta. 2000. [“**Experiments with random projection**](https://cseweb.ucsd.edu/~dasgupta/papers/randomf.pdf)[”](https://drive.google.com/file/d/1-VIy31JaOmvgtsWInL4ppR-j64DB8m7f/view?usp=drive_link). In Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence (UAI’00), Craig Boutilier and Moisés Goldszmidt (Eds.). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 143-151.\n",
    "\n",
    "- Ella Bingham and Heikki Mannila. 2001. [“**Random projection in dimensionality reduction: applications to image and text data**](https://citeseerx.ist.psu.edu/doc_view/pid/aed77346f737b0ed5890b61ad02e5eb4ab2f3dc6)[”](https://drive.google.com/file/d/1NdIw9ZL8sEft3aqLyVi1Ljv0Hj-7UDY5/view?usp=drive_link). In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining (KDD ‘01). ACM, New York, NY, USA, 245-250.\n",
    "\n",
    "#### Plan\n",
    "\n",
    "✔  6.6. [**Projection aléatoire**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_6_random_projection.ipynb#random-projection)\n",
    "([*Random Projection*](https://scikit-learn.org/stable/modules/random_projection.html#random-projection))\n",
    "* **Volume** : 5 pages, 1 exemples, 5 papiers\n",
    "- ✔ 6.6.1. [**Le lemme de Johnson-Lindenstrauss**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_6_random_projection.ipynb#the-johnson-lindenstrauss-lemma)\n",
    "([*The Johnson-Lindenstrauss lemma*](https://scikit-learn.org/stable/modules/random_projection.html#the-johnson-lindenstrauss-lemma))\n",
    "- ✔ 6.6.2. [**Projection aléatoire gaussienne**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_6_random_projection.ipynb#gaussian-random-projection)\n",
    "([*Gaussian random projection*](https://scikit-learn.org/stable/modules/random_projection.html#gaussian-random-projection))\n",
    "- ✔ 6.6.3. [**Projection aléatoire creuse**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_6_random_projection.ipynb#sparse-random-projection)\n",
    "([*Sparse random projection*](https://scikit-learn.org/stable/modules/random_projection.html#sparse-random-projection))\n",
    "- ✔ 6.6.4. [**Transformation inverse**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_6_random_projection.ipynb#inverse-transform)\n",
    "([*Inverse Transform*](https://scikit-learn.org/stable/modules/random_projection.html#inverse-transform))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='the-johnson-lindenstrauss-lemma'></a> 6.6.1. Le lemme de Johnson-Lindenstrauss\n",
    "\n",
    "Le résultat théorique principal derrière l'efficacité de la projection aléatoire est le [lemme de Johnson-Lindenstrauss (citant Wikipedia)](https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma) :\n",
    "\n",
    "> En mathématiques, le lemme de Johnson-Lindenstrauss est un résultat concernant les plongements à faible distorsion de points d'un espace de grande dimension dans un espace euclidien de plus basse dimension. Le lemme affirme qu'un petit ensemble de points dans un espace de grande dimension peut être plongé dans un espace de dimension beaucoup plus basse de manière à quasiment préserver les distances entre les points. La fonction utilisée pour le plongement est au moins Lipschitzienne et peut même être prise comme une projection orthogonale.\n",
    "\n",
    "En ne connaissant que le nombre d'échantillons, la fonction [**`johnson_lindenstrauss_min_dim`**](https://scikit-learn.org/stable/modules/generated/sklearn.random_projection.johnson_lindenstrauss_min_dim.html) estime de manière conservatrice la taille minimale du sous-espace aléatoire pour garantir une distorsion bornée introduite par la projection aléatoire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7894,  9868, 11841], dtype=int64)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "johnson_lindenstrauss_min_dim(n_samples=1e6, eps=0.5)\n",
    "# 663\n",
    "johnson_lindenstrauss_min_dim(n_samples=1e6, eps=[0.5, 0.1, 0.01])\n",
    "# array([    663,   11841, 1112658])\n",
    "johnson_lindenstrauss_min_dim(n_samples=[1e4, 1e5, 1e6], eps=0.1)\n",
    "# array([ 7894,  9868, 11841])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png)\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_johnson_lindenstrauss_bound_002.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple\n",
    "\n",
    "Consultez la [**La borne de Johnson-Lindenstrauss pour le plongement avec des projections aléatoires**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/misc/plot_johnson_lindenstrauss_bound.ipynb)<br/>([*The Johnson-Lindenstrauss bound for embedding with random projections*](https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_johnson_lindenstrauss_bound.html)), afin d'obtenir une explication théorique sur le lemme de Johnson-Lindenstrauss et une validation empirique utilisant des matrices aléatoires creuses."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Références\n",
    "\n",
    "- Sanjoy Dasgupta and Anupam Gupta, 1999. [“**An elementary proof of the Johnson-Lindenstrauss Lemma**](https://cseweb.ucsd.edu/~dasgupta/papers/jl.pdf)[”](https://drive.google.com/file/d/1qwf2ScFtGK5bfsfHUEdANgptllT3CB06/view?usp=drive_link)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='gaussian-random-projection'></a> 6.6.2. Projection aléatoire gaussienne\n",
    "\n",
    "La classe [**`GaussianRandomProjection`**](https://scikit-learn.org/stable/modules/generated/sklearn.random_projection.GaussianRandomProjection.html) réduit la dimensionnalité en projetant l'espace d'entrée d'origine sur une matrice générée aléatoirement, dont les composantes sont tirées selon la distribution suivante : $\\mathcal{N}(0, \\frac{1}{n_{components}})$\n",
    "\n",
    "Voici un petit extrait qui illustre comment utiliser le transformateur de projection aléatoire gaussienne :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3947)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import random_projection\n",
    "X = np.random.rand(100, 10000)\n",
    "transformer = random_projection.GaussianRandomProjection()\n",
    "X_new = transformer.fit_transform(X)\n",
    "X_new.shape\n",
    "# (100, 3947)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='sparse-random-projection'></a> 6.6.3. Projection aléatoire creuse\n",
    "\n",
    "La classe [**`SparseRandomProjection`**](https://scikit-learn.org/stable/modules/generated/sklearn.random_projection.SparseRandomProjection.html#sklearn.random_projection.SparseRandomProjection) réduit la dimensionnalité en projetant l'espace d'entrée d'origine à l'aide d'une matrice aléatoire creuse.\n",
    "\n",
    "Les matrices aléatoires creuses sont une alternative aux matrices de projection aléatoires gaussiennes denses, garantissant une qualité d'incorporation similaire tout en étant beaucoup plus efficaces en termes de mémoire et permettant un calcul plus rapide des données projetées.\n",
    "\n",
    "Si nous définissons `s = 1 / density`, les éléments de la matrice aléatoire sont tirés selon la distribution suivante :\n",
    "\n",
    "$$\\begin{split}\\left\\{\n",
    "\\begin{array}{c c l}\n",
    "-\\sqrt{\\frac{s}{n_{\\text{components}}}} & & 1 / 2s\\\\\n",
    "0 &\\text{with probability}  & 1 - 1 / s \\\\\n",
    "+\\sqrt{\\frac{s}{n_{\\text{components}}}} & & 1 / 2s\\\\\n",
    "\\end{array}\n",
    "\\right.\\end{split}\n",
    "$$\n",
    "\n",
    "Où $n_{components}$ est la taille du sous-espace projeté. Par défaut, la densité des éléments non nuls est fixée à la densité minimale recommandée par Ping Li et al. : $1 / \\sqrt{n_{\\text{features}}}$\n",
    "\n",
    "Voici un petit extrait qui illustre comment utiliser le transformateur de projection aléatoire creuse :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3947)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import random_projection\n",
    "X = np.random.rand(100, 10000)\n",
    "transformer = random_projection.SparseRandomProjection()\n",
    "X_new = transformer.fit_transform(X)\n",
    "X_new.shape\n",
    "# (100, 3947)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Références\n",
    "\n",
    "- D. Achlioptas. 2003. [“**Database-friendly random projections: Johnson-Lindenstrauss with binary coins**](https://www.sciencedirect.com/science/article/pii/S0022000003000254)[”](https://drive.google.com/file/d/1pa6Yr8bZfpmWfzqqLdLLNDItgFS15ChY/view?usp=drive_link). Journal of Computer and System Sciences 66 (2003) 671–687\n",
    "\n",
    "- Ping Li, Trevor J. Hastie, and Kenneth W. Church. 2006. [“**Very sparse random projections**](https://hastie.su.domains/Papers/Ping/KDD06_rp.pdf)[”](https://drive.google.com/file/d/10l1qq26uzYK_O7ToBU-C1jMdsjyzpgm7/view?usp=drive_link). In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD ‘06). ACM, New York, NY, USA, 287-296."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='inverse-transform'></a> 6.6.4. Transformation inverse\n",
    "\n",
    "Les transformeurs de projection aléatoire ont un paramètre `compute_inverse_components`. Lorsqu'il est défini sur `True`, après la création de la matrice aléatoire `components_` lors de l'ajustement, le transformateur calcule la pseudo-inverse de cette matrice et la stocke en tant que `inverse_components_`. La matrice `inverse_components_` a une forme de $n_{features} \\times n_{components}$ et est toujours une matrice dense, indépendamment de la densité de la matrice `components_`. Ainsi, en fonction du nombre de caractéristiques et de composantes, cela peut utiliser beaucoup de mémoire.\n",
    "\n",
    "Lorsque la méthode `inverse_transform` est appelée, elle calcule le produit de l'entrée `X` et de la transposée des composantes inverses. Si les composantes inverses ont été calculées lors de l'ajustement, elles sont réutilisées à chaque appel de `inverse_transform`. Sinon, elles sont recalculées à chaque fois, ce qui peut être coûteux. Le résultat est toujours dense, même si `X` est creux.\n",
    "\n",
    "Voici un petit exemple de code qui illustre comment utiliser la fonctionnalité de transformation inverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "X = np.random.rand(100, 10000)\n",
    "transformer = SparseRandomProjection(compute_inverse_components=True)\n",
    "\n",
    "X_new = transformer.fit_transform(X)\n",
    "X_new.shape\n",
    "\n",
    "X_new_inversed = transformer.inverse_transform(X_new)\n",
    "X_new_inversed.shape\n",
    "\n",
    "X_new_again = transformer.transform(X_new_inversed)\n",
    "np.allclose(X_new, X_new_again)\n",
    "# True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
