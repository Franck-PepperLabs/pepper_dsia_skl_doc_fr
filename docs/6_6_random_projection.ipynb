{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='random-projection'></a> 6.6. [**Projection aléatoire**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_6_random_projection.ipynb#random-projection)</br>([*Random Projection*](https://scikit-learn.org/stable/modules/random_projection.html#random-projection))\n",
    "\n",
    "Le module [**`sklearn.random_projection`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.random_projection) implémente une méthode simple et efficace en termes de calcul pour réduire la dimensionnalité des données en échangeant une certaine quantité de précision (sous la forme d'une variance supplémentaire) contre des temps de traitement plus rapides et des tailles de modèle plus petites. Ce module met en œuvre deux types de matrices aléatoires non structurées : une [**Matrice aléatoire gaussienne** (6.6.2.)](https://scikit-learn.org/stable/modules/random_projection.html#gaussian-random-matrix) et une [**Matrice aléatoire creuse** (6.6.3.)](https://scikit-learn.org/stable/modules/random_projection.html#sparse-random-matrix).\n",
    "\n",
    "Les dimensions et la distribution des matrices de projection aléatoire sont contrôlées de manière à préserver les distances mutuelles entre deux échantillons de l'ensemble de données. Ainsi, la projection aléatoire est une technique d'approximation appropriée pour les méthodes basées sur les distances.\n",
    "\n",
    "#### Références\n",
    "\n",
    "- Sanjoy Dasgupta. 2000. [“**Experiments with random projection**](https://cseweb.ucsd.edu/~dasgupta/papers/randomf.pdf)[”](https://drive.google.com/file/d/1-VIy31JaOmvgtsWInL4ppR-j64DB8m7f/view?usp=drive_link). In Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence (UAI’00), Craig Boutilier and Moisés Goldszmidt (Eds.). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 143-151.\n",
    "\n",
    "- Ella Bingham and Heikki Mannila. 2001. [“**Random projection in dimensionality reduction: applications to image and text data**](https://citeseerx.ist.psu.edu/doc_view/pid/aed77346f737b0ed5890b61ad02e5eb4ab2f3dc6)[”](https://drive.google.com/file/d/1NdIw9ZL8sEft3aqLyVi1Ljv0Hj-7UDY5/view?usp=drive_link). In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining (KDD ‘01). ACM, New York, NY, USA, 245-250.\n",
    "\n",
    "\n",
    "✘ 6.6. [**Projection aléatoire**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_6_random_projection.ipynb#random-projection)\n",
    "([*Random Projection*](https://scikit-learn.org/stable/modules/random_projection.html#random-projection))\n",
    "* **Volume** : 5 pages, 1 exemples, 5 papiers\n",
    "* **Reste** : 5 pages, 1 exemples, 5 papiers\n",
    "* ✘ 6.6.1. [**Le lemme de Johnson-Lindenstrauss**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_6_random_projection.ipynb#the-johnson-lindenstrauss-lemma)\n",
    "([*The Johnson-Lindenstrauss lemma*](https://scikit-learn.org/stable/modules/random_projection.html#the-johnson-lindenstrauss-lemma))\n",
    "* ✘ 6.6.2. [**Projection aléatoire gaussienne**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_6_random_projection.ipynb#gaussian-random-projection)\n",
    "([*Gaussian random projection*](https://scikit-learn.org/stable/modules/random_projection.html#gaussian-random-projection))\n",
    "* ✘ 6.6.3. [**Projection aléatoire parcimonieuse**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_6_random_projection.ipynb#sparse-random-projection)\n",
    "([*Sparse random projection*](https://scikit-learn.org/stable/modules/random_projection.html#sparse-random-projection))\n",
    "* ✘ 6.6.4. [**Transformation inverse**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/6_6_random_projection.ipynb#inverse-transform)\n",
    "([*Inverse Transform*](https://scikit-learn.org/stable/modules/random_projection.html#inverse-transform))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='the-johnson-lindenstrauss-lemma'></a> 6.6.1. Le lemme de Johnson-Lindenstrauss\n",
    "\n",
    "Le résultat théorique principal derrière l'efficacité de la projection aléatoire est le [lemme de Johnson-Lindenstrauss (citant Wikipedia)](https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma) :\n",
    "\n",
    "> En mathématiques, le lemme de Johnson-Lindenstrauss est un résultat concernant les plongements à faible distorsion de points d'un espace de grande dimension dans un espace euclidien de plus basse dimension. Le lemme affirme qu'un petit ensemble de points dans un espace de grande dimension peut être plongé dans un espace de dimension beaucoup plus basse de manière à quasiment préserver les distances entre les points. La fonction utilisée pour le plongement est au moins Lipschitzienne et peut même être prise comme une projection orthogonale.\n",
    "\n",
    "En ne connaissant que le nombre d'échantillons, la fonction [**`johnson_lindenstrauss_min_dim`**](https://scikit-learn.org/stable/modules/generated/sklearn.random_projection.johnson_lindenstrauss_min_dim.html) estime de manière conservatrice la taille minimale du sous-espace aléatoire pour garantir une distorsion bornée introduite par la projection aléatoire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7894,  9868, 11841], dtype=int64)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "johnson_lindenstrauss_min_dim(n_samples=1e6, eps=0.5)\n",
    "# 663\n",
    "johnson_lindenstrauss_min_dim(n_samples=1e6, eps=[0.5, 0.1, 0.01])\n",
    "# array([    663,   11841, 1112658])\n",
    "johnson_lindenstrauss_min_dim(n_samples=[1e4, 1e5, 1e6], eps=0.1)\n",
    "# array([ 7894,  9868, 11841])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png)\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_johnson_lindenstrauss_bound_002.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple\n",
    "\n",
    "Consultez la **borne de Johnson-Lindenstrauss pour le plongement avec des projections aléatoires**, afin d'obtenir une explication théorique sur le lemme de Johnson-Lindenstrauss et une validation empirique utilisant des matrices aléatoires creuses."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Références\n",
    "\n",
    "- Sanjoy Dasgupta and Anupam Gupta, 1999. [“**An elementary proof of the Johnson-Lindenstrauss Lemma**](https://cseweb.ucsd.edu/~dasgupta/papers/jl.pdf)[”](https://drive.google.com/file/d/1qwf2ScFtGK5bfsfHUEdANgptllT3CB06/view?usp=drive_link)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
