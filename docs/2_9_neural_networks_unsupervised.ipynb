{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='unsupervised-learning'></a> 2. [**Apprentissage non supervis√©**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_9_neural_networks_unsupervised.ipynb#model-selection-and-evaluation)</br>([*Unsupervised learning*](https://scikit-learn.org/stable/unsupervised_learning.html#unsupervised-learning))\n",
    "\n",
    "# 2.9. [**Mod√®les de r√©seaux de neurones (non supervis√©s)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_9_neural_networks_unsupervised.ipynb#neural-network-models-unsupervised)<br/>([_Neural network models (unsupervised)_](https://scikit-learn.org/stable/neural_networks_unsupervised#neural-network-models-unsupervised))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 4 pages, 1 exemples, 2 papiers\n",
    "- 2.9.1. [**Machines Boltzmann restreintes**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_9_neural_networks_unsupervised.ipynb#restricted-boltzmann-machines)<br/>([_Restricted Boltzmann machines_](https://scikit-learn.org/stable/neural_networks_unsupervised.html#restricted-boltzmann-machines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='neural-network-models-unsupervised'></a> 2.9. Mod√®les de r√©seaux de neurones (non supervis√©s)\n",
    "\n",
    "## <a id='restricted-boltzmann-machines'></a> 2.9.1. Machines de Boltzmann restreintes\n",
    "\n",
    "Les machines de Boltzmann restreintes (RBM) sont des apprenants non supervis√©s de caract√©ristiques non lin√©aires bas√©s sur un mod√®le probabiliste. Les caract√©ristiques extraites par un RBM ou une hi√©rarchie de RBM donnent souvent de bons r√©sultats lorsqu'elles sont utilis√©es dans un classifieur lin√©aire tel qu'un SVM lin√©aire ou un perceptron.\n",
    "\n",
    "Le mod√®le fait des hypoth√®ses concernant la distribution des entr√©es. Pour le moment, scikit-learn ne fournit que [**`BernoulliRBM`**](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.BernoulliRBM.html), qui suppose que les entr√©es sont soit des valeurs binaires, soit des valeurs comprises entre 0 et 1, chacune codant la probabilit√© que la caract√©ristique sp√©cifique soit activ√©e.\n",
    "\n",
    "Le RBM tente de maximiser la vraisemblance des donn√©es √† l'aide d'un mod√®le graphique particulier. L'algorithme d'apprentissage des param√®tres utilis√© ([**Maximum de vraisemblance stochastique** (2.9.1.3)](https://scikit-learn.org/stable/modules/neural_networks_unsupervised.html#sml)) emp√™che les repr√©sentations de s'√©loigner trop des donn√©es d'entr√©e, ce qui leur permet de capturer des r√©gularit√©s int√©ressantes, mais rend le mod√®le moins utile pour les petits ensembles de donn√©es, et g√©n√©ralement peu utile pour l'estimation de densit√©.\n",
    "\n",
    "Cette m√©thode a gagn√© en popularit√© pour l'initialisation des r√©seaux de neurones profonds avec les poids des RBMs ind√©pendants. Cette m√©thode est connue sous le nom de pr√©-entra√Ænement non supervis√©.\n",
    "\n",
    "![Image](https://scikit-learn.org/stable/_images/sphx_glr_plot_rbm_logistic_classification_001.png)\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Caract√©ristiques de la machine de Boltzmann restreinte pour la classification de chiffres**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/neural_networks/plot_rbm_logistic_classification.ipynb)<br/>([_Restricted Boltzmann Machine features for digit classification_](https://scikit-learn.org/stable/auto_examples/neural_networks/plot_rbm_logistic_classification.html))\n",
    "\n",
    "### <a id='graphical-model-and-parametrization'></a> 2.9.1.1. Mod√®le graphique et param√©trisation\n",
    "\n",
    "Le mod√®le graphique d'un RBM est un graphe biparti enti√®rement connect√©.\n",
    "\n",
    "<div style=\"background-color: white; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/rbm_graph.png\"\n",
    "    alt=\"Mod√®le graphique d'un RBM\"\n",
    "    style=\"max-width: 50%; height: auto;\">\n",
    "</div>\n",
    "\n",
    "Les n≈ìuds sont des variables al√©atoires dont les √©tats d√©pendent de l'√©tat des autres n≈ìuds auxquels ils sont connect√©s. Le mod√®le est donc param√©tr√© par les poids des connexions, ainsi qu'un terme d'interception (biais) pour chaque unit√© visible et cach√©e, omis de l'image pour simplifier.\n",
    "\n",
    "La fonction d'√©nergie mesure la qualit√© d'une attribution conjointe :\n",
    "\n",
    "$$E(\\mathbf{v}, \\mathbf{h}) = -\\sum_i \\sum_j w_{ij}v_ih_j - \\sum_i b_iv_i - \\sum_j c_jh_j$$\n",
    "\n",
    "Dans la formule ci-dessus, $\\mathbf{b}$ et $\\mathbf{c}$ sont les vecteurs d'interception pour les couches visibles et cach√©es, respectivement. La probabilit√© conjointe du mod√®le est d√©finie en termes de l'√©nergie :\n",
    "\n",
    "$$P(\\mathbf{v}, \\mathbf{h}) = \\frac{e^{-E(\\mathbf{v}, \\mathbf{h})}}{Z}$$\n",
    "\n",
    "Le mot \"restreint\" se r√©f√®re √† la structure bipartie du mod√®le, qui interdit toute interaction directe entre les unit√©s cach√©es ou entre les unit√©s visibles. Cela signifie que les ind√©pendances conditionnelles suivantes sont suppos√©es :\n",
    "\n",
    "$$\\begin{split}h_i \\bot h_j | \\mathbf{v} \\\\\n",
    "v_i \\bot v_j | \\mathbf{h}\\end{split}$$\n",
    "\n",
    "La structure bipartie permet l'utilisation d'un √©chantillonnage de Gibbs par blocs efficace pour l'inf√©rence.\n",
    "\n",
    "### <a id='bernoulli-restricted-boltzmann-machines'></a> 2.9.1.2. Machines de Boltzmann restreintes de Bernoulli\n",
    "\n",
    "Dans la [**`BernoulliRBM`**](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.BernoulliRBM.html), toutes les unit√©s sont des unit√©s stochastiques binaires. Cela signifie que les donn√©es d'entr√©e doivent √™tre soit binaires, soit r√©elles entre 0 et 1, signifiant la probabilit√© que l'unit√© visible soit activ√©e ou d√©sactiv√©e. C'est un bon mod√®le pour la reconnaissance de caract√®res, o√π l'int√©r√™t porte sur les pixels actifs et inactifs. Pour les images de sc√®nes naturelles, cela ne convient plus en raison de l'arri√®re-plan, de la profondeur et de la tendance des pixels voisins √† prendre les m√™mes valeurs.\n",
    "\n",
    "La distribution de probabilit√© conditionnelle de chaque unit√© est donn√©e par la fonction d'activation sigmo√Øde logistique de l'entr√©e qu'elle re√ßoit :\n",
    "\n",
    "$$\\begin{split}P(v_i=1|\\mathbf{h}) = \\sigma(\\sum_j w_{ij}h_j + b_i) \\\\\n",
    "P(h_i=1|\\mathbf{v}) = \\sigma(\\sum_i w_{ij}v_i + c_j)\\end{split}$$\n",
    "\n",
    "o√π $\\sigma$ est la fonction d'activation sigmo√Øde logistique :\n",
    "\n",
    "### <a id='stochastic-maximum-likelihood-learning'></a> 2.9.1.3. Apprentissage par maximum de vraisemblance stochastique\n",
    "\n",
    "L'algorithme d'apprentissage mis en ≈ìuvre dans [**`BernoulliRBM`**](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.BernoulliRBM.html) est connu sous le nom de maximum de vraisemblance stochastique (SML) ou de divergence contrastive persistante (PCD). Maximiser directement la vraisemblance est irr√©alisable en raison de la forme de la vraisemblance des donn√©es :\n",
    "\n",
    "$$\\log P(v) = \\log \\sum_h e^{-E(v, h)} - \\log \\sum_{x, y} e^{-E(x, y)}$$\n",
    "\n",
    "Pour simplifier, l'√©quation ci-dessus est √©crite pour un seul exemple d'entra√Ænement. Le gradient par rapport aux poids est form√© de deux termes correspondant √† ceux ci-dessus. Ils sont g√©n√©ralement appel√©s gradient positif et gradient n√©gatif, en raison de leurs signes respectifs. Dans cette impl√©mentation, les gradients sont estim√©s sur des mini-lots d'√©chantillons.\n",
    "\n",
    "En maximisant la log-vraisemblance, le gradient positif incite le mod√®le √† pr√©f√©rer des √©tats cach√©s compatibles avec les donn√©es d'entra√Ænement observ√©es. En raison de la structure bipartie des RBMs, il peut √™tre calcul√© efficacement. Cependant, le gradient n√©gatif est intractable. Son but est de r√©duire l'√©nergie des √©tats conjoints que le mod√®le pr√©f√®re, afin de rester fid√®le aux donn√©es. Il peut √™tre approxim√© par une m√©thode de Monte-Carlo par cha√Ænes de Markov en utilisant un √©chantillonnage de Gibbs par blocs en √©chantillonnant de mani√®re it√©rative chaque √©l√©ment de $v$ et $h$ donn√© l'autre, jusqu'√† ce que la cha√Æne se m√©lange. Les √©chantillons g√©n√©r√©s de cette mani√®re sont parfois appel√©s \"particules de fantaisie\". Cela est inefficace et il est difficile de d√©terminer si la cha√Æne de Markov se m√©lange.\n",
    "\n",
    "La m√©thode de divergence contrastive sugg√®re d'arr√™ter la cha√Æne apr√®s un petit nombre d'it√©rations, $k$, g√©n√©ralement m√™me 1. Cette m√©thode est rapide et a une faible variance, mais les √©chantillons sont loin de la distribution du mod√®le.\n",
    "\n",
    "La divergence contrastive persistante corrige cela. Au lieu de d√©marrer une nouvelle cha√Æne √† chaque fois que le gradient est n√©cessaire et de r√©aliser une seule √©tape d'√©chantillonnage de Gibbs, dans la PCD, nous conservons un certain nombre de cha√Ænes (particules de fantaisie) qui sont mises √† jour avec des √©tapes de Gibbs apr√®s chaque mise √† jour des poids. Cela permet aux particules d'explorer plus en profondeur l'espace.\n",
    "\n",
    "#### R√©f√©rences\n",
    "\n",
    "## <a id='restricted-boltzmann-machines'></a> 2.9.1. Machines de Boltzmann restreintes\n",
    "\n",
    "üî¨ G. Hinton, S. Osindero, Y.-W. Teh, [**‚ÄúA fast learning algorithm for deep belief nets‚Äù**](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf), 2006\n",
    "\n",
    "üî¨ T. Tieleman, [**‚ÄúTraining Restricted Boltzmann Machines using Approximations to the Likelihood Gradient‚Äù**](https://www.cs.toronto.edu/~tijmen/pcd/pcd.pdf), 2008"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
