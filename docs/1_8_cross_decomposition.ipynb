{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='supervised-learning'></a> 1. [**Apprentissage supervis√©**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#supervised-learning)</br>([*Supervised learning*](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning))\n",
    "\n",
    "# <a id='cross-decomposition'></a> 1.8. [**D√©composition crois√©e**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_8_cross_decomposition.ipynb)<br/>([_Cross decomposition_](https://scikit-learn.org/stable/modules/cross_decomposition.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 4 pages, 2 exemples, 1 papiers\n",
    "- 1.8.1. [**`PLSCanonical`**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_8_cross_decomposition.ipynb#plscanonical)<br/>([_`PLSCanonical`_](https://scikit-learn.org/stable/modules/cross_decomposition.html#plscanonical))\n",
    "    - 1.8.1.1. [**Transformation des donn√©es**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_8_cross_decomposition.ipynb#transforming-data)<br/>([_Transforming data_](https://scikit-learn.org/stable/modules/cross_decomposition.html#transforming-data))\n",
    "    - 1.8.1.2. [**Pr√©diction des cibles `Y`**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_8_cross_decomposition.ipynb#predicting-the-targets-y)<br/>([_Predicting the targets `Y`_](https://scikit-learn.org/stable/modules/cross_decomposition.html#predicting-the-targets-y))\n",
    "- 1.8.2. [**`PLSSVD`**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_8_cross_decomposition.ipynb#plssvd)<br/>([_`PLSSVD`_](https://scikit-learn.org/stable/modules/cross_decomposition.html#plssvd))\n",
    "- 1.8.3. [**`PLSRegression`**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_8_cross_decomposition.ipynb#plsregression)<br/>([_`PLSRegression`_](https://scikit-learn.org/stable/modules/cross_decomposition.html#plsregression))\n",
    "- 1.8.4. [**Analyse de corr√©lation canonique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_8_cross_decomposition.ipynb#canonical-correlation-analysis)<br/>([_Canonical Correlation Analysis_](https://scikit-learn.org/stable/modules/cross_decomposition.html#canonical-correlation-analysis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='cross-decomposition'></a> 1.8. **D√©composition crois√©e**<br/>([_Cross decomposition_](https://scikit-learn.org/stable/modules/cross_decomposition.html))\n",
    "\n",
    "Le module de d√©composition crois√©e contient des estimateurs **supervis√©s** pour la r√©duction de dimension et la r√©gression, qui appartiennent √† la famille des \"Partial Least Squares\" (PLS).\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_compare_cross_decomposition_001.png\"\n",
    "    alt=\"Compare cross decomposition methods\"\n",
    "    style=\"max-width: 75%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Les algorithmes de d√©composition crois√©e trouvent les relations fondamentales entre deux matrices (`X` et `Y`). Ce sont des approches √† variables latentes pour mod√©liser les structures de covariance dans ces deux espaces. Ils essaient de trouver la direction multidimensionnelle dans l'espace `X` qui explique la direction de variance multidimensionnelle maximale dans l'espace `Y`. En d'autres termes, PLS projette √† la fois `X` et `Y` dans un sous-espace de plus basse dimension de telle sorte que la covariance entre `transformed(X)` et `transformed(Y)` soit maximale.\n",
    "\n",
    "PLS pr√©sente des similitudes avec la [**R√©gression des Composantes Principales**](https://en.wikipedia.org/wiki/Principal_component_regression) (PCR), o√π les √©chantillons sont d'abord projet√©s dans un sous-espace de plus basse dimension, et les cibles `y` sont pr√©dites √† l'aide de `transformed(X)`. Un probl√®me avec la PCR est que la r√©duction de dimension est non supervis√©e et peut faire perdre certaines variables importantes : la PCR conserverait les caract√©ristiques ayant la plus grande variance, mais il est possible que les caract√©ristiques ayant une faible variance soient pertinentes pour la pr√©diction de la cible. En quelque sorte, le PLS permet le m√™me type de r√©duction de dimension, mais en prenant en compte les cibles `y`. Une illustration de ce fait est donn√©e dans l'exemple suivant : **R√©gression des Composantes Principales vs R√©gression des Moindres Carr√©s Partiels**.\n",
    "\n",
    "Mis √† part le CCA, les estimateurs PLS sont particuli√®rement adapt√©s lorsque la matrice des pr√©dicteurs a plus de variables que d'observations et qu'il y a une multicollin√©arit√© parmi les caract√©ristiques. En revanche, la r√©gression lin√©aire standard √©chouerait dans ces cas, sauf si elle est r√©gularis√©e.\n",
    "\n",
    "Les classes incluses dans ce module sont [**`PLSRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html#sklearn.cross_decomposition.PLSRegression), [**`PLSCanonical`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSCanonical.html#sklearn.cross_decomposition.PLSCanonical), [**`CCA`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html#sklearn.cross_decomposition.CCA) et [**`PLSSVD`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSSVD.html#sklearn.cross_decomposition.PLSSVD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='plscanonical'></a> 1.8.1. [**`PLSCanonical`**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_8_cross_decomposition.ipynb#plscanonical)<br/>([_`PLSCanonical`_](https://scikit-learn.org/stable/modules/cross_decomposition.html#plscanonical))\n",
    "\n",
    "Nous d√©crivons ici l'algorithme utilis√© dans [**`PLSCanonical`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSCanonical.html#sklearn.cross_decomposition.PLSCanonical). Les autres estimateurs utilisent des variantes de cet algorithme, qui sont d√©taill√©es ci-dessous. Nous recommandons la section [1] pour plus de d√©tails et de comparaisons entre ces algorithmes. Dans [1], [**`PLSCanonical`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSCanonical.html#sklearn.cross_decomposition.PLSCanonical) correspond √† \"PLSW2A\".\n",
    "\n",
    "√âtant donn√© deux matrices centr√©es $X \\in \\mathbb{R}^{n \\times d}$ et $Y \\in \\mathbb{R}^{n \\times t}$, ainsi qu'un nombre de composantes $K$, [**`PLSCanonical`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSCanonical.html#sklearn.cross_decomposition.PLSCanonical) proc√®de comme suit :\n",
    "\n",
    "On fixe $X_1$ √† $X$ et $Y_1$ √† $Y$. Ensuite, pour chaque $k \\in [1, K]$ :\n",
    "\n",
    "- a) On calcule $u_k \\in \\mathbb{R}^d$ et $v_k \\in \\mathbb{R}^t$, qui sont les premiers vecteurs singuliers √† gauche et √† droite de la matrice de covariance crois√©e $C = X_k^T Y_k$. On appelle $u_k$ et $v_k$ les _poids_. Par d√©finition, $u_k$ et $v_k$ sont choisis de mani√®re √† maximiser la covariance entre $X_k$ projet√© et la cible projet√©e, c'est-√†-dire $\\text{Cov}(X_k u_k, Y_k v_k)$.\n",
    "- b) On projette $X_k$ et $Y_k$ sur les vecteurs singuliers pour obtenir des _scores_ : $\\xi_k = X_k u_k$ et $\\omega_k = Y_k v_k$\n",
    "- c) On r√©gresse $X_k$ sur $\\xi_k$, c'est-√†-dire qu'on trouve un vecteur $\\gamma_k \\in \\mathbb{R}^d$ tel que la matrice de rang 1, $\\xi_k \\gamma_k^T$ soit la plus proche possible de $\\xi_k$. On fait de m√™me sur $Y_k$ avec $\\omega_k$ pour obtenir $\\delta_k$. Les vecteurs $\\gamma_k$ et $\\delta_k$ sont appel√©s les _chargements_.\n",
    "- d) _D√©flation_ de $X_k$ et $Y_k$, c'est-√†-dire soustraction des approximations de rang 1 : $X_{k+1} = X_k - \\xi_k \\gamma_k^T$ et $Y_{k + 1} = Y_k - \\omega_k \\delta_k^T$.\n",
    "\n",
    "√Ä la fin, nous avons approch√© $X$ comme une somme de matrices de rang 1 : $X = \\Xi \\Gamma^T$ o√π $\\Xi \\in \\mathbb{R}^{n \\times K}$ contient les scores en colonnes, et $\\Gamma^T \\in \\mathbb{R}^{K \\times d}$ contient les chargements en lignes. De m√™me pour $Y$, nous avons $Y = \\Omega \\Delta^T$.\n",
    "\n",
    "Notez que les matrices de scores $\\Xi$ et $\\Omega$ correspondent aux projections des donn√©es d'entra√Ænement $X$ et $Y$, respectivement.\n",
    "\n",
    "L'√©tape a) peut √™tre effectu√©e de deux mani√®res : soit en calculant l'ensemble de la SVD de $C$ et en ne conservant que les vecteurs singuliers avec les plus grandes valeurs singuli√®res, soit en calculant directement les vecteurs singuliers √† l'aide de la m√©thode de puissance (cf section 11.3 dans [1]), ce qui correspond √† l'option `'nipals'` du param√®tre `algorithm`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='transforming-data'></a> 1.8.1.1. **Transformation des donn√©es**<br/>([_Transforming data_](https://scikit-learn.org/stable/modules/cross_decomposition.html#transforming-data))\n",
    "\n",
    "Pour transformer $X$ en $\\bar{X}$, nous devons trouver une matrice de projection $P$ telle que $\\bar{X} = XP$. Nous savons que pour les donn√©es d'entra√Ænement, $\\Xi = XP$, et $X = \\Xi \\Gamma^T$. En fixant $P = U(\\Gamma^T U)^{-1}$ o√π $U$ est la matrice avec les $u_k$ dans les colonnes, nous avons $XP = X U(\\Gamma^T U)^{-1} = \\Xi (\\Gamma^T U) (\\Gamma^T U)^{-1} = \\Xi$ comme souhait√©. La matrice de rotation $P$ peut √™tre obtenue √† partir de l'attribut `x_rotations_`.\n",
    "\n",
    "De m√™me, $Y$ peut √™tre transform√© en utilisant la matrice de rotation $V(\\Delta^T V)^{-1}$, accessible via l'attribut `y_rotations_`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='predicting-the-targets-y'></a> 1.8.1.2. **Pr√©diction des cibles `Y`**<br/>([_Predicting the targets `Y`_](https://scikit-learn.org/stable/modules/cross_decomposition.html#predicting-the-targets-y))\n",
    "\n",
    "Pour pr√©dire les cibles de certaines donn√©es $X$, nous recherchons une matrice de coefficients $\\beta \\in R^{d \\times t}$ telle que $Y = X\\beta$.\n",
    "\n",
    "L'id√©e est de tenter de pr√©dire les cibles transform√©es $\\Omega$ en fonction des √©chantillons transform√©s $\\Xi$, en calculant $\\alpha \\in \\mathbb{R}$ de telle mani√®re que $\\Omega = \\alpha \\Xi$.\n",
    "\n",
    "Ensuite, nous avons $Y = \\Omega \\Delta^T = \\alpha \\Xi \\Delta^T$, et puisque $\\Xi$ repr√©sente les donn√©es d'entra√Ænement transform√©es, nous avons que $Y = X \\alpha P \\Delta^T$, et en cons√©quence la matrice de coefficients $\\beta = \\alpha P \\Delta^T$.\n",
    "\n",
    "La matrice $\\beta$ est accessible via l'attribut `coef_`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='plssvd'></a> 1.8.2. **`PLSSVD`**<br/>([_`PLSSVD`_](https://scikit-learn.org/stable/modules/cross_decomposition.html#plssvd))\n",
    "\n",
    "[**`PLSSVD`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSSVD.html#sklearn.cross_decomposition.PLSSVD) est une version simplifi√©e de [**`PLSCanonical`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSCanonical.html#sklearn.cross_decomposition.PLSCanonical) d√©crite pr√©c√©demment : au lieu de d√©gonfler de mani√®re it√©rative les matrices $X_k$ et $Y_k$, [**`PLSSVD`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSSVD.html#sklearn.cross_decomposition.PLSSVD) calcule la SVD de $C = X^TY$ une seule fois, et stocke les `n_components` vecteurs singuliers correspondant aux plus grandes valeurs singuli√®res dans les matrices `U` et `V`, correspondant aux attributs `x_weights_` et `y_weights_`. Ici, les donn√©es transform√©es sont simplement `transformed(X) = XU` et `transformed(Y) = YV`.\n",
    "\n",
    "Si `n_components == 1`, [**`PLSSVD`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSSVD.html#sklearn.cross_decomposition.PLSSVD) et [**`PLSCanonical`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSCanonical.html#sklearn.cross_decomposition.PLSCanonical) sont strictement √©quivalents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='plsregression'></a> 1.8.3. **`PLSRegression`**<br/>([_`PLSRegression`_](https://scikit-learn.org/stable/modules/cross_decomposition.html#plsregression))\n",
    "\n",
    "L'estimateur [**`PLSRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html#sklearn.cross_decomposition.PLSRegression) est similaire √† [**`PLSCanonical`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSCanonical.html#sklearn.cross_decomposition.PLSCanonical) avec `algorithm='nipals'`, mais pr√©sente deux diff√©rences significatives :\n",
    "\n",
    "- √Ä l'√©tape a) de la m√©thode de puissance pour calculer $u_k$ et $v_k$, $v_k$ n'est jamais normalis√©.\n",
    "- √Ä l'√©tape c), les cibles $Y_k$ sont approxim√©es en utilisant la projection de $X_k$ (c'est-√†-dire $\\xi_k$) au lieu de la projection de $Y_k$ (c'est-√†-dire $\\omega_k$). En d'autres termes, le calcul des chargements est diff√©rent. Par cons√©quent, la d√©flation √† l'√©tape d) sera √©galement affect√©e.\n",
    "\n",
    "Ces deux modifications affectent les sorties de `predict` et `transform`, qui ne sont pas identiques √† celles de [**`PLSCanonical`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSCanonical.html#sklearn.cross_decomposition.PLSCanonical). De plus, alors que le nombre de composants est limit√© par `min(n_samples, n_features, n_targets)` dans [**`PLSCanonical`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSCanonical.html#sklearn.cross_decomposition.PLSCanonical), ici la limite est le rang de $X^TX$, c'est-√†-dire `min(n_samples, n_features)`.\n",
    "\n",
    "[**`PLSRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html#sklearn.cross_decomposition.PLSRegression) est √©galement connu sous le nom de PLS1 (cibles uniques) et PLS2 (cibles multiples). Tout comme [**`Lasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso), [**`PLSRegression`**](https://scikit-learn.org/stable/modules/generated.sklearn.cross_decomposition.PLSRegression.html#sklearn.cross_decomposition.PLSRegression) est une forme de r√©gression lin√©aire r√©gularis√©e o√π le nombre de composants contr√¥le la force de la r√©gularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='canonical-correlation-analysis'></a> 1.8.4. **Analyse de corr√©lation canonique**<br/>([_Canonical Correlation Analysis_](https://scikit-learn.org/stable/modules/cross_decomposition.html#canonical-correlation-analysis))\n",
    "\n",
    "L'**Analyse de Corr√©lation Canonique** (CCA) a √©t√© d√©velopp√©e avant et ind√©pendamment de PLS. Cependant, il s'av√®re que CCA est un cas particulier de PLS, correspondant √† PLS en \"Mode B\" dans la litt√©rature.\n",
    "\n",
    "[**`CCA`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html#sklearn.cross_decomposition.CCA) diff√®re de [**`PLSCanonical`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSCanonical.html#sklearn.cross_decomposition.PLSCanonical) dans la mani√®re dont les poids $u_k$ et $v_k$ sont calcul√©s dans la m√©thode de puissance de l'√©tape a). Des d√©tails peuvent √™tre trouv√©s dans la section 10 de [1].\n",
    "\n",
    "√âtant donn√© que [**`CCA`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html#sklearn.cross_decomposition.CCA) implique l'inversion de $X_k^TX_k$ et $Y_k^TY_k$, cet estimateur peut √™tre instable si le nombre de caract√©ristiques ou de cibles est sup√©rieur au nombre d'√©chantillons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R√©f√©rences\n",
    "\n",
    "üìö [1] (1,2,3,4) [**‚ÄúA survey of Partial Least Squares (PLS) methods, with Emphasis on the Two-Block Case‚Äù**](https://stat.uw.edu/sites/default/files/files/reports/2000/tr371.pdf) JA Wegelin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemples\n",
    "\n",
    "#### [**Comparaison des m√©thodes de d√©composition crois√©e**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_8_cross_decomposition/plot_compare_cross_decomposition.ipynb)<br/>([_Compare cross decomposition methods_](https://scikit-learn.org/stable/auto_examples/cross_decomposition/plot_compare_cross_decomposition.html))\n",
    "\n",
    "#### [**R√©gression des composantes principales vs R√©gression des moindres carr√©s partiels**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_8_cross_decomposition/plot_pcr_vs_pls.ipynb)<br/>([_Principal Component Regression vs Partial Least Squares Regression_](https://scikit-learn.org/stable/auto_examples/cross_decomposition/plot_pcr_vs_pls.html))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
