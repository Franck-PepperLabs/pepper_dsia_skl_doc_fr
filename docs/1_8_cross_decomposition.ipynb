{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='supervised-learning'></a> 1. [**Apprentissage supervisé**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_supervised_learning.ipynb#supervised-learning)</br>([*Supervised learning*](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning))\n",
    "\n",
    "# <a id='cross-decomposition'></a> 1.8. [**Décomposition croisée**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_8_cross_decomposition.ipynb)<br/>([_Cross decomposition_](https://scikit-learn.org/stable/modules/cross_decomposition.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 4 pages, 2 exemples, 1 papiers\n",
    "- 1.8.1. [**`PLSCanonical`**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_8_cross_decomposition.ipynb#plscanonical)<br/>([_`PLSCanonical`_](https://scikit-learn.org/stable/modules/cross_decomposition.html#plscanonical))\n",
    "    - 1.8.1.1. [**Transformation des données**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_8_cross_decomposition.ipynb#transforming-data)<br/>([_Transforming data_](https://scikit-learn.org/stable/modules/cross_decomposition.html#transforming-data))\n",
    "    - 1.8.1.2. [**Prédiction des cibles `Y`**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_8_cross_decomposition.ipynb#predicting-the-targets-y)<br/>([_Predicting the targets `Y`_](https://scikit-learn.org/stable/modules/cross_decomposition.html#predicting-the-targets-y))\n",
    "- 1.8.2. [**`PLSSVD`**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_8_cross_decomposition.ipynb#plssvd)<br/>([_`PLSSVD`_](https://scikit-learn.org/stable/modules/cross_decomposition.html#plssvd))\n",
    "- 1.8.3. [**`PLSRegression`**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_8_cross_decomposition.ipynb#plsregression)<br/>([_`PLSRegression`_](https://scikit-learn.org/stable/modules/cross_decomposition.html#plsregression))\n",
    "- 1.8.4. [**Analyse de corrélation canonique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_8_cross_decomposition.ipynb#canonical-correlation-analysis)<br/>([_Canonical Correlation Analysis_](https://scikit-learn.org/stable/modules/cross_decomposition.html#canonical-correlation-analysis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='cross-decomposition'></a> 1.8. **Décomposition croisée**<br/>([_Cross decomposition_](https://scikit-learn.org/stable/modules/cross_decomposition.html))\n",
    "\n",
    "Le module de décomposition croisée contient des estimateurs **supervisés** pour la réduction de dimension et la régression, qui appartiennent à la famille des \"Partial Least Squares\" (PLS).\n",
    "\n",
    "<div style=\"background-color: white; color: black; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_compare_cross_decomposition_001.png\"\n",
    "    alt=\"Compare cross decomposition methods\"\n",
    "    style=\"max-width: 75%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "Les algorithmes de décomposition croisée trouvent les relations fondamentales entre deux matrices (`X` et `Y`). Ce sont des approches à variables latentes pour modéliser les structures de covariance dans ces deux espaces. Ils essaient de trouver la direction multidimensionnelle dans l'espace `X` qui explique la direction de variance multidimensionnelle maximale dans l'espace `Y`. En d'autres termes, PLS projette à la fois `X` et `Y` dans un sous-espace de plus basse dimension de telle sorte que la covariance entre `transformed(X)` et `transformed(Y)` soit maximale.\n",
    "\n",
    "PLS présente des similitudes avec la [**Régression des Composantes Principales**](https://en.wikipedia.org/wiki/Principal_component_regression) (PCR), où les échantillons sont d'abord projetés dans un sous-espace de plus basse dimension, et les cibles `y` sont prédites à l'aide de `transformed(X)`. Un problème avec la PCR est que la réduction de dimension est non supervisée et peut faire perdre certaines variables importantes : la PCR conserverait les caractéristiques ayant la plus grande variance, mais il est possible que les caractéristiques ayant une faible variance soient pertinentes pour la prédiction de la cible. En quelque sorte, le PLS permet le même type de réduction de dimension, mais en prenant en compte les cibles `y`. Une illustration de ce fait est donnée dans l'exemple suivant : **Régression des Composantes Principales vs Régression des Moindres Carrés Partiels**.\n",
    "\n",
    "Mis à part le CCA, les estimateurs PLS sont particulièrement adaptés lorsque la matrice des prédicteurs a plus de variables que d'observations et qu'il y a une multicollinéarité parmi les caractéristiques. En revanche, la régression linéaire standard échouerait dans ces cas, sauf si elle est régularisée.\n",
    "\n",
    "Les classes incluses dans ce module sont [**`PLSRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html#sklearn.cross_decomposition.PLSRegression), [**`PLSCanonical`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSCanonical.html#sklearn.cross_decomposition.PLSCanonical), [**`CCA`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html#sklearn.cross_decomposition.CCA) et [**`PLSSVD`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSSVD.html#sklearn.cross_decomposition.PLSSVD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='plscanonical'></a> 1.8.1. [**`PLSCanonical`**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/1_8_cross_decomposition.ipynb#plscanonical)<br/>([_`PLSCanonical`_](https://scikit-learn.org/stable/modules/cross_decomposition.html#plscanonical))\n",
    "\n",
    "Nous décrivons ici l'algorithme utilisé dans [**`PLSCanonical`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSCanonical.html#sklearn.cross_decomposition.PLSCanonical). Les autres estimateurs utilisent des variantes de cet algorithme, qui sont détaillées ci-dessous. Nous recommandons la section [1] pour plus de détails et de comparaisons entre ces algorithmes. Dans [1], [**`PLSCanonical`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSCanonical.html#sklearn.cross_decomposition.PLSCanonical) correspond à \"PLSW2A\".\n",
    "\n",
    "Étant donné deux matrices centrées $X \\in \\mathbb{R}^{n \\times d}$ et $Y \\in \\mathbb{R}^{n \\times t}$, ainsi qu'un nombre de composantes $K$, [**`PLSCanonical`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSCanonical.html#sklearn.cross_decomposition.PLSCanonical) procède comme suit :\n",
    "\n",
    "On fixe $X_1$ à $X$ et $Y_1$ à $Y$. Ensuite, pour chaque $k \\in [1, K]$ :\n",
    "\n",
    "- a) On calcule $u_k \\in \\mathbb{R}^d$ et $v_k \\in \\mathbb{R}^t$, qui sont les premiers vecteurs singuliers à gauche et à droite de la matrice de covariance croisée $C = X_k^T Y_k$. On appelle $u_k$ et $v_k$ les _poids_. Par définition, $u_k$ et $v_k$ sont choisis de manière à maximiser la covariance entre $X_k$ projeté et la cible projetée, c'est-à-dire $\\text{Cov}(X_k u_k, Y_k v_k)$.\n",
    "- b) On projette $X_k$ et $Y_k$ sur les vecteurs singuliers pour obtenir des _scores_ : $\\xi_k = X_k u_k$ et $\\omega_k = Y_k v_k$\n",
    "- c) On régresse $X_k$ sur $\\xi_k$, c'est-à-dire qu'on trouve un vecteur $\\gamma_k \\in \\mathbb{R}^d$ tel que la matrice de rang 1, $\\xi_k \\gamma_k^T$ soit la plus proche possible de $\\xi_k$. On fait de même sur $Y_k$ avec $\\omega_k$ pour obtenir $\\delta_k$. Les vecteurs $\\gamma_k$ et $\\delta_k$ sont appelés les _chargements_.\n",
    "- d) _Déflation_ de $X_k$ et $Y_k$, c'est-à-dire soustraction des approximations de rang 1 : $X_{k+1} = X_k - \\xi_k \\gamma_k^T$ et $Y_{k + 1} = Y_k - \\omega_k \\delta_k^T$.\n",
    "\n",
    "À la fin, nous avons approché $X$ comme une somme de matrices de rang 1 : $X = \\Xi \\Gamma^T$ où $\\Xi \\in \\mathbb{R}^{n \\times K}$ contient les scores en colonnes, et $\\Gamma^T \\in \\mathbb{R}^{K \\times d}$ contient les chargements en lignes. De même pour $Y$, nous avons $Y = \\Omega \\Delta^T$.\n",
    "\n",
    "Notez que les matrices de scores $\\Xi$ et $\\Omega$ correspondent aux projections des données d'entraînement $X$ et $Y$, respectivement.\n",
    "\n",
    "L'étape a) peut être effectuée de deux manières : soit en calculant l'ensemble de la SVD de $C$ et en ne conservant que les vecteurs singuliers avec les plus grandes valeurs singulières, soit en calculant directement les vecteurs singuliers à l'aide de la méthode de puissance (cf section 11.3 dans [1]), ce qui correspond à l'option `'nipals'` du paramètre `algorithm`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='transforming-data'></a> 1.8.1.1. **Transformation des données**<br/>([_Transforming data_](https://scikit-learn.org/stable/modules/cross_decomposition.html#transforming-data))\n",
    "\n",
    "Pour transformer $X$ en $\\bar{X}$, nous devons trouver une matrice de projection $P$ telle que $\\bar{X} = XP$. Nous savons que pour les données d'entraînement, $\\Xi = XP$, et $X = \\Xi \\Gamma^T$. En fixant $P = U(\\Gamma^T U)^{-1}$ où $U$ est la matrice avec les $u_k$ dans les colonnes, nous avons $XP = X U(\\Gamma^T U)^{-1} = \\Xi (\\Gamma^T U) (\\Gamma^T U)^{-1} = \\Xi$ comme souhaité. La matrice de rotation $P$ peut être obtenue à partir de l'attribut `x_rotations_`.\n",
    "\n",
    "De même, $Y$ peut être transformé en utilisant la matrice de rotation $V(\\Delta^T V)^{-1}$, accessible via l'attribut `y_rotations_`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='predicting-the-targets-y'></a> 1.8.1.2. **Prédiction des cibles `Y`**<br/>([_Predicting the targets `Y`_](https://scikit-learn.org/stable/modules/cross_decomposition.html#predicting-the-targets-y))\n",
    "\n",
    "Pour prédire les cibles de certaines données $X$, nous recherchons une matrice de coefficients $\\beta \\in R^{d \\times t}$ telle que $Y = X\\beta$.\n",
    "\n",
    "L'idée est de tenter de prédire les cibles transformées $\\Omega$ en fonction des échantillons transformés $\\Xi$, en calculant $\\alpha \\in \\mathbb{R}$ de telle manière que $\\Omega = \\alpha \\Xi$.\n",
    "\n",
    "Ensuite, nous avons $Y = \\Omega \\Delta^T = \\alpha \\Xi \\Delta^T$, et puisque $\\Xi$ représente les données d'entraînement transformées, nous avons que $Y = X \\alpha P \\Delta^T$, et en conséquence la matrice de coefficients $\\beta = \\alpha P \\Delta^T$.\n",
    "\n",
    "La matrice $\\beta$ est accessible via l'attribut `coef_`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='plssvd'></a> 1.8.2. **`PLSSVD`**<br/>([_`PLSSVD`_](https://scikit-learn.org/stable/modules/cross_decomposition.html#plssvd))\n",
    "\n",
    "[**`PLSSVD`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSSVD.html#sklearn.cross_decomposition.PLSSVD) est une version simplifiée de [**`PLSCanonical`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSCanonical.html#sklearn.cross_decomposition.PLSCanonical) décrite précédemment : au lieu de dégonfler de manière itérative les matrices $X_k$ et $Y_k$, [**`PLSSVD`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSSVD.html#sklearn.cross_decomposition.PLSSVD) calcule la SVD de $C = X^TY$ une seule fois, et stocke les `n_components` vecteurs singuliers correspondant aux plus grandes valeurs singulières dans les matrices `U` et `V`, correspondant aux attributs `x_weights_` et `y_weights_`. Ici, les données transformées sont simplement `transformed(X) = XU` et `transformed(Y) = YV`.\n",
    "\n",
    "Si `n_components == 1`, [**`PLSSVD`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSSVD.html#sklearn.cross_decomposition.PLSSVD) et [**`PLSCanonical`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSCanonical.html#sklearn.cross_decomposition.PLSCanonical) sont strictement équivalents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='plsregression'></a> 1.8.3. **`PLSRegression`**<br/>([_`PLSRegression`_](https://scikit-learn.org/stable/modules/cross_decomposition.html#plsregression))\n",
    "\n",
    "L'estimateur [**`PLSRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html#sklearn.cross_decomposition.PLSRegression) est similaire à [**`PLSCanonical`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSCanonical.html#sklearn.cross_decomposition.PLSCanonical) avec `algorithm='nipals'`, mais présente deux différences significatives :\n",
    "\n",
    "- À l'étape a) de la méthode de puissance pour calculer $u_k$ et $v_k$, $v_k$ n'est jamais normalisé.\n",
    "- À l'étape c), les cibles $Y_k$ sont approximées en utilisant la projection de $X_k$ (c'est-à-dire $\\xi_k$) au lieu de la projection de $Y_k$ (c'est-à-dire $\\omega_k$). En d'autres termes, le calcul des chargements est différent. Par conséquent, la déflation à l'étape d) sera également affectée.\n",
    "\n",
    "Ces deux modifications affectent les sorties de `predict` et `transform`, qui ne sont pas identiques à celles de [**`PLSCanonical`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSCanonical.html#sklearn.cross_decomposition.PLSCanonical). De plus, alors que le nombre de composants est limité par `min(n_samples, n_features, n_targets)` dans [**`PLSCanonical`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSCanonical.html#sklearn.cross_decomposition.PLSCanonical), ici la limite est le rang de $X^TX$, c'est-à-dire `min(n_samples, n_features)`.\n",
    "\n",
    "[**`PLSRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html#sklearn.cross_decomposition.PLSRegression) est également connu sous le nom de PLS1 (cibles uniques) et PLS2 (cibles multiples). Tout comme [**`Lasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso), [**`PLSRegression`**](https://scikit-learn.org/stable/modules/generated.sklearn.cross_decomposition.PLSRegression.html#sklearn.cross_decomposition.PLSRegression) est une forme de régression linéaire régularisée où le nombre de composants contrôle la force de la régularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='canonical-correlation-analysis'></a> 1.8.4. **Analyse de corrélation canonique**<br/>([_Canonical Correlation Analysis_](https://scikit-learn.org/stable/modules/cross_decomposition.html#canonical-correlation-analysis))\n",
    "\n",
    "L'**Analyse de Corrélation Canonique** (CCA) a été développée avant et indépendamment de PLS. Cependant, il s'avère que CCA est un cas particulier de PLS, correspondant à PLS en \"Mode B\" dans la littérature.\n",
    "\n",
    "[**`CCA`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html#sklearn.cross_decomposition.CCA) diffère de [**`PLSCanonical`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSCanonical.html#sklearn.cross_decomposition.PLSCanonical) dans la manière dont les poids $u_k$ et $v_k$ sont calculés dans la méthode de puissance de l'étape a). Des détails peuvent être trouvés dans la section 10 de [1].\n",
    "\n",
    "Étant donné que [**`CCA`**](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html#sklearn.cross_decomposition.CCA) implique l'inversion de $X_k^TX_k$ et $Y_k^TY_k$, cet estimateur peut être instable si le nombre de caractéristiques ou de cibles est supérieur au nombre d'échantillons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Références\n",
    "\n",
    "📚 [1] (1,2,3,4) [**“A survey of Partial Least Squares (PLS) methods, with Emphasis on the Two-Block Case”**](https://stat.uw.edu/sites/default/files/files/reports/2000/tr371.pdf) JA Wegelin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemples\n",
    "\n",
    "#### [**Comparaison des méthodes de décomposition croisée**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_8_cross_decomposition/plot_compare_cross_decomposition.ipynb)<br/>([_Compare cross decomposition methods_](https://scikit-learn.org/stable/auto_examples/cross_decomposition/plot_compare_cross_decomposition.html))\n",
    "\n",
    "#### [**Régression des composantes principales vs Régression des moindres carrés partiels**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/1_8_cross_decomposition/plot_pcr_vs_pls.ipynb)<br/>([_Principal Component Regression vs Partial Least Squares Regression_](https://scikit-learn.org/stable/auto_examples/cross_decomposition/plot_pcr_vs_pls.html))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
