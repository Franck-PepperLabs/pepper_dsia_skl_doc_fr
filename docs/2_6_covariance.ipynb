{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='unsupervised-learning'></a> 2. [**Apprentissage non supervis√©**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_unsupervised_learning.ipynb#model-selection-and-evaluation)</br>([*Unsupervised learning*](https://scikit-learn.org/stable/unsupervised_learning.html#unsupervised-learning))\n",
    "\n",
    "# 2.6. [**Estimation de la covariance**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_6_covariance.ipynb#covariance-estimation)<br/>([_Covariance estimation_](https://scikit-learn.org/stable/modules/covariance.html#covariance-estimation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 9 pages, 6 exemples, 5 papiers\n",
    "- 2.6.1. [**Covariance empirique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_6_covariance.ipynb#empirical-covariance)<br/>([_Empirical covariance_](https://scikit-learn.org/stable/modules/covariance.html#empirical-covariance))\n",
    "- 2.6.2. [**Covariance r√©duite**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_6_covariance.ipynb#shrunk-covariance)<br/>([_Shrunk Covariance_](https://scikit-learn.org/stable/modules/covariance.html#shrunk-covariance))\n",
    "- 2.6.3. [**Covariance inverse creuse**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_6_covariance.ipynb#sparse-inverse-covariance)<br/>([_Sparse inverse covariance_](https://scikit-learn.org/stable/modules/covariance.html#sparse-inverse-covariance))\n",
    "- 2.6.4. [**Estimation robuste de la covariance**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_6_covariance.ipynb#robust-covariance-estimation)<br/>([_Robust Covariance Estimation_](https://scikit-learn.org/stable/modules/covariance.html#robust-covariance-estimation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='covariance-estimation'></a> 2.6. Estimation de covariance\n",
    "\n",
    "De nombreux probl√®mes statistiques n√©cessitent l'estimation de la matrice de covariance d'une population, qui peut √™tre consid√©r√©e comme une estimation de la forme du nuage de points d'un ensemble de donn√©es. La plupart du temps, une telle estimation doit √™tre effectu√©e sur un √©chantillon dont les propri√©t√©s (taille, structure, homog√©n√©it√©) ont une grande influence sur la qualit√© de l'estimation. Le module [**`sklearn.covariance`**](https://scikit-learn.org/stable/modules/classes.html) fournit des outils pour estimer avec pr√©cision la matrice de covariance d'une population dans divers contextes.\n",
    "\n",
    "Nous supposons que les observations sont ind√©pendantes et identiquement distribu√©es (i.i.d.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='empirical-covariance'></a> 2.6.1. Covariance empirique\n",
    "\n",
    "Il est bien connu que la matrice de covariance d'un ensemble de donn√©es peut √™tre bien approxim√©e par l'estimateur _maximum de vraisemblance_ classique (ou \"covariance empirique\"), √† condition que le nombre d'observations soit suffisamment grand par rapport au nombre de variables (les variables d√©crivant les observations). Plus pr√©cis√©ment, l'estimateur du maximum de vraisemblance d'un √©chantillon est un estimateur asymptotiquement non biais√© de la matrice de covariance de la population correspondante.\n",
    "\n",
    "La matrice de covariance empirique d'un √©chantillon peut √™tre calcul√©e √† l'aide de la fonction [**`empirical_covariance`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.empirical_covariance.html) du module, ou en ajustant un objet [**`EmpiricalCovariance`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EmpiricalCovariance.html) aux donn√©es de l'√©chantillon avec la m√©thode [**`EmpiricalCovariance.fit`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EmpiricalCovariance.html#sklearn.covariance.EmpiricalCovariance.fit). Notez que les r√©sultats d√©pendent de la centralit√© des donn√©es, donc on peut souhaiter utiliser pr√©cis√©ment le param√®tre `assume_centered`. Plus pr√©cis√©ment, si `assume_centered=False`, alors on suppose que l'ensemble de test a le m√™me vecteur de moyenne que l'ensemble d'entra√Ænement. Sinon, les deux ensembles doivent √™tre centr√©s par l'utilisateur, et `assume_centered=True` doit √™tre utilis√©.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Estimation de covariance par r√©tr√©cissement : LedoitWolf vs OAS et maximum de vraisemblance**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_6_covariance/plot_covariance_estimation.ipynb)<br/>([_Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood_](https://scikit-learn.org/stable/auto_examples/covariance/plot_covariance_estimation.html))\n",
    "\n",
    "Exemple montrant comment ajuster un objet [**`EmpiricalCovariance`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EmpiricalCovariance.html) aux donn√©es."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='shrunk-covariance'></a> 2.6.2. Covariance r√©tr√©cie\n",
    "\n",
    "### <a id='basic-shrinkage'></a> 2.6.2.1. R√©tr√©cissement de base\n",
    "\n",
    "Bien que l'estimateur du maximum de vraisemblance soit un estimateur asymptotiquement non biais√© de la matrice de covariance, il n'est pas un bon estimateur des valeurs propres de la matrice de covariance, de sorte que la matrice de pr√©cision obtenue √† partir de son inversion n'est pas exacte. Parfois, il arrive m√™me que la matrice de covariance empirique ne puisse pas √™tre invers√©e pour des raisons num√©riques. Pour √©viter ce probl√®me d'inversion, une transformation de la matrice de covariance empirique a √©t√© introduite : le `shrinkage` (r√©tr√©cissement).\n",
    "\n",
    "Dans scikit-learn, cette transformation (avec un coefficient de r√©tr√©cissement d√©fini par l'utilisateur) peut √™tre directement appliqu√©e √† une covariance pr√©calcul√©e avec la m√©thode [**`shrunk_covariance`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.shrunk_covariance.html). De plus, un estimateur r√©tr√©ci de la covariance peut √™tre ajust√© aux donn√©es avec un objet [**`ShrunkCovariance`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.ShrunkCovariance.html) et sa m√©thode [**`ShrunkCovariance.fit`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.ShrunkCovariance.html#sklearn.covariance.ShrunkCovariance.fit). Encore une fois, les r√©sultats d√©pendent de la centralit√© des donn√©es, donc on peut souhaiter utiliser pr√©cis√©ment le param√®tre `assume_centered`.\n",
    "\n",
    "Math√©matiquement, ce r√©tr√©cissement consiste √† r√©duire le rapport entre les plus petites et les plus grandes valeurs propres de la matrice de covariance empirique. Cela peut √™tre fait en d√©calant simplement chaque valeur propre selon un d√©calage donn√©, ce qui revient √† trouver l'estimateur du maximum de vraisemblance p√©nalis√© par la norme $\\ell_2$ de la matrice de covariance. En pratique, le r√©tr√©cissement se r√©sume √† une simple transformation convexe : $\\Sigma_{\\rm r√©tr√©cie} = (1-\\alpha)\\hat{\\Sigma} + \\alpha\\frac{{\\rm Tr}\\hat{\\Sigma}}{p}\\rm Id$.\n",
    "\n",
    "Le choix du taux de r√©tr√©cissement, $\\alpha$, √©quivaut √† d√©finir un compromis biais/variance, et est discut√© ci-dessous.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Estimation de covariance par r√©tr√©cissement : LedoitWolf vs OAS et maximum de vraisemblance**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_6_covariance/plot_covariance_estimation.ipynb)<br/>([_Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood_](https://scikit-learn.org/stable/auto_examples/covariance/plot_covariance_estimation.html))\n",
    "\n",
    "Exemple montrant comment ajuster un objet [**`ShrunkCovariance`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.ShrunkCovariance.html) aux donn√©es."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ledoit-wolf-shrinkage'></a> 2.6.2.2. R√©tr√©cissement Ledoit-Wolf\n",
    "\n",
    "Dans leur article de 2004 [1], O. Ledoit et M. Wolf proposent une formule pour calculer le coefficient de r√©tr√©cissement optimal qui minimise l'erreur quadratique moyenne entre la matrice de covariance estim√©e et la matrice de covariance r√©elle.\n",
    "\n",
    "L'estimateur de la matrice de covariance Ledoit-Wolf peut √™tre calcul√© sur un √©chantillon avec la fonction [**`ledoit_wolf`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.ledoit_wolf.html) du package [**`sklearn.covariance`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.covariance), ou il peut √™tre obtenu autrement en ajustant un objet [**`LedoitWolf`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.LedoitWolf.html) au m√™me √©chantillon.\n",
    "\n",
    "> **Note : Cas o√π la matrice de covariance de la population est isotropique**  \n",
    "> Il est important de noter que lorsque le nombre d'√©chantillons est beaucoup plus grand que le nombre de variables, on s'attendrait √† ce qu'aucun r√©tr√©cissement ne soit n√©cessaire. L'intuition derri√®re cela est que si la covariance de la population est de rang complet, lorsque le nombre d'√©chantillons augmente, la covariance de l'√©chantillon deviendra √©galement d√©finie positive. Par cons√©quent, aucun r√©tr√©cissement ne serait n√©cessaire et la m√©thode devrait le faire automatiquement.\n",
    ">\n",
    "> Cependant, ce n'est pas le cas dans la proc√©dure Ledoit-Wolf lorsque la covariance de la population est un multiple de la matrice identit√©. Dans ce cas, l'estimation du r√©tr√©cissement Ledoit-Wolf approche 1 lorsque le nombre d'√©chantillons augmente. Cela indique que l'estimation optimale de la matrice de covariance dans le sens Ledoit-Wolf est un multiple de la matrice identit√©. Puisque la covariance de la population est d√©j√† un multiple de la matrice identit√©, la solution Ledoit-Wolf est effectivement une estimation raisonnable.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Estimation de covariance par r√©tr√©cissement : LedoitWolf vs OAS et maximum de vraisemblance**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_6_covariance/plot_covariance_estimation.ipynb)<br/>([_Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood_](https://scikit-learn.org/stable/auto_examples/covariance/plot_covariance_estimation.html))\n",
    "\n",
    "Exemple montrant comment ajuster un objet [**`LedoitWolf`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.LedoitWolf.html) aux donn√©es et visualiser les performances de l'estimateur Ledoit-Wolf en termes de vraisemblance.\n",
    "\n",
    "#### R√©f√©rences\n",
    "\n",
    "üî¨ [1] O. Ledoit and M. Wolf, [**‚ÄúA Well-Conditioned Estimator for Large-Dimensional Covariance Matrices‚Äù**](https://e-archivo.uc3m.es/bitstream/handle/10016/10087/ws0076.pdf;jsessionid=4C9B01BB15A802AF69A4E6D5149912F1?sequence=1), Journal of Multivariate Analysis, Volume 88, Issue 2, February 2004, pages 365-411."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='oracle-approximating-shrinkage'></a> 2.6.2.3. R√©tr√©cissement d'Oracle Approximatif\n",
    "\n",
    "En supposant que les donn√©es sont distribu√©es selon une loi gaussienne, Chen et al. [2] ont d√©riv√© une formule visant √† choisir un coefficient de r√©tr√©cissement qui donne une erreur quadratique moyenne plus faible que celle donn√©e par la formule de Ledoit et Wolf. L'estimateur r√©sultant est connu sous le nom d'estimateur de r√©tr√©cissement d'Oracle Approximatif de la covariance.\n",
    "\n",
    "L'estimateur OAS de la matrice de covariance peut √™tre calcul√© sur un √©chantillon avec la fonction [**`oas`**](https://scikit-learn.org/stable/modules/generated/oas-function.html#sklearn.covariance.oas) du package [**`sklearn.covariance`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.covariance), ou il peut √™tre obtenu autrement en ajustant un objet [**`OAS`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.OAS.html) au m√™me √©chantillon.\n",
    "\n",
    "<div style=\"background-color: white; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_covariance_estimation_001.png\"\n",
    "    alt=\"Comparaison des choix des estimateurs Ledoit-Wolf et OAS\"\n",
    "    style=\"max-width: 50%; height: auto;\">\n",
    "</div>\n",
    "\n",
    "Compromis biais-variance lors du r√©glage du r√©tr√©cissement : comparaison des choix des estimateurs Ledoit-Wolf et OAS\n",
    "\n",
    "#### R√©f√©rences\n",
    "\n",
    "üî¨ [2] [**‚ÄúShrinkage algorithms for MMSE covariance estimation‚Äù**](https://arxiv.org/pdf/0907.4698.pdf). Chen, Y., Wiesel, A., Eldar, Y. C., & Hero, A. O. IEEE Transactions on Signal Processing, 58(10), 5016-5029, 2010\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Estimation de covariance par r√©tr√©cissement : LedoitWolf vs OAS et maximum de vraisemblance**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_6_covariance/plot_covariance_estimation.ipynb)<br/>([_Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood_](https://scikit-learn.org/stable/auto_examples/covariance/plot_covariance_estimation.html))\n",
    "\n",
    "Exemple montrant comment ajuster un objet [**`OAS`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.OAS.html) aux donn√©es.\n",
    "\n",
    "##### [**Estimation Ledoit-Wolf vs OAS**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_6_covariance/plot_lw_vs_oas.ipynb)<br/>([_Ledoit-Wolf vs OAS estimation_](https://scikit-learn.org/stable/auto_examples/covariance/plot_lw_vs_oas.html))\n",
    "\n",
    "Visualisation de la diff√©rence d'erreur quadratique moyenne entre un estimateur [**`LedoitWolf`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.LedoitWolf.html) et un estimateur [**`OAS`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.OAS.html) de la covariance.\n",
    "\n",
    "<div style=\"background-color: white; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lw_vs_oas_001.png\"\n",
    "    alt=\"Estimation Ledoit-Wolf vs OAS\"\n",
    "    style=\"max-width: 50%; height: auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='sparse-inverse-covariance'></a> 2.6.3. Covariance inverse creuse\n",
    "\n",
    "La matrice inverse de la matrice de covariance, souvent appel√©e matrice de pr√©cision, est proportionnelle √† la matrice de corr√©lation partielle. Elle donne la relation d'ind√©pendance partielle. En d'autres termes, si deux caract√©ristiques sont ind√©pendantes conditionnellement aux autres, le coefficient correspondant dans la matrice de pr√©cision sera nul. C'est pourquoi il est logique d'estimer une matrice de pr√©cision creuse : l'estimation de la matrice de covariance est mieux conditionn√©e en apprenant des relations d'ind√©pendance √† partir des donn√©es. C'est ce qu'on appelle la _s√©lection de covariance_.\n",
    "\n",
    "Dans les situations de petits √©chantillons, o√π le nombre d'√©chantillons (`n_samples`) est de l'ordre de `n_features` ou moins, les estimateurs de covariance inverse creux ont tendance √† mieux fonctionner que les estimateurs de covariance r√©tr√©cis. Cependant, dans la situation inverse, ou pour des donn√©es tr√®s corr√©l√©es, ils peuvent √™tre num√©riquement instables. De plus, contrairement aux estimateurs r√©tr√©cis, les estimateurs creux sont capables de r√©cup√©rer une structure hors diagonale.\n",
    "\n",
    "L'estimateur [**`GraphicalLasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.GraphicalLasso.html) utilise une p√©nalit√© l1 pour imposer la parcimonie sur la matrice de pr√©cision : plus son param√®tre `alpha` est √©lev√©, plus la matrice de pr√©cision est creuse. L'objet [**`GraphicalLassoCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.GraphicalLassoCV.html) correspondant utilise la validation crois√©e pour r√©gler automatiquement le param√®tre `alpha`.\n",
    "\n",
    "<div style=\"background-color: white; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_sparse_cov_001.png\"\n",
    "    alt=\"Comparaison des estimations maximum de vraisemblance, r√©tr√©cissement et √©parses\"\n",
    "    style=\"max-width: 50%; height: auto;\">\n",
    "</div>\n",
    "\n",
    "Comparaison des estimations maximum de vraisemblance, r√©tr√©cissement et parcimonie de la matrice de covariance et de la matrice de pr√©cision dans les param√®tres de tr√®s petits √©chantillons.\n",
    "\n",
    "> **Note : R√©cup√©ration de la structure**\n",
    "> R√©cup√©rer une structure graphique √† partir des corr√©lations dans les donn√©es est une t√¢che difficile. Si vous √™tes int√©ress√© par cette r√©cup√©ration, gardez √† l'esprit que :\n",
    "> - La r√©cup√©ration est plus facile √† partir d'une matrice de corr√©lation qu'√† partir d'une matrice de covariance : standardisez vos observations avant d'ex√©cuter [**`GraphicalLasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.GraphicalLasso.html)\n",
    "> - Si le graphe sous-jacent a des n≈ìuds avec beaucoup plus de connexions que le n≈ìud moyen, l'algorithme en manquera certaines.\n",
    "> - Si votre nombre d'observations n'est pas grand par rapport au nombre d'ar√™tes dans votre graphe sous-jacent, vous ne le r√©cup√©rerez pas.\n",
    "> - M√™me si vous √™tes dans des conditions de r√©cup√©ration favorables, le param√®tre `alpha` choisi par validation crois√©e (par exemple, en utilisant l'objet [**`GraphicalLassoCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.GraphicalLassoCV.html)) conduira √† s√©lectionner trop d'ar√™tes. Cependant, les ar√™tes pertinentes auront des poids plus importants que les ar√™tes non pertinentes.\n",
    "\n",
    "La formulation math√©matique est la suivante :\n",
    "\n",
    "$$\\hat{K} = \\mathrm{argmin}_K \\big(\n",
    "            \\mathrm{tr} S K - \\mathrm{log} \\mathrm{det} K\n",
    "            + \\alpha \\|K\\|_1\n",
    "            \\big)$$\n",
    "\n",
    "O√π $K$ est la matrice de pr√©cision √† estimer, et $S$ est la matrice de covariance de l'√©chantillon. $\\|K\\|_1$ est la somme des valeurs absolues des coefficients hors diagonale de $K$. L'algorithme utilis√© pour r√©soudre ce probl√®me est l'algorithme GLasso, issu de l'article de Friedman de 2008 sur les biostatistiques. C'est le m√™me algorithme que dans le package R `glasso`.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Estimation de covariance inverse creuse**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_6_covariance/plot_sparse_cov.ipynb)<br/>([_Sparse inverse covariance estimation_](https://scikit-learn.org/stable/auto_examples/covariance/plot_sparse_cov.html))\n",
    "\n",
    "Exemple sur des donn√©es synth√©tiques montrant une certaine r√©cup√©ration d'une structure, et comparaison avec d'autres estimateurs de covariance.\n",
    "\n",
    "##### [**Visualisation de la structure du march√© boursier**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/applications/plot_stock_market.ipynb)<br/>([_Visualizing the stock market structure_](https://scikit-learn.org/stable/auto_examples/applications/plot_stock_market.html))\n",
    "\n",
    "Exemple sur de vraies donn√©es du march√© boursier, pour trouver quels symboles sont les plus li√©s. \n",
    "\n",
    "#### R√©f√©rences\n",
    "\n",
    "üî¨ Friedman et al, [**‚ÄúSparse inverse covariance estimation with the graphical lasso‚Äù**](https://watermark.silverchair.com/kxm045.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAA2QwggNgBgkqhkiG9w0BBwagggNRMIIDTQIBADCCA0YGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMUr3szU4SCdu9nx_qAgEQgIIDFyBF_2vLMcbxDR7pXo9Oe36XsXL6haJ9ZsDdy2nYFw29U9ZFQoLnxVdS7XRG0ulKm4k2lbGkhWUMlECv9Ag1kZLD0AoBMV06XgRKoainz5_eASh_uMIybKLwPLnKP10_tOwT9IhhtpX9LH4O7KQNg-wtvQYCCih0gT_xoTW0A2SmY3WzW5eWlTepZIE8NLcLm3fEhgjwxzCxgFFherLnuU48HQEhJLNVn3gDT658gg_Fn1zldZenGPZ6lnvsm_f3KhX6wptSy9ZnGOjTY2X-GTXsbsMUva_bsm-Ws9B-e0JTri0sSBjWYjXiz2SxEI29WEtfKq6zWghDGf_JRA1qKvBsmMw8TWkjuNT1D5JMylalJZ8np87uTdZTr1fLZkndFkJlxIbSnTV78-1foRVx_0HItmCqWN3cJiGeNkXpqvDQmXvYRqzZxoN-U-2MI4hE5A1XWOy54pdMCL1Vudu5ce-Pe8Rej493okjZoKdojh7hCf6yoQ6RFa5jamTFTYBF7cxsbkWcSNuX3lYpWOHaai03RUk9WuU_pbX4k8ghup7BdkclgoTaFB2IMn9qwVHsgkra01bDXJGL2eqeNP1fU5CbqTIYfK7WxggzV0PMd5rM5kB4AJoh_OMuRT4L6N1jpsMvq12E-ESCnAy02iQw0PTu8MwUTKt1AAlPD1eCz3x1P9gBPIxztUvCWDE7sLSmz-2v5n7s0EMHRvrik3eh1H-l3DGOHkOYMIC5RMlAnq0gDM0iirYA1aWlIPtZ-BrY4qoy1iZwZk5SSotUuSeNjNToa2gTQKx-c1S63y2_tUWsERlMNQF2223jN-Sur50gKQYYv3aqgQLesf8-A1KAZZhZc7SpMJqM-XRZbWTcmpasJLwL0257rOso1zvlddjjgP0L_HYD3xJIwmsSmZj9-I8qXPdc9U2_DjxUkH2eFMTf7sW2F9EK2q0qM-AAOPQueL0MMXNYIaVXICML4xUBoubNFIviQqFBTCQbdI8x2F583RFmv-7Hv-rN8ImvlmO5LVaL8EvXlqXdbReaD50vrs1lb39nUO05), Biostatistics 9, pp 432, 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='robust-covariance-estimation'></a> 2.6.4. Estimation de covariance robuste\n",
    "\n",
    "Les ensembles de donn√©es r√©els sont souvent sujets √† des erreurs de mesure ou d'enregistrement. Des observations r√©guli√®res mais peu communes peuvent √©galement appara√Ætre pour diverses raisons. Les observations tr√®s inhabituelles sont appel√©es valeurs aberrantes (ou _outliers_). L'estimateur de covariance empirique et les estimateurs de covariance r√©tr√©cis pr√©sent√©s ci-dessus sont tr√®s sensibles √† la pr√©sence de valeurs aberrantes dans les donn√©es. Par cons√©quent, il est pr√©f√©rable d'utiliser des estimateurs de covariance robustes pour estimer la covariance de vos ensembles de donn√©es r√©els. Alternativement, les estimateurs de covariance robustes peuvent √™tre utilis√©s pour la d√©tection des valeurs aberrantes et pour exclure ou pond√©rer certaines observations lors du traitement ult√©rieur des donn√©es.\n",
    "\n",
    "La biblioth√®que `sklearn.covariance` met en ≈ìuvre un estimateur robuste de covariance, le Minimum Covariance Determinant (MCD) [3].\n",
    "\n",
    "### <a id='minimum-covariance-determinant'></a> 2.6.4.1. Minimum Covariance Determinant\n",
    "\n",
    "L'estimateur Minimum Covariance Determinant est un estimateur robuste de la covariance d'un ensemble de donn√©es introduit par P.J. Rousseeuw dans [3]. L'id√©e est de trouver une proportion donn√©e ($h$) de \"bonnes\" observations qui ne sont pas des valeurs atypiques et de calculer leur matrice de covariance empirique. Cette matrice de covariance empirique est ensuite mise √† l'√©chelle pour compenser la s√©lection des observations effectu√©e (\"√©tape de coh√©rence\"). Une fois l'estimateur Minimum Covariance Determinant calcul√©, il est possible d'attribuer des poids aux observations en fonction de leur distance de Mahalanobis, ce qui conduit √† une estimation pond√©r√©e de la matrice de covariance de l'ensemble de donn√©es (\"√©tape de pond√©ration\").\n",
    "\n",
    "Rousseeuw et Van Driessen [4] ont d√©velopp√© l'algorithme FastMCD pour calculer le Minimum Covariance Determinant. Cet algorithme est utilis√© dans scikit-learn lors de l'ajustement d'un objet MCD aux donn√©es. L'algorithme FastMCD calcule √©galement une estimation robuste de la position de l'ensemble de donn√©es en m√™me temps.\n",
    "\n",
    "Les estimations brutes peuvent √™tre consult√©es √† l'aide des attributs `raw_location_` et `raw_covariance_` d'un objet estimateur de covariance robuste [**`MinCovDet`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.MinCovDet.html).\n",
    "\n",
    "#### R√©f√©rences\n",
    "\n",
    "üî¨ [3] (1,2) P. J. Rousseeuw. [**‚ÄúLeast median of squares regression‚Äù**](https://web.ipac.caltech.edu/staff/fmasci/home/astro_refs/LeastMedianOfSquares.pdf). J. Am Stat Ass, 79:871, 1984.\n",
    "\n",
    "üî¨ [4] Rousseeuw, P.J., Van Driessen, K. [**‚ÄúA Fast Algorithm for the Minimum Covariance Determinant Estimator‚Äù**](https://wis.kuleuven.be/statdatascience/robust/papers/1999/rousseeuwvandriessen-fastalgorithmformcd-technomet.pdf) Technometrics 41(3), 212 (1999)\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Estimation de covariance robuste vs. empirique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_6_covariance/plot_robust_vs_empirical_covariance.ipynb)<br/>([_Robust covariance estimation and Mahalanobis distances relevance_](https://scikit-learn.org/stable/auto_examples/covariance/plot_robust_vs_empirical_covariance.html))\n",
    "\n",
    "Exemple sur la fa√ßon d'ajuster un objet MinCovDet aux donn√©es et de voir comment l'estimation reste pr√©cise malgr√© la pr√©sence de valeurs aberrantes.\n",
    "\n",
    "##### [**Estimation de covariance robuste et pertinence des distances de Mahalanobis**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_6_covariance/plot_mahalanobis_distances.ipynb)<br/>([_Robust covariance estimation and Mahalanobis distances relevance_](https://scikit-learn.org/stable/auto_examples/covariance/plot_mahalanobis_distances.html))\n",
    "\n",
    "Visualisation de la diff√©rence entre les estimateurs de covariance [**`EmpiricalCovariance`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EmpiricalCovariance.html) et [**`MinCovDet`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.MinCovDet.html) en termes de distance de Mahalanobis (afin d'obtenir une meilleure estimation de la matrice de pr√©cision √©galement).\n",
    "\n",
    "**Influence des valeurs aberrantes sur les estimations de la position et de la covariance**\n",
    "\n",
    "<div style=\"background-color: white; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_robust_vs_empirical_covariance_001.png\"\n",
    "    alt=\"Estimation robuste vs empirique de covariance\"\n",
    "    style=\"max-width: 50%; height: auto;\">\n",
    "</div>\n",
    "\n",
    "**S√©paration des valeurs aberrantes des valeurs normales √† l'aide de la distance de Mahalanobis**\n",
    "\n",
    "<div style=\"background-color: white; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_mahalanobis_distances_001.png\"\n",
    "    alt=\"Distances de Mahalanobis\"\n",
    "    style=\"max-width: 50%; height: auto;\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
