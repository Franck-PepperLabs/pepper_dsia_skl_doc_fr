{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='unsupervised-learning'></a> 2. [**Apprentissage non supervisé**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_unsupervised_learning.ipynb#model-selection-and-evaluation)</br>([*Unsupervised learning*](https://scikit-learn.org/stable/unsupervised_learning.html#unsupervised-learning))\n",
    "\n",
    "# 2.6. [**Estimation de la covariance**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_6_covariance.ipynb#covariance-estimation)<br/>([_Covariance estimation_](https://scikit-learn.org/stable/modules/covariance.html#covariance-estimation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "- **Volume** : 9 pages, 6 exemples, 5 papiers\n",
    "- 2.6.1. [**Covariance empirique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_6_covariance.ipynb#empirical-covariance)<br/>([_Empirical covariance_](https://scikit-learn.org/stable/modules/covariance.html#empirical-covariance))\n",
    "- 2.6.2. [**Covariance réduite**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_6_covariance.ipynb#shrunk-covariance)<br/>([_Shrunk Covariance_](https://scikit-learn.org/stable/modules/covariance.html#shrunk-covariance))\n",
    "- 2.6.3. [**Covariance inverse creuse**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_6_covariance.ipynb#sparse-inverse-covariance)<br/>([_Sparse inverse covariance_](https://scikit-learn.org/stable/modules/covariance.html#sparse-inverse-covariance))\n",
    "- 2.6.4. [**Estimation robuste de la covariance**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_6_covariance.ipynb#robust-covariance-estimation)<br/>([_Robust Covariance Estimation_](https://scikit-learn.org/stable/modules/covariance.html#robust-covariance-estimation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='covariance-estimation'></a> 2.6. Estimation de covariance\n",
    "\n",
    "De nombreux problèmes statistiques nécessitent l'estimation de la matrice de covariance d'une population, qui peut être considérée comme une estimation de la forme du nuage de points d'un ensemble de données. La plupart du temps, une telle estimation doit être effectuée sur un échantillon dont les propriétés (taille, structure, homogénéité) ont une grande influence sur la qualité de l'estimation. Le module [**`sklearn.covariance`**](https://scikit-learn.org/stable/modules/classes.html) fournit des outils pour estimer avec précision la matrice de covariance d'une population dans divers contextes.\n",
    "\n",
    "Nous supposons que les observations sont indépendantes et identiquement distribuées (i.i.d.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='empirical-covariance'></a> 2.6.1. Covariance empirique\n",
    "\n",
    "Il est bien connu que la matrice de covariance d'un ensemble de données peut être bien approximée par l'estimateur _maximum de vraisemblance_ classique (ou \"covariance empirique\"), à condition que le nombre d'observations soit suffisamment grand par rapport au nombre de variables (les variables décrivant les observations). Plus précisément, l'estimateur du maximum de vraisemblance d'un échantillon est un estimateur asymptotiquement non biaisé de la matrice de covariance de la population correspondante.\n",
    "\n",
    "La matrice de covariance empirique d'un échantillon peut être calculée à l'aide de la fonction [**`empirical_covariance`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.empirical_covariance.html) du module, ou en ajustant un objet [**`EmpiricalCovariance`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EmpiricalCovariance.html) aux données de l'échantillon avec la méthode [**`EmpiricalCovariance.fit`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EmpiricalCovariance.html#sklearn.covariance.EmpiricalCovariance.fit). Notez que les résultats dépendent de la centralité des données, donc on peut souhaiter utiliser précisément le paramètre `assume_centered`. Plus précisément, si `assume_centered=False`, alors on suppose que l'ensemble de test a le même vecteur de moyenne que l'ensemble d'entraînement. Sinon, les deux ensembles doivent être centrés par l'utilisateur, et `assume_centered=True` doit être utilisé.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Estimation de covariance par rétrécissement : LedoitWolf vs OAS et maximum de vraisemblance**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_6_covariance/plot_covariance_estimation.ipynb)<br/>([_Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood_](https://scikit-learn.org/stable/auto_examples/covariance/plot_covariance_estimation.html))\n",
    "\n",
    "Exemple montrant comment ajuster un objet [**`EmpiricalCovariance`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EmpiricalCovariance.html) aux données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='shrunk-covariance'></a> 2.6.2. Covariance rétrécie\n",
    "\n",
    "### <a id='basic-shrinkage'></a> 2.6.2.1. Rétrécissement de base\n",
    "\n",
    "Bien que l'estimateur du maximum de vraisemblance soit un estimateur asymptotiquement non biaisé de la matrice de covariance, il n'est pas un bon estimateur des valeurs propres de la matrice de covariance, de sorte que la matrice de précision obtenue à partir de son inversion n'est pas exacte. Parfois, il arrive même que la matrice de covariance empirique ne puisse pas être inversée pour des raisons numériques. Pour éviter ce problème d'inversion, une transformation de la matrice de covariance empirique a été introduite : le `shrinkage` (rétrécissement).\n",
    "\n",
    "Dans scikit-learn, cette transformation (avec un coefficient de rétrécissement défini par l'utilisateur) peut être directement appliquée à une covariance précalculée avec la méthode [**`shrunk_covariance`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.shrunk_covariance.html). De plus, un estimateur rétréci de la covariance peut être ajusté aux données avec un objet [**`ShrunkCovariance`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.ShrunkCovariance.html) et sa méthode [**`ShrunkCovariance.fit`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.ShrunkCovariance.html#sklearn.covariance.ShrunkCovariance.fit). Encore une fois, les résultats dépendent de la centralité des données, donc on peut souhaiter utiliser précisément le paramètre `assume_centered`.\n",
    "\n",
    "Mathématiquement, ce rétrécissement consiste à réduire le rapport entre les plus petites et les plus grandes valeurs propres de la matrice de covariance empirique. Cela peut être fait en décalant simplement chaque valeur propre selon un décalage donné, ce qui revient à trouver l'estimateur du maximum de vraisemblance pénalisé par la norme $\\ell_2$ de la matrice de covariance. En pratique, le rétrécissement se résume à une simple transformation convexe : $\\Sigma_{\\rm rétrécie} = (1-\\alpha)\\hat{\\Sigma} + \\alpha\\frac{{\\rm Tr}\\hat{\\Sigma}}{p}\\rm Id$.\n",
    "\n",
    "Le choix du taux de rétrécissement, $\\alpha$, équivaut à définir un compromis biais/variance, et est discuté ci-dessous.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Estimation de covariance par rétrécissement : LedoitWolf vs OAS et maximum de vraisemblance**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_6_covariance/plot_covariance_estimation.ipynb)<br/>([_Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood_](https://scikit-learn.org/stable/auto_examples/covariance/plot_covariance_estimation.html))\n",
    "\n",
    "Exemple montrant comment ajuster un objet [**`ShrunkCovariance`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.ShrunkCovariance.html) aux données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ledoit-wolf-shrinkage'></a> 2.6.2.2. Rétrécissement Ledoit-Wolf\n",
    "\n",
    "Dans leur article de 2004 [1], O. Ledoit et M. Wolf proposent une formule pour calculer le coefficient de rétrécissement optimal qui minimise l'erreur quadratique moyenne entre la matrice de covariance estimée et la matrice de covariance réelle.\n",
    "\n",
    "L'estimateur de la matrice de covariance Ledoit-Wolf peut être calculé sur un échantillon avec la fonction [**`ledoit_wolf`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.ledoit_wolf.html) du package [**`sklearn.covariance`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.covariance), ou il peut être obtenu autrement en ajustant un objet [**`LedoitWolf`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.LedoitWolf.html) au même échantillon.\n",
    "\n",
    "> **Note : Cas où la matrice de covariance de la population est isotropique**  \n",
    "> Il est important de noter que lorsque le nombre d'échantillons est beaucoup plus grand que le nombre de variables, on s'attendrait à ce qu'aucun rétrécissement ne soit nécessaire. L'intuition derrière cela est que si la covariance de la population est de rang complet, lorsque le nombre d'échantillons augmente, la covariance de l'échantillon deviendra également définie positive. Par conséquent, aucun rétrécissement ne serait nécessaire et la méthode devrait le faire automatiquement.\n",
    ">\n",
    "> Cependant, ce n'est pas le cas dans la procédure Ledoit-Wolf lorsque la covariance de la population est un multiple de la matrice identité. Dans ce cas, l'estimation du rétrécissement Ledoit-Wolf approche 1 lorsque le nombre d'échantillons augmente. Cela indique que l'estimation optimale de la matrice de covariance dans le sens Ledoit-Wolf est un multiple de la matrice identité. Puisque la covariance de la population est déjà un multiple de la matrice identité, la solution Ledoit-Wolf est effectivement une estimation raisonnable.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Estimation de covariance par rétrécissement : LedoitWolf vs OAS et maximum de vraisemblance**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_6_covariance/plot_covariance_estimation.ipynb)<br/>([_Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood_](https://scikit-learn.org/stable/auto_examples/covariance/plot_covariance_estimation.html))\n",
    "\n",
    "Exemple montrant comment ajuster un objet [**`LedoitWolf`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.LedoitWolf.html) aux données et visualiser les performances de l'estimateur Ledoit-Wolf en termes de vraisemblance.\n",
    "\n",
    "#### Références\n",
    "\n",
    "🔬 [1] O. Ledoit and M. Wolf, [**“A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices”**](https://e-archivo.uc3m.es/bitstream/handle/10016/10087/ws0076.pdf;jsessionid=4C9B01BB15A802AF69A4E6D5149912F1?sequence=1), Journal of Multivariate Analysis, Volume 88, Issue 2, February 2004, pages 365-411."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='oracle-approximating-shrinkage'></a> 2.6.2.3. Rétrécissement d'Oracle Approximatif\n",
    "\n",
    "En supposant que les données sont distribuées selon une loi gaussienne, Chen et al. [2] ont dérivé une formule visant à choisir un coefficient de rétrécissement qui donne une erreur quadratique moyenne plus faible que celle donnée par la formule de Ledoit et Wolf. L'estimateur résultant est connu sous le nom d'estimateur de rétrécissement d'Oracle Approximatif de la covariance.\n",
    "\n",
    "L'estimateur OAS de la matrice de covariance peut être calculé sur un échantillon avec la fonction [**`oas`**](https://scikit-learn.org/stable/modules/generated/oas-function.html#sklearn.covariance.oas) du package [**`sklearn.covariance`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.covariance), ou il peut être obtenu autrement en ajustant un objet [**`OAS`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.OAS.html) au même échantillon.\n",
    "\n",
    "<div style=\"background-color: white; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_covariance_estimation_001.png\"\n",
    "    alt=\"Comparaison des choix des estimateurs Ledoit-Wolf et OAS\"\n",
    "    style=\"max-width: 50%; height: auto;\">\n",
    "</div>\n",
    "\n",
    "Compromis biais-variance lors du réglage du rétrécissement : comparaison des choix des estimateurs Ledoit-Wolf et OAS\n",
    "\n",
    "#### Références\n",
    "\n",
    "🔬 [2] [**“Shrinkage algorithms for MMSE covariance estimation”**](https://arxiv.org/pdf/0907.4698.pdf). Chen, Y., Wiesel, A., Eldar, Y. C., & Hero, A. O. IEEE Transactions on Signal Processing, 58(10), 5016-5029, 2010\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Estimation de covariance par rétrécissement : LedoitWolf vs OAS et maximum de vraisemblance**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_6_covariance/plot_covariance_estimation.ipynb)<br/>([_Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood_](https://scikit-learn.org/stable/auto_examples/covariance/plot_covariance_estimation.html))\n",
    "\n",
    "Exemple montrant comment ajuster un objet [**`OAS`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.OAS.html) aux données.\n",
    "\n",
    "##### [**Estimation Ledoit-Wolf vs OAS**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_6_covariance/plot_lw_vs_oas.ipynb)<br/>([_Ledoit-Wolf vs OAS estimation_](https://scikit-learn.org/stable/auto_examples/covariance/plot_lw_vs_oas.html))\n",
    "\n",
    "Visualisation de la différence d'erreur quadratique moyenne entre un estimateur [**`LedoitWolf`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.LedoitWolf.html) et un estimateur [**`OAS`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.OAS.html) de la covariance.\n",
    "\n",
    "<div style=\"background-color: white; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lw_vs_oas_001.png\"\n",
    "    alt=\"Estimation Ledoit-Wolf vs OAS\"\n",
    "    style=\"max-width: 50%; height: auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='sparse-inverse-covariance'></a> 2.6.3. Covariance inverse creuse\n",
    "\n",
    "La matrice inverse de la matrice de covariance, souvent appelée matrice de précision, est proportionnelle à la matrice de corrélation partielle. Elle donne la relation d'indépendance partielle. En d'autres termes, si deux caractéristiques sont indépendantes conditionnellement aux autres, le coefficient correspondant dans la matrice de précision sera nul. C'est pourquoi il est logique d'estimer une matrice de précision creuse : l'estimation de la matrice de covariance est mieux conditionnée en apprenant des relations d'indépendance à partir des données. C'est ce qu'on appelle la _sélection de covariance_.\n",
    "\n",
    "Dans les situations de petits échantillons, où le nombre d'échantillons (`n_samples`) est de l'ordre de `n_features` ou moins, les estimateurs de covariance inverse creux ont tendance à mieux fonctionner que les estimateurs de covariance rétrécis. Cependant, dans la situation inverse, ou pour des données très corrélées, ils peuvent être numériquement instables. De plus, contrairement aux estimateurs rétrécis, les estimateurs creux sont capables de récupérer une structure hors diagonale.\n",
    "\n",
    "L'estimateur [**`GraphicalLasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.GraphicalLasso.html) utilise une pénalité l1 pour imposer la parcimonie sur la matrice de précision : plus son paramètre `alpha` est élevé, plus la matrice de précision est creuse. L'objet [**`GraphicalLassoCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.GraphicalLassoCV.html) correspondant utilise la validation croisée pour régler automatiquement le paramètre `alpha`.\n",
    "\n",
    "<div style=\"background-color: white; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_sparse_cov_001.png\"\n",
    "    alt=\"Comparaison des estimations maximum de vraisemblance, rétrécissement et éparses\"\n",
    "    style=\"max-width: 50%; height: auto;\">\n",
    "</div>\n",
    "\n",
    "Comparaison des estimations maximum de vraisemblance, rétrécissement et parcimonie de la matrice de covariance et de la matrice de précision dans les paramètres de très petits échantillons.\n",
    "\n",
    "> **Note : Récupération de la structure**\n",
    "> Récupérer une structure graphique à partir des corrélations dans les données est une tâche difficile. Si vous êtes intéressé par cette récupération, gardez à l'esprit que :\n",
    "> - La récupération est plus facile à partir d'une matrice de corrélation qu'à partir d'une matrice de covariance : standardisez vos observations avant d'exécuter [**`GraphicalLasso`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.GraphicalLasso.html)\n",
    "> - Si le graphe sous-jacent a des nœuds avec beaucoup plus de connexions que le nœud moyen, l'algorithme en manquera certaines.\n",
    "> - Si votre nombre d'observations n'est pas grand par rapport au nombre d'arêtes dans votre graphe sous-jacent, vous ne le récupérerez pas.\n",
    "> - Même si vous êtes dans des conditions de récupération favorables, le paramètre `alpha` choisi par validation croisée (par exemple, en utilisant l'objet [**`GraphicalLassoCV`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.GraphicalLassoCV.html)) conduira à sélectionner trop d'arêtes. Cependant, les arêtes pertinentes auront des poids plus importants que les arêtes non pertinentes.\n",
    "\n",
    "La formulation mathématique est la suivante :\n",
    "\n",
    "$$\\hat{K} = \\mathrm{argmin}_K \\big(\n",
    "            \\mathrm{tr} S K - \\mathrm{log} \\mathrm{det} K\n",
    "            + \\alpha \\|K\\|_1\n",
    "            \\big)$$\n",
    "\n",
    "Où $K$ est la matrice de précision à estimer, et $S$ est la matrice de covariance de l'échantillon. $\\|K\\|_1$ est la somme des valeurs absolues des coefficients hors diagonale de $K$. L'algorithme utilisé pour résoudre ce problème est l'algorithme GLasso, issu de l'article de Friedman de 2008 sur les biostatistiques. C'est le même algorithme que dans le package R `glasso`.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Estimation de covariance inverse creuse**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_6_covariance/plot_sparse_cov.ipynb)<br/>([_Sparse inverse covariance estimation_](https://scikit-learn.org/stable/auto_examples/covariance/plot_sparse_cov.html))\n",
    "\n",
    "Exemple sur des données synthétiques montrant une certaine récupération d'une structure, et comparaison avec d'autres estimateurs de covariance.\n",
    "\n",
    "##### [**Visualisation de la structure du marché boursier**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/applications/plot_stock_market.ipynb)<br/>([_Visualizing the stock market structure_](https://scikit-learn.org/stable/auto_examples/applications/plot_stock_market.html))\n",
    "\n",
    "Exemple sur de vraies données du marché boursier, pour trouver quels symboles sont les plus liés. \n",
    "\n",
    "#### Références\n",
    "\n",
    "🔬 Friedman et al, [**“Sparse inverse covariance estimation with the graphical lasso”**](https://watermark.silverchair.com/kxm045.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAA2QwggNgBgkqhkiG9w0BBwagggNRMIIDTQIBADCCA0YGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMUr3szU4SCdu9nx_qAgEQgIIDFyBF_2vLMcbxDR7pXo9Oe36XsXL6haJ9ZsDdy2nYFw29U9ZFQoLnxVdS7XRG0ulKm4k2lbGkhWUMlECv9Ag1kZLD0AoBMV06XgRKoainz5_eASh_uMIybKLwPLnKP10_tOwT9IhhtpX9LH4O7KQNg-wtvQYCCih0gT_xoTW0A2SmY3WzW5eWlTepZIE8NLcLm3fEhgjwxzCxgFFherLnuU48HQEhJLNVn3gDT658gg_Fn1zldZenGPZ6lnvsm_f3KhX6wptSy9ZnGOjTY2X-GTXsbsMUva_bsm-Ws9B-e0JTri0sSBjWYjXiz2SxEI29WEtfKq6zWghDGf_JRA1qKvBsmMw8TWkjuNT1D5JMylalJZ8np87uTdZTr1fLZkndFkJlxIbSnTV78-1foRVx_0HItmCqWN3cJiGeNkXpqvDQmXvYRqzZxoN-U-2MI4hE5A1XWOy54pdMCL1Vudu5ce-Pe8Rej493okjZoKdojh7hCf6yoQ6RFa5jamTFTYBF7cxsbkWcSNuX3lYpWOHaai03RUk9WuU_pbX4k8ghup7BdkclgoTaFB2IMn9qwVHsgkra01bDXJGL2eqeNP1fU5CbqTIYfK7WxggzV0PMd5rM5kB4AJoh_OMuRT4L6N1jpsMvq12E-ESCnAy02iQw0PTu8MwUTKt1AAlPD1eCz3x1P9gBPIxztUvCWDE7sLSmz-2v5n7s0EMHRvrik3eh1H-l3DGOHkOYMIC5RMlAnq0gDM0iirYA1aWlIPtZ-BrY4qoy1iZwZk5SSotUuSeNjNToa2gTQKx-c1S63y2_tUWsERlMNQF2223jN-Sur50gKQYYv3aqgQLesf8-A1KAZZhZc7SpMJqM-XRZbWTcmpasJLwL0257rOso1zvlddjjgP0L_HYD3xJIwmsSmZj9-I8qXPdc9U2_DjxUkH2eFMTf7sW2F9EK2q0qM-AAOPQueL0MMXNYIaVXICML4xUBoubNFIviQqFBTCQbdI8x2F583RFmv-7Hv-rN8ImvlmO5LVaL8EvXlqXdbReaD50vrs1lb39nUO05), Biostatistics 9, pp 432, 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='robust-covariance-estimation'></a> 2.6.4. Estimation de covariance robuste\n",
    "\n",
    "Les ensembles de données réels sont souvent sujets à des erreurs de mesure ou d'enregistrement. Des observations régulières mais peu communes peuvent également apparaître pour diverses raisons. Les observations très inhabituelles sont appelées valeurs aberrantes (ou _outliers_). L'estimateur de covariance empirique et les estimateurs de covariance rétrécis présentés ci-dessus sont très sensibles à la présence de valeurs aberrantes dans les données. Par conséquent, il est préférable d'utiliser des estimateurs de covariance robustes pour estimer la covariance de vos ensembles de données réels. Alternativement, les estimateurs de covariance robustes peuvent être utilisés pour la détection des valeurs aberrantes et pour exclure ou pondérer certaines observations lors du traitement ultérieur des données.\n",
    "\n",
    "La bibliothèque `sklearn.covariance` met en œuvre un estimateur robuste de covariance, le Minimum Covariance Determinant (MCD) [3].\n",
    "\n",
    "### <a id='minimum-covariance-determinant'></a> 2.6.4.1. Minimum Covariance Determinant\n",
    "\n",
    "L'estimateur Minimum Covariance Determinant est un estimateur robuste de la covariance d'un ensemble de données introduit par P.J. Rousseeuw dans [3]. L'idée est de trouver une proportion donnée ($h$) de \"bonnes\" observations qui ne sont pas des valeurs atypiques et de calculer leur matrice de covariance empirique. Cette matrice de covariance empirique est ensuite mise à l'échelle pour compenser la sélection des observations effectuée (\"étape de cohérence\"). Une fois l'estimateur Minimum Covariance Determinant calculé, il est possible d'attribuer des poids aux observations en fonction de leur distance de Mahalanobis, ce qui conduit à une estimation pondérée de la matrice de covariance de l'ensemble de données (\"étape de pondération\").\n",
    "\n",
    "Rousseeuw et Van Driessen [4] ont développé l'algorithme FastMCD pour calculer le Minimum Covariance Determinant. Cet algorithme est utilisé dans scikit-learn lors de l'ajustement d'un objet MCD aux données. L'algorithme FastMCD calcule également une estimation robuste de la position de l'ensemble de données en même temps.\n",
    "\n",
    "Les estimations brutes peuvent être consultées à l'aide des attributs `raw_location_` et `raw_covariance_` d'un objet estimateur de covariance robuste [**`MinCovDet`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.MinCovDet.html).\n",
    "\n",
    "#### Références\n",
    "\n",
    "🔬 [3] (1,2) P. J. Rousseeuw. [**“Least median of squares regression”**](https://web.ipac.caltech.edu/staff/fmasci/home/astro_refs/LeastMedianOfSquares.pdf). J. Am Stat Ass, 79:871, 1984.\n",
    "\n",
    "🔬 [4] Rousseeuw, P.J., Van Driessen, K. [**“A Fast Algorithm for the Minimum Covariance Determinant Estimator”**](https://wis.kuleuven.be/statdatascience/robust/papers/1999/rousseeuwvandriessen-fastalgorithmformcd-technomet.pdf) Technometrics 41(3), 212 (1999)\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "##### [**Estimation de covariance robuste vs. empirique**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_6_covariance/plot_robust_vs_empirical_covariance.ipynb)<br/>([_Robust covariance estimation and Mahalanobis distances relevance_](https://scikit-learn.org/stable/auto_examples/covariance/plot_robust_vs_empirical_covariance.html))\n",
    "\n",
    "Exemple sur la façon d'ajuster un objet MinCovDet aux données et de voir comment l'estimation reste précise malgré la présence de valeurs aberrantes.\n",
    "\n",
    "##### [**Estimation de covariance robuste et pertinence des distances de Mahalanobis**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_6_covariance/plot_mahalanobis_distances.ipynb)<br/>([_Robust covariance estimation and Mahalanobis distances relevance_](https://scikit-learn.org/stable/auto_examples/covariance/plot_mahalanobis_distances.html))\n",
    "\n",
    "Visualisation de la différence entre les estimateurs de covariance [**`EmpiricalCovariance`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EmpiricalCovariance.html) et [**`MinCovDet`**](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.MinCovDet.html) en termes de distance de Mahalanobis (afin d'obtenir une meilleure estimation de la matrice de précision également).\n",
    "\n",
    "**Influence des valeurs aberrantes sur les estimations de la position et de la covariance**\n",
    "\n",
    "<div style=\"background-color: white; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_robust_vs_empirical_covariance_001.png\"\n",
    "    alt=\"Estimation robuste vs empirique de covariance\"\n",
    "    style=\"max-width: 50%; height: auto;\">\n",
    "</div>\n",
    "\n",
    "**Séparation des valeurs aberrantes des valeurs normales à l'aide de la distance de Mahalanobis**\n",
    "\n",
    "<div style=\"background-color: white; text-align: center;\">\n",
    "  <img\n",
    "    src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_mahalanobis_distances_001.png\"\n",
    "    alt=\"Distances de Mahalanobis\"\n",
    "    style=\"max-width: 50%; height: auto;\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
